Traceback (most recent call last):
  File "/home/airi/Desktop/Shaxboz/Paddle_face_recognition/insightface/recognition/arcface_paddle/tools/train.py", line 24, in <module>
    args = parser.parse_args()
  File "/home/airi/Desktop/Shaxboz/Paddle_face_recognition/insightface/recognition/arcface_paddle/configs/argparser.py", line 72, in parse_args
    cfg = get_config(user_namespace.config_file)
  File "/home/airi/Desktop/Shaxboz/Paddle_face_recognition/insightface/recognition/arcface_paddle/configs/argparser.py", line 51, in get_config
    config = importlib.import_module("configs.config")
  File "/home/airi/anaconda3/envs/paddle/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/home/airi/Desktop/Shaxboz/Paddle_face_recognition/insightface/recognition/arcface_paddle/configs/config.py", line 15, in <module>
    from easydict import EasyDict as edict
ModuleNotFoundError: No module named 'easydict'
Traceback (most recent call last):
  File "/home/airi/Desktop/Shaxboz/Paddle_face_recognition/insightface/recognition/arcface_paddle/tools/train.py", line 24, in <module>
    args = parser.parse_args()
  File "/home/airi/Desktop/Shaxboz/Paddle_face_recognition/insightface/recognition/arcface_paddle/configs/argparser.py", line 72, in parse_args
    cfg = get_config(user_namespace.config_file)
  File "/home/airi/Desktop/Shaxboz/Paddle_face_recognition/insightface/recognition/arcface_paddle/configs/argparser.py", line 51, in get_config
    config = importlib.import_module("configs.config")
  File "/home/airi/anaconda3/envs/paddle/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/home/airi/Desktop/Shaxboz/Paddle_face_recognition/insightface/recognition/arcface_paddle/configs/config.py", line 15, in <module>
    from easydict import EasyDict as edict
ModuleNotFoundError: No module named 'easydict'
/home/airi/Desktop/Shaxboz/Paddle_face_recognition/insightface/recognition/arcface_paddle/static/train.py:231: SyntaxWarning: "is" with a literal. Did you mean "=="?
  if rank is 0 and global_step > 0 and global_step % args.log_interval_step == 0:
Traceback (most recent call last):
  File "/home/airi/Desktop/Shaxboz/Paddle_face_recognition/insightface/recognition/arcface_paddle/tools/train.py", line 26, in <module>
    from static.train import train
  File "/home/airi/Desktop/Shaxboz/Paddle_face_recognition/insightface/recognition/arcface_paddle/static/train.py", line 28, in <module>
    from .utils.verification import CallBackVerification
  File "/home/airi/Desktop/Shaxboz/Paddle_face_recognition/insightface/recognition/arcface_paddle/static/utils/verification.py", line 18, in <module>
    import sklearn
ModuleNotFoundError: No module named 'sklearn'
[2023-09-27 09:00:18,297] [    INFO] logging.py:57 - rank: 0
Training: 2023-09-27 09:00:18,297 - rank: 0
[2023-09-27 09:00:18,297] [    INFO] argparser.py:22 - --------args----------
Training: 2023-09-27 09:00:18,297 - --------args----------
[2023-09-27 09:00:18,297] [    INFO] argparser.py:24 - config_file: configs/ms1mv3_r50.py
Training: 2023-09-27 09:00:18,297 - config_file: configs/ms1mv3_r50.py
[2023-09-27 09:00:18,297] [    INFO] argparser.py:24 - is_static: True
Training: 2023-09-27 09:00:18,297 - is_static: True
[2023-09-27 09:00:18,297] [    INFO] argparser.py:24 - backbone: FresResNet50
Training: 2023-09-27 09:00:18,297 - backbone: FresResNet50
[2023-09-27 09:00:18,297] [    INFO] argparser.py:24 - classifier: LargeScaleClassifier
Training: 2023-09-27 09:00:18,297 - classifier: LargeScaleClassifier
[2023-09-27 09:00:18,297] [    INFO] argparser.py:24 - embedding_size: 512
Training: 2023-09-27 09:00:18,297 - embedding_size: 512
[2023-09-27 09:00:18,297] [    INFO] argparser.py:24 - model_parallel: True
Training: 2023-09-27 09:00:18,297 - model_parallel: True
[2023-09-27 09:00:18,297] [    INFO] argparser.py:24 - sample_ratio: 0.1
Training: 2023-09-27 09:00:18,297 - sample_ratio: 0.1
[2023-09-27 09:00:18,297] [    INFO] argparser.py:24 - loss: ArcFace
Training: 2023-09-27 09:00:18,297 - loss: ArcFace
[2023-09-27 09:00:18,298] [    INFO] argparser.py:24 - dropout: 0.0
Training: 2023-09-27 09:00:18,298 - dropout: 0.0
[2023-09-27 09:00:18,298] [    INFO] argparser.py:24 - fp16: True
Training: 2023-09-27 09:00:18,298 - fp16: True
[2023-09-27 09:00:18,298] [    INFO] argparser.py:24 - init_loss_scaling: 27648.0
Training: 2023-09-27 09:00:18,298 - init_loss_scaling: 27648.0
[2023-09-27 09:00:18,298] [    INFO] argparser.py:24 - max_loss_scaling: 128.0
Training: 2023-09-27 09:00:18,298 - max_loss_scaling: 128.0
[2023-09-27 09:00:18,298] [    INFO] argparser.py:24 - incr_every_n_steps: 2000
Training: 2023-09-27 09:00:18,298 - incr_every_n_steps: 2000
[2023-09-27 09:00:18,298] [    INFO] argparser.py:24 - decr_every_n_nan_or_inf: 1
Training: 2023-09-27 09:00:18,298 - decr_every_n_nan_or_inf: 1
[2023-09-27 09:00:18,298] [    INFO] argparser.py:24 - incr_ratio: 2.0
Training: 2023-09-27 09:00:18,298 - incr_ratio: 2.0
[2023-09-27 09:00:18,298] [    INFO] argparser.py:24 - decr_ratio: 0.5
Training: 2023-09-27 09:00:18,298 - decr_ratio: 0.5
[2023-09-27 09:00:18,298] [    INFO] argparser.py:24 - use_dynamic_loss_scaling: True
Training: 2023-09-27 09:00:18,298 - use_dynamic_loss_scaling: True
[2023-09-27 09:00:18,298] [    INFO] argparser.py:24 - custom_white_list: []
Training: 2023-09-27 09:00:18,298 - custom_white_list: []
[2023-09-27 09:00:18,298] [    INFO] argparser.py:24 - custom_black_list: []
Training: 2023-09-27 09:00:18,298 - custom_black_list: []
[2023-09-27 09:00:18,298] [    INFO] argparser.py:24 - lr: 0.1
Training: 2023-09-27 09:00:18,298 - lr: 0.1
[2023-09-27 09:00:18,298] [    INFO] argparser.py:24 - lr_decay: 0.1
Training: 2023-09-27 09:00:18,298 - lr_decay: 0.1
[2023-09-27 09:00:18,298] [    INFO] argparser.py:24 - weight_decay: 0.0005
Training: 2023-09-27 09:00:18,298 - weight_decay: 0.0005
[2023-09-27 09:00:18,298] [    INFO] argparser.py:24 - momentum: 0.9
Training: 2023-09-27 09:00:18,298 - momentum: 0.9
[2023-09-27 09:00:18,298] [    INFO] argparser.py:24 - train_unit: epoch
Training: 2023-09-27 09:00:18,298 - train_unit: epoch
[2023-09-27 09:00:18,298] [    INFO] argparser.py:24 - warmup_num: 0
Training: 2023-09-27 09:00:18,298 - warmup_num: 0
[2023-09-27 09:00:18,298] [    INFO] argparser.py:24 - train_num: 25
Training: 2023-09-27 09:00:18,298 - train_num: 25
[2023-09-27 09:00:18,298] [    INFO] argparser.py:24 - decay_boundaries: [10, 16, 22]
Training: 2023-09-27 09:00:18,298 - decay_boundaries: [10, 16, 22]
[2023-09-27 09:00:18,298] [    INFO] argparser.py:24 - use_synthetic_dataset: False
Training: 2023-09-27 09:00:18,298 - use_synthetic_dataset: False
[2023-09-27 09:00:18,298] [    INFO] argparser.py:24 - dataset: MS1M_v2
Training: 2023-09-27 09:00:18,298 - dataset: MS1M_v2
[2023-09-27 09:00:18,298] [    INFO] argparser.py:24 - data_dir: data_out/
Training: 2023-09-27 09:00:18,298 - data_dir: data_out/
[2023-09-27 09:00:18,298] [    INFO] argparser.py:24 - label_file: data_out/label.txt
Training: 2023-09-27 09:00:18,298 - label_file: data_out/label.txt
[2023-09-27 09:00:18,298] [    INFO] argparser.py:24 - is_bin: False
Training: 2023-09-27 09:00:18,298 - is_bin: False
[2023-09-27 09:00:18,298] [    INFO] argparser.py:24 - num_classes: 85741
Training: 2023-09-27 09:00:18,298 - num_classes: 85741
[2023-09-27 09:00:18,298] [    INFO] argparser.py:24 - batch_size: 128
Training: 2023-09-27 09:00:18,298 - batch_size: 128
[2023-09-27 09:00:18,298] [    INFO] argparser.py:24 - num_workers: 2
Training: 2023-09-27 09:00:18,298 - num_workers: 2
[2023-09-27 09:00:18,298] [    INFO] argparser.py:24 - do_validation_while_train: True
Training: 2023-09-27 09:00:18,298 - do_validation_while_train: True
[2023-09-27 09:00:18,298] [    INFO] argparser.py:24 - validation_interval_step: 2000
Training: 2023-09-27 09:00:18,298 - validation_interval_step: 2000
[2023-09-27 09:00:18,298] [    INFO] argparser.py:24 - val_targets: ['lfw', 'cfp_fp', 'agedb_30']
Training: 2023-09-27 09:00:18,298 - val_targets: ['lfw', 'cfp_fp', 'agedb_30']
[2023-09-27 09:00:18,298] [    INFO] argparser.py:24 - logdir: ./log
Training: 2023-09-27 09:00:18,298 - logdir: ./log
[2023-09-27 09:00:18,298] [    INFO] argparser.py:24 - log_interval_step: 100
Training: 2023-09-27 09:00:18,298 - log_interval_step: 100
[2023-09-27 09:00:18,298] [    INFO] argparser.py:24 - output: MS1M_v3_arcface_static_0.1
Training: 2023-09-27 09:00:18,298 - output: MS1M_v3_arcface_static_0.1
[2023-09-27 09:00:18,298] [    INFO] argparser.py:24 - resume: False
Training: 2023-09-27 09:00:18,298 - resume: False
[2023-09-27 09:00:18,299] [    INFO] argparser.py:24 - checkpoint_dir: None
Training: 2023-09-27 09:00:18,299 - checkpoint_dir: None
[2023-09-27 09:00:18,299] [    INFO] argparser.py:24 - max_num_last_checkpoint: 1
Training: 2023-09-27 09:00:18,299 - max_num_last_checkpoint: 1
[2023-09-27 09:00:18,299] [    INFO] argparser.py:25 - ------------------------

Training: 2023-09-27 09:00:18,299 - ------------------------

[2023-09-27 09:00:18,299] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
Traceback (most recent call last):
  File "/home/airi/Desktop/Shaxboz/Paddle_face_recognition/insightface/recognition/arcface_paddle/tools/train.py", line 35, in <module>
    train(args)
  File "/home/airi/Desktop/Shaxboz/Paddle_face_recognition/insightface/recognition/arcface_paddle/static/train.py", line 63, in train
    trainset = CommonDataset(
  File "/home/airi/Desktop/Shaxboz/Paddle_face_recognition/insightface/recognition/arcface_paddle/datasets/common_dataset.py", line 48, in __init__
    with open(label_file, "r") as fin:
FileNotFoundError: [Errno 2] No such file or directory: 'data_out/label.txt'
[2023-09-27 09:01:50,911] [    INFO] logging.py:57 - rank: 0
Training: 2023-09-27 09:01:50,911 - rank: 0
[2023-09-27 09:01:50,912] [    INFO] argparser.py:22 - --------args----------
Training: 2023-09-27 09:01:50,912 - --------args----------
[2023-09-27 09:01:50,912] [    INFO] argparser.py:24 - config_file: configs/ms1mv3_r50.py
Training: 2023-09-27 09:01:50,912 - config_file: configs/ms1mv3_r50.py
[2023-09-27 09:01:50,912] [    INFO] argparser.py:24 - is_static: True
Training: 2023-09-27 09:01:50,912 - is_static: True
[2023-09-27 09:01:50,912] [    INFO] argparser.py:24 - backbone: FresResNet50
Training: 2023-09-27 09:01:50,912 - backbone: FresResNet50
[2023-09-27 09:01:50,912] [    INFO] argparser.py:24 - classifier: LargeScaleClassifier
Training: 2023-09-27 09:01:50,912 - classifier: LargeScaleClassifier
[2023-09-27 09:01:50,912] [    INFO] argparser.py:24 - embedding_size: 512
Training: 2023-09-27 09:01:50,912 - embedding_size: 512
[2023-09-27 09:01:50,912] [    INFO] argparser.py:24 - model_parallel: True
Training: 2023-09-27 09:01:50,912 - model_parallel: True
[2023-09-27 09:01:50,912] [    INFO] argparser.py:24 - sample_ratio: 0.1
Training: 2023-09-27 09:01:50,912 - sample_ratio: 0.1
[2023-09-27 09:01:50,912] [    INFO] argparser.py:24 - loss: ArcFace
Training: 2023-09-27 09:01:50,912 - loss: ArcFace
[2023-09-27 09:01:50,912] [    INFO] argparser.py:24 - dropout: 0.0
Training: 2023-09-27 09:01:50,912 - dropout: 0.0
[2023-09-27 09:01:50,912] [    INFO] argparser.py:24 - fp16: True
Training: 2023-09-27 09:01:50,912 - fp16: True
[2023-09-27 09:01:50,912] [    INFO] argparser.py:24 - init_loss_scaling: 27648.0
Training: 2023-09-27 09:01:50,912 - init_loss_scaling: 27648.0
[2023-09-27 09:01:50,912] [    INFO] argparser.py:24 - max_loss_scaling: 128.0
Training: 2023-09-27 09:01:50,912 - max_loss_scaling: 128.0
[2023-09-27 09:01:50,912] [    INFO] argparser.py:24 - incr_every_n_steps: 2000
Training: 2023-09-27 09:01:50,912 - incr_every_n_steps: 2000
[2023-09-27 09:01:50,912] [    INFO] argparser.py:24 - decr_every_n_nan_or_inf: 1
Training: 2023-09-27 09:01:50,912 - decr_every_n_nan_or_inf: 1
[2023-09-27 09:01:50,912] [    INFO] argparser.py:24 - incr_ratio: 2.0
Training: 2023-09-27 09:01:50,912 - incr_ratio: 2.0
[2023-09-27 09:01:50,912] [    INFO] argparser.py:24 - decr_ratio: 0.5
Training: 2023-09-27 09:01:50,912 - decr_ratio: 0.5
[2023-09-27 09:01:50,912] [    INFO] argparser.py:24 - use_dynamic_loss_scaling: True
Training: 2023-09-27 09:01:50,912 - use_dynamic_loss_scaling: True
[2023-09-27 09:01:50,912] [    INFO] argparser.py:24 - custom_white_list: []
Training: 2023-09-27 09:01:50,912 - custom_white_list: []
[2023-09-27 09:01:50,912] [    INFO] argparser.py:24 - custom_black_list: []
Training: 2023-09-27 09:01:50,912 - custom_black_list: []
[2023-09-27 09:01:50,912] [    INFO] argparser.py:24 - lr: 0.1
Training: 2023-09-27 09:01:50,912 - lr: 0.1
[2023-09-27 09:01:50,912] [    INFO] argparser.py:24 - lr_decay: 0.1
Training: 2023-09-27 09:01:50,912 - lr_decay: 0.1
[2023-09-27 09:01:50,912] [    INFO] argparser.py:24 - weight_decay: 0.0005
Training: 2023-09-27 09:01:50,912 - weight_decay: 0.0005
[2023-09-27 09:01:50,912] [    INFO] argparser.py:24 - momentum: 0.9
Training: 2023-09-27 09:01:50,912 - momentum: 0.9
[2023-09-27 09:01:50,912] [    INFO] argparser.py:24 - train_unit: epoch
Training: 2023-09-27 09:01:50,912 - train_unit: epoch
[2023-09-27 09:01:50,912] [    INFO] argparser.py:24 - warmup_num: 0
Training: 2023-09-27 09:01:50,912 - warmup_num: 0
[2023-09-27 09:01:50,912] [    INFO] argparser.py:24 - train_num: 25
Training: 2023-09-27 09:01:50,912 - train_num: 25
[2023-09-27 09:01:50,912] [    INFO] argparser.py:24 - decay_boundaries: [10, 16, 22]
Training: 2023-09-27 09:01:50,912 - decay_boundaries: [10, 16, 22]
[2023-09-27 09:01:50,912] [    INFO] argparser.py:24 - use_synthetic_dataset: False
Training: 2023-09-27 09:01:50,912 - use_synthetic_dataset: False
[2023-09-27 09:01:50,912] [    INFO] argparser.py:24 - dataset: MS1M_v2
Training: 2023-09-27 09:01:50,912 - dataset: MS1M_v2
[2023-09-27 09:01:50,912] [    INFO] argparser.py:24 - data_dir: /home/airi/Desktop/Shaxboz/Paddle_face_recognition/data_out/
Training: 2023-09-27 09:01:50,912 - data_dir: /home/airi/Desktop/Shaxboz/Paddle_face_recognition/data_out/
[2023-09-27 09:01:50,913] [    INFO] argparser.py:24 - label_file: data_out/label.txt
Training: 2023-09-27 09:01:50,913 - label_file: data_out/label.txt
[2023-09-27 09:01:50,913] [    INFO] argparser.py:24 - is_bin: False
Training: 2023-09-27 09:01:50,913 - is_bin: False
[2023-09-27 09:01:50,913] [    INFO] argparser.py:24 - num_classes: 85741
Training: 2023-09-27 09:01:50,913 - num_classes: 85741
[2023-09-27 09:01:50,913] [    INFO] argparser.py:24 - batch_size: 128
Training: 2023-09-27 09:01:50,913 - batch_size: 128
[2023-09-27 09:01:50,913] [    INFO] argparser.py:24 - num_workers: 2
Training: 2023-09-27 09:01:50,913 - num_workers: 2
[2023-09-27 09:01:50,913] [    INFO] argparser.py:24 - do_validation_while_train: True
Training: 2023-09-27 09:01:50,913 - do_validation_while_train: True
[2023-09-27 09:01:50,913] [    INFO] argparser.py:24 - validation_interval_step: 2000
Training: 2023-09-27 09:01:50,913 - validation_interval_step: 2000
[2023-09-27 09:01:50,913] [    INFO] argparser.py:24 - val_targets: ['lfw', 'cfp_fp', 'agedb_30']
Training: 2023-09-27 09:01:50,913 - val_targets: ['lfw', 'cfp_fp', 'agedb_30']
[2023-09-27 09:01:50,913] [    INFO] argparser.py:24 - logdir: ./log
Training: 2023-09-27 09:01:50,913 - logdir: ./log
[2023-09-27 09:01:50,913] [    INFO] argparser.py:24 - log_interval_step: 100
Training: 2023-09-27 09:01:50,913 - log_interval_step: 100
[2023-09-27 09:01:50,913] [    INFO] argparser.py:24 - output: MS1M_v3_arcface_static_0.1
Training: 2023-09-27 09:01:50,913 - output: MS1M_v3_arcface_static_0.1
[2023-09-27 09:01:50,913] [    INFO] argparser.py:24 - resume: False
Training: 2023-09-27 09:01:50,913 - resume: False
[2023-09-27 09:01:50,913] [    INFO] argparser.py:24 - checkpoint_dir: None
Training: 2023-09-27 09:01:50,913 - checkpoint_dir: None
[2023-09-27 09:01:50,913] [    INFO] argparser.py:24 - max_num_last_checkpoint: 1
Training: 2023-09-27 09:01:50,913 - max_num_last_checkpoint: 1
[2023-09-27 09:01:50,913] [    INFO] argparser.py:25 - ------------------------

Training: 2023-09-27 09:01:50,913 - ------------------------

[2023-09-27 09:01:50,913] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
Traceback (most recent call last):
  File "/home/airi/Desktop/Shaxboz/Paddle_face_recognition/insightface/recognition/arcface_paddle/tools/train.py", line 35, in <module>
    train(args)
  File "/home/airi/Desktop/Shaxboz/Paddle_face_recognition/insightface/recognition/arcface_paddle/static/train.py", line 63, in train
    trainset = CommonDataset(
  File "/home/airi/Desktop/Shaxboz/Paddle_face_recognition/insightface/recognition/arcface_paddle/datasets/common_dataset.py", line 48, in __init__
    with open(label_file, "r") as fin:
FileNotFoundError: [Errno 2] No such file or directory: 'data_out/label.txt'
[2023-09-27 09:02:20,681] [    INFO] logging.py:57 - rank: 0
Training: 2023-09-27 09:02:20,681 - rank: 0
[2023-09-27 09:02:20,681] [    INFO] argparser.py:22 - --------args----------
Training: 2023-09-27 09:02:20,681 - --------args----------
[2023-09-27 09:02:20,681] [    INFO] argparser.py:24 - config_file: configs/ms1mv3_r50.py
Training: 2023-09-27 09:02:20,681 - config_file: configs/ms1mv3_r50.py
[2023-09-27 09:02:20,682] [    INFO] argparser.py:24 - is_static: True
Training: 2023-09-27 09:02:20,682 - is_static: True
[2023-09-27 09:02:20,682] [    INFO] argparser.py:24 - backbone: FresResNet50
Training: 2023-09-27 09:02:20,682 - backbone: FresResNet50
[2023-09-27 09:02:20,682] [    INFO] argparser.py:24 - classifier: LargeScaleClassifier
Training: 2023-09-27 09:02:20,682 - classifier: LargeScaleClassifier
[2023-09-27 09:02:20,682] [    INFO] argparser.py:24 - embedding_size: 512
Training: 2023-09-27 09:02:20,682 - embedding_size: 512
[2023-09-27 09:02:20,682] [    INFO] argparser.py:24 - model_parallel: True
Training: 2023-09-27 09:02:20,682 - model_parallel: True
[2023-09-27 09:02:20,682] [    INFO] argparser.py:24 - sample_ratio: 0.1
Training: 2023-09-27 09:02:20,682 - sample_ratio: 0.1
[2023-09-27 09:02:20,682] [    INFO] argparser.py:24 - loss: ArcFace
Training: 2023-09-27 09:02:20,682 - loss: ArcFace
[2023-09-27 09:02:20,682] [    INFO] argparser.py:24 - dropout: 0.0
Training: 2023-09-27 09:02:20,682 - dropout: 0.0
[2023-09-27 09:02:20,682] [    INFO] argparser.py:24 - fp16: True
Training: 2023-09-27 09:02:20,682 - fp16: True
[2023-09-27 09:02:20,682] [    INFO] argparser.py:24 - init_loss_scaling: 27648.0
Training: 2023-09-27 09:02:20,682 - init_loss_scaling: 27648.0
[2023-09-27 09:02:20,682] [    INFO] argparser.py:24 - max_loss_scaling: 128.0
Training: 2023-09-27 09:02:20,682 - max_loss_scaling: 128.0
[2023-09-27 09:02:20,682] [    INFO] argparser.py:24 - incr_every_n_steps: 2000
Training: 2023-09-27 09:02:20,682 - incr_every_n_steps: 2000
[2023-09-27 09:02:20,682] [    INFO] argparser.py:24 - decr_every_n_nan_or_inf: 1
Training: 2023-09-27 09:02:20,682 - decr_every_n_nan_or_inf: 1
[2023-09-27 09:02:20,682] [    INFO] argparser.py:24 - incr_ratio: 2.0
Training: 2023-09-27 09:02:20,682 - incr_ratio: 2.0
[2023-09-27 09:02:20,682] [    INFO] argparser.py:24 - decr_ratio: 0.5
Training: 2023-09-27 09:02:20,682 - decr_ratio: 0.5
[2023-09-27 09:02:20,682] [    INFO] argparser.py:24 - use_dynamic_loss_scaling: True
Training: 2023-09-27 09:02:20,682 - use_dynamic_loss_scaling: True
[2023-09-27 09:02:20,682] [    INFO] argparser.py:24 - custom_white_list: []
Training: 2023-09-27 09:02:20,682 - custom_white_list: []
[2023-09-27 09:02:20,682] [    INFO] argparser.py:24 - custom_black_list: []
Training: 2023-09-27 09:02:20,682 - custom_black_list: []
[2023-09-27 09:02:20,682] [    INFO] argparser.py:24 - lr: 0.1
Training: 2023-09-27 09:02:20,682 - lr: 0.1
[2023-09-27 09:02:20,682] [    INFO] argparser.py:24 - lr_decay: 0.1
Training: 2023-09-27 09:02:20,682 - lr_decay: 0.1
[2023-09-27 09:02:20,682] [    INFO] argparser.py:24 - weight_decay: 0.0005
Training: 2023-09-27 09:02:20,682 - weight_decay: 0.0005
[2023-09-27 09:02:20,682] [    INFO] argparser.py:24 - momentum: 0.9
Training: 2023-09-27 09:02:20,682 - momentum: 0.9
[2023-09-27 09:02:20,682] [    INFO] argparser.py:24 - train_unit: epoch
Training: 2023-09-27 09:02:20,682 - train_unit: epoch
[2023-09-27 09:02:20,682] [    INFO] argparser.py:24 - warmup_num: 0
Training: 2023-09-27 09:02:20,682 - warmup_num: 0
[2023-09-27 09:02:20,682] [    INFO] argparser.py:24 - train_num: 25
Training: 2023-09-27 09:02:20,682 - train_num: 25
[2023-09-27 09:02:20,682] [    INFO] argparser.py:24 - decay_boundaries: [10, 16, 22]
Training: 2023-09-27 09:02:20,682 - decay_boundaries: [10, 16, 22]
[2023-09-27 09:02:20,682] [    INFO] argparser.py:24 - use_synthetic_dataset: False
Training: 2023-09-27 09:02:20,682 - use_synthetic_dataset: False
[2023-09-27 09:02:20,682] [    INFO] argparser.py:24 - dataset: MS1M_v2
Training: 2023-09-27 09:02:20,682 - dataset: MS1M_v2
[2023-09-27 09:02:20,682] [    INFO] argparser.py:24 - data_dir: /home/airi/Desktop/Shaxboz/Paddle_face_recognition/data_out/
Training: 2023-09-27 09:02:20,682 - data_dir: /home/airi/Desktop/Shaxboz/Paddle_face_recognition/data_out/
[2023-09-27 09:02:20,682] [    INFO] argparser.py:24 - label_file: /home/airi/Desktop/Shaxboz/Paddle_face_recognition/data_out/label.txt
Training: 2023-09-27 09:02:20,682 - label_file: /home/airi/Desktop/Shaxboz/Paddle_face_recognition/data_out/label.txt
[2023-09-27 09:02:20,682] [    INFO] argparser.py:24 - is_bin: False
Training: 2023-09-27 09:02:20,682 - is_bin: False
[2023-09-27 09:02:20,682] [    INFO] argparser.py:24 - num_classes: 85741
Training: 2023-09-27 09:02:20,682 - num_classes: 85741
[2023-09-27 09:02:20,682] [    INFO] argparser.py:24 - batch_size: 128
Training: 2023-09-27 09:02:20,682 - batch_size: 128
[2023-09-27 09:02:20,682] [    INFO] argparser.py:24 - num_workers: 2
Training: 2023-09-27 09:02:20,682 - num_workers: 2
[2023-09-27 09:02:20,682] [    INFO] argparser.py:24 - do_validation_while_train: True
Training: 2023-09-27 09:02:20,682 - do_validation_while_train: True
[2023-09-27 09:02:20,682] [    INFO] argparser.py:24 - validation_interval_step: 2000
Training: 2023-09-27 09:02:20,682 - validation_interval_step: 2000
[2023-09-27 09:02:20,683] [    INFO] argparser.py:24 - val_targets: ['lfw', 'cfp_fp', 'agedb_30']
Training: 2023-09-27 09:02:20,683 - val_targets: ['lfw', 'cfp_fp', 'agedb_30']
[2023-09-27 09:02:20,683] [    INFO] argparser.py:24 - logdir: ./log
Training: 2023-09-27 09:02:20,683 - logdir: ./log
[2023-09-27 09:02:20,683] [    INFO] argparser.py:24 - log_interval_step: 100
Training: 2023-09-27 09:02:20,683 - log_interval_step: 100
[2023-09-27 09:02:20,683] [    INFO] argparser.py:24 - output: MS1M_v3_arcface_static_0.1
Training: 2023-09-27 09:02:20,683 - output: MS1M_v3_arcface_static_0.1
[2023-09-27 09:02:20,683] [    INFO] argparser.py:24 - resume: False
Training: 2023-09-27 09:02:20,683 - resume: False
[2023-09-27 09:02:20,683] [    INFO] argparser.py:24 - checkpoint_dir: None
Training: 2023-09-27 09:02:20,683 - checkpoint_dir: None
[2023-09-27 09:02:20,683] [    INFO] argparser.py:24 - max_num_last_checkpoint: 1
Training: 2023-09-27 09:02:20,683 - max_num_last_checkpoint: 1
[2023-09-27 09:02:20,683] [    INFO] argparser.py:25 - ------------------------

Training: 2023-09-27 09:02:20,683 - ------------------------

[2023-09-27 09:02:20,683] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[2023-09-27 09:02:20,981] [    INFO] common_dataset.py:55 - read label file finished, total num: 5822653
Training: 2023-09-27 09:02:20,981 - read label file finished, total num: 5822653
[2023-09-27 09:02:20,981] [    INFO] train.py:84 - world_size: 2
Training: 2023-09-27 09:02:20,981 - world_size: 2
[2023-09-27 09:02:20,981] [    INFO] train.py:85 - total_batch_size: 256
Training: 2023-09-27 09:02:20,981 - total_batch_size: 256
[2023-09-27 09:02:20,981] [    INFO] train.py:86 - warmup_steps: 0
Training: 2023-09-27 09:02:20,981 - warmup_steps: 0
[2023-09-27 09:02:20,981] [    INFO] train.py:87 - steps_per_epoch: 22744
Training: 2023-09-27 09:02:20,981 - steps_per_epoch: 22744
[2023-09-27 09:02:20,981] [    INFO] train.py:88 - total_steps: 568600
Training: 2023-09-27 09:02:20,981 - total_steps: 568600
[2023-09-27 09:02:20,981] [    INFO] train.py:89 - total_epoch: 25
Training: 2023-09-27 09:02:20,981 - total_epoch: 25
[2023-09-27 09:02:20,982] [    INFO] train.py:90 - decay_steps: [227440, 363904, 500368]
Training: 2023-09-27 09:02:20,982 - decay_steps: [227440, 363904, 500368]
/home/airi/anaconda3/envs/paddle/lib/python3.9/site-packages/paddle/fluid/data_feeder.py:177: UserWarning: The data type of 'input' in conv2d only support float16 in GPU now. 
  warnings.warn(
/home/airi/anaconda3/envs/paddle/lib/python3.9/site-packages/paddle/fluid/data_feeder.py:177: UserWarning: The data type of 'Out' in guassian_random only support float16 in GPU now. 
  warnings.warn(
/home/airi/anaconda3/envs/paddle/lib/python3.9/site-packages/paddle/fluid/data_feeder.py:177: UserWarning: The data type of 'input' in batch_norm only support float16 in GPU now. 
  warnings.warn(
/home/airi/anaconda3/envs/paddle/lib/python3.9/site-packages/paddle/fluid/data_feeder.py:177: UserWarning: The data type of 'x' in prelu only support float16 in GPU now. 
  warnings.warn(
/home/airi/anaconda3/envs/paddle/lib/python3.9/site-packages/paddle/fluid/data_feeder.py:177: UserWarning: The data type of 'x' in elementwise_add only support float16 in GPU now. 
  warnings.warn(
/home/airi/anaconda3/envs/paddle/lib/python3.9/site-packages/paddle/fluid/data_feeder.py:177: UserWarning: The data type of 'y' in elementwise_add only support float16 in GPU now. 
  warnings.warn(
/home/airi/anaconda3/envs/paddle/lib/python3.9/site-packages/paddle/fluid/data_feeder.py:177: UserWarning: The data type of 'input' in fc only support float16 in GPU now. 
  warnings.warn(
/home/airi/anaconda3/envs/paddle/lib/python3.9/site-packages/paddle/fluid/data_feeder.py:177: UserWarning: The data type of 'Out' in xavier_init only support float16 in GPU now. 
  warnings.warn(
/home/airi/anaconda3/envs/paddle/lib/python3.9/site-packages/paddle/fluid/data_feeder.py:177: UserWarning: The data type of 'dtype' in create_parameter only support float16 in GPU now. 
  warnings.warn(
/home/airi/anaconda3/envs/paddle/lib/python3.9/site-packages/paddle/fluid/data_feeder.py:177: UserWarning: The data type of 'tensor' in all_gather only support float16 in GPU now. 
  warnings.warn(
/home/airi/anaconda3/envs/paddle/lib/python3.9/site-packages/paddle/fluid/data_feeder.py:177: UserWarning: The data type of 'input' in split only support float16 in GPU now. 
  warnings.warn(
/home/airi/anaconda3/envs/paddle/lib/python3.9/site-packages/paddle/fluid/data_feeder.py:177: UserWarning: The data type of 'input[0]' in concat only support float16 in GPU now. 
  warnings.warn(
/home/airi/anaconda3/envs/paddle/lib/python3.9/site-packages/paddle/fluid/data_feeder.py:177: UserWarning: The data type of 'input[1]' in concat only support float16 in GPU now. 
  warnings.warn(
/home/airi/anaconda3/envs/paddle/lib/python3.9/site-packages/paddle/fluid/data_feeder.py:177: UserWarning: The data type of 'x' in gather only support float16 in GPU now. 
  warnings.warn(
Traceback (most recent call last):
  File "/home/airi/Desktop/Shaxboz/Paddle_face_recognition/insightface/recognition/arcface_paddle/tools/train.py", line 35, in <module>
    train(args)
  File "/home/airi/Desktop/Shaxboz/Paddle_face_recognition/insightface/recognition/arcface_paddle/static/train.py", line 107, in train
    train_model = StaticModel(
  File "/home/airi/Desktop/Shaxboz/Paddle_face_recognition/insightface/recognition/arcface_paddle/static/static_model.py", line 86, in __init__
    self.classifier = eval("classifiers.{}".format(
  File "/home/airi/Desktop/Shaxboz/Paddle_face_recognition/insightface/recognition/arcface_paddle/static/classifiers/lsc.py", line 110, in __init__
    norm_feature = paddle.fluid.layers.l2_normalize(total_feature, axis=1)
AttributeError: module 'paddle.fluid.layers' has no attribute 'l2_normalize'
grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
/paddle/insightface/recognition/arcface_paddle/utils/logging.py:46: SyntaxWarning: "is" with a literal. Did you mean "=="?
  if rank is 0:
/paddle/insightface/recognition/arcface_paddle/utils/logging.py:80: SyntaxWarning: "is" with a literal. Did you mean "=="?
  if self.rank is 0 and global_step > 0 and global_step % self.frequent == 0:
Traceback (most recent call last):
  File "/paddle/insightface/recognition/arcface_paddle/tools/train.py", line 24, in <module>
    args = parser.parse_args()
  File "/paddle/insightface/recognition/arcface_paddle/configs/argparser.py", line 72, in parse_args
    cfg = get_config(user_namespace.config_file)
  File "/paddle/insightface/recognition/arcface_paddle/configs/argparser.py", line 51, in get_config
    config = importlib.import_module("configs.config")
  File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/paddle/insightface/recognition/arcface_paddle/configs/config.py", line 15, in <module>
    from easydict import EasyDict as edict
ModuleNotFoundError: No module named 'easydict'
grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
Traceback (most recent call last):
  File "/paddle/insightface/recognition/arcface_paddle/tools/train.py", line 26, in <module>
    from static.train import train
  File "/paddle/insightface/recognition/arcface_paddle/static/train.py", line 22, in <module>
    from visualdl import LogWriter
ModuleNotFoundError: No module named 'visualdl'


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1695788172 (unix time) try "date -d @1695788172" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0xa0) received by PID 235 (TID 0x7f636b49a740) from PID 160 ***]

grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
Traceback (most recent call last):
  File "/paddle/insightface/recognition/arcface_paddle/tools/train.py", line 26, in <module>
    from static.train import train
  File "/paddle/insightface/recognition/arcface_paddle/static/train.py", line 25, in <module>
    from datasets import CommonDataset, SyntheticDataset
  File "/paddle/insightface/recognition/arcface_paddle/datasets/__init__.py", line 15, in <module>
    from .common_dataset import CommonDataset, SyntheticDataset, load_bin
  File "/paddle/insightface/recognition/arcface_paddle/datasets/common_dataset.py", line 18, in <module>
    import cv2
ModuleNotFoundError: No module named 'cv2'
grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
[2023-09-27 04:18:41,273] [    INFO] logging.py:57 - rank: 0
Training: 2023-09-27 04:18:41,273 - rank: 0
[2023-09-27 04:18:41,273] [    INFO] argparser.py:22 - --------args----------
Training: 2023-09-27 04:18:41,273 - --------args----------
[2023-09-27 04:18:41,273] [    INFO] argparser.py:24 - config_file: configs/ms1mv3_r50.py
Training: 2023-09-27 04:18:41,273 - config_file: configs/ms1mv3_r50.py
[2023-09-27 04:18:41,273] [    INFO] argparser.py:24 - is_static: True
Training: 2023-09-27 04:18:41,273 - is_static: True
[2023-09-27 04:18:41,273] [    INFO] argparser.py:24 - backbone: FresResNet50
Training: 2023-09-27 04:18:41,273 - backbone: FresResNet50
[2023-09-27 04:18:41,273] [    INFO] argparser.py:24 - classifier: LargeScaleClassifier
Training: 2023-09-27 04:18:41,273 - classifier: LargeScaleClassifier
[2023-09-27 04:18:41,273] [    INFO] argparser.py:24 - embedding_size: 512
Training: 2023-09-27 04:18:41,273 - embedding_size: 512
[2023-09-27 04:18:41,273] [    INFO] argparser.py:24 - model_parallel: True
Training: 2023-09-27 04:18:41,273 - model_parallel: True
[2023-09-27 04:18:41,273] [    INFO] argparser.py:24 - sample_ratio: 0.1
Training: 2023-09-27 04:18:41,273 - sample_ratio: 0.1
[2023-09-27 04:18:41,273] [    INFO] argparser.py:24 - loss: ArcFace
Training: 2023-09-27 04:18:41,273 - loss: ArcFace
[2023-09-27 04:18:41,273] [    INFO] argparser.py:24 - dropout: 0.0
Training: 2023-09-27 04:18:41,273 - dropout: 0.0
[2023-09-27 04:18:41,273] [    INFO] argparser.py:24 - fp16: True
Training: 2023-09-27 04:18:41,273 - fp16: True
[2023-09-27 04:18:41,273] [    INFO] argparser.py:24 - init_loss_scaling: 27648.0
Training: 2023-09-27 04:18:41,273 - init_loss_scaling: 27648.0
[2023-09-27 04:18:41,273] [    INFO] argparser.py:24 - max_loss_scaling: 128.0
Training: 2023-09-27 04:18:41,273 - max_loss_scaling: 128.0
[2023-09-27 04:18:41,273] [    INFO] argparser.py:24 - incr_every_n_steps: 2000
Training: 2023-09-27 04:18:41,273 - incr_every_n_steps: 2000
[2023-09-27 04:18:41,273] [    INFO] argparser.py:24 - decr_every_n_nan_or_inf: 1
Training: 2023-09-27 04:18:41,273 - decr_every_n_nan_or_inf: 1
[2023-09-27 04:18:41,273] [    INFO] argparser.py:24 - incr_ratio: 2.0
Training: 2023-09-27 04:18:41,273 - incr_ratio: 2.0
[2023-09-27 04:18:41,273] [    INFO] argparser.py:24 - decr_ratio: 0.5
Training: 2023-09-27 04:18:41,273 - decr_ratio: 0.5
[2023-09-27 04:18:41,274] [    INFO] argparser.py:24 - use_dynamic_loss_scaling: True
Training: 2023-09-27 04:18:41,274 - use_dynamic_loss_scaling: True
[2023-09-27 04:18:41,274] [    INFO] argparser.py:24 - custom_white_list: []
Training: 2023-09-27 04:18:41,274 - custom_white_list: []
[2023-09-27 04:18:41,274] [    INFO] argparser.py:24 - custom_black_list: []
Training: 2023-09-27 04:18:41,274 - custom_black_list: []
[2023-09-27 04:18:41,274] [    INFO] argparser.py:24 - lr: 0.1
Training: 2023-09-27 04:18:41,274 - lr: 0.1
[2023-09-27 04:18:41,274] [    INFO] argparser.py:24 - lr_decay: 0.1
Training: 2023-09-27 04:18:41,274 - lr_decay: 0.1
[2023-09-27 04:18:41,274] [    INFO] argparser.py:24 - weight_decay: 0.0005
Training: 2023-09-27 04:18:41,274 - weight_decay: 0.0005
[2023-09-27 04:18:41,274] [    INFO] argparser.py:24 - momentum: 0.9
Training: 2023-09-27 04:18:41,274 - momentum: 0.9
[2023-09-27 04:18:41,274] [    INFO] argparser.py:24 - train_unit: epoch
Training: 2023-09-27 04:18:41,274 - train_unit: epoch
[2023-09-27 04:18:41,274] [    INFO] argparser.py:24 - warmup_num: 0
Training: 2023-09-27 04:18:41,274 - warmup_num: 0
[2023-09-27 04:18:41,274] [    INFO] argparser.py:24 - train_num: 25
Training: 2023-09-27 04:18:41,274 - train_num: 25
[2023-09-27 04:18:41,274] [    INFO] argparser.py:24 - decay_boundaries: [10, 16, 22]
Training: 2023-09-27 04:18:41,274 - decay_boundaries: [10, 16, 22]
[2023-09-27 04:18:41,274] [    INFO] argparser.py:24 - use_synthetic_dataset: False
Training: 2023-09-27 04:18:41,274 - use_synthetic_dataset: False
[2023-09-27 04:18:41,274] [    INFO] argparser.py:24 - dataset: MS1M_v2
Training: 2023-09-27 04:18:41,274 - dataset: MS1M_v2
[2023-09-27 04:18:41,274] [    INFO] argparser.py:24 - data_dir: /home/airi/Desktop/Shaxboz/Paddle_face_recognition/data_out/
Training: 2023-09-27 04:18:41,274 - data_dir: /home/airi/Desktop/Shaxboz/Paddle_face_recognition/data_out/
[2023-09-27 04:18:41,274] [    INFO] argparser.py:24 - label_file: /home/airi/Desktop/Shaxboz/Paddle_face_recognition/data_out/label.txt
Training: 2023-09-27 04:18:41,274 - label_file: /home/airi/Desktop/Shaxboz/Paddle_face_recognition/data_out/label.txt
[2023-09-27 04:18:41,274] [    INFO] argparser.py:24 - is_bin: False
Training: 2023-09-27 04:18:41,274 - is_bin: False
[2023-09-27 04:18:41,274] [    INFO] argparser.py:24 - num_classes: 85741
Training: 2023-09-27 04:18:41,274 - num_classes: 85741
[2023-09-27 04:18:41,274] [    INFO] argparser.py:24 - batch_size: 128
Training: 2023-09-27 04:18:41,274 - batch_size: 128
[2023-09-27 04:18:41,274] [    INFO] argparser.py:24 - num_workers: 2
Training: 2023-09-27 04:18:41,274 - num_workers: 2
[2023-09-27 04:18:41,274] [    INFO] argparser.py:24 - do_validation_while_train: True
Training: 2023-09-27 04:18:41,274 - do_validation_while_train: True
[2023-09-27 04:18:41,274] [    INFO] argparser.py:24 - validation_interval_step: 2000
Training: 2023-09-27 04:18:41,274 - validation_interval_step: 2000
[2023-09-27 04:18:41,274] [    INFO] argparser.py:24 - val_targets: ['lfw', 'cfp_fp', 'agedb_30']
Training: 2023-09-27 04:18:41,274 - val_targets: ['lfw', 'cfp_fp', 'agedb_30']
[2023-09-27 04:18:41,274] [    INFO] argparser.py:24 - logdir: ./log
Training: 2023-09-27 04:18:41,274 - logdir: ./log
[2023-09-27 04:18:41,274] [    INFO] argparser.py:24 - log_interval_step: 100
Training: 2023-09-27 04:18:41,274 - log_interval_step: 100
[2023-09-27 04:18:41,274] [    INFO] argparser.py:24 - output: MS1M_v3_arcface_static_0.1
Training: 2023-09-27 04:18:41,274 - output: MS1M_v3_arcface_static_0.1
[2023-09-27 04:18:41,274] [    INFO] argparser.py:24 - resume: False
Training: 2023-09-27 04:18:41,274 - resume: False
[2023-09-27 04:18:41,274] [    INFO] argparser.py:24 - checkpoint_dir: None
Training: 2023-09-27 04:18:41,274 - checkpoint_dir: None
[2023-09-27 04:18:41,274] [    INFO] argparser.py:24 - max_num_last_checkpoint: 1
Training: 2023-09-27 04:18:41,274 - max_num_last_checkpoint: 1
[2023-09-27 04:18:41,274] [    INFO] argparser.py:25 - ------------------------

Training: 2023-09-27 04:18:41,274 - ------------------------

[2023-09-27 04:18:41,275] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
Traceback (most recent call last):
  File "/paddle/insightface/recognition/arcface_paddle/tools/train.py", line 35, in <module>
    train(args)
  File "/paddle/insightface/recognition/arcface_paddle/static/train.py", line 63, in train
    trainset = CommonDataset(
  File "/paddle/insightface/recognition/arcface_paddle/datasets/common_dataset.py", line 48, in __init__
    with open(label_file, "r") as fin:
FileNotFoundError: [Errno 2] No such file or directory: '/home/airi/Desktop/Shaxboz/Paddle_face_recognition/data_out/label.txt'
grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
[2023-09-27 04:21:51,365] [    INFO] logging.py:57 - rank: 0
Training: 2023-09-27 04:21:51,365 - rank: 0
[2023-09-27 04:21:51,365] [    INFO] argparser.py:22 - --------args----------
Training: 2023-09-27 04:21:51,365 - --------args----------
[2023-09-27 04:21:51,365] [    INFO] argparser.py:24 - config_file: configs/ms1mv3_r50.py
Training: 2023-09-27 04:21:51,365 - config_file: configs/ms1mv3_r50.py
[2023-09-27 04:21:51,365] [    INFO] argparser.py:24 - is_static: True
Training: 2023-09-27 04:21:51,365 - is_static: True
[2023-09-27 04:21:51,365] [    INFO] argparser.py:24 - backbone: FresResNet50
Training: 2023-09-27 04:21:51,365 - backbone: FresResNet50
[2023-09-27 04:21:51,365] [    INFO] argparser.py:24 - classifier: LargeScaleClassifier
Training: 2023-09-27 04:21:51,365 - classifier: LargeScaleClassifier
[2023-09-27 04:21:51,365] [    INFO] argparser.py:24 - embedding_size: 512
Training: 2023-09-27 04:21:51,365 - embedding_size: 512
[2023-09-27 04:21:51,365] [    INFO] argparser.py:24 - model_parallel: True
Training: 2023-09-27 04:21:51,365 - model_parallel: True
[2023-09-27 04:21:51,365] [    INFO] argparser.py:24 - sample_ratio: 0.1
Training: 2023-09-27 04:21:51,365 - sample_ratio: 0.1
[2023-09-27 04:21:51,365] [    INFO] argparser.py:24 - loss: ArcFace
Training: 2023-09-27 04:21:51,365 - loss: ArcFace
[2023-09-27 04:21:51,365] [    INFO] argparser.py:24 - dropout: 0.0
Training: 2023-09-27 04:21:51,365 - dropout: 0.0
[2023-09-27 04:21:51,365] [    INFO] argparser.py:24 - fp16: True
Training: 2023-09-27 04:21:51,365 - fp16: True
[2023-09-27 04:21:51,365] [    INFO] argparser.py:24 - init_loss_scaling: 27648.0
Training: 2023-09-27 04:21:51,365 - init_loss_scaling: 27648.0
[2023-09-27 04:21:51,365] [    INFO] argparser.py:24 - max_loss_scaling: 128.0
Training: 2023-09-27 04:21:51,365 - max_loss_scaling: 128.0
[2023-09-27 04:21:51,366] [    INFO] argparser.py:24 - incr_every_n_steps: 2000
Training: 2023-09-27 04:21:51,366 - incr_every_n_steps: 2000
[2023-09-27 04:21:51,366] [    INFO] argparser.py:24 - decr_every_n_nan_or_inf: 1
Training: 2023-09-27 04:21:51,366 - decr_every_n_nan_or_inf: 1
[2023-09-27 04:21:51,366] [    INFO] argparser.py:24 - incr_ratio: 2.0
Training: 2023-09-27 04:21:51,366 - incr_ratio: 2.0
[2023-09-27 04:21:51,366] [    INFO] argparser.py:24 - decr_ratio: 0.5
Training: 2023-09-27 04:21:51,366 - decr_ratio: 0.5
[2023-09-27 04:21:51,366] [    INFO] argparser.py:24 - use_dynamic_loss_scaling: True
Training: 2023-09-27 04:21:51,366 - use_dynamic_loss_scaling: True
[2023-09-27 04:21:51,366] [    INFO] argparser.py:24 - custom_white_list: []
Training: 2023-09-27 04:21:51,366 - custom_white_list: []
[2023-09-27 04:21:51,366] [    INFO] argparser.py:24 - custom_black_list: []
Training: 2023-09-27 04:21:51,366 - custom_black_list: []
[2023-09-27 04:21:51,366] [    INFO] argparser.py:24 - lr: 0.1
Training: 2023-09-27 04:21:51,366 - lr: 0.1
[2023-09-27 04:21:51,366] [    INFO] argparser.py:24 - lr_decay: 0.1
Training: 2023-09-27 04:21:51,366 - lr_decay: 0.1
[2023-09-27 04:21:51,366] [    INFO] argparser.py:24 - weight_decay: 0.0005
Training: 2023-09-27 04:21:51,366 - weight_decay: 0.0005
[2023-09-27 04:21:51,366] [    INFO] argparser.py:24 - momentum: 0.9
Training: 2023-09-27 04:21:51,366 - momentum: 0.9
[2023-09-27 04:21:51,366] [    INFO] argparser.py:24 - train_unit: epoch
Training: 2023-09-27 04:21:51,366 - train_unit: epoch
[2023-09-27 04:21:51,366] [    INFO] argparser.py:24 - warmup_num: 0
Training: 2023-09-27 04:21:51,366 - warmup_num: 0
[2023-09-27 04:21:51,366] [    INFO] argparser.py:24 - train_num: 25
Training: 2023-09-27 04:21:51,366 - train_num: 25
[2023-09-27 04:21:51,366] [    INFO] argparser.py:24 - decay_boundaries: [10, 16, 22]
Training: 2023-09-27 04:21:51,366 - decay_boundaries: [10, 16, 22]
[2023-09-27 04:21:51,366] [    INFO] argparser.py:24 - use_synthetic_dataset: False
Training: 2023-09-27 04:21:51,366 - use_synthetic_dataset: False
[2023-09-27 04:21:51,366] [    INFO] argparser.py:24 - dataset: MS1M_v2
Training: 2023-09-27 04:21:51,366 - dataset: MS1M_v2
[2023-09-27 04:21:51,366] [    INFO] argparser.py:24 - data_dir: /paddle/insightface/recognition/arcface_paddle/data_out/
Training: 2023-09-27 04:21:51,366 - data_dir: /paddle/insightface/recognition/arcface_paddle/data_out/
[2023-09-27 04:21:51,366] [    INFO] argparser.py:24 - label_file: /paddle/insightface/recognition/arcface_paddle/data_out/label.txt
Training: 2023-09-27 04:21:51,366 - label_file: /paddle/insightface/recognition/arcface_paddle/data_out/label.txt
[2023-09-27 04:21:51,366] [    INFO] argparser.py:24 - is_bin: False
Training: 2023-09-27 04:21:51,366 - is_bin: False
[2023-09-27 04:21:51,366] [    INFO] argparser.py:24 - num_classes: 85741
Training: 2023-09-27 04:21:51,366 - num_classes: 85741
[2023-09-27 04:21:51,366] [    INFO] argparser.py:24 - batch_size: 128
Training: 2023-09-27 04:21:51,366 - batch_size: 128
[2023-09-27 04:21:51,366] [    INFO] argparser.py:24 - num_workers: 2
Training: 2023-09-27 04:21:51,366 - num_workers: 2
[2023-09-27 04:21:51,366] [    INFO] argparser.py:24 - do_validation_while_train: True
Training: 2023-09-27 04:21:51,366 - do_validation_while_train: True
[2023-09-27 04:21:51,366] [    INFO] argparser.py:24 - validation_interval_step: 2000
Training: 2023-09-27 04:21:51,366 - validation_interval_step: 2000
[2023-09-27 04:21:51,366] [    INFO] argparser.py:24 - val_targets: ['lfw', 'cfp_fp', 'agedb_30']
Training: 2023-09-27 04:21:51,366 - val_targets: ['lfw', 'cfp_fp', 'agedb_30']
[2023-09-27 04:21:51,366] [    INFO] argparser.py:24 - logdir: ./log
Training: 2023-09-27 04:21:51,366 - logdir: ./log
[2023-09-27 04:21:51,366] [    INFO] argparser.py:24 - log_interval_step: 100
Training: 2023-09-27 04:21:51,366 - log_interval_step: 100
[2023-09-27 04:21:51,366] [    INFO] argparser.py:24 - output: MS1M_v3_arcface_static_0.1
Training: 2023-09-27 04:21:51,366 - output: MS1M_v3_arcface_static_0.1
[2023-09-27 04:21:51,366] [    INFO] argparser.py:24 - resume: False
Training: 2023-09-27 04:21:51,366 - resume: False
[2023-09-27 04:21:51,366] [    INFO] argparser.py:24 - checkpoint_dir: None
Training: 2023-09-27 04:21:51,366 - checkpoint_dir: None
[2023-09-27 04:21:51,366] [    INFO] argparser.py:24 - max_num_last_checkpoint: 1
Training: 2023-09-27 04:21:51,366 - max_num_last_checkpoint: 1
[2023-09-27 04:21:51,366] [    INFO] argparser.py:25 - ------------------------

Training: 2023-09-27 04:21:51,366 - ------------------------

[2023-09-27 04:21:51,367] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
Traceback (most recent call last):
  File "/paddle/insightface/recognition/arcface_paddle/tools/train.py", line 35, in <module>
    train(args)
  File "/paddle/insightface/recognition/arcface_paddle/static/train.py", line 63, in train
    trainset = CommonDataset(
  File "/paddle/insightface/recognition/arcface_paddle/datasets/common_dataset.py", line 48, in __init__
    with open(label_file, "r") as fin:
FileNotFoundError: [Errno 2] No such file or directory: '/paddle/insightface/recognition/arcface_paddle/data_out/label.txt'
grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
[2023-09-27 04:23:31,441] [    INFO] logging.py:57 - rank: 0
Training: 2023-09-27 04:23:31,441 - rank: 0
[2023-09-27 04:23:31,441] [    INFO] argparser.py:22 - --------args----------
Training: 2023-09-27 04:23:31,441 - --------args----------
[2023-09-27 04:23:31,441] [    INFO] argparser.py:24 - config_file: configs/ms1mv3_r50.py
Training: 2023-09-27 04:23:31,441 - config_file: configs/ms1mv3_r50.py
[2023-09-27 04:23:31,442] [    INFO] argparser.py:24 - is_static: True
Training: 2023-09-27 04:23:31,442 - is_static: True
[2023-09-27 04:23:31,442] [    INFO] argparser.py:24 - backbone: FresResNet50
Training: 2023-09-27 04:23:31,442 - backbone: FresResNet50
[2023-09-27 04:23:31,442] [    INFO] argparser.py:24 - classifier: LargeScaleClassifier
Training: 2023-09-27 04:23:31,442 - classifier: LargeScaleClassifier
[2023-09-27 04:23:31,442] [    INFO] argparser.py:24 - embedding_size: 512
Training: 2023-09-27 04:23:31,442 - embedding_size: 512
[2023-09-27 04:23:31,442] [    INFO] argparser.py:24 - model_parallel: True
Training: 2023-09-27 04:23:31,442 - model_parallel: True
[2023-09-27 04:23:31,442] [    INFO] argparser.py:24 - sample_ratio: 0.1
Training: 2023-09-27 04:23:31,442 - sample_ratio: 0.1
[2023-09-27 04:23:31,442] [    INFO] argparser.py:24 - loss: ArcFace
Training: 2023-09-27 04:23:31,442 - loss: ArcFace
[2023-09-27 04:23:31,442] [    INFO] argparser.py:24 - dropout: 0.0
Training: 2023-09-27 04:23:31,442 - dropout: 0.0
[2023-09-27 04:23:31,442] [    INFO] argparser.py:24 - fp16: True
Training: 2023-09-27 04:23:31,442 - fp16: True
[2023-09-27 04:23:31,442] [    INFO] argparser.py:24 - init_loss_scaling: 27648.0
Training: 2023-09-27 04:23:31,442 - init_loss_scaling: 27648.0
[2023-09-27 04:23:31,442] [    INFO] argparser.py:24 - max_loss_scaling: 128.0
Training: 2023-09-27 04:23:31,442 - max_loss_scaling: 128.0
[2023-09-27 04:23:31,442] [    INFO] argparser.py:24 - incr_every_n_steps: 2000
Training: 2023-09-27 04:23:31,442 - incr_every_n_steps: 2000
[2023-09-27 04:23:31,442] [    INFO] argparser.py:24 - decr_every_n_nan_or_inf: 1
Training: 2023-09-27 04:23:31,442 - decr_every_n_nan_or_inf: 1
[2023-09-27 04:23:31,442] [    INFO] argparser.py:24 - incr_ratio: 2.0
Training: 2023-09-27 04:23:31,442 - incr_ratio: 2.0
[2023-09-27 04:23:31,442] [    INFO] argparser.py:24 - decr_ratio: 0.5
Training: 2023-09-27 04:23:31,442 - decr_ratio: 0.5
[2023-09-27 04:23:31,442] [    INFO] argparser.py:24 - use_dynamic_loss_scaling: True
Training: 2023-09-27 04:23:31,442 - use_dynamic_loss_scaling: True
[2023-09-27 04:23:31,442] [    INFO] argparser.py:24 - custom_white_list: []
Training: 2023-09-27 04:23:31,442 - custom_white_list: []
[2023-09-27 04:23:31,442] [    INFO] argparser.py:24 - custom_black_list: []
Training: 2023-09-27 04:23:31,442 - custom_black_list: []
[2023-09-27 04:23:31,442] [    INFO] argparser.py:24 - lr: 0.1
Training: 2023-09-27 04:23:31,442 - lr: 0.1
[2023-09-27 04:23:31,442] [    INFO] argparser.py:24 - lr_decay: 0.1
Training: 2023-09-27 04:23:31,442 - lr_decay: 0.1
[2023-09-27 04:23:31,442] [    INFO] argparser.py:24 - weight_decay: 0.0005
Training: 2023-09-27 04:23:31,442 - weight_decay: 0.0005
[2023-09-27 04:23:31,442] [    INFO] argparser.py:24 - momentum: 0.9
Training: 2023-09-27 04:23:31,442 - momentum: 0.9
[2023-09-27 04:23:31,442] [    INFO] argparser.py:24 - train_unit: epoch
Training: 2023-09-27 04:23:31,442 - train_unit: epoch
[2023-09-27 04:23:31,442] [    INFO] argparser.py:24 - warmup_num: 0
Training: 2023-09-27 04:23:31,442 - warmup_num: 0
[2023-09-27 04:23:31,442] [    INFO] argparser.py:24 - train_num: 25
Training: 2023-09-27 04:23:31,442 - train_num: 25
[2023-09-27 04:23:31,442] [    INFO] argparser.py:24 - decay_boundaries: [10, 16, 22]
Training: 2023-09-27 04:23:31,442 - decay_boundaries: [10, 16, 22]
[2023-09-27 04:23:31,442] [    INFO] argparser.py:24 - use_synthetic_dataset: False
Training: 2023-09-27 04:23:31,442 - use_synthetic_dataset: False
[2023-09-27 04:23:31,442] [    INFO] argparser.py:24 - dataset: MS1M_v2
Training: 2023-09-27 04:23:31,442 - dataset: MS1M_v2
[2023-09-27 04:23:31,442] [    INFO] argparser.py:24 - data_dir: /paddle/data_out/
Training: 2023-09-27 04:23:31,442 - data_dir: /paddle/data_out/
[2023-09-27 04:23:31,442] [    INFO] argparser.py:24 - label_file: /paddle/data_out/label.txt
Training: 2023-09-27 04:23:31,442 - label_file: /paddle/data_out/label.txt
[2023-09-27 04:23:31,442] [    INFO] argparser.py:24 - is_bin: False
Training: 2023-09-27 04:23:31,442 - is_bin: False
[2023-09-27 04:23:31,442] [    INFO] argparser.py:24 - num_classes: 85741
Training: 2023-09-27 04:23:31,442 - num_classes: 85741
[2023-09-27 04:23:31,442] [    INFO] argparser.py:24 - batch_size: 128
Training: 2023-09-27 04:23:31,442 - batch_size: 128
[2023-09-27 04:23:31,442] [    INFO] argparser.py:24 - num_workers: 2
Training: 2023-09-27 04:23:31,442 - num_workers: 2
[2023-09-27 04:23:31,443] [    INFO] argparser.py:24 - do_validation_while_train: True
Training: 2023-09-27 04:23:31,443 - do_validation_while_train: True
[2023-09-27 04:23:31,443] [    INFO] argparser.py:24 - validation_interval_step: 2000
Training: 2023-09-27 04:23:31,443 - validation_interval_step: 2000
[2023-09-27 04:23:31,443] [    INFO] argparser.py:24 - val_targets: ['lfw', 'cfp_fp', 'agedb_30']
Training: 2023-09-27 04:23:31,443 - val_targets: ['lfw', 'cfp_fp', 'agedb_30']
[2023-09-27 04:23:31,443] [    INFO] argparser.py:24 - logdir: ./log
Training: 2023-09-27 04:23:31,443 - logdir: ./log
[2023-09-27 04:23:31,443] [    INFO] argparser.py:24 - log_interval_step: 100
Training: 2023-09-27 04:23:31,443 - log_interval_step: 100
[2023-09-27 04:23:31,443] [    INFO] argparser.py:24 - output: MS1M_v3_arcface_static_0.1
Training: 2023-09-27 04:23:31,443 - output: MS1M_v3_arcface_static_0.1
[2023-09-27 04:23:31,443] [    INFO] argparser.py:24 - resume: False
Training: 2023-09-27 04:23:31,443 - resume: False
[2023-09-27 04:23:31,443] [    INFO] argparser.py:24 - checkpoint_dir: None
Training: 2023-09-27 04:23:31,443 - checkpoint_dir: None
[2023-09-27 04:23:31,443] [    INFO] argparser.py:24 - max_num_last_checkpoint: 1
Training: 2023-09-27 04:23:31,443 - max_num_last_checkpoint: 1
[2023-09-27 04:23:31,443] [    INFO] argparser.py:25 - ------------------------

Training: 2023-09-27 04:23:31,443 - ------------------------

[2023-09-27 04:23:31,443] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[2023-09-27 04:23:31,707] [    INFO] common_dataset.py:55 - read label file finished, total num: 5822653
Training: 2023-09-27 04:23:31,707 - read label file finished, total num: 5822653
[2023-09-27 04:23:31,708] [    INFO] train.py:84 - world_size: 2
Training: 2023-09-27 04:23:31,708 - world_size: 2
[2023-09-27 04:23:31,708] [    INFO] train.py:85 - total_batch_size: 256
Training: 2023-09-27 04:23:31,708 - total_batch_size: 256
[2023-09-27 04:23:31,708] [    INFO] train.py:86 - warmup_steps: 0
Training: 2023-09-27 04:23:31,708 - warmup_steps: 0
[2023-09-27 04:23:31,708] [    INFO] train.py:87 - steps_per_epoch: 22744
Training: 2023-09-27 04:23:31,708 - steps_per_epoch: 22744
[2023-09-27 04:23:31,708] [    INFO] train.py:88 - total_steps: 568600
Training: 2023-09-27 04:23:31,708 - total_steps: 568600
[2023-09-27 04:23:31,708] [    INFO] train.py:89 - total_epoch: 25
Training: 2023-09-27 04:23:31,708 - total_epoch: 25
[2023-09-27 04:23:31,708] [    INFO] train.py:90 - decay_steps: [227440, 363904, 500368]
Training: 2023-09-27 04:23:31,708 - decay_steps: [227440, 363904, 500368]
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:177: UserWarning: The data type of 'input' in conv2d only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:177: UserWarning: The data type of 'Out' in guassian_random only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:177: UserWarning: The data type of 'input' in batch_norm only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:177: UserWarning: The data type of 'x' in prelu only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:177: UserWarning: The data type of 'x' in elementwise_add only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:177: UserWarning: The data type of 'y' in elementwise_add only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:177: UserWarning: The data type of 'input' in fc only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:177: UserWarning: The data type of 'Out' in xavier_init only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:177: UserWarning: The data type of 'dtype' in create_parameter only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:177: UserWarning: The data type of 'tensor' in all_gather only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:177: UserWarning: The data type of 'input' in split only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:177: UserWarning: The data type of 'input[0]' in concat only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:177: UserWarning: The data type of 'input[1]' in concat only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:177: UserWarning: The data type of 'x' in gather only support float16 in GPU now. 
  warnings.warn(
Traceback (most recent call last):
  File "/paddle/insightface/recognition/arcface_paddle/tools/train.py", line 35, in <module>
    train(args)
  File "/paddle/insightface/recognition/arcface_paddle/static/train.py", line 107, in train
    train_model = StaticModel(
  File "/paddle/insightface/recognition/arcface_paddle/static/static_model.py", line 86, in __init__
    self.classifier = eval("classifiers.{}".format(
  File "/paddle/insightface/recognition/arcface_paddle/static/classifiers/lsc.py", line 110, in __init__
    norm_feature = paddle.fluid.layers.l2_normalize(total_feature, axis=1)
AttributeError: module 'paddle.fluid.layers' has no attribute 'l2_normalize'
grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
/usr/local/lib/python3.10/dist-packages/setuptools-50.3.2-py3.10.egg/setuptools/__init__.py:10: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives
/usr/lib/python3.10/distutils/command/install.py:13: DeprecationWarning: The distutils.sysconfig module is deprecated, use sysconfig instead
  from distutils.sysconfig import get_config_vars
INFO 2023-09-27 04:35:52,018 logging.py:57] rank: 0
Training: 2023-09-27 04:35:52,018 - rank: 0
INFO 2023-09-27 04:35:52,018 argparser.py:22] --------args----------
Training: 2023-09-27 04:35:52,018 - --------args----------
INFO 2023-09-27 04:35:52,019 argparser.py:24] config_file: configs/ms1mv3_r50.py
Training: 2023-09-27 04:35:52,019 - config_file: configs/ms1mv3_r50.py
INFO 2023-09-27 04:35:52,019 argparser.py:24] is_static: True
Training: 2023-09-27 04:35:52,019 - is_static: True
INFO 2023-09-27 04:35:52,019 argparser.py:24] backbone: FresResNet50
Training: 2023-09-27 04:35:52,019 - backbone: FresResNet50
INFO 2023-09-27 04:35:52,019 argparser.py:24] classifier: LargeScaleClassifier
Training: 2023-09-27 04:35:52,019 - classifier: LargeScaleClassifier
INFO 2023-09-27 04:35:52,019 argparser.py:24] embedding_size: 512
Training: 2023-09-27 04:35:52,019 - embedding_size: 512
INFO 2023-09-27 04:35:52,019 argparser.py:24] model_parallel: True
Training: 2023-09-27 04:35:52,019 - model_parallel: True
INFO 2023-09-27 04:35:52,019 argparser.py:24] sample_ratio: 0.1
Training: 2023-09-27 04:35:52,019 - sample_ratio: 0.1
INFO 2023-09-27 04:35:52,019 argparser.py:24] loss: ArcFace
Training: 2023-09-27 04:35:52,019 - loss: ArcFace
INFO 2023-09-27 04:35:52,019 argparser.py:24] dropout: 0.0
Training: 2023-09-27 04:35:52,019 - dropout: 0.0
INFO 2023-09-27 04:35:52,019 argparser.py:24] fp16: True
Training: 2023-09-27 04:35:52,019 - fp16: True
INFO 2023-09-27 04:35:52,019 argparser.py:24] init_loss_scaling: 27648.0
Training: 2023-09-27 04:35:52,019 - init_loss_scaling: 27648.0
INFO 2023-09-27 04:35:52,019 argparser.py:24] max_loss_scaling: 128.0
Training: 2023-09-27 04:35:52,019 - max_loss_scaling: 128.0
INFO 2023-09-27 04:35:52,019 argparser.py:24] incr_every_n_steps: 2000
Training: 2023-09-27 04:35:52,019 - incr_every_n_steps: 2000
INFO 2023-09-27 04:35:52,019 argparser.py:24] decr_every_n_nan_or_inf: 1
Training: 2023-09-27 04:35:52,019 - decr_every_n_nan_or_inf: 1
INFO 2023-09-27 04:35:52,019 argparser.py:24] incr_ratio: 2.0
Training: 2023-09-27 04:35:52,019 - incr_ratio: 2.0
INFO 2023-09-27 04:35:52,019 argparser.py:24] decr_ratio: 0.5
Training: 2023-09-27 04:35:52,019 - decr_ratio: 0.5
INFO 2023-09-27 04:35:52,019 argparser.py:24] use_dynamic_loss_scaling: True
Training: 2023-09-27 04:35:52,019 - use_dynamic_loss_scaling: True
INFO 2023-09-27 04:35:52,019 argparser.py:24] custom_white_list: []
Training: 2023-09-27 04:35:52,019 - custom_white_list: []
INFO 2023-09-27 04:35:52,019 argparser.py:24] custom_black_list: []
Training: 2023-09-27 04:35:52,019 - custom_black_list: []
INFO 2023-09-27 04:35:52,019 argparser.py:24] lr: 0.1
Training: 2023-09-27 04:35:52,019 - lr: 0.1
INFO 2023-09-27 04:35:52,019 argparser.py:24] lr_decay: 0.1
Training: 2023-09-27 04:35:52,019 - lr_decay: 0.1
INFO 2023-09-27 04:35:52,019 argparser.py:24] weight_decay: 0.0005
Training: 2023-09-27 04:35:52,019 - weight_decay: 0.0005
INFO 2023-09-27 04:35:52,019 argparser.py:24] momentum: 0.9
Training: 2023-09-27 04:35:52,019 - momentum: 0.9
INFO 2023-09-27 04:35:52,019 argparser.py:24] train_unit: epoch
Training: 2023-09-27 04:35:52,019 - train_unit: epoch
INFO 2023-09-27 04:35:52,019 argparser.py:24] warmup_num: 0
Training: 2023-09-27 04:35:52,019 - warmup_num: 0
INFO 2023-09-27 04:35:52,019 argparser.py:24] train_num: 25
Training: 2023-09-27 04:35:52,019 - train_num: 25
INFO 2023-09-27 04:35:52,019 argparser.py:24] decay_boundaries: [10, 16, 22]
Training: 2023-09-27 04:35:52,019 - decay_boundaries: [10, 16, 22]
INFO 2023-09-27 04:35:52,019 argparser.py:24] use_synthetic_dataset: False
Training: 2023-09-27 04:35:52,019 - use_synthetic_dataset: False
INFO 2023-09-27 04:35:52,019 argparser.py:24] dataset: MS1M_v2
Training: 2023-09-27 04:35:52,019 - dataset: MS1M_v2
INFO 2023-09-27 04:35:52,019 argparser.py:24] data_dir: /paddle/data_out/
Training: 2023-09-27 04:35:52,019 - data_dir: /paddle/data_out/
INFO 2023-09-27 04:35:52,019 argparser.py:24] label_file: /paddle/data_out/label.txt
Training: 2023-09-27 04:35:52,019 - label_file: /paddle/data_out/label.txt
INFO 2023-09-27 04:35:52,019 argparser.py:24] is_bin: False
Training: 2023-09-27 04:35:52,019 - is_bin: False
INFO 2023-09-27 04:35:52,020 argparser.py:24] num_classes: 85741
Training: 2023-09-27 04:35:52,020 - num_classes: 85741
INFO 2023-09-27 04:35:52,020 argparser.py:24] batch_size: 128
Training: 2023-09-27 04:35:52,020 - batch_size: 128
INFO 2023-09-27 04:35:52,020 argparser.py:24] num_workers: 2
Training: 2023-09-27 04:35:52,020 - num_workers: 2
INFO 2023-09-27 04:35:52,020 argparser.py:24] do_validation_while_train: True
Training: 2023-09-27 04:35:52,020 - do_validation_while_train: True
INFO 2023-09-27 04:35:52,020 argparser.py:24] validation_interval_step: 2000
Training: 2023-09-27 04:35:52,020 - validation_interval_step: 2000
INFO 2023-09-27 04:35:52,020 argparser.py:24] val_targets: ['lfw', 'cfp_fp', 'agedb_30']
Training: 2023-09-27 04:35:52,020 - val_targets: ['lfw', 'cfp_fp', 'agedb_30']
INFO 2023-09-27 04:35:52,020 argparser.py:24] logdir: ./log
Training: 2023-09-27 04:35:52,020 - logdir: ./log
INFO 2023-09-27 04:35:52,020 argparser.py:24] log_interval_step: 100
Training: 2023-09-27 04:35:52,020 - log_interval_step: 100
INFO 2023-09-27 04:35:52,020 argparser.py:24] output: MS1M_v3_arcface_static_0.1
Training: 2023-09-27 04:35:52,020 - output: MS1M_v3_arcface_static_0.1
INFO 2023-09-27 04:35:52,020 argparser.py:24] resume: False
Training: 2023-09-27 04:35:52,020 - resume: False
INFO 2023-09-27 04:35:52,020 argparser.py:24] checkpoint_dir: None
Training: 2023-09-27 04:35:52,020 - checkpoint_dir: None
INFO 2023-09-27 04:35:52,020 argparser.py:24] max_num_last_checkpoint: 1
Training: 2023-09-27 04:35:52,020 - max_num_last_checkpoint: 1
INFO 2023-09-27 04:35:52,020 argparser.py:25] ------------------------

Training: 2023-09-27 04:35:52,020 - ------------------------

INFO 2023-09-27 04:35:52,325 common_dataset.py:55] read label file finished, total num: 5822653
Training: 2023-09-27 04:35:52,325 - read label file finished, total num: 5822653
INFO 2023-09-27 04:35:52,326 train.py:84] world_size: 2
Training: 2023-09-27 04:35:52,326 - world_size: 2
INFO 2023-09-27 04:35:52,326 train.py:85] total_batch_size: 256
Training: 2023-09-27 04:35:52,326 - total_batch_size: 256
INFO 2023-09-27 04:35:52,326 train.py:86] warmup_steps: 0
Training: 2023-09-27 04:35:52,326 - warmup_steps: 0
INFO 2023-09-27 04:35:52,326 train.py:87] steps_per_epoch: 22744
Training: 2023-09-27 04:35:52,326 - steps_per_epoch: 22744
INFO 2023-09-27 04:35:52,326 train.py:88] total_steps: 568600
Training: 2023-09-27 04:35:52,326 - total_steps: 568600
INFO 2023-09-27 04:35:52,326 train.py:89] total_epoch: 25
Training: 2023-09-27 04:35:52,326 - total_epoch: 25
INFO 2023-09-27 04:35:52,326 train.py:90] decay_steps: [227440, 363904, 500368]
Training: 2023-09-27 04:35:52,326 - decay_steps: [227440, 363904, 500368]
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'input' in conv2d only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'Out' in guassian_random only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'input' in batch_norm only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'x' in prelu only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'x' in elementwise_add only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'y' in elementwise_add only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'input' in fc only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'Out' in xavier_init only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'dtype' in create_parameter only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'tensor' in all_gather only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'input' in split only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'input[0]' in concat only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'input[1]' in concat only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'x' in gather only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'X' in norm only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'x' in matmul only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'y' in matmul only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'logits' in margin_cross_entropy only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/framework.py:1104: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.
  elif dtype == np.bool:
Traceback (most recent call last):
  File "/paddle/insightface/recognition/arcface_paddle/tools/train.py", line 35, in <module>
    train(args)
  File "/paddle/insightface/recognition/arcface_paddle/static/train.py", line 107, in train
    train_model = StaticModel(
  File "/paddle/insightface/recognition/arcface_paddle/static/static_model.py", line 132, in __init__
    dist_optimizer.minimize(self.classifier.output_dict[
  File "/usr/local/lib/python3.10/dist-packages/paddle/distributed/fleet/base/fleet_base.py", line 1494, in minimize
    return self._minimize_impl(loss, startup_program, parameter_list,
  File "/usr/local/lib/python3.10/dist-packages/paddle/distributed/fleet/base/fleet_base.py", line 1633, in _minimize_impl
    optimize_ops, params_grads = meta_optimizer.minimize(
  File "/usr/local/lib/python3.10/dist-packages/paddle/distributed/fleet/meta_optimizers/meta_optimizer_base.py", line 94, in minimize
    optimize_ops, params_grads = self.minimize_impl(
  File "/usr/local/lib/python3.10/dist-packages/paddle/distributed/fleet/meta_optimizers/raw_program_optimizer.py", line 126, in minimize_impl
    optimize_ops, params_grads = self.inner_opt.minimize(
  File "/usr/local/lib/python3.10/dist-packages/paddle/fluid/contrib/mixed_precision/decorator.py", line 523, in minimize
    optimize_ops = self.apply_optimize(loss, startup_program,
  File "/usr/local/lib/python3.10/dist-packages/paddle/fluid/contrib/mixed_precision/decorator.py", line 487, in apply_optimize
    optimize_ops = self.apply_gradients(params_grads)
  File "/usr/local/lib/python3.10/dist-packages/paddle/fluid/contrib/mixed_precision/decorator.py", line 336, in apply_gradients
    found_inf = self._check_finite_and_unscale(params_grads)
  File "/usr/local/lib/python3.10/dist-packages/paddle/fluid/contrib/mixed_precision/decorator.py", line 393, in _check_finite_and_unscale
    _, fp32_found_inf = check_finite_and_unscale(
  File "/usr/local/lib/python3.10/dist-packages/paddle/fluid/contrib/mixed_precision/amp_nn.py", line 45, in check_finite_and_unscale
    found_inf = helper.create_variable_for_type_inference(dtype='bool')
  File "/usr/local/lib/python3.10/dist-packages/paddle/fluid/layer_helper_base.py", line 406, in create_variable_for_type_inference
    return self.main_program.current_block().create_var(
  File "/usr/local/lib/python3.10/dist-packages/paddle/fluid/framework.py", line 3428, in create_var
    var = Variable(block=self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/paddle/fluid/framework.py", line 1275, in __init__
    dtype = convert_np_dtype_to_dtype_(dtype)
  File "/usr/local/lib/python3.10/dist-packages/paddle/fluid/framework.py", line 1104, in convert_np_dtype_to_dtype_
    elif dtype == np.bool:
  File "/usr/local/lib/python3.10/dist-packages/numpy/__init__.py", line 305, in __getattr__
    raise AttributeError(__former_attrs__[attr])
AttributeError: module 'numpy' has no attribute 'bool'.
`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:
    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations. Did you mean: 'bool_'?
grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
/usr/local/lib/python3.10/dist-packages/setuptools-50.3.2-py3.10.egg/setuptools/__init__.py:10: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives
/usr/lib/python3.10/distutils/command/install.py:13: DeprecationWarning: The distutils.sysconfig module is deprecated, use sysconfig instead
  from distutils.sysconfig import get_config_vars
INFO 2023-09-27 04:36:33,288 logging.py:57] rank: 0
Training: 2023-09-27 04:36:33,288 - rank: 0
INFO 2023-09-27 04:36:33,288 argparser.py:22] --------args----------
Training: 2023-09-27 04:36:33,288 - --------args----------
INFO 2023-09-27 04:36:33,288 argparser.py:24] config_file: configs/ms1mv3_r50.py
Training: 2023-09-27 04:36:33,288 - config_file: configs/ms1mv3_r50.py
INFO 2023-09-27 04:36:33,288 argparser.py:24] is_static: True
Training: 2023-09-27 04:36:33,288 - is_static: True
INFO 2023-09-27 04:36:33,288 argparser.py:24] backbone: FresResNet50
Training: 2023-09-27 04:36:33,288 - backbone: FresResNet50
INFO 2023-09-27 04:36:33,288 argparser.py:24] classifier: LargeScaleClassifier
Training: 2023-09-27 04:36:33,288 - classifier: LargeScaleClassifier
INFO 2023-09-27 04:36:33,288 argparser.py:24] embedding_size: 512
Training: 2023-09-27 04:36:33,288 - embedding_size: 512
INFO 2023-09-27 04:36:33,288 argparser.py:24] model_parallel: True
Training: 2023-09-27 04:36:33,288 - model_parallel: True
INFO 2023-09-27 04:36:33,288 argparser.py:24] sample_ratio: 0.1
Training: 2023-09-27 04:36:33,288 - sample_ratio: 0.1
INFO 2023-09-27 04:36:33,288 argparser.py:24] loss: ArcFace
Training: 2023-09-27 04:36:33,288 - loss: ArcFace
INFO 2023-09-27 04:36:33,288 argparser.py:24] dropout: 0.0
Training: 2023-09-27 04:36:33,288 - dropout: 0.0
INFO 2023-09-27 04:36:33,288 argparser.py:24] fp16: True
Training: 2023-09-27 04:36:33,288 - fp16: True
INFO 2023-09-27 04:36:33,288 argparser.py:24] init_loss_scaling: 27648.0
Training: 2023-09-27 04:36:33,288 - init_loss_scaling: 27648.0
INFO 2023-09-27 04:36:33,288 argparser.py:24] max_loss_scaling: 128.0
Training: 2023-09-27 04:36:33,288 - max_loss_scaling: 128.0
INFO 2023-09-27 04:36:33,288 argparser.py:24] incr_every_n_steps: 2000
Training: 2023-09-27 04:36:33,288 - incr_every_n_steps: 2000
INFO 2023-09-27 04:36:33,288 argparser.py:24] decr_every_n_nan_or_inf: 1
Training: 2023-09-27 04:36:33,288 - decr_every_n_nan_or_inf: 1
INFO 2023-09-27 04:36:33,289 argparser.py:24] incr_ratio: 2.0
Training: 2023-09-27 04:36:33,289 - incr_ratio: 2.0
INFO 2023-09-27 04:36:33,289 argparser.py:24] decr_ratio: 0.5
Training: 2023-09-27 04:36:33,289 - decr_ratio: 0.5
INFO 2023-09-27 04:36:33,289 argparser.py:24] use_dynamic_loss_scaling: True
Training: 2023-09-27 04:36:33,289 - use_dynamic_loss_scaling: True
INFO 2023-09-27 04:36:33,289 argparser.py:24] custom_white_list: []
Training: 2023-09-27 04:36:33,289 - custom_white_list: []
INFO 2023-09-27 04:36:33,289 argparser.py:24] custom_black_list: []
Training: 2023-09-27 04:36:33,289 - custom_black_list: []
INFO 2023-09-27 04:36:33,289 argparser.py:24] lr: 0.1
Training: 2023-09-27 04:36:33,289 - lr: 0.1
INFO 2023-09-27 04:36:33,289 argparser.py:24] lr_decay: 0.1
Training: 2023-09-27 04:36:33,289 - lr_decay: 0.1
INFO 2023-09-27 04:36:33,289 argparser.py:24] weight_decay: 0.0005
Training: 2023-09-27 04:36:33,289 - weight_decay: 0.0005
INFO 2023-09-27 04:36:33,289 argparser.py:24] momentum: 0.9
Training: 2023-09-27 04:36:33,289 - momentum: 0.9
INFO 2023-09-27 04:36:33,289 argparser.py:24] train_unit: epoch
Training: 2023-09-27 04:36:33,289 - train_unit: epoch
INFO 2023-09-27 04:36:33,289 argparser.py:24] warmup_num: 0
Training: 2023-09-27 04:36:33,289 - warmup_num: 0
INFO 2023-09-27 04:36:33,289 argparser.py:24] train_num: 25
Training: 2023-09-27 04:36:33,289 - train_num: 25
INFO 2023-09-27 04:36:33,289 argparser.py:24] decay_boundaries: [10, 16, 22]
Training: 2023-09-27 04:36:33,289 - decay_boundaries: [10, 16, 22]
INFO 2023-09-27 04:36:33,289 argparser.py:24] use_synthetic_dataset: False
Training: 2023-09-27 04:36:33,289 - use_synthetic_dataset: False
INFO 2023-09-27 04:36:33,289 argparser.py:24] dataset: MS1M_v2
Training: 2023-09-27 04:36:33,289 - dataset: MS1M_v2
INFO 2023-09-27 04:36:33,289 argparser.py:24] data_dir: /paddle/data_out/
Training: 2023-09-27 04:36:33,289 - data_dir: /paddle/data_out/
INFO 2023-09-27 04:36:33,289 argparser.py:24] label_file: /paddle/data_out/label.txt
Training: 2023-09-27 04:36:33,289 - label_file: /paddle/data_out/label.txt
INFO 2023-09-27 04:36:33,289 argparser.py:24] is_bin: False
Training: 2023-09-27 04:36:33,289 - is_bin: False
INFO 2023-09-27 04:36:33,289 argparser.py:24] num_classes: 85741
Training: 2023-09-27 04:36:33,289 - num_classes: 85741
INFO 2023-09-27 04:36:33,289 argparser.py:24] batch_size: 128
Training: 2023-09-27 04:36:33,289 - batch_size: 128
INFO 2023-09-27 04:36:33,289 argparser.py:24] num_workers: 2
Training: 2023-09-27 04:36:33,289 - num_workers: 2
INFO 2023-09-27 04:36:33,289 argparser.py:24] do_validation_while_train: True
Training: 2023-09-27 04:36:33,289 - do_validation_while_train: True
INFO 2023-09-27 04:36:33,289 argparser.py:24] validation_interval_step: 2000
Training: 2023-09-27 04:36:33,289 - validation_interval_step: 2000
INFO 2023-09-27 04:36:33,289 argparser.py:24] val_targets: ['lfw', 'cfp_fp', 'agedb_30']
Training: 2023-09-27 04:36:33,289 - val_targets: ['lfw', 'cfp_fp', 'agedb_30']
INFO 2023-09-27 04:36:33,289 argparser.py:24] logdir: ./log
Training: 2023-09-27 04:36:33,289 - logdir: ./log
INFO 2023-09-27 04:36:33,289 argparser.py:24] log_interval_step: 100
Training: 2023-09-27 04:36:33,289 - log_interval_step: 100
INFO 2023-09-27 04:36:33,289 argparser.py:24] output: MS1M_v3_arcface_static_0.1
Training: 2023-09-27 04:36:33,289 - output: MS1M_v3_arcface_static_0.1
INFO 2023-09-27 04:36:33,289 argparser.py:24] resume: False
Training: 2023-09-27 04:36:33,289 - resume: False
INFO 2023-09-27 04:36:33,289 argparser.py:24] checkpoint_dir: None
Training: 2023-09-27 04:36:33,289 - checkpoint_dir: None
INFO 2023-09-27 04:36:33,289 argparser.py:24] max_num_last_checkpoint: 1
Training: 2023-09-27 04:36:33,289 - max_num_last_checkpoint: 1
INFO 2023-09-27 04:36:33,289 argparser.py:25] ------------------------

Training: 2023-09-27 04:36:33,289 - ------------------------

INFO 2023-09-27 04:36:33,580 common_dataset.py:55] read label file finished, total num: 5822653
Training: 2023-09-27 04:36:33,580 - read label file finished, total num: 5822653
INFO 2023-09-27 04:36:33,580 train.py:84] world_size: 2
Training: 2023-09-27 04:36:33,580 - world_size: 2
INFO 2023-09-27 04:36:33,581 train.py:85] total_batch_size: 256
Training: 2023-09-27 04:36:33,581 - total_batch_size: 256
INFO 2023-09-27 04:36:33,581 train.py:86] warmup_steps: 0
Training: 2023-09-27 04:36:33,581 - warmup_steps: 0
INFO 2023-09-27 04:36:33,581 train.py:87] steps_per_epoch: 22744
Training: 2023-09-27 04:36:33,581 - steps_per_epoch: 22744
INFO 2023-09-27 04:36:33,581 train.py:88] total_steps: 568600
Training: 2023-09-27 04:36:33,581 - total_steps: 568600
INFO 2023-09-27 04:36:33,581 train.py:89] total_epoch: 25
Training: 2023-09-27 04:36:33,581 - total_epoch: 25
INFO 2023-09-27 04:36:33,581 train.py:90] decay_steps: [227440, 363904, 500368]
Training: 2023-09-27 04:36:33,581 - decay_steps: [227440, 363904, 500368]
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'input' in conv2d only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'Out' in guassian_random only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'input' in batch_norm only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'x' in prelu only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'x' in elementwise_add only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'y' in elementwise_add only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'input' in fc only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'Out' in xavier_init only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'dtype' in create_parameter only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'tensor' in all_gather only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'input' in split only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'input[0]' in concat only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'input[1]' in concat only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'x' in gather only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'X' in norm only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'x' in matmul only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'y' in matmul only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'logits' in margin_cross_entropy only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/framework.py:1104: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  elif dtype == np.bool:
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'x' in check_finite_and_unscale only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'x' in update_loss_scaling only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/optimizer/momentum.py:270: UserWarning: Accumulating with FP16 in optimizer can lead to poor accuracy or slow convergence.Consider using multi_precision=True option of the Momentum optimizer.
  warnings.warn(
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:42839']
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
(14000, 3, 112, 112)
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
/usr/local/lib/python3.10/dist-packages/paddle/fluid/executor.py:400: UserWarning: do not use standalone executor in fleet by default
  warnings.warn("do not use standalone executor in fleet by default")
W0927 04:36:50.159798  7363 gpu_context.cc:278] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 12.0, Runtime API Version: 10.1
W0927 04:36:50.160488  7363 gpu_context.cc:306] device: 0, cuDNN Version: 8.9.
I0927 04:36:51.893617  7363 gen_comm_id_helper.cc:205] Server listening on: 127.0.0.1:44389 successful.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::framework::Executor::Run(paddle::framework::ProgramDesc const&, paddle::framework::Scope*, int, bool, bool, std::vector<std::string, std::allocator<std::string > > const&, bool, bool)
1   paddle::framework::Executor::RunPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, bool, bool, bool)
2   paddle::framework::Executor::RunPartialPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, long, long, bool, bool, bool)
3   paddle::framework::OperatorBase::Run(paddle::framework::Scope const&, phi::Place const&)
4   paddle::operators::CCommInitOp::RunImpl(paddle::framework::Scope const&, phi::Place const&) const
5   paddle::platform::NCCLCommContext::CreateComm(ncclUniqueId*, int, int, int, int)
6   pncclCommInitRank

----------------------
Error Message Summary:
----------------------
FatalError: `Access to an undefined portion of a memory object` is detected by the operating system.
  [TimeInfo: *** Aborted at 1695789412 (unix time) try "date -d @1695789412" if you are using GNU date ***]
  [SignalInfo: *** SIGBUS (@0x7f708cd9afe0) received by PID 7363 (TID 0x7f7797d17740) from PID 18446744071777660896 ***]

grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
/usr/local/lib/python3.10/dist-packages/setuptools-50.3.2-py3.10.egg/setuptools/__init__.py:10: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives
/usr/lib/python3.10/distutils/command/install.py:13: DeprecationWarning: The distutils.sysconfig module is deprecated, use sysconfig instead
  from distutils.sysconfig import get_config_vars
INFO 2023-09-27 04:39:13,554 logging.py:57] rank: 0
Training: 2023-09-27 04:39:13,554 - rank: 0
INFO 2023-09-27 04:39:13,554 argparser.py:22] --------args----------
Training: 2023-09-27 04:39:13,554 - --------args----------
INFO 2023-09-27 04:39:13,554 argparser.py:24] config_file: configs/ms1mv3_r50.py
Training: 2023-09-27 04:39:13,554 - config_file: configs/ms1mv3_r50.py
INFO 2023-09-27 04:39:13,554 argparser.py:24] is_static: True
Training: 2023-09-27 04:39:13,554 - is_static: True
INFO 2023-09-27 04:39:13,554 argparser.py:24] backbone: FresResNet50
Training: 2023-09-27 04:39:13,554 - backbone: FresResNet50
INFO 2023-09-27 04:39:13,554 argparser.py:24] classifier: LargeScaleClassifier
Training: 2023-09-27 04:39:13,554 - classifier: LargeScaleClassifier
INFO 2023-09-27 04:39:13,554 argparser.py:24] embedding_size: 512
Training: 2023-09-27 04:39:13,554 - embedding_size: 512
INFO 2023-09-27 04:39:13,554 argparser.py:24] model_parallel: True
Training: 2023-09-27 04:39:13,554 - model_parallel: True
INFO 2023-09-27 04:39:13,554 argparser.py:24] sample_ratio: 0.1
Training: 2023-09-27 04:39:13,554 - sample_ratio: 0.1
INFO 2023-09-27 04:39:13,554 argparser.py:24] loss: ArcFace
Training: 2023-09-27 04:39:13,554 - loss: ArcFace
INFO 2023-09-27 04:39:13,554 argparser.py:24] dropout: 0.0
Training: 2023-09-27 04:39:13,554 - dropout: 0.0
INFO 2023-09-27 04:39:13,554 argparser.py:24] fp16: True
Training: 2023-09-27 04:39:13,554 - fp16: True
INFO 2023-09-27 04:39:13,554 argparser.py:24] init_loss_scaling: 27648.0
Training: 2023-09-27 04:39:13,554 - init_loss_scaling: 27648.0
INFO 2023-09-27 04:39:13,554 argparser.py:24] max_loss_scaling: 128.0
Training: 2023-09-27 04:39:13,554 - max_loss_scaling: 128.0
INFO 2023-09-27 04:39:13,554 argparser.py:24] incr_every_n_steps: 2000
Training: 2023-09-27 04:39:13,554 - incr_every_n_steps: 2000
INFO 2023-09-27 04:39:13,554 argparser.py:24] decr_every_n_nan_or_inf: 1
Training: 2023-09-27 04:39:13,554 - decr_every_n_nan_or_inf: 1
INFO 2023-09-27 04:39:13,554 argparser.py:24] incr_ratio: 2.0
Training: 2023-09-27 04:39:13,554 - incr_ratio: 2.0
INFO 2023-09-27 04:39:13,554 argparser.py:24] decr_ratio: 0.5
Training: 2023-09-27 04:39:13,554 - decr_ratio: 0.5
INFO 2023-09-27 04:39:13,554 argparser.py:24] use_dynamic_loss_scaling: True
Training: 2023-09-27 04:39:13,554 - use_dynamic_loss_scaling: True
INFO 2023-09-27 04:39:13,554 argparser.py:24] custom_white_list: []
Training: 2023-09-27 04:39:13,554 - custom_white_list: []
INFO 2023-09-27 04:39:13,554 argparser.py:24] custom_black_list: []
Training: 2023-09-27 04:39:13,554 - custom_black_list: []
INFO 2023-09-27 04:39:13,554 argparser.py:24] lr: 0.1
Training: 2023-09-27 04:39:13,554 - lr: 0.1
INFO 2023-09-27 04:39:13,554 argparser.py:24] lr_decay: 0.1
Training: 2023-09-27 04:39:13,554 - lr_decay: 0.1
INFO 2023-09-27 04:39:13,554 argparser.py:24] weight_decay: 0.0005
Training: 2023-09-27 04:39:13,554 - weight_decay: 0.0005
INFO 2023-09-27 04:39:13,554 argparser.py:24] momentum: 0.9
Training: 2023-09-27 04:39:13,554 - momentum: 0.9
INFO 2023-09-27 04:39:13,554 argparser.py:24] train_unit: epoch
Training: 2023-09-27 04:39:13,554 - train_unit: epoch
INFO 2023-09-27 04:39:13,554 argparser.py:24] warmup_num: 0
Training: 2023-09-27 04:39:13,554 - warmup_num: 0
INFO 2023-09-27 04:39:13,555 argparser.py:24] train_num: 25
Training: 2023-09-27 04:39:13,555 - train_num: 25
INFO 2023-09-27 04:39:13,555 argparser.py:24] decay_boundaries: [10, 16, 22]
Training: 2023-09-27 04:39:13,555 - decay_boundaries: [10, 16, 22]
INFO 2023-09-27 04:39:13,555 argparser.py:24] use_synthetic_dataset: False
Training: 2023-09-27 04:39:13,555 - use_synthetic_dataset: False
INFO 2023-09-27 04:39:13,555 argparser.py:24] dataset: MS1M_v2
Training: 2023-09-27 04:39:13,555 - dataset: MS1M_v2
INFO 2023-09-27 04:39:13,555 argparser.py:24] data_dir: /paddle/data_out/
Training: 2023-09-27 04:39:13,555 - data_dir: /paddle/data_out/
INFO 2023-09-27 04:39:13,555 argparser.py:24] label_file: /paddle/data_out/label.txt
Training: 2023-09-27 04:39:13,555 - label_file: /paddle/data_out/label.txt
INFO 2023-09-27 04:39:13,555 argparser.py:24] is_bin: False
Training: 2023-09-27 04:39:13,555 - is_bin: False
INFO 2023-09-27 04:39:13,555 argparser.py:24] num_classes: 85741
Training: 2023-09-27 04:39:13,555 - num_classes: 85741
INFO 2023-09-27 04:39:13,555 argparser.py:24] batch_size: 64
Training: 2023-09-27 04:39:13,555 - batch_size: 64
INFO 2023-09-27 04:39:13,555 argparser.py:24] num_workers: 2
Training: 2023-09-27 04:39:13,555 - num_workers: 2
INFO 2023-09-27 04:39:13,555 argparser.py:24] do_validation_while_train: True
Training: 2023-09-27 04:39:13,555 - do_validation_while_train: True
INFO 2023-09-27 04:39:13,555 argparser.py:24] validation_interval_step: 2000
Training: 2023-09-27 04:39:13,555 - validation_interval_step: 2000
INFO 2023-09-27 04:39:13,555 argparser.py:24] val_targets: ['lfw', 'cfp_fp', 'agedb_30']
Training: 2023-09-27 04:39:13,555 - val_targets: ['lfw', 'cfp_fp', 'agedb_30']
INFO 2023-09-27 04:39:13,555 argparser.py:24] logdir: ./log
Training: 2023-09-27 04:39:13,555 - logdir: ./log
INFO 2023-09-27 04:39:13,555 argparser.py:24] log_interval_step: 100
Training: 2023-09-27 04:39:13,555 - log_interval_step: 100
INFO 2023-09-27 04:39:13,555 argparser.py:24] output: MS1M_v3_arcface_static_0.1
Training: 2023-09-27 04:39:13,555 - output: MS1M_v3_arcface_static_0.1
INFO 2023-09-27 04:39:13,555 argparser.py:24] resume: False
Training: 2023-09-27 04:39:13,555 - resume: False
INFO 2023-09-27 04:39:13,555 argparser.py:24] checkpoint_dir: None
Training: 2023-09-27 04:39:13,555 - checkpoint_dir: None
INFO 2023-09-27 04:39:13,555 argparser.py:24] max_num_last_checkpoint: 1
Training: 2023-09-27 04:39:13,555 - max_num_last_checkpoint: 1
INFO 2023-09-27 04:39:13,555 argparser.py:25] ------------------------

Training: 2023-09-27 04:39:13,555 - ------------------------

INFO 2023-09-27 04:39:13,850 common_dataset.py:55] read label file finished, total num: 5822653
Training: 2023-09-27 04:39:13,850 - read label file finished, total num: 5822653
INFO 2023-09-27 04:39:13,850 train.py:84] world_size: 2
Training: 2023-09-27 04:39:13,850 - world_size: 2
INFO 2023-09-27 04:39:13,850 train.py:85] total_batch_size: 128
Training: 2023-09-27 04:39:13,850 - total_batch_size: 128
INFO 2023-09-27 04:39:13,850 train.py:86] warmup_steps: 0
Training: 2023-09-27 04:39:13,850 - warmup_steps: 0
INFO 2023-09-27 04:39:13,850 train.py:87] steps_per_epoch: 45489
Training: 2023-09-27 04:39:13,850 - steps_per_epoch: 45489
INFO 2023-09-27 04:39:13,850 train.py:88] total_steps: 1137225
Training: 2023-09-27 04:39:13,850 - total_steps: 1137225
INFO 2023-09-27 04:39:13,850 train.py:89] total_epoch: 25
Training: 2023-09-27 04:39:13,850 - total_epoch: 25
INFO 2023-09-27 04:39:13,850 train.py:90] decay_steps: [454890, 727824, 1000758]
Training: 2023-09-27 04:39:13,850 - decay_steps: [454890, 727824, 1000758]
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'input' in conv2d only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'Out' in guassian_random only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'input' in batch_norm only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'x' in prelu only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'x' in elementwise_add only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'y' in elementwise_add only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'input' in fc only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'Out' in xavier_init only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'dtype' in create_parameter only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'tensor' in all_gather only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'input' in split only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'input[0]' in concat only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'input[1]' in concat only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'x' in gather only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'X' in norm only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'x' in matmul only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'y' in matmul only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'logits' in margin_cross_entropy only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/framework.py:1104: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  elif dtype == np.bool:
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'x' in check_finite_and_unscale only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'x' in update_loss_scaling only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/optimizer/momentum.py:270: UserWarning: Accumulating with FP16 in optimizer can lead to poor accuracy or slow convergence.Consider using multi_precision=True option of the Momentum optimizer.
  warnings.warn(
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:44493']
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
(14000, 3, 112, 112)
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
/usr/local/lib/python3.10/dist-packages/paddle/fluid/executor.py:400: UserWarning: do not use standalone executor in fleet by default
  warnings.warn("do not use standalone executor in fleet by default")
W0927 04:39:30.398232  7482 gpu_context.cc:278] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 12.0, Runtime API Version: 10.1
W0927 04:39:30.398910  7482 gpu_context.cc:306] device: 0, cuDNN Version: 8.9.
I0927 04:39:32.137579  7482 gen_comm_id_helper.cc:205] Server listening on: 127.0.0.1:53001 successful.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::framework::Executor::Run(paddle::framework::ProgramDesc const&, paddle::framework::Scope*, int, bool, bool, std::vector<std::string, std::allocator<std::string > > const&, bool, bool)
1   paddle::framework::Executor::RunPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, bool, bool, bool)
2   paddle::framework::Executor::RunPartialPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, long, long, bool, bool, bool)
3   paddle::framework::OperatorBase::Run(paddle::framework::Scope const&, phi::Place const&)
4   paddle::operators::CCommInitOp::RunImpl(paddle::framework::Scope const&, phi::Place const&) const
5   paddle::platform::NCCLCommContext::CreateComm(ncclUniqueId*, int, int, int, int)
6   pncclCommInitRank

----------------------
Error Message Summary:
----------------------
FatalError: `Access to an undefined portion of a memory object` is detected by the operating system.
  [TimeInfo: *** Aborted at 1695789572 (unix time) try "date -d @1695789572" if you are using GNU date ***]
  [SignalInfo: *** SIGBUS (@0x7fc0811d0000) received by PID 7482 (TID 0x7fc7887ff740) from PID 18446744071580745728 ***]

grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
/usr/local/lib/python3.10/dist-packages/setuptools-50.3.2-py3.10.egg/setuptools/__init__.py:10: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives
/usr/lib/python3.10/distutils/command/install.py:13: DeprecationWarning: The distutils.sysconfig module is deprecated, use sysconfig instead
  from distutils.sysconfig import get_config_vars
INFO 2023-09-27 04:40:15,383 logging.py:57] rank: 0
Training: 2023-09-27 04:40:15,383 - rank: 0
INFO 2023-09-27 04:40:15,384 argparser.py:22] --------args----------
Training: 2023-09-27 04:40:15,384 - --------args----------
INFO 2023-09-27 04:40:15,384 argparser.py:24] config_file: configs/ms1mv3_r50.py
Training: 2023-09-27 04:40:15,384 - config_file: configs/ms1mv3_r50.py
INFO 2023-09-27 04:40:15,384 argparser.py:24] is_static: True
Training: 2023-09-27 04:40:15,384 - is_static: True
INFO 2023-09-27 04:40:15,384 argparser.py:24] backbone: FresResNet50
Training: 2023-09-27 04:40:15,384 - backbone: FresResNet50
INFO 2023-09-27 04:40:15,384 argparser.py:24] classifier: LargeScaleClassifier
Training: 2023-09-27 04:40:15,384 - classifier: LargeScaleClassifier
INFO 2023-09-27 04:40:15,384 argparser.py:24] embedding_size: 512
Training: 2023-09-27 04:40:15,384 - embedding_size: 512
INFO 2023-09-27 04:40:15,384 argparser.py:24] model_parallel: True
Training: 2023-09-27 04:40:15,384 - model_parallel: True
INFO 2023-09-27 04:40:15,384 argparser.py:24] sample_ratio: 0.1
Training: 2023-09-27 04:40:15,384 - sample_ratio: 0.1
INFO 2023-09-27 04:40:15,384 argparser.py:24] loss: ArcFace
Training: 2023-09-27 04:40:15,384 - loss: ArcFace
INFO 2023-09-27 04:40:15,384 argparser.py:24] dropout: 0.0
Training: 2023-09-27 04:40:15,384 - dropout: 0.0
INFO 2023-09-27 04:40:15,384 argparser.py:24] fp16: True
Training: 2023-09-27 04:40:15,384 - fp16: True
INFO 2023-09-27 04:40:15,384 argparser.py:24] init_loss_scaling: 27648.0
Training: 2023-09-27 04:40:15,384 - init_loss_scaling: 27648.0
INFO 2023-09-27 04:40:15,384 argparser.py:24] max_loss_scaling: 128.0
Training: 2023-09-27 04:40:15,384 - max_loss_scaling: 128.0
INFO 2023-09-27 04:40:15,384 argparser.py:24] incr_every_n_steps: 2000
Training: 2023-09-27 04:40:15,384 - incr_every_n_steps: 2000
INFO 2023-09-27 04:40:15,384 argparser.py:24] decr_every_n_nan_or_inf: 1
Training: 2023-09-27 04:40:15,384 - decr_every_n_nan_or_inf: 1
INFO 2023-09-27 04:40:15,384 argparser.py:24] incr_ratio: 2.0
Training: 2023-09-27 04:40:15,384 - incr_ratio: 2.0
INFO 2023-09-27 04:40:15,384 argparser.py:24] decr_ratio: 0.5
Training: 2023-09-27 04:40:15,384 - decr_ratio: 0.5
INFO 2023-09-27 04:40:15,384 argparser.py:24] use_dynamic_loss_scaling: True
Training: 2023-09-27 04:40:15,384 - use_dynamic_loss_scaling: True
INFO 2023-09-27 04:40:15,384 argparser.py:24] custom_white_list: []
Training: 2023-09-27 04:40:15,384 - custom_white_list: []
INFO 2023-09-27 04:40:15,384 argparser.py:24] custom_black_list: []
Training: 2023-09-27 04:40:15,384 - custom_black_list: []
INFO 2023-09-27 04:40:15,384 argparser.py:24] lr: 0.1
Training: 2023-09-27 04:40:15,384 - lr: 0.1
INFO 2023-09-27 04:40:15,384 argparser.py:24] lr_decay: 0.1
Training: 2023-09-27 04:40:15,384 - lr_decay: 0.1
INFO 2023-09-27 04:40:15,384 argparser.py:24] weight_decay: 0.0005
Training: 2023-09-27 04:40:15,384 - weight_decay: 0.0005
INFO 2023-09-27 04:40:15,384 argparser.py:24] momentum: 0.9
Training: 2023-09-27 04:40:15,384 - momentum: 0.9
INFO 2023-09-27 04:40:15,384 argparser.py:24] train_unit: epoch
Training: 2023-09-27 04:40:15,384 - train_unit: epoch
INFO 2023-09-27 04:40:15,384 argparser.py:24] warmup_num: 0
Training: 2023-09-27 04:40:15,384 - warmup_num: 0
INFO 2023-09-27 04:40:15,384 argparser.py:24] train_num: 25
Training: 2023-09-27 04:40:15,384 - train_num: 25
INFO 2023-09-27 04:40:15,384 argparser.py:24] decay_boundaries: [10, 16, 22]
Training: 2023-09-27 04:40:15,384 - decay_boundaries: [10, 16, 22]
INFO 2023-09-27 04:40:15,384 argparser.py:24] use_synthetic_dataset: False
Training: 2023-09-27 04:40:15,384 - use_synthetic_dataset: False
INFO 2023-09-27 04:40:15,384 argparser.py:24] dataset: MS1M_v2
Training: 2023-09-27 04:40:15,384 - dataset: MS1M_v2
INFO 2023-09-27 04:40:15,385 argparser.py:24] data_dir: /paddle/data_out/
Training: 2023-09-27 04:40:15,385 - data_dir: /paddle/data_out/
INFO 2023-09-27 04:40:15,385 argparser.py:24] label_file: /paddle/data_out/label.txt
Training: 2023-09-27 04:40:15,385 - label_file: /paddle/data_out/label.txt
INFO 2023-09-27 04:40:15,385 argparser.py:24] is_bin: False
Training: 2023-09-27 04:40:15,385 - is_bin: False
INFO 2023-09-27 04:40:15,385 argparser.py:24] num_classes: 85741
Training: 2023-09-27 04:40:15,385 - num_classes: 85741
INFO 2023-09-27 04:40:15,385 argparser.py:24] batch_size: 2
Training: 2023-09-27 04:40:15,385 - batch_size: 2
INFO 2023-09-27 04:40:15,385 argparser.py:24] num_workers: 2
Training: 2023-09-27 04:40:15,385 - num_workers: 2
INFO 2023-09-27 04:40:15,385 argparser.py:24] do_validation_while_train: True
Training: 2023-09-27 04:40:15,385 - do_validation_while_train: True
INFO 2023-09-27 04:40:15,385 argparser.py:24] validation_interval_step: 2000
Training: 2023-09-27 04:40:15,385 - validation_interval_step: 2000
INFO 2023-09-27 04:40:15,385 argparser.py:24] val_targets: ['lfw', 'cfp_fp', 'agedb_30']
Training: 2023-09-27 04:40:15,385 - val_targets: ['lfw', 'cfp_fp', 'agedb_30']
INFO 2023-09-27 04:40:15,385 argparser.py:24] logdir: ./log
Training: 2023-09-27 04:40:15,385 - logdir: ./log
INFO 2023-09-27 04:40:15,385 argparser.py:24] log_interval_step: 100
Training: 2023-09-27 04:40:15,385 - log_interval_step: 100
INFO 2023-09-27 04:40:15,385 argparser.py:24] output: MS1M_v3_arcface_static_0.1
Training: 2023-09-27 04:40:15,385 - output: MS1M_v3_arcface_static_0.1
INFO 2023-09-27 04:40:15,385 argparser.py:24] resume: False
Training: 2023-09-27 04:40:15,385 - resume: False
INFO 2023-09-27 04:40:15,385 argparser.py:24] checkpoint_dir: None
Training: 2023-09-27 04:40:15,385 - checkpoint_dir: None
INFO 2023-09-27 04:40:15,385 argparser.py:24] max_num_last_checkpoint: 1
Training: 2023-09-27 04:40:15,385 - max_num_last_checkpoint: 1
INFO 2023-09-27 04:40:15,385 argparser.py:25] ------------------------

Training: 2023-09-27 04:40:15,385 - ------------------------

INFO 2023-09-27 04:40:15,684 common_dataset.py:55] read label file finished, total num: 5822653
Training: 2023-09-27 04:40:15,684 - read label file finished, total num: 5822653
INFO 2023-09-27 04:40:15,685 train.py:84] world_size: 2
Training: 2023-09-27 04:40:15,685 - world_size: 2
INFO 2023-09-27 04:40:15,685 train.py:85] total_batch_size: 4
Training: 2023-09-27 04:40:15,685 - total_batch_size: 4
INFO 2023-09-27 04:40:15,685 train.py:86] warmup_steps: 0
Training: 2023-09-27 04:40:15,685 - warmup_steps: 0
INFO 2023-09-27 04:40:15,685 train.py:87] steps_per_epoch: 1455663
Training: 2023-09-27 04:40:15,685 - steps_per_epoch: 1455663
INFO 2023-09-27 04:40:15,685 train.py:88] total_steps: 36391575
Training: 2023-09-27 04:40:15,685 - total_steps: 36391575
INFO 2023-09-27 04:40:15,685 train.py:89] total_epoch: 25
Training: 2023-09-27 04:40:15,685 - total_epoch: 25
INFO 2023-09-27 04:40:15,685 train.py:90] decay_steps: [14556630, 23290608, 32024586]
Training: 2023-09-27 04:40:15,685 - decay_steps: [14556630, 23290608, 32024586]
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'input' in conv2d only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'Out' in guassian_random only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'input' in batch_norm only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'x' in prelu only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'x' in elementwise_add only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'y' in elementwise_add only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'input' in fc only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'Out' in xavier_init only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'dtype' in create_parameter only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'tensor' in all_gather only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'input' in split only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'input[0]' in concat only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'input[1]' in concat only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'x' in gather only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'X' in norm only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'x' in matmul only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'y' in matmul only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'logits' in margin_cross_entropy only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/framework.py:1104: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  elif dtype == np.bool:
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'x' in check_finite_and_unscale only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/fluid/data_feeder.py:136: UserWarning: The data type of 'x' in update_loss_scaling only support float16 in GPU now. 
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/paddle/optimizer/momentum.py:270: UserWarning: Accumulating with FP16 in optimizer can lead to poor accuracy or slow convergence.Consider using multi_precision=True option of the Momentum optimizer.
  warnings.warn(
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:50627']
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
(14000, 3, 112, 112)
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
/usr/local/lib/python3.10/dist-packages/paddle/fluid/executor.py:400: UserWarning: do not use standalone executor in fleet by default
  warnings.warn("do not use standalone executor in fleet by default")
W0927 04:40:32.176399  7601 gpu_context.cc:278] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 12.0, Runtime API Version: 10.1
W0927 04:40:32.177079  7601 gpu_context.cc:306] device: 0, cuDNN Version: 8.9.
I0927 04:40:33.898622  7601 gen_comm_id_helper.cc:205] Server listening on: 127.0.0.1:52651 successful.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::framework::Executor::Run(paddle::framework::ProgramDesc const&, paddle::framework::Scope*, int, bool, bool, std::vector<std::string, std::allocator<std::string > > const&, bool, bool)
1   paddle::framework::Executor::RunPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, bool, bool, bool)
2   paddle::framework::Executor::RunPartialPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, long, long, bool, bool, bool)
3   paddle::framework::OperatorBase::Run(paddle::framework::Scope const&, phi::Place const&)
4   paddle::operators::CCommInitOp::RunImpl(paddle::framework::Scope const&, phi::Place const&) const
5   paddle::platform::NCCLCommContext::CreateComm(ncclUniqueId*, int, int, int, int)
6   pncclCommInitRank

----------------------
Error Message Summary:
----------------------
FatalError: `Access to an undefined portion of a memory object` is detected by the operating system.
  [TimeInfo: *** Aborted at 1695789634 (unix time) try "date -d @1695789634" if you are using GNU date ***]
  [SignalInfo: *** SIGBUS (@0x7fca4d1d0000) received by PID 7601 (TID 0x7fd15563b740) from PID 1293746176 ***]

grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
Traceback (most recent call last):
  File "/paddle/insightface/recognition/arcface_paddle/tools/train.py", line 24, in <module>
    args = parser.parse_args()
  File "/paddle/insightface/recognition/arcface_paddle/configs/argparser.py", line 72, in parse_args
    cfg = get_config(user_namespace.config_file)
  File "/paddle/insightface/recognition/arcface_paddle/configs/argparser.py", line 51, in get_config
    config = importlib.import_module("configs.config")
  File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/paddle/insightface/recognition/arcface_paddle/configs/config.py", line 15, in <module>
    from easydict import EasyDict as edict
ModuleNotFoundError: No module named 'easydict'
grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
Traceback (most recent call last):
  File "tools/train.py", line 26, in <module>
    from static.train import train
  File "/paddle/insightface/recognition/arcface_paddle/static/train.py", line 22, in <module>
    from visualdl import LogWriter
ModuleNotFoundError: No module named 'visualdl'
grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
Traceback (most recent call last):
  File "tools/train.py", line 26, in <module>
    from static.train import train
  File "/paddle/insightface/recognition/arcface_paddle/static/train.py", line 22, in <module>
    from visualdl import LogWriter
  File "/usr/local/lib/python3.7/dist-packages/visualdl/__init__.py", line 20, in <module>
    from visualdl.writer.writer import LogWriter  # noqa: F401
  File "/usr/local/lib/python3.7/dist-packages/visualdl/writer/writer.py", line 20, in <module>
    from visualdl.component.base_component import audio
  File "/usr/local/lib/python3.7/dist-packages/visualdl/component/base_component.py", line 18, in <module>
    from visualdl.proto.record_pb2 import Record
  File "/usr/local/lib/python3.7/dist-packages/visualdl/proto/record_pb2.py", line 9, in <module>
    from google.protobuf.internal import builder as _builder
ImportError: cannot import name 'builder' from 'google.protobuf.internal' (/usr/local/lib/python3.7/dist-packages/google/protobuf/internal/__init__.py)
grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
INFO:root:rank: 0
Training: 2023-09-27 05:55:03,519 - rank: 0
INFO:root:--------args----------
Training: 2023-09-27 05:55:03,519 - --------args----------
INFO:root:config_file: configs/ms1mv3_r50.py
Training: 2023-09-27 05:55:03,519 - config_file: configs/ms1mv3_r50.py
INFO:root:is_static: True
Training: 2023-09-27 05:55:03,519 - is_static: True
INFO:root:backbone: FresResNet50
Training: 2023-09-27 05:55:03,519 - backbone: FresResNet50
INFO:root:classifier: LargeScaleClassifier
Training: 2023-09-27 05:55:03,519 - classifier: LargeScaleClassifier
INFO:root:embedding_size: 512
Training: 2023-09-27 05:55:03,519 - embedding_size: 512
INFO:root:model_parallel: True
Training: 2023-09-27 05:55:03,519 - model_parallel: True
INFO:root:sample_ratio: 0.1
Training: 2023-09-27 05:55:03,519 - sample_ratio: 0.1
INFO:root:loss: ArcFace
Training: 2023-09-27 05:55:03,520 - loss: ArcFace
INFO:root:dropout: 0.0
Training: 2023-09-27 05:55:03,520 - dropout: 0.0
INFO:root:fp16: True
Training: 2023-09-27 05:55:03,520 - fp16: True
INFO:root:init_loss_scaling: 27648.0
Training: 2023-09-27 05:55:03,520 - init_loss_scaling: 27648.0
INFO:root:max_loss_scaling: 128.0
Training: 2023-09-27 05:55:03,520 - max_loss_scaling: 128.0
INFO:root:incr_every_n_steps: 2000
Training: 2023-09-27 05:55:03,520 - incr_every_n_steps: 2000
INFO:root:decr_every_n_nan_or_inf: 1
Training: 2023-09-27 05:55:03,520 - decr_every_n_nan_or_inf: 1
INFO:root:incr_ratio: 2.0
Training: 2023-09-27 05:55:03,520 - incr_ratio: 2.0
INFO:root:decr_ratio: 0.5
Training: 2023-09-27 05:55:03,520 - decr_ratio: 0.5
INFO:root:use_dynamic_loss_scaling: True
Training: 2023-09-27 05:55:03,520 - use_dynamic_loss_scaling: True
INFO:root:custom_white_list: []
Training: 2023-09-27 05:55:03,520 - custom_white_list: []
INFO:root:custom_black_list: []
Training: 2023-09-27 05:55:03,520 - custom_black_list: []
INFO:root:lr: 0.1
Training: 2023-09-27 05:55:03,520 - lr: 0.1
INFO:root:lr_decay: 0.1
Training: 2023-09-27 05:55:03,520 - lr_decay: 0.1
INFO:root:weight_decay: 0.0005
Training: 2023-09-27 05:55:03,520 - weight_decay: 0.0005
INFO:root:momentum: 0.9
Training: 2023-09-27 05:55:03,520 - momentum: 0.9
INFO:root:train_unit: epoch
Training: 2023-09-27 05:55:03,520 - train_unit: epoch
INFO:root:warmup_num: 0
Training: 2023-09-27 05:55:03,520 - warmup_num: 0
INFO:root:train_num: 25
Training: 2023-09-27 05:55:03,520 - train_num: 25
INFO:root:decay_boundaries: [10, 16, 22]
Training: 2023-09-27 05:55:03,520 - decay_boundaries: [10, 16, 22]
INFO:root:use_synthetic_dataset: False
Training: 2023-09-27 05:55:03,520 - use_synthetic_dataset: False
INFO:root:dataset: MS1M_v2
Training: 2023-09-27 05:55:03,520 - dataset: MS1M_v2
INFO:root:data_dir: /paddle/data_out/
Training: 2023-09-27 05:55:03,520 - data_dir: /paddle/data_out/
INFO:root:label_file: /paddle/data_out/label.txt
Training: 2023-09-27 05:55:03,520 - label_file: /paddle/data_out/label.txt
INFO:root:is_bin: False
Training: 2023-09-27 05:55:03,520 - is_bin: False
INFO:root:num_classes: 85741
Training: 2023-09-27 05:55:03,520 - num_classes: 85741
INFO:root:batch_size: 2
Training: 2023-09-27 05:55:03,520 - batch_size: 2
INFO:root:num_workers: 2
Training: 2023-09-27 05:55:03,520 - num_workers: 2
INFO:root:do_validation_while_train: True
Training: 2023-09-27 05:55:03,520 - do_validation_while_train: True
INFO:root:validation_interval_step: 2000
Training: 2023-09-27 05:55:03,520 - validation_interval_step: 2000
INFO:root:val_targets: ['lfw', 'cfp_fp', 'agedb_30']
Training: 2023-09-27 05:55:03,520 - val_targets: ['lfw', 'cfp_fp', 'agedb_30']
INFO:root:logdir: ./log
Training: 2023-09-27 05:55:03,521 - logdir: ./log
INFO:root:log_interval_step: 100
Training: 2023-09-27 05:55:03,521 - log_interval_step: 100
INFO:root:output: MS1M_v3_arcface_static_0.1
Training: 2023-09-27 05:55:03,521 - output: MS1M_v3_arcface_static_0.1
INFO:root:resume: False
Training: 2023-09-27 05:55:03,521 - resume: False
INFO:root:checkpoint_dir: None
Training: 2023-09-27 05:55:03,521 - checkpoint_dir: None
INFO:root:max_num_last_checkpoint: 1
Training: 2023-09-27 05:55:03,521 - max_num_last_checkpoint: 1
INFO:root:------------------------

Training: 2023-09-27 05:55:03,521 - ------------------------

INFO:root:read label file finished, total num: 5822653
Training: 2023-09-27 05:55:03,849 - read label file finished, total num: 5822653
INFO:root:world_size: 2
Training: 2023-09-27 05:55:03,850 - world_size: 2
INFO:root:total_batch_size: 4
Training: 2023-09-27 05:55:03,850 - total_batch_size: 4
INFO:root:warmup_steps: 0
Training: 2023-09-27 05:55:03,850 - warmup_steps: 0
INFO:root:steps_per_epoch: 1455663
Training: 2023-09-27 05:55:03,850 - steps_per_epoch: 1455663
INFO:root:total_steps: 36391575
Training: 2023-09-27 05:55:03,850 - total_steps: 36391575
INFO:root:total_epoch: 25
Training: 2023-09-27 05:55:03,850 - total_epoch: 25
INFO:root:decay_steps: [14556630, 23290608, 32024586]
Training: 2023-09-27 05:55:03,850 - decay_steps: [14556630, 23290608, 32024586]
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in conv2d only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'Out' in guassian_random only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in batch_norm only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in prelu only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in elementwise_add only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'y' in elementwise_add only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in fc only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'Out' in xavier_init only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'dtype' in create_parameter only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'tensor' in all_gather only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in split only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input[0]' in concat only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input[1]' in concat only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in gather only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'X' in norm only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in matmul only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'y' in matmul only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'logits' in margin_cross_entropy only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/framework.py:744: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  elif dtype == np.bool:
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in check_finite_and_unscale only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in update_loss_scaling only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/optimizer/momentum.py:262: UserWarning: Accumulating with FP16 in optimizer can lead to poor accuracy or slow convergence.Consider using multi_precision=True option of the Momentum optimizer.
  "Accumulating with FP16 in optimizer can lead to poor accuracy or slow convergence."
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:51895']
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
(14000, 3, 112, 112)
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
W0927 05:55:24.766564 10713 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 12.0, Runtime API Version: 11.2
W0927 05:55:24.768213 10713 device_context.cc:465] device: 0, cuDNN Version: 8.1.
I0927 05:55:26.329439 10713 gen_comm_id_helper.cc:190] Server listening on: 127.0.0.1:55393 successful.
INFO:root:loss nan, lr: 0.000781, epoch: 0, step: 100, eta: 885.07 hours, avg_reader_cost: 0.01677 sec, avg_batch_cost: 0.07120 sec, avg_samples: 2.00000, ips: 56.17791 images/sec
Training: 2023-09-27 05:55:33,564 - loss nan, lr: 0.000781, epoch: 0, step: 100, eta: 885.07 hours, avg_reader_cost: 0.01677 sec, avg_batch_cost: 0.07120 sec, avg_samples: 2.00000, ips: 56.17791 images/sec
INFO:root:loss nan, lr: 0.000781, epoch: 0, step: 200, eta: 663.57 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.04346 sec, avg_samples: 2.00000, ips: 92.03650 images/sec
Training: 2023-09-27 05:55:37,915 - loss nan, lr: 0.000781, epoch: 0, step: 200, eta: 663.57 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.04346 sec, avg_samples: 2.00000, ips: 92.03650 images/sec
INFO:root:loss nan, lr: 0.000781, epoch: 0, step: 300, eta: 589.88 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.04365 sec, avg_samples: 2.00000, ips: 91.63141 images/sec
Training: 2023-09-27 05:55:42,285 - loss nan, lr: 0.000781, epoch: 0, step: 300, eta: 589.88 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.04365 sec, avg_samples: 2.00000, ips: 91.63141 images/sec
INFO:root:loss nan, lr: 0.000781, epoch: 0, step: 400, eta: 552.97 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.04367 sec, avg_samples: 2.00000, ips: 91.60347 images/sec
Training: 2023-09-27 05:55:46,657 - loss nan, lr: 0.000781, epoch: 0, step: 400, eta: 552.97 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.04367 sec, avg_samples: 2.00000, ips: 91.60347 images/sec
INFO:root:loss nan, lr: 0.000781, epoch: 0, step: 500, eta: 531.39 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.04395 sec, avg_samples: 2.00000, ips: 91.00326 images/sec
Training: 2023-09-27 05:55:51,057 - loss nan, lr: 0.000781, epoch: 0, step: 500, eta: 531.39 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.04395 sec, avg_samples: 2.00000, ips: 91.00326 images/sec
INFO:root:loss nan, lr: 0.000781, epoch: 0, step: 600, eta: 516.20 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.04350 sec, avg_samples: 2.00000, ips: 91.95843 images/sec
Training: 2023-09-27 05:55:55,411 - loss nan, lr: 0.000781, epoch: 0, step: 600, eta: 516.20 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.04350 sec, avg_samples: 2.00000, ips: 91.95843 images/sec
INFO:root:loss nan, lr: 0.000781, epoch: 0, step: 700, eta: 505.23 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.04341 sec, avg_samples: 2.00000, ips: 92.13742 images/sec
Training: 2023-09-27 05:55:59,757 - loss nan, lr: 0.000781, epoch: 0, step: 700, eta: 505.23 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.04341 sec, avg_samples: 2.00000, ips: 92.13742 images/sec
INFO:root:loss nan, lr: 0.000781, epoch: 0, step: 800, eta: 497.37 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.04371 sec, avg_samples: 2.00000, ips: 91.50274 images/sec
Training: 2023-09-27 05:56:04,132 - loss nan, lr: 0.000781, epoch: 0, step: 800, eta: 497.37 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.04371 sec, avg_samples: 2.00000, ips: 91.50274 images/sec
INFO:root:loss nan, lr: 0.000781, epoch: 0, step: 900, eta: 490.88 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.04338 sec, avg_samples: 2.00000, ips: 92.20285 images/sec
Training: 2023-09-27 05:56:08,475 - loss nan, lr: 0.000781, epoch: 0, step: 900, eta: 490.88 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.04338 sec, avg_samples: 2.00000, ips: 92.20285 images/sec
INFO:root:loss nan, lr: 0.000781, epoch: 0, step: 1000, eta: 485.80 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.04348 sec, avg_samples: 2.00000, ips: 91.98658 images/sec
Training: 2023-09-27 05:56:12,828 - loss nan, lr: 0.000781, epoch: 0, step: 1000, eta: 485.80 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.04348 sec, avg_samples: 2.00000, ips: 91.98658 images/sec
INFO:root:loss nan, lr: 0.000781, epoch: 0, step: 1100, eta: 481.77 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.04362 sec, avg_samples: 2.00000, ips: 91.69189 images/sec
Training: 2023-09-27 05:56:17,195 - loss nan, lr: 0.000781, epoch: 0, step: 1100, eta: 481.77 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.04362 sec, avg_samples: 2.00000, ips: 91.69189 images/sec
INFO:root:loss nan, lr: 0.000781, epoch: 0, step: 1200, eta: 478.42 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.04363 sec, avg_samples: 2.00000, ips: 91.67373 images/sec
Training: 2023-09-27 05:56:21,563 - loss nan, lr: 0.000781, epoch: 0, step: 1200, eta: 478.42 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.04363 sec, avg_samples: 2.00000, ips: 91.67373 images/sec
INFO:root:loss nan, lr: 0.000781, epoch: 0, step: 1300, eta: 475.58 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.04362 sec, avg_samples: 2.00000, ips: 91.69292 images/sec
Training: 2023-09-27 05:56:25,930 - loss nan, lr: 0.000781, epoch: 0, step: 1300, eta: 475.58 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.04362 sec, avg_samples: 2.00000, ips: 91.69292 images/sec
INFO:root:loss nan, lr: 0.000781, epoch: 0, step: 1400, eta: 473.19 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.04369 sec, avg_samples: 2.00000, ips: 91.55077 images/sec
Training: 2023-09-27 05:56:30,304 - loss nan, lr: 0.000781, epoch: 0, step: 1400, eta: 473.19 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.04369 sec, avg_samples: 2.00000, ips: 91.55077 images/sec
INFO:root:loss nan, lr: 0.000781, epoch: 0, step: 1500, eta: 470.90 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.04337 sec, avg_samples: 2.00000, ips: 92.22452 images/sec
Training: 2023-09-27 05:56:34,646 - loss nan, lr: 0.000781, epoch: 0, step: 1500, eta: 470.90 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.04337 sec, avg_samples: 2.00000, ips: 92.22452 images/sec
INFO:root:loss nan, lr: 0.000781, epoch: 0, step: 1600, eta: 468.98 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.04349 sec, avg_samples: 2.00000, ips: 91.98222 images/sec
Training: 2023-09-27 05:56:38,999 - loss nan, lr: 0.000781, epoch: 0, step: 1600, eta: 468.98 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.04349 sec, avg_samples: 2.00000, ips: 91.98222 images/sec
INFO:root:loss nan, lr: 0.000781, epoch: 0, step: 1700, eta: 467.37 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.04364 sec, avg_samples: 2.00000, ips: 91.66342 images/sec
Training: 2023-09-27 05:56:43,368 - loss nan, lr: 0.000781, epoch: 0, step: 1700, eta: 467.37 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.04364 sec, avg_samples: 2.00000, ips: 91.66342 images/sec
INFO:root:loss nan, lr: 0.000781, epoch: 0, step: 1800, eta: 465.76 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.04334 sec, avg_samples: 2.00000, ips: 92.30253 images/sec
Training: 2023-09-27 05:56:47,706 - loss nan, lr: 0.000781, epoch: 0, step: 1800, eta: 465.76 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.04334 sec, avg_samples: 2.00000, ips: 92.30253 images/sec
INFO:root:loss nan, lr: 0.000781, epoch: 0, step: 1900, eta: 464.33 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.04335 sec, avg_samples: 2.00000, ips: 92.27877 images/sec
Training: 2023-09-27 05:56:52,044 - loss nan, lr: 0.000781, epoch: 0, step: 1900, eta: 464.33 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.04335 sec, avg_samples: 2.00000, ips: 92.27877 images/sec
INFO:root:loss nan, lr: 0.000781, epoch: 0, step: 2000, eta: 463.49 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.04423 sec, avg_samples: 2.00000, ips: 90.44241 images/sec
Training: 2023-09-27 05:56:56,472 - loss nan, lr: 0.000781, epoch: 0, step: 2000, eta: 463.49 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.04423 sec, avg_samples: 2.00000, ips: 90.44241 images/sec
INFO:root:[lfw][2000]XNorm: 21.498826
Training: 2023-09-27 05:58:09,122 - [lfw][2000]XNorm: 21.498826
INFO:root:[lfw][2000]Accuracy-Flip: 0.68500+-0.01350
Training: 2023-09-27 05:58:09,123 - [lfw][2000]Accuracy-Flip: 0.68500+-0.01350
INFO:root:[lfw][2000]Accuracy-Highest: 0.68500
Training: 2023-09-27 05:58:09,123 - [lfw][2000]Accuracy-Highest: 0.68500
INFO:root:test time: 72.6506
Training: 2023-09-27 05:58:09,123 - test time: 72.6506
INFO:root:[cfp_fp][2000]XNorm: 28.531154
Training: 2023-09-27 05:59:33,895 - [cfp_fp][2000]XNorm: 28.531154
INFO:root:[cfp_fp][2000]Accuracy-Flip: 0.57486+-0.02341
Training: 2023-09-27 05:59:33,895 - [cfp_fp][2000]Accuracy-Flip: 0.57486+-0.02341
INFO:root:[cfp_fp][2000]Accuracy-Highest: 0.57486
Training: 2023-09-27 05:59:33,895 - [cfp_fp][2000]Accuracy-Highest: 0.57486
INFO:root:test time: 84.7722
Training: 2023-09-27 05:59:33,895 - test time: 84.7722


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::framework::Executor::RunPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, bool, bool, bool)
1   paddle::framework::Executor::RunPartialPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, long, long, bool, bool, bool)
2   paddle::framework::OperatorBase::Run(paddle::framework::Scope const&, paddle::platform::Place const&)
3   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&) const
4   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&, paddle::framework::RuntimeContext*) const
5   std::_Function_handler<void (paddle::framework::ExecutionContext const&), paddle::framework::OpKernelRegistrarFunctor<paddle::platform::CUDAPlace, false, 2ul, paddle::operators::CUDNNConvOpKernel<float>, paddle::operators::CUDNNConvOpKernel<double>, paddle::operators::CUDNNConvOpKernel<paddle::platform::float16>, paddle::operators::CUDNNConvOpKernel<paddle::platform::bfloat16> >::operator()(char const*, char const*, int) const::{lambda(paddle::framework::ExecutionContext const&)#1}>::_M_invoke(std::_Any_data const&, paddle::framework::ExecutionContext const&)
6   paddle::operators::CUDNNConvOpKernel<paddle::platform::float16>::Compute(paddle::framework::ExecutionContext const&) const

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1695794391 (unix time) try "date -d @1695794391" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x2987) received by PID 10713 (TID 0x7f3247a49740) from PID 10631 ***]

grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
INFO:root:rank: 0
Training: 2023-09-27 06:00:14,725 - rank: 0
INFO:root:--------args----------
Training: 2023-09-27 06:00:14,725 - --------args----------
INFO:root:config_file: configs/ms1mv3_r50.py
Training: 2023-09-27 06:00:14,726 - config_file: configs/ms1mv3_r50.py
INFO:root:is_static: True
Training: 2023-09-27 06:00:14,726 - is_static: True
INFO:root:backbone: FresResNet50
Training: 2023-09-27 06:00:14,726 - backbone: FresResNet50
INFO:root:classifier: LargeScaleClassifier
Training: 2023-09-27 06:00:14,726 - classifier: LargeScaleClassifier
INFO:root:embedding_size: 512
Training: 2023-09-27 06:00:14,726 - embedding_size: 512
INFO:root:model_parallel: True
Training: 2023-09-27 06:00:14,726 - model_parallel: True
INFO:root:sample_ratio: 0.1
Training: 2023-09-27 06:00:14,726 - sample_ratio: 0.1
INFO:root:loss: ArcFace
Training: 2023-09-27 06:00:14,726 - loss: ArcFace
INFO:root:dropout: 0.0
Training: 2023-09-27 06:00:14,726 - dropout: 0.0
INFO:root:fp16: True
Training: 2023-09-27 06:00:14,726 - fp16: True
INFO:root:init_loss_scaling: 27648.0
Training: 2023-09-27 06:00:14,726 - init_loss_scaling: 27648.0
INFO:root:max_loss_scaling: 128.0
Training: 2023-09-27 06:00:14,726 - max_loss_scaling: 128.0
INFO:root:incr_every_n_steps: 2000
Training: 2023-09-27 06:00:14,726 - incr_every_n_steps: 2000
INFO:root:decr_every_n_nan_or_inf: 1
Training: 2023-09-27 06:00:14,726 - decr_every_n_nan_or_inf: 1
INFO:root:incr_ratio: 2.0
Training: 2023-09-27 06:00:14,726 - incr_ratio: 2.0
INFO:root:decr_ratio: 0.5
Training: 2023-09-27 06:00:14,726 - decr_ratio: 0.5
INFO:root:use_dynamic_loss_scaling: True
Training: 2023-09-27 06:00:14,726 - use_dynamic_loss_scaling: True
INFO:root:custom_white_list: []
Training: 2023-09-27 06:00:14,726 - custom_white_list: []
INFO:root:custom_black_list: []
Training: 2023-09-27 06:00:14,726 - custom_black_list: []
INFO:root:lr: 0.1
Training: 2023-09-27 06:00:14,726 - lr: 0.1
INFO:root:lr_decay: 0.1
Training: 2023-09-27 06:00:14,726 - lr_decay: 0.1
INFO:root:weight_decay: 0.0005
Training: 2023-09-27 06:00:14,726 - weight_decay: 0.0005
INFO:root:momentum: 0.9
Training: 2023-09-27 06:00:14,726 - momentum: 0.9
INFO:root:train_unit: epoch
Training: 2023-09-27 06:00:14,726 - train_unit: epoch
INFO:root:warmup_num: 0
Training: 2023-09-27 06:00:14,726 - warmup_num: 0
INFO:root:train_num: 25
Training: 2023-09-27 06:00:14,726 - train_num: 25
INFO:root:decay_boundaries: [10, 16, 22]
Training: 2023-09-27 06:00:14,726 - decay_boundaries: [10, 16, 22]
INFO:root:use_synthetic_dataset: False
Training: 2023-09-27 06:00:14,726 - use_synthetic_dataset: False
INFO:root:dataset: MS1M_v2
Training: 2023-09-27 06:00:14,726 - dataset: MS1M_v2
INFO:root:data_dir: /paddle/data_out/
Training: 2023-09-27 06:00:14,726 - data_dir: /paddle/data_out/
INFO:root:label_file: /paddle/data_out/label.txt
Training: 2023-09-27 06:00:14,726 - label_file: /paddle/data_out/label.txt
INFO:root:is_bin: False
Training: 2023-09-27 06:00:14,727 - is_bin: False
INFO:root:num_classes: 85741
Training: 2023-09-27 06:00:14,727 - num_classes: 85741
INFO:root:batch_size: 128
Training: 2023-09-27 06:00:14,727 - batch_size: 128
INFO:root:num_workers: 2
Training: 2023-09-27 06:00:14,727 - num_workers: 2
INFO:root:do_validation_while_train: True
Training: 2023-09-27 06:00:14,727 - do_validation_while_train: True
INFO:root:validation_interval_step: 2000
Training: 2023-09-27 06:00:14,727 - validation_interval_step: 2000
INFO:root:val_targets: ['lfw', 'cfp_fp', 'agedb_30']
Training: 2023-09-27 06:00:14,727 - val_targets: ['lfw', 'cfp_fp', 'agedb_30']
INFO:root:logdir: ./log
Training: 2023-09-27 06:00:14,727 - logdir: ./log
INFO:root:log_interval_step: 100
Training: 2023-09-27 06:00:14,727 - log_interval_step: 100
INFO:root:output: MS1M_v3_arcface_static_0.1
Training: 2023-09-27 06:00:14,727 - output: MS1M_v3_arcface_static_0.1
INFO:root:resume: False
Training: 2023-09-27 06:00:14,727 - resume: False
INFO:root:checkpoint_dir: None
Training: 2023-09-27 06:00:14,727 - checkpoint_dir: None
INFO:root:max_num_last_checkpoint: 1
Training: 2023-09-27 06:00:14,727 - max_num_last_checkpoint: 1
INFO:root:------------------------

Training: 2023-09-27 06:00:14,727 - ------------------------

INFO:root:read label file finished, total num: 5822653
Training: 2023-09-27 06:00:15,069 - read label file finished, total num: 5822653
INFO:root:world_size: 2
Training: 2023-09-27 06:00:15,069 - world_size: 2
INFO:root:total_batch_size: 256
Training: 2023-09-27 06:00:15,069 - total_batch_size: 256
INFO:root:warmup_steps: 0
Training: 2023-09-27 06:00:15,069 - warmup_steps: 0
INFO:root:steps_per_epoch: 22744
Training: 2023-09-27 06:00:15,069 - steps_per_epoch: 22744
INFO:root:total_steps: 568600
Training: 2023-09-27 06:00:15,070 - total_steps: 568600
INFO:root:total_epoch: 25
Training: 2023-09-27 06:00:15,070 - total_epoch: 25
INFO:root:decay_steps: [227440, 363904, 500368]
Training: 2023-09-27 06:00:15,070 - decay_steps: [227440, 363904, 500368]
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in conv2d only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'Out' in guassian_random only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in batch_norm only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in prelu only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in elementwise_add only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'y' in elementwise_add only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in fc only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'Out' in xavier_init only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'dtype' in create_parameter only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'tensor' in all_gather only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in split only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input[0]' in concat only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input[1]' in concat only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in gather only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'X' in norm only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in matmul only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'y' in matmul only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'logits' in margin_cross_entropy only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/framework.py:744: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  elif dtype == np.bool:
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in check_finite_and_unscale only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in update_loss_scaling only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/optimizer/momentum.py:262: UserWarning: Accumulating with FP16 in optimizer can lead to poor accuracy or slow convergence.Consider using multi_precision=True option of the Momentum optimizer.
  "Accumulating with FP16 in optimizer can lead to poor accuracy or slow convergence."
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:41303']
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
(14000, 3, 112, 112)
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
W0927 06:00:35.800076 10872 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 12.0, Runtime API Version: 11.2
W0927 06:00:35.801728 10872 device_context.cc:465] device: 0, cuDNN Version: 8.1.
I0927 06:00:37.373100 10872 gen_comm_id_helper.cc:190] Server listening on: 127.0.0.1:39283 successful.
ERROR: Unexpected BUS error encountered in DataLoader worker. This might be caused by insufficient shared memory (shm), please check whether use_shared_memory is set and storage space in /dev/shm is enough
 ERROR: Unexpected BUS error encountered in DataLoader worker. This might be caused by insufficient shared memory (shm), please check whether use_shared_memory is set and storage space in /dev/shm is enough
 Traceback (most recent call last):
  File "tools/train.py", line 35, in <module>
    train(args)
  File "/paddle/insightface/recognition/arcface_paddle/static/train.py", line 199, in train
    for step, data in enumerate(train_loader):
  File "/usr/local/lib/python3.7/dist-packages/paddle/fluid/dataloader/dataloader_iter.py", line 715, in __next__
    data = self._reader.read_next()
  File "/usr/local/lib/python3.7/dist-packages/paddle/fluid/multiprocess_utils.py", line 134, in __handler__
    core._throw_error_if_process_failed()
SystemError: (Fatal) DataLoader process (pid   1. If run DataLoader by DataLoader.from_generator(...), queue capacity is set by from_generator(..., capacity=xx, ...).
  2. If run DataLoader by DataLoader(dataset, ...), queue capacity is set as 2 times of the max value of num_workers and len(places).
  3. If run by DataLoader(dataset, ..., use_shared_memory=True), set use_shared_memory=False for not using shared memory.) exited is killed by signal: 10923.
  It may be caused by insufficient shared storage space. This problem usually occurs when using docker as a development environment.
  Please use command `df -h` to check the storage space of `/dev/shm`. Shared storage space needs to be greater than (DataLoader Num * DataLoader queue capacity * 1 batch data size).
  You can solve this problem by increasing the shared storage space or reducing the queue capacity appropriately.
Bus error (at /paddle/paddle/fluid/imperative/data_loader.cc:177)

grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
INFO:root:rank: 0
Training: 2023-09-27 06:01:09,016 - rank: 0
INFO:root:--------args----------
Training: 2023-09-27 06:01:09,016 - --------args----------
INFO:root:config_file: configs/ms1mv3_r50.py
Training: 2023-09-27 06:01:09,016 - config_file: configs/ms1mv3_r50.py
INFO:root:is_static: True
Training: 2023-09-27 06:01:09,016 - is_static: True
INFO:root:backbone: FresResNet50
Training: 2023-09-27 06:01:09,016 - backbone: FresResNet50
INFO:root:classifier: LargeScaleClassifier
Training: 2023-09-27 06:01:09,016 - classifier: LargeScaleClassifier
INFO:root:embedding_size: 512
Training: 2023-09-27 06:01:09,016 - embedding_size: 512
INFO:root:model_parallel: True
Training: 2023-09-27 06:01:09,016 - model_parallel: True
INFO:root:sample_ratio: 0.1
Training: 2023-09-27 06:01:09,016 - sample_ratio: 0.1
INFO:root:loss: ArcFace
Training: 2023-09-27 06:01:09,016 - loss: ArcFace
INFO:root:dropout: 0.0
Training: 2023-09-27 06:01:09,016 - dropout: 0.0
INFO:root:fp16: True
Training: 2023-09-27 06:01:09,016 - fp16: True
INFO:root:init_loss_scaling: 27648.0
Training: 2023-09-27 06:01:09,016 - init_loss_scaling: 27648.0
INFO:root:max_loss_scaling: 128.0
Training: 2023-09-27 06:01:09,016 - max_loss_scaling: 128.0
INFO:root:incr_every_n_steps: 2000
Training: 2023-09-27 06:01:09,016 - incr_every_n_steps: 2000
INFO:root:decr_every_n_nan_or_inf: 1
Training: 2023-09-27 06:01:09,016 - decr_every_n_nan_or_inf: 1
INFO:root:incr_ratio: 2.0
Training: 2023-09-27 06:01:09,016 - incr_ratio: 2.0
INFO:root:decr_ratio: 0.5
Training: 2023-09-27 06:01:09,016 - decr_ratio: 0.5
INFO:root:use_dynamic_loss_scaling: True
Training: 2023-09-27 06:01:09,016 - use_dynamic_loss_scaling: True
INFO:root:custom_white_list: []
Training: 2023-09-27 06:01:09,016 - custom_white_list: []
INFO:root:custom_black_list: []
Training: 2023-09-27 06:01:09,016 - custom_black_list: []
INFO:root:lr: 0.1
Training: 2023-09-27 06:01:09,016 - lr: 0.1
INFO:root:lr_decay: 0.1
Training: 2023-09-27 06:01:09,016 - lr_decay: 0.1
INFO:root:weight_decay: 0.0005
Training: 2023-09-27 06:01:09,016 - weight_decay: 0.0005
INFO:root:momentum: 0.9
Training: 2023-09-27 06:01:09,016 - momentum: 0.9
INFO:root:train_unit: epoch
Training: 2023-09-27 06:01:09,017 - train_unit: epoch
INFO:root:warmup_num: 0
Training: 2023-09-27 06:01:09,017 - warmup_num: 0
INFO:root:train_num: 25
Training: 2023-09-27 06:01:09,017 - train_num: 25
INFO:root:decay_boundaries: [10, 16, 22]
Training: 2023-09-27 06:01:09,017 - decay_boundaries: [10, 16, 22]
INFO:root:use_synthetic_dataset: False
Training: 2023-09-27 06:01:09,017 - use_synthetic_dataset: False
INFO:root:dataset: MS1M_v2
Training: 2023-09-27 06:01:09,017 - dataset: MS1M_v2
INFO:root:data_dir: /paddle/data_out/
Training: 2023-09-27 06:01:09,017 - data_dir: /paddle/data_out/
INFO:root:label_file: /paddle/data_out/label.txt
Training: 2023-09-27 06:01:09,017 - label_file: /paddle/data_out/label.txt
INFO:root:is_bin: False
Training: 2023-09-27 06:01:09,017 - is_bin: False
INFO:root:num_classes: 85741
Training: 2023-09-27 06:01:09,017 - num_classes: 85741
INFO:root:batch_size: 64
Training: 2023-09-27 06:01:09,017 - batch_size: 64
INFO:root:num_workers: 2
Training: 2023-09-27 06:01:09,017 - num_workers: 2
INFO:root:do_validation_while_train: True
Training: 2023-09-27 06:01:09,017 - do_validation_while_train: True
INFO:root:validation_interval_step: 2000
Training: 2023-09-27 06:01:09,017 - validation_interval_step: 2000
INFO:root:val_targets: ['lfw', 'cfp_fp', 'agedb_30']
Training: 2023-09-27 06:01:09,017 - val_targets: ['lfw', 'cfp_fp', 'agedb_30']
INFO:root:logdir: ./log
Training: 2023-09-27 06:01:09,017 - logdir: ./log
INFO:root:log_interval_step: 100
Training: 2023-09-27 06:01:09,017 - log_interval_step: 100
INFO:root:output: MS1M_v3_arcface_static_0.1
Training: 2023-09-27 06:01:09,017 - output: MS1M_v3_arcface_static_0.1
INFO:root:resume: False
Training: 2023-09-27 06:01:09,017 - resume: False
INFO:root:checkpoint_dir: None
Training: 2023-09-27 06:01:09,017 - checkpoint_dir: None
INFO:root:max_num_last_checkpoint: 1
Training: 2023-09-27 06:01:09,017 - max_num_last_checkpoint: 1
INFO:root:------------------------

Training: 2023-09-27 06:01:09,017 - ------------------------

INFO:root:read label file finished, total num: 5822653
Training: 2023-09-27 06:01:09,376 - read label file finished, total num: 5822653
INFO:root:world_size: 2
Training: 2023-09-27 06:01:09,376 - world_size: 2
INFO:root:total_batch_size: 128
Training: 2023-09-27 06:01:09,376 - total_batch_size: 128
INFO:root:warmup_steps: 0
Training: 2023-09-27 06:01:09,376 - warmup_steps: 0
INFO:root:steps_per_epoch: 45489
Training: 2023-09-27 06:01:09,376 - steps_per_epoch: 45489
INFO:root:total_steps: 1137225
Training: 2023-09-27 06:01:09,376 - total_steps: 1137225
INFO:root:total_epoch: 25
Training: 2023-09-27 06:01:09,376 - total_epoch: 25
INFO:root:decay_steps: [454890, 727824, 1000758]
Training: 2023-09-27 06:01:09,376 - decay_steps: [454890, 727824, 1000758]
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in conv2d only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'Out' in guassian_random only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in batch_norm only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in prelu only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in elementwise_add only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'y' in elementwise_add only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in fc only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'Out' in xavier_init only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'dtype' in create_parameter only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'tensor' in all_gather only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in split only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input[0]' in concat only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input[1]' in concat only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in gather only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'X' in norm only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in matmul only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'y' in matmul only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'logits' in margin_cross_entropy only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/framework.py:744: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  elif dtype == np.bool:
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in check_finite_and_unscale only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in update_loss_scaling only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/optimizer/momentum.py:262: UserWarning: Accumulating with FP16 in optimizer can lead to poor accuracy or slow convergence.Consider using multi_precision=True option of the Momentum optimizer.
  "Accumulating with FP16 in optimizer can lead to poor accuracy or slow convergence."
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:37355']
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
(14000, 3, 112, 112)
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
W0927 06:01:30.113579 11029 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 12.0, Runtime API Version: 11.2
W0927 06:01:30.115219 11029 device_context.cc:465] device: 0, cuDNN Version: 8.1.
I0927 06:01:31.672538 11029 gen_comm_id_helper.cc:190] Server listening on: 127.0.0.1:58233 successful.
ERROR: Unexpected BUS error encountered in DataLoader worker. This might be caused by insufficient shared memory (shm), please check whether use_shared_memory is set and storage space in /dev/shm is enough
 ERROR: Unexpected BUS error encountered in DataLoader worker. This might be caused by insufficient shared memory (shm), please check whether use_shared_memory is set and storage space in /dev/shm is enough
 Traceback (most recent call last):
  File "tools/train.py", line 35, in <module>
    train(args)
  File "/paddle/insightface/recognition/arcface_paddle/static/train.py", line 199, in train
    for step, data in enumerate(train_loader):
  File "/usr/local/lib/python3.7/dist-packages/paddle/fluid/dataloader/dataloader_iter.py", line 715, in __next__
    data = self._reader.read_next()
  File "/usr/local/lib/python3.7/dist-packages/paddle/fluid/multiprocess_utils.py", line 134, in __handler__
    core._throw_error_if_process_failed()
SystemError: (Fatal) DataLoader process (pid   1. If run DataLoader by DataLoader.from_generator(...), queue capacity is set by from_generator(..., capacity=xx, ...).
  2. If run DataLoader by DataLoader(dataset, ...), queue capacity is set as 2 times of the max value of num_workers and len(places).
  3. If run by DataLoader(dataset, ..., use_shared_memory=True), set use_shared_memory=False for not using shared memory.) exited is killed by signal: 11081.
  It may be caused by insufficient shared storage space. This problem usually occurs when using docker as a development environment.
  Please use command `df -h` to check the storage space of `/dev/shm`. Shared storage space needs to be greater than (DataLoader Num * DataLoader queue capacity * 1 batch data size).
  You can solve this problem by increasing the shared storage space or reducing the queue capacity appropriately.
Bus error (at /paddle/paddle/fluid/imperative/data_loader.cc:177)

grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
INFO:root:rank: 0
Training: 2023-09-27 06:01:56,091 - rank: 0
INFO:root:--------args----------
Training: 2023-09-27 06:01:56,091 - --------args----------
INFO:root:config_file: configs/ms1mv3_r50.py
Training: 2023-09-27 06:01:56,091 - config_file: configs/ms1mv3_r50.py
INFO:root:is_static: True
Training: 2023-09-27 06:01:56,091 - is_static: True
INFO:root:backbone: FresResNet50
Training: 2023-09-27 06:01:56,091 - backbone: FresResNet50
INFO:root:classifier: LargeScaleClassifier
Training: 2023-09-27 06:01:56,091 - classifier: LargeScaleClassifier
INFO:root:embedding_size: 512
Training: 2023-09-27 06:01:56,091 - embedding_size: 512
INFO:root:model_parallel: True
Training: 2023-09-27 06:01:56,091 - model_parallel: True
INFO:root:sample_ratio: 0.1
Training: 2023-09-27 06:01:56,091 - sample_ratio: 0.1
INFO:root:loss: ArcFace
Training: 2023-09-27 06:01:56,091 - loss: ArcFace
INFO:root:dropout: 0.0
Training: 2023-09-27 06:01:56,091 - dropout: 0.0
INFO:root:fp16: True
Training: 2023-09-27 06:01:56,091 - fp16: True
INFO:root:init_loss_scaling: 27648.0
Training: 2023-09-27 06:01:56,092 - init_loss_scaling: 27648.0
INFO:root:max_loss_scaling: 128.0
Training: 2023-09-27 06:01:56,092 - max_loss_scaling: 128.0
INFO:root:incr_every_n_steps: 2000
Training: 2023-09-27 06:01:56,092 - incr_every_n_steps: 2000
INFO:root:decr_every_n_nan_or_inf: 1
Training: 2023-09-27 06:01:56,092 - decr_every_n_nan_or_inf: 1
INFO:root:incr_ratio: 2.0
Training: 2023-09-27 06:01:56,092 - incr_ratio: 2.0
INFO:root:decr_ratio: 0.5
Training: 2023-09-27 06:01:56,092 - decr_ratio: 0.5
INFO:root:use_dynamic_loss_scaling: True
Training: 2023-09-27 06:01:56,092 - use_dynamic_loss_scaling: True
INFO:root:custom_white_list: []
Training: 2023-09-27 06:01:56,092 - custom_white_list: []
INFO:root:custom_black_list: []
Training: 2023-09-27 06:01:56,092 - custom_black_list: []
INFO:root:lr: 0.1
Training: 2023-09-27 06:01:56,092 - lr: 0.1
INFO:root:lr_decay: 0.1
Training: 2023-09-27 06:01:56,092 - lr_decay: 0.1
INFO:root:weight_decay: 0.0005
Training: 2023-09-27 06:01:56,092 - weight_decay: 0.0005
INFO:root:momentum: 0.9
Training: 2023-09-27 06:01:56,092 - momentum: 0.9
INFO:root:train_unit: epoch
Training: 2023-09-27 06:01:56,092 - train_unit: epoch
INFO:root:warmup_num: 0
Training: 2023-09-27 06:01:56,092 - warmup_num: 0
INFO:root:train_num: 25
Training: 2023-09-27 06:01:56,092 - train_num: 25
INFO:root:decay_boundaries: [10, 16, 22]
Training: 2023-09-27 06:01:56,092 - decay_boundaries: [10, 16, 22]
INFO:root:use_synthetic_dataset: False
Training: 2023-09-27 06:01:56,092 - use_synthetic_dataset: False
INFO:root:dataset: MS1M_v2
Training: 2023-09-27 06:01:56,092 - dataset: MS1M_v2
INFO:root:data_dir: /paddle/data_out/
Training: 2023-09-27 06:01:56,092 - data_dir: /paddle/data_out/
INFO:root:label_file: /paddle/data_out/label.txt
Training: 2023-09-27 06:01:56,092 - label_file: /paddle/data_out/label.txt
INFO:root:is_bin: False
Training: 2023-09-27 06:01:56,092 - is_bin: False
INFO:root:num_classes: 85741
Training: 2023-09-27 06:01:56,092 - num_classes: 85741
INFO:root:batch_size: 16
Training: 2023-09-27 06:01:56,092 - batch_size: 16
INFO:root:num_workers: 2
Training: 2023-09-27 06:01:56,092 - num_workers: 2
INFO:root:do_validation_while_train: True
Training: 2023-09-27 06:01:56,092 - do_validation_while_train: True
INFO:root:validation_interval_step: 2000
Training: 2023-09-27 06:01:56,092 - validation_interval_step: 2000
INFO:root:val_targets: ['lfw', 'cfp_fp', 'agedb_30']
Training: 2023-09-27 06:01:56,092 - val_targets: ['lfw', 'cfp_fp', 'agedb_30']
INFO:root:logdir: ./log
Training: 2023-09-27 06:01:56,092 - logdir: ./log
INFO:root:log_interval_step: 100
Training: 2023-09-27 06:01:56,092 - log_interval_step: 100
INFO:root:output: MS1M_v3_arcface_static_0.1
Training: 2023-09-27 06:01:56,092 - output: MS1M_v3_arcface_static_0.1
INFO:root:resume: False
Training: 2023-09-27 06:01:56,092 - resume: False
INFO:root:checkpoint_dir: None
Training: 2023-09-27 06:01:56,093 - checkpoint_dir: None
INFO:root:max_num_last_checkpoint: 1
Training: 2023-09-27 06:01:56,093 - max_num_last_checkpoint: 1
INFO:root:------------------------

Training: 2023-09-27 06:01:56,093 - ------------------------

INFO:root:read label file finished, total num: 5822653
Training: 2023-09-27 06:01:56,444 - read label file finished, total num: 5822653
INFO:root:world_size: 2
Training: 2023-09-27 06:01:56,444 - world_size: 2
INFO:root:total_batch_size: 32
Training: 2023-09-27 06:01:56,444 - total_batch_size: 32
INFO:root:warmup_steps: 0
Training: 2023-09-27 06:01:56,444 - warmup_steps: 0
INFO:root:steps_per_epoch: 181957
Training: 2023-09-27 06:01:56,444 - steps_per_epoch: 181957
INFO:root:total_steps: 4548925
Training: 2023-09-27 06:01:56,444 - total_steps: 4548925
INFO:root:total_epoch: 25
Training: 2023-09-27 06:01:56,444 - total_epoch: 25
INFO:root:decay_steps: [1819570, 2911312, 4003054]
Training: 2023-09-27 06:01:56,444 - decay_steps: [1819570, 2911312, 4003054]
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in conv2d only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'Out' in guassian_random only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in batch_norm only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in prelu only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in elementwise_add only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'y' in elementwise_add only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in fc only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'Out' in xavier_init only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'dtype' in create_parameter only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'tensor' in all_gather only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in split only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input[0]' in concat only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input[1]' in concat only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in gather only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'X' in norm only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in matmul only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'y' in matmul only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'logits' in margin_cross_entropy only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/framework.py:744: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  elif dtype == np.bool:
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in check_finite_and_unscale only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in update_loss_scaling only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/optimizer/momentum.py:262: UserWarning: Accumulating with FP16 in optimizer can lead to poor accuracy or slow convergence.Consider using multi_precision=True option of the Momentum optimizer.
  "Accumulating with FP16 in optimizer can lead to poor accuracy or slow convergence."
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:43767']
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
(14000, 3, 112, 112)
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
W0927 06:02:17.358150 11188 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 12.0, Runtime API Version: 11.2
W0927 06:02:17.359834 11188 device_context.cc:465] device: 0, cuDNN Version: 8.1.
I0927 06:02:18.929576 11188 gen_comm_id_helper.cc:190] Server listening on: 127.0.0.1:42251 successful.
INFO:root:loss 45.3714, lr: 0.006250, epoch: 0, step: 100, eta: 157.98 hours, avg_reader_cost: 0.01399 sec, avg_batch_cost: 0.10861 sec, avg_samples: 16.00000, ips: 294.63261 images/sec
Training: 2023-09-27 06:02:29,941 - loss 45.3714, lr: 0.006250, epoch: 0, step: 100, eta: 157.98 hours, avg_reader_cost: 0.01399 sec, avg_batch_cost: 0.10861 sec, avg_samples: 16.00000, ips: 294.63261 images/sec
INFO:root:loss 44.7132, lr: 0.006250, epoch: 0, step: 200, eta: 130.04 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08053 sec, avg_samples: 16.00000, ips: 397.38017 images/sec
Training: 2023-09-27 06:02:37,999 - loss 44.7132, lr: 0.006250, epoch: 0, step: 200, eta: 130.04 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08053 sec, avg_samples: 16.00000, ips: 397.38017 images/sec
INFO:root:loss 44.3886, lr: 0.006250, epoch: 0, step: 300, eta: 120.63 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08046 sec, avg_samples: 16.00000, ips: 397.70268 images/sec
Training: 2023-09-27 06:02:46,049 - loss 44.3886, lr: 0.006250, epoch: 0, step: 300, eta: 120.63 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08046 sec, avg_samples: 16.00000, ips: 397.70268 images/sec
INFO:root:loss 44.2691, lr: 0.006250, epoch: 0, step: 400, eta: 116.03 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08084 sec, avg_samples: 16.00000, ips: 395.82831 images/sec
Training: 2023-09-27 06:02:54,139 - loss 44.2691, lr: 0.006250, epoch: 0, step: 400, eta: 116.03 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08084 sec, avg_samples: 16.00000, ips: 395.82831 images/sec
INFO:root:loss 44.0990, lr: 0.006250, epoch: 0, step: 500, eta: 113.22 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08066 sec, avg_samples: 16.00000, ips: 396.75048 images/sec
Training: 2023-09-27 06:03:02,209 - loss 44.0990, lr: 0.006250, epoch: 0, step: 500, eta: 113.22 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08066 sec, avg_samples: 16.00000, ips: 396.75048 images/sec
INFO:root:loss 44.1642, lr: 0.006250, epoch: 0, step: 600, eta: 111.33 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08059 sec, avg_samples: 16.00000, ips: 397.07479 images/sec
Training: 2023-09-27 06:03:10,273 - loss 44.1642, lr: 0.006250, epoch: 0, step: 600, eta: 111.33 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08059 sec, avg_samples: 16.00000, ips: 397.07479 images/sec
INFO:root:loss 44.1568, lr: 0.006250, epoch: 0, step: 700, eta: 110.02 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08080 sec, avg_samples: 16.00000, ips: 396.03285 images/sec
Training: 2023-09-27 06:03:18,358 - loss 44.1568, lr: 0.006250, epoch: 0, step: 700, eta: 110.02 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08080 sec, avg_samples: 16.00000, ips: 396.03285 images/sec
INFO:root:loss 44.0475, lr: 0.006250, epoch: 0, step: 800, eta: 109.04 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08084 sec, avg_samples: 16.00000, ips: 395.84032 images/sec
Training: 2023-09-27 06:03:26,448 - loss 44.0475, lr: 0.006250, epoch: 0, step: 800, eta: 109.04 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08084 sec, avg_samples: 16.00000, ips: 395.84032 images/sec
INFO:root:loss 43.9775, lr: 0.006250, epoch: 0, step: 900, eta: 108.28 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08087 sec, avg_samples: 16.00000, ips: 395.69625 images/sec
Training: 2023-09-27 06:03:34,540 - loss 43.9775, lr: 0.006250, epoch: 0, step: 900, eta: 108.28 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08087 sec, avg_samples: 16.00000, ips: 395.69625 images/sec
INFO:root:loss 43.9570, lr: 0.006250, epoch: 0, step: 1000, eta: 107.66 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08075 sec, avg_samples: 16.00000, ips: 396.28898 images/sec
Training: 2023-09-27 06:03:42,620 - loss 43.9570, lr: 0.006250, epoch: 0, step: 1000, eta: 107.66 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08075 sec, avg_samples: 16.00000, ips: 396.28898 images/sec
INFO:root:loss 43.9781, lr: 0.006250, epoch: 0, step: 1100, eta: 107.16 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08081 sec, avg_samples: 16.00000, ips: 396.01460 images/sec
Training: 2023-09-27 06:03:50,706 - loss 43.9781, lr: 0.006250, epoch: 0, step: 1100, eta: 107.16 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08081 sec, avg_samples: 16.00000, ips: 396.01460 images/sec
INFO:root:loss 43.8951, lr: 0.006250, epoch: 0, step: 1200, eta: 106.73 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08075 sec, avg_samples: 16.00000, ips: 396.29463 images/sec
Training: 2023-09-27 06:03:58,786 - loss 43.8951, lr: 0.006250, epoch: 0, step: 1200, eta: 106.73 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08075 sec, avg_samples: 16.00000, ips: 396.29463 images/sec
INFO:root:loss 44.0537, lr: 0.006250, epoch: 0, step: 1300, eta: 106.38 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08083 sec, avg_samples: 16.00000, ips: 395.90394 images/sec
Training: 2023-09-27 06:04:06,873 - loss 44.0537, lr: 0.006250, epoch: 0, step: 1300, eta: 106.38 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08083 sec, avg_samples: 16.00000, ips: 395.90394 images/sec
INFO:root:loss 43.8135, lr: 0.006250, epoch: 0, step: 1400, eta: 106.10 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08103 sec, avg_samples: 16.00000, ips: 394.89875 images/sec
Training: 2023-09-27 06:04:14,983 - loss 43.8135, lr: 0.006250, epoch: 0, step: 1400, eta: 106.10 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08103 sec, avg_samples: 16.00000, ips: 394.89875 images/sec
INFO:root:loss 43.9016, lr: 0.006250, epoch: 0, step: 1500, eta: 105.86 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08111 sec, avg_samples: 16.00000, ips: 394.50969 images/sec
Training: 2023-09-27 06:04:23,101 - loss 43.9016, lr: 0.006250, epoch: 0, step: 1500, eta: 105.86 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08111 sec, avg_samples: 16.00000, ips: 394.50969 images/sec
INFO:root:loss 43.7949, lr: 0.006250, epoch: 0, step: 1600, eta: 105.64 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08100 sec, avg_samples: 16.00000, ips: 395.04957 images/sec
Training: 2023-09-27 06:04:31,207 - loss 43.7949, lr: 0.006250, epoch: 0, step: 1600, eta: 105.64 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08100 sec, avg_samples: 16.00000, ips: 395.04957 images/sec
INFO:root:loss 43.8857, lr: 0.006250, epoch: 0, step: 1700, eta: 105.46 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08119 sec, avg_samples: 16.00000, ips: 394.13776 images/sec
Training: 2023-09-27 06:04:39,332 - loss 43.8857, lr: 0.006250, epoch: 0, step: 1700, eta: 105.46 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08119 sec, avg_samples: 16.00000, ips: 394.13776 images/sec
INFO:root:loss 43.7837, lr: 0.006250, epoch: 0, step: 1800, eta: 105.30 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08120 sec, avg_samples: 16.00000, ips: 394.06874 images/sec
Training: 2023-09-27 06:04:47,459 - loss 43.7837, lr: 0.006250, epoch: 0, step: 1800, eta: 105.30 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08120 sec, avg_samples: 16.00000, ips: 394.06874 images/sec
INFO:root:loss 43.6355, lr: 0.006250, epoch: 0, step: 1900, eta: 105.16 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08119 sec, avg_samples: 16.00000, ips: 394.12720 images/sec
Training: 2023-09-27 06:04:55,585 - loss 43.6355, lr: 0.006250, epoch: 0, step: 1900, eta: 105.16 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08119 sec, avg_samples: 16.00000, ips: 394.12720 images/sec
INFO:root:loss 43.7022, lr: 0.006250, epoch: 0, step: 2000, eta: 105.02 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08110 sec, avg_samples: 16.00000, ips: 394.56970 images/sec
Training: 2023-09-27 06:05:03,701 - loss 43.7022, lr: 0.006250, epoch: 0, step: 2000, eta: 105.02 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08110 sec, avg_samples: 16.00000, ips: 394.56970 images/sec
INFO:root:[lfw][2000]XNorm: 21.781275
Training: 2023-09-27 06:05:33,762 - [lfw][2000]XNorm: 21.781275
INFO:root:[lfw][2000]Accuracy-Flip: 0.69617+-0.01600
Training: 2023-09-27 06:05:33,762 - [lfw][2000]Accuracy-Flip: 0.69617+-0.01600
INFO:root:[lfw][2000]Accuracy-Highest: 0.69617
Training: 2023-09-27 06:05:33,762 - [lfw][2000]Accuracy-Highest: 0.69617
INFO:root:test time: 30.0616
Training: 2023-09-27 06:05:33,762 - test time: 30.0616
INFO:root:[cfp_fp][2000]XNorm: 22.309554
Training: 2023-09-27 06:06:08,673 - [cfp_fp][2000]XNorm: 22.309554
INFO:root:[cfp_fp][2000]Accuracy-Flip: 0.55457+-0.01505
Training: 2023-09-27 06:06:08,673 - [cfp_fp][2000]Accuracy-Flip: 0.55457+-0.01505
INFO:root:[cfp_fp][2000]Accuracy-Highest: 0.55457
Training: 2023-09-27 06:06:08,673 - [cfp_fp][2000]Accuracy-Highest: 0.55457
INFO:root:test time: 34.9108
Training: 2023-09-27 06:06:08,673 - test time: 34.9108
INFO:root:[agedb_30][2000]XNorm: 21.962322
Training: 2023-09-27 06:06:38,728 - [agedb_30][2000]XNorm: 21.962322
INFO:root:[agedb_30][2000]Accuracy-Flip: 0.53883+-0.01682
Training: 2023-09-27 06:06:38,729 - [agedb_30][2000]Accuracy-Flip: 0.53883+-0.01682
INFO:root:[agedb_30][2000]Accuracy-Highest: 0.53883
Training: 2023-09-27 06:06:38,729 - [agedb_30][2000]Accuracy-Highest: 0.53883
INFO:root:test time: 30.0553
Training: 2023-09-27 06:06:38,729 - test time: 30.0553
INFO:root:loss 43.7930, lr: 0.006250, epoch: 0, step: 2100, eta: 161.99 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08045 sec, avg_samples: 16.00000, ips: 397.74523 images/sec
Training: 2023-09-27 06:06:46,779 - loss 43.7930, lr: 0.006250, epoch: 0, step: 2100, eta: 161.99 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08045 sec, avg_samples: 16.00000, ips: 397.74523 images/sec
INFO:root:loss 43.6489, lr: 0.006250, epoch: 0, step: 2200, eta: 159.26 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08065 sec, avg_samples: 16.00000, ips: 396.78665 images/sec
Training: 2023-09-27 06:06:54,850 - loss 43.6489, lr: 0.006250, epoch: 0, step: 2200, eta: 159.26 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08065 sec, avg_samples: 16.00000, ips: 396.78665 images/sec
INFO:root:loss 43.6348, lr: 0.006250, epoch: 0, step: 2300, eta: 156.78 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08094 sec, avg_samples: 16.00000, ips: 395.36115 images/sec
Training: 2023-09-27 06:07:02,950 - loss 43.6348, lr: 0.006250, epoch: 0, step: 2300, eta: 156.78 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08094 sec, avg_samples: 16.00000, ips: 395.36115 images/sec
INFO:root:loss 43.6773, lr: 0.006250, epoch: 0, step: 2400, eta: 154.51 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08102 sec, avg_samples: 16.00000, ips: 394.95006 images/sec
Training: 2023-09-27 06:07:11,058 - loss 43.6773, lr: 0.006250, epoch: 0, step: 2400, eta: 154.51 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08102 sec, avg_samples: 16.00000, ips: 394.95006 images/sec
INFO:root:loss 43.6229, lr: 0.006250, epoch: 0, step: 2500, eta: 152.42 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08095 sec, avg_samples: 16.00000, ips: 395.28804 images/sec
Training: 2023-09-27 06:07:19,159 - loss 43.6229, lr: 0.006250, epoch: 0, step: 2500, eta: 152.42 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08095 sec, avg_samples: 16.00000, ips: 395.28804 images/sec
INFO:root:loss 43.6405, lr: 0.006250, epoch: 0, step: 2600, eta: 150.49 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08099 sec, avg_samples: 16.00000, ips: 395.11167 images/sec
Training: 2023-09-27 06:07:27,264 - loss 43.6405, lr: 0.006250, epoch: 0, step: 2600, eta: 150.49 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08099 sec, avg_samples: 16.00000, ips: 395.11167 images/sec
INFO:root:loss 43.6858, lr: 0.006250, epoch: 0, step: 2700, eta: 148.71 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08113 sec, avg_samples: 16.00000, ips: 394.44874 images/sec
Training: 2023-09-27 06:07:35,382 - loss 43.6858, lr: 0.006250, epoch: 0, step: 2700, eta: 148.71 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08113 sec, avg_samples: 16.00000, ips: 394.44874 images/sec
INFO:root:loss 43.6107, lr: 0.006250, epoch: 0, step: 2800, eta: 147.05 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08087 sec, avg_samples: 16.00000, ips: 395.71070 images/sec
Training: 2023-09-27 06:07:43,475 - loss 43.6107, lr: 0.006250, epoch: 0, step: 2800, eta: 147.05 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08087 sec, avg_samples: 16.00000, ips: 395.71070 images/sec
INFO:root:loss 43.5142, lr: 0.006250, epoch: 0, step: 2900, eta: 145.49 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08070 sec, avg_samples: 16.00000, ips: 396.54174 images/sec
Training: 2023-09-27 06:07:51,549 - loss 43.5142, lr: 0.006250, epoch: 0, step: 2900, eta: 145.49 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08070 sec, avg_samples: 16.00000, ips: 396.54174 images/sec
INFO:root:loss 43.4079, lr: 0.006250, epoch: 0, step: 3000, eta: 144.04 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08068 sec, avg_samples: 16.00000, ips: 396.63676 images/sec
Training: 2023-09-27 06:07:59,622 - loss 43.4079, lr: 0.006250, epoch: 0, step: 3000, eta: 144.04 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08068 sec, avg_samples: 16.00000, ips: 396.63676 images/sec
INFO:root:loss 43.5226, lr: 0.006250, epoch: 0, step: 3100, eta: 142.69 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08090 sec, avg_samples: 16.00000, ips: 395.52735 images/sec
Training: 2023-09-27 06:08:07,718 - loss 43.5226, lr: 0.006250, epoch: 0, step: 3100, eta: 142.69 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08090 sec, avg_samples: 16.00000, ips: 395.52735 images/sec
INFO:root:loss 43.4467, lr: 0.006250, epoch: 0, step: 3200, eta: 141.42 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08101 sec, avg_samples: 16.00000, ips: 395.01657 images/sec
Training: 2023-09-27 06:08:15,824 - loss 43.4467, lr: 0.006250, epoch: 0, step: 3200, eta: 141.42 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08101 sec, avg_samples: 16.00000, ips: 395.01657 images/sec
INFO:root:loss 43.4234, lr: 0.006250, epoch: 0, step: 3300, eta: 140.24 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08101 sec, avg_samples: 16.00000, ips: 395.03178 images/sec
Training: 2023-09-27 06:08:23,929 - loss 43.4234, lr: 0.006250, epoch: 0, step: 3300, eta: 140.24 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08101 sec, avg_samples: 16.00000, ips: 395.03178 images/sec
INFO:root:loss 43.3262, lr: 0.006250, epoch: 0, step: 3400, eta: 139.12 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08099 sec, avg_samples: 16.00000, ips: 395.13143 images/sec
Training: 2023-09-27 06:08:32,033 - loss 43.3262, lr: 0.006250, epoch: 0, step: 3400, eta: 139.12 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08099 sec, avg_samples: 16.00000, ips: 395.13143 images/sec
INFO:root:loss 43.3995, lr: 0.006250, epoch: 0, step: 3500, eta: 138.07 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08119 sec, avg_samples: 16.00000, ips: 394.13506 images/sec
Training: 2023-09-27 06:08:40,160 - loss 43.3995, lr: 0.006250, epoch: 0, step: 3500, eta: 138.07 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08119 sec, avg_samples: 16.00000, ips: 394.13506 images/sec
INFO:root:loss 43.2851, lr: 0.006250, epoch: 0, step: 3600, eta: 137.08 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08113 sec, avg_samples: 16.00000, ips: 394.43030 images/sec
Training: 2023-09-27 06:08:48,279 - loss 43.2851, lr: 0.006250, epoch: 0, step: 3600, eta: 137.08 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08113 sec, avg_samples: 16.00000, ips: 394.43030 images/sec
INFO:root:loss 43.2577, lr: 0.006250, epoch: 0, step: 3700, eta: 136.14 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08108 sec, avg_samples: 16.00000, ips: 394.67954 images/sec
Training: 2023-09-27 06:08:56,393 - loss 43.2577, lr: 0.006250, epoch: 0, step: 3700, eta: 136.14 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08108 sec, avg_samples: 16.00000, ips: 394.67954 images/sec
INFO:root:loss 43.2497, lr: 0.006250, epoch: 0, step: 3800, eta: 135.25 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08105 sec, avg_samples: 16.00000, ips: 394.83617 images/sec
Training: 2023-09-27 06:09:04,503 - loss 43.2497, lr: 0.006250, epoch: 0, step: 3800, eta: 135.25 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08105 sec, avg_samples: 16.00000, ips: 394.83617 images/sec
INFO:root:loss 43.2827, lr: 0.006250, epoch: 0, step: 3900, eta: 134.41 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08117 sec, avg_samples: 16.00000, ips: 394.25105 images/sec
Training: 2023-09-27 06:09:12,626 - loss 43.2827, lr: 0.006250, epoch: 0, step: 3900, eta: 134.41 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08117 sec, avg_samples: 16.00000, ips: 394.25105 images/sec
INFO:root:loss 43.1924, lr: 0.006250, epoch: 0, step: 4000, eta: 133.62 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08136 sec, avg_samples: 16.00000, ips: 393.30781 images/sec
Training: 2023-09-27 06:09:20,768 - loss 43.1924, lr: 0.006250, epoch: 0, step: 4000, eta: 133.62 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08136 sec, avg_samples: 16.00000, ips: 393.30781 images/sec
INFO:root:[lfw][4000]XNorm: 19.989206
Training: 2023-09-27 06:09:50,822 - [lfw][4000]XNorm: 19.989206
INFO:root:[lfw][4000]Accuracy-Flip: 0.74133+-0.01533
Training: 2023-09-27 06:09:50,822 - [lfw][4000]Accuracy-Flip: 0.74133+-0.01533
INFO:root:[lfw][4000]Accuracy-Highest: 0.74133
Training: 2023-09-27 06:09:50,822 - [lfw][4000]Accuracy-Highest: 0.74133
INFO:root:test time: 30.0526
Training: 2023-09-27 06:09:50,822 - test time: 30.0526
INFO:root:[cfp_fp][4000]XNorm: 19.595640
Training: 2023-09-27 06:10:25,763 - [cfp_fp][4000]XNorm: 19.595640
INFO:root:[cfp_fp][4000]Accuracy-Flip: 0.56543+-0.01335
Training: 2023-09-27 06:10:25,763 - [cfp_fp][4000]Accuracy-Flip: 0.56543+-0.01335
INFO:root:[cfp_fp][4000]Accuracy-Highest: 0.56543
Training: 2023-09-27 06:10:25,763 - [cfp_fp][4000]Accuracy-Highest: 0.56543
INFO:root:test time: 34.9413
Training: 2023-09-27 06:10:25,763 - test time: 34.9413
INFO:root:[agedb_30][4000]XNorm: 20.081696
Training: 2023-09-27 06:10:55,817 - [agedb_30][4000]XNorm: 20.081696
INFO:root:[agedb_30][4000]Accuracy-Flip: 0.56867+-0.01132
Training: 2023-09-27 06:10:55,817 - [agedb_30][4000]Accuracy-Flip: 0.56867+-0.01132
INFO:root:[agedb_30][4000]Accuracy-Highest: 0.56867
Training: 2023-09-27 06:10:55,817 - [agedb_30][4000]Accuracy-Highest: 0.56867
INFO:root:test time: 30.0541
Training: 2023-09-27 06:10:55,817 - test time: 30.0541
INFO:root:loss 43.2158, lr: 0.006250, epoch: 0, step: 4100, eta: 162.12 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08115 sec, avg_samples: 16.00000, ips: 394.33823 images/sec
Training: 2023-09-27 06:11:03,938 - loss 43.2158, lr: 0.006250, epoch: 0, step: 4100, eta: 162.12 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08115 sec, avg_samples: 16.00000, ips: 394.33823 images/sec
INFO:root:loss 43.2380, lr: 0.006250, epoch: 0, step: 4200, eta: 160.70 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08128 sec, avg_samples: 16.00000, ips: 393.68984 images/sec
Training: 2023-09-27 06:11:12,072 - loss 43.2380, lr: 0.006250, epoch: 0, step: 4200, eta: 160.70 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08128 sec, avg_samples: 16.00000, ips: 393.68984 images/sec
INFO:root:loss 43.2617, lr: 0.006250, epoch: 0, step: 4300, eta: 159.35 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08134 sec, avg_samples: 16.00000, ips: 393.39552 images/sec
Training: 2023-09-27 06:11:20,212 - loss 43.2617, lr: 0.006250, epoch: 0, step: 4300, eta: 159.35 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08134 sec, avg_samples: 16.00000, ips: 393.39552 images/sec
INFO:root:loss 43.1883, lr: 0.006250, epoch: 0, step: 4400, eta: 158.06 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08130 sec, avg_samples: 16.00000, ips: 393.59113 images/sec
Training: 2023-09-27 06:11:28,349 - loss 43.1883, lr: 0.006250, epoch: 0, step: 4400, eta: 158.06 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08130 sec, avg_samples: 16.00000, ips: 393.59113 images/sec
INFO:root:loss 43.1211, lr: 0.006250, epoch: 0, step: 4500, eta: 156.82 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08131 sec, avg_samples: 16.00000, ips: 393.56073 images/sec
Training: 2023-09-27 06:11:36,485 - loss 43.1211, lr: 0.006250, epoch: 0, step: 4500, eta: 156.82 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08131 sec, avg_samples: 16.00000, ips: 393.56073 images/sec
INFO:root:loss 43.0890, lr: 0.006250, epoch: 0, step: 4600, eta: 155.65 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08138 sec, avg_samples: 16.00000, ips: 393.22228 images/sec
Training: 2023-09-27 06:11:44,629 - loss 43.0890, lr: 0.006250, epoch: 0, step: 4600, eta: 155.65 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08138 sec, avg_samples: 16.00000, ips: 393.22228 images/sec
INFO:root:loss 43.0206, lr: 0.006250, epoch: 0, step: 4700, eta: 154.52 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08134 sec, avg_samples: 16.00000, ips: 393.43316 images/sec
Training: 2023-09-27 06:11:52,769 - loss 43.0206, lr: 0.006250, epoch: 0, step: 4700, eta: 154.52 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08134 sec, avg_samples: 16.00000, ips: 393.43316 images/sec
INFO:root:loss 42.9767, lr: 0.006250, epoch: 0, step: 4800, eta: 153.44 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08133 sec, avg_samples: 16.00000, ips: 393.43798 images/sec
Training: 2023-09-27 06:12:00,908 - loss 42.9767, lr: 0.006250, epoch: 0, step: 4800, eta: 153.44 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08133 sec, avg_samples: 16.00000, ips: 393.43798 images/sec
INFO:root:loss 42.9574, lr: 0.006250, epoch: 0, step: 4900, eta: 152.40 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08137 sec, avg_samples: 16.00000, ips: 393.25127 images/sec
Training: 2023-09-27 06:12:09,051 - loss 42.9574, lr: 0.006250, epoch: 0, step: 4900, eta: 152.40 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08137 sec, avg_samples: 16.00000, ips: 393.25127 images/sec
INFO:root:loss 43.0479, lr: 0.006250, epoch: 0, step: 5000, eta: 151.40 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08136 sec, avg_samples: 16.00000, ips: 393.29308 images/sec
Training: 2023-09-27 06:12:17,194 - loss 43.0479, lr: 0.006250, epoch: 0, step: 5000, eta: 151.40 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08136 sec, avg_samples: 16.00000, ips: 393.29308 images/sec
INFO:root:loss 42.8617, lr: 0.006250, epoch: 0, step: 5100, eta: 150.45 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08138 sec, avg_samples: 16.00000, ips: 393.23212 images/sec
Training: 2023-09-27 06:12:25,338 - loss 42.8617, lr: 0.006250, epoch: 0, step: 5100, eta: 150.45 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08138 sec, avg_samples: 16.00000, ips: 393.23212 images/sec
INFO:root:loss 42.9070, lr: 0.006250, epoch: 0, step: 5200, eta: 149.53 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08139 sec, avg_samples: 16.00000, ips: 393.17812 images/sec
Training: 2023-09-27 06:12:33,482 - loss 42.9070, lr: 0.006250, epoch: 0, step: 5200, eta: 149.53 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08139 sec, avg_samples: 16.00000, ips: 393.17812 images/sec
INFO:root:loss 42.8255, lr: 0.006250, epoch: 0, step: 5300, eta: 148.64 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08144 sec, avg_samples: 16.00000, ips: 392.93453 images/sec
Training: 2023-09-27 06:12:41,632 - loss 42.8255, lr: 0.006250, epoch: 0, step: 5300, eta: 148.64 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08144 sec, avg_samples: 16.00000, ips: 392.93453 images/sec
INFO:root:loss 42.8711, lr: 0.006250, epoch: 0, step: 5400, eta: 147.79 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08131 sec, avg_samples: 16.00000, ips: 393.56588 images/sec
Training: 2023-09-27 06:12:49,769 - loss 42.8711, lr: 0.006250, epoch: 0, step: 5400, eta: 147.79 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08131 sec, avg_samples: 16.00000, ips: 393.56588 images/sec
INFO:root:loss 42.8179, lr: 0.006250, epoch: 0, step: 5500, eta: 146.96 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08099 sec, avg_samples: 16.00000, ips: 395.10403 images/sec
Training: 2023-09-27 06:12:57,874 - loss 42.8179, lr: 0.006250, epoch: 0, step: 5500, eta: 146.96 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08099 sec, avg_samples: 16.00000, ips: 395.10403 images/sec
INFO:root:loss 42.8751, lr: 0.006250, epoch: 0, step: 5600, eta: 146.16 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08095 sec, avg_samples: 16.00000, ips: 395.28476 images/sec
Training: 2023-09-27 06:13:05,975 - loss 42.8751, lr: 0.006250, epoch: 0, step: 5600, eta: 146.16 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08095 sec, avg_samples: 16.00000, ips: 395.28476 images/sec
INFO:root:loss 42.6940, lr: 0.006250, epoch: 0, step: 5700, eta: 145.39 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08101 sec, avg_samples: 16.00000, ips: 395.03093 images/sec
Training: 2023-09-27 06:13:14,082 - loss 42.6940, lr: 0.006250, epoch: 0, step: 5700, eta: 145.39 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08101 sec, avg_samples: 16.00000, ips: 395.03093 images/sec
INFO:root:loss 42.7377, lr: 0.006250, epoch: 0, step: 5800, eta: 144.64 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08097 sec, avg_samples: 16.00000, ips: 395.22478 images/sec
Training: 2023-09-27 06:13:22,185 - loss 42.7377, lr: 0.006250, epoch: 0, step: 5800, eta: 144.64 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08097 sec, avg_samples: 16.00000, ips: 395.22478 images/sec
INFO:root:loss 42.6787, lr: 0.006250, epoch: 0, step: 5900, eta: 143.93 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08135 sec, avg_samples: 16.00000, ips: 393.38125 images/sec
Training: 2023-09-27 06:13:30,326 - loss 42.6787, lr: 0.006250, epoch: 0, step: 5900, eta: 143.93 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08135 sec, avg_samples: 16.00000, ips: 393.38125 images/sec
INFO:root:loss 42.5534, lr: 0.006250, epoch: 0, step: 6000, eta: 143.24 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08158 sec, avg_samples: 16.00000, ips: 392.26485 images/sec
Training: 2023-09-27 06:13:38,491 - loss 42.5534, lr: 0.006250, epoch: 0, step: 6000, eta: 143.24 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08158 sec, avg_samples: 16.00000, ips: 392.26485 images/sec
INFO:root:[lfw][6000]XNorm: 19.320402
Training: 2023-09-27 06:14:08,568 - [lfw][6000]XNorm: 19.320402
INFO:root:[lfw][6000]Accuracy-Flip: 0.78433+-0.01587
Training: 2023-09-27 06:14:08,568 - [lfw][6000]Accuracy-Flip: 0.78433+-0.01587
INFO:root:[lfw][6000]Accuracy-Highest: 0.78433
Training: 2023-09-27 06:14:08,568 - [lfw][6000]Accuracy-Highest: 0.78433
INFO:root:test time: 30.0770
Training: 2023-09-27 06:14:08,568 - test time: 30.0770
INFO:root:[cfp_fp][6000]XNorm: 18.675342
Training: 2023-09-27 06:14:43,485 - [cfp_fp][6000]XNorm: 18.675342
INFO:root:[cfp_fp][6000]Accuracy-Flip: 0.58986+-0.00999
Training: 2023-09-27 06:14:43,485 - [cfp_fp][6000]Accuracy-Flip: 0.58986+-0.00999
INFO:root:[cfp_fp][6000]Accuracy-Highest: 0.58986
Training: 2023-09-27 06:14:43,485 - [cfp_fp][6000]Accuracy-Highest: 0.58986
INFO:root:test time: 34.9174
Training: 2023-09-27 06:14:43,485 - test time: 34.9174
INFO:root:[agedb_30][6000]XNorm: 19.282434
Training: 2023-09-27 06:15:13,492 - [agedb_30][6000]XNorm: 19.282434
INFO:root:[agedb_30][6000]Accuracy-Flip: 0.60133+-0.01612
Training: 2023-09-27 06:15:13,492 - [agedb_30][6000]Accuracy-Flip: 0.60133+-0.01612
INFO:root:[agedb_30][6000]Accuracy-Highest: 0.60133
Training: 2023-09-27 06:15:13,492 - [agedb_30][6000]Accuracy-Highest: 0.60133
INFO:root:test time: 30.0063
Training: 2023-09-27 06:15:13,492 - test time: 30.0063
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 6100, eta: 162.21 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08077 sec, avg_samples: 16.00000, ips: 396.16698 images/sec
Training: 2023-09-27 06:15:21,574 - loss nan, lr: 0.006250, epoch: 0, step: 6100, eta: 162.21 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08077 sec, avg_samples: 16.00000, ips: 396.16698 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 6200, eta: 161.24 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08091 sec, avg_samples: 16.00000, ips: 395.51369 images/sec
Training: 2023-09-27 06:15:29,670 - loss nan, lr: 0.006250, epoch: 0, step: 6200, eta: 161.24 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08091 sec, avg_samples: 16.00000, ips: 395.51369 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 6300, eta: 160.31 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08128 sec, avg_samples: 16.00000, ips: 393.71417 images/sec
Training: 2023-09-27 06:15:37,803 - loss nan, lr: 0.006250, epoch: 0, step: 6300, eta: 160.31 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08128 sec, avg_samples: 16.00000, ips: 393.71417 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 6400, eta: 159.40 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08143 sec, avg_samples: 16.00000, ips: 392.98429 images/sec
Training: 2023-09-27 06:15:45,952 - loss nan, lr: 0.006250, epoch: 0, step: 6400, eta: 159.40 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08143 sec, avg_samples: 16.00000, ips: 392.98429 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 6500, eta: 158.53 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08131 sec, avg_samples: 16.00000, ips: 393.54595 images/sec
Training: 2023-09-27 06:15:54,089 - loss nan, lr: 0.006250, epoch: 0, step: 6500, eta: 158.53 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08131 sec, avg_samples: 16.00000, ips: 393.54595 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 6600, eta: 157.68 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08129 sec, avg_samples: 16.00000, ips: 393.63546 images/sec
Training: 2023-09-27 06:16:02,224 - loss nan, lr: 0.006250, epoch: 0, step: 6600, eta: 157.68 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08129 sec, avg_samples: 16.00000, ips: 393.63546 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 6700, eta: 156.85 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08125 sec, avg_samples: 16.00000, ips: 393.85412 images/sec
Training: 2023-09-27 06:16:10,355 - loss nan, lr: 0.006250, epoch: 0, step: 6700, eta: 156.85 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08125 sec, avg_samples: 16.00000, ips: 393.85412 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 6800, eta: 156.05 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08121 sec, avg_samples: 16.00000, ips: 394.04557 images/sec
Training: 2023-09-27 06:16:18,481 - loss nan, lr: 0.006250, epoch: 0, step: 6800, eta: 156.05 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08121 sec, avg_samples: 16.00000, ips: 394.04557 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 6900, eta: 155.27 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08114 sec, avg_samples: 16.00000, ips: 394.35915 images/sec
Training: 2023-09-27 06:16:26,601 - loss nan, lr: 0.006250, epoch: 0, step: 6900, eta: 155.27 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08114 sec, avg_samples: 16.00000, ips: 394.35915 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 7000, eta: 154.51 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08125 sec, avg_samples: 16.00000, ips: 393.87010 images/sec
Training: 2023-09-27 06:16:34,730 - loss nan, lr: 0.006250, epoch: 0, step: 7000, eta: 154.51 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08125 sec, avg_samples: 16.00000, ips: 393.87010 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 7100, eta: 153.78 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08125 sec, avg_samples: 16.00000, ips: 393.85951 images/sec
Training: 2023-09-27 06:16:42,860 - loss nan, lr: 0.006250, epoch: 0, step: 7100, eta: 153.78 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08125 sec, avg_samples: 16.00000, ips: 393.85951 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 7200, eta: 153.06 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08117 sec, avg_samples: 16.00000, ips: 394.25614 images/sec
Training: 2023-09-27 06:16:50,982 - loss nan, lr: 0.006250, epoch: 0, step: 7200, eta: 153.06 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08117 sec, avg_samples: 16.00000, ips: 394.25614 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 7300, eta: 152.37 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08123 sec, avg_samples: 16.00000, ips: 393.92405 images/sec
Training: 2023-09-27 06:16:59,111 - loss nan, lr: 0.006250, epoch: 0, step: 7300, eta: 152.37 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08123 sec, avg_samples: 16.00000, ips: 393.92405 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 7400, eta: 151.70 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08153 sec, avg_samples: 16.00000, ips: 392.48782 images/sec
Training: 2023-09-27 06:17:07,271 - loss nan, lr: 0.006250, epoch: 0, step: 7400, eta: 151.70 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08153 sec, avg_samples: 16.00000, ips: 392.48782 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 7500, eta: 151.04 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08158 sec, avg_samples: 16.00000, ips: 392.27636 images/sec
Training: 2023-09-27 06:17:15,434 - loss nan, lr: 0.006250, epoch: 0, step: 7500, eta: 151.04 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08158 sec, avg_samples: 16.00000, ips: 392.27636 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 7600, eta: 150.41 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08143 sec, avg_samples: 16.00000, ips: 392.97533 images/sec
Training: 2023-09-27 06:17:23,583 - loss nan, lr: 0.006250, epoch: 0, step: 7600, eta: 150.41 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08143 sec, avg_samples: 16.00000, ips: 392.97533 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 7700, eta: 149.79 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08150 sec, avg_samples: 16.00000, ips: 392.61999 images/sec
Training: 2023-09-27 06:17:31,739 - loss nan, lr: 0.006250, epoch: 0, step: 7700, eta: 149.79 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08150 sec, avg_samples: 16.00000, ips: 392.61999 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 7800, eta: 149.18 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08156 sec, avg_samples: 16.00000, ips: 392.37308 images/sec
Training: 2023-09-27 06:17:39,902 - loss nan, lr: 0.006250, epoch: 0, step: 7800, eta: 149.18 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08156 sec, avg_samples: 16.00000, ips: 392.37308 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 7900, eta: 148.59 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08144 sec, avg_samples: 16.00000, ips: 392.91515 images/sec
Training: 2023-09-27 06:17:48,053 - loss nan, lr: 0.006250, epoch: 0, step: 7900, eta: 148.59 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08144 sec, avg_samples: 16.00000, ips: 392.91515 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 8000, eta: 148.02 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08137 sec, avg_samples: 16.00000, ips: 393.28314 images/sec
Training: 2023-09-27 06:17:56,196 - loss nan, lr: 0.006250, epoch: 0, step: 8000, eta: 148.02 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08137 sec, avg_samples: 16.00000, ips: 393.28314 images/sec
INFO:root:[lfw][8000]XNorm: 17.740854
Training: 2023-09-27 06:18:26,246 - [lfw][8000]XNorm: 17.740854
INFO:root:[lfw][8000]Accuracy-Flip: 0.79050+-0.01986
Training: 2023-09-27 06:18:26,246 - [lfw][8000]Accuracy-Flip: 0.79050+-0.01986
INFO:root:[lfw][8000]Accuracy-Highest: 0.79050
Training: 2023-09-27 06:18:26,246 - [lfw][8000]Accuracy-Highest: 0.79050
INFO:root:test time: 30.0495
Training: 2023-09-27 06:18:26,246 - test time: 30.0495
INFO:root:[cfp_fp][8000]XNorm: 17.304387
Training: 2023-09-27 06:19:01,191 - [cfp_fp][8000]XNorm: 17.304387
INFO:root:[cfp_fp][8000]Accuracy-Flip: 0.57500+-0.01231
Training: 2023-09-27 06:19:01,191 - [cfp_fp][8000]Accuracy-Flip: 0.57500+-0.01231
INFO:root:[cfp_fp][8000]Accuracy-Highest: 0.58986
Training: 2023-09-27 06:19:01,191 - [cfp_fp][8000]Accuracy-Highest: 0.58986
INFO:root:test time: 34.9453
Training: 2023-09-27 06:19:01,191 - test time: 34.9453
INFO:root:[agedb_30][8000]XNorm: 17.807431
Training: 2023-09-27 06:19:31,267 - [agedb_30][8000]XNorm: 17.807431
INFO:root:[agedb_30][8000]Accuracy-Flip: 0.60100+-0.01551
Training: 2023-09-27 06:19:31,267 - [agedb_30][8000]Accuracy-Flip: 0.60100+-0.01551
INFO:root:[agedb_30][8000]Accuracy-Highest: 0.60133
Training: 2023-09-27 06:19:31,267 - [agedb_30][8000]Accuracy-Highest: 0.60133
INFO:root:test time: 30.0758
Training: 2023-09-27 06:19:31,267 - test time: 30.0758
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 8100, eta: 162.26 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08132 sec, avg_samples: 16.00000, ips: 393.48982 images/sec
Training: 2023-09-27 06:19:39,406 - loss nan, lr: 0.006250, epoch: 0, step: 8100, eta: 162.26 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08132 sec, avg_samples: 16.00000, ips: 393.48982 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 8200, eta: 161.53 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08130 sec, avg_samples: 16.00000, ips: 393.60089 images/sec
Training: 2023-09-27 06:19:47,542 - loss nan, lr: 0.006250, epoch: 0, step: 8200, eta: 161.53 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08130 sec, avg_samples: 16.00000, ips: 393.60089 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 8300, eta: 160.81 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08132 sec, avg_samples: 16.00000, ips: 393.52128 images/sec
Training: 2023-09-27 06:19:55,681 - loss nan, lr: 0.006250, epoch: 0, step: 8300, eta: 160.81 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08132 sec, avg_samples: 16.00000, ips: 393.52128 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 8400, eta: 160.12 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08137 sec, avg_samples: 16.00000, ips: 393.24304 images/sec
Training: 2023-09-27 06:20:03,824 - loss nan, lr: 0.006250, epoch: 0, step: 8400, eta: 160.12 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08137 sec, avg_samples: 16.00000, ips: 393.24304 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 8500, eta: 159.43 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08111 sec, avg_samples: 16.00000, ips: 394.51773 images/sec
Training: 2023-09-27 06:20:11,941 - loss nan, lr: 0.006250, epoch: 0, step: 8500, eta: 159.43 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08111 sec, avg_samples: 16.00000, ips: 394.51773 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 8600, eta: 158.78 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.08163 sec, avg_samples: 16.00000, ips: 392.00513 images/sec
Training: 2023-09-27 06:20:20,111 - loss nan, lr: 0.006250, epoch: 0, step: 8600, eta: 158.78 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.08163 sec, avg_samples: 16.00000, ips: 392.00513 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 8700, eta: 158.13 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08161 sec, avg_samples: 16.00000, ips: 392.11466 images/sec
Training: 2023-09-27 06:20:28,280 - loss nan, lr: 0.006250, epoch: 0, step: 8700, eta: 158.13 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08161 sec, avg_samples: 16.00000, ips: 392.11466 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 8800, eta: 157.50 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08152 sec, avg_samples: 16.00000, ips: 392.55957 images/sec
Training: 2023-09-27 06:20:36,438 - loss nan, lr: 0.006250, epoch: 0, step: 8800, eta: 157.50 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08152 sec, avg_samples: 16.00000, ips: 392.55957 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 8900, eta: 156.88 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08129 sec, avg_samples: 16.00000, ips: 393.63398 images/sec
Training: 2023-09-27 06:20:44,573 - loss nan, lr: 0.006250, epoch: 0, step: 8900, eta: 156.88 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08129 sec, avg_samples: 16.00000, ips: 393.63398 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 9000, eta: 156.27 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08141 sec, avg_samples: 16.00000, ips: 393.09303 images/sec
Training: 2023-09-27 06:20:52,720 - loss nan, lr: 0.006250, epoch: 0, step: 9000, eta: 156.27 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08141 sec, avg_samples: 16.00000, ips: 393.09303 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 9100, eta: 155.68 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08141 sec, avg_samples: 16.00000, ips: 393.05594 images/sec
Training: 2023-09-27 06:21:00,869 - loss nan, lr: 0.006250, epoch: 0, step: 9100, eta: 155.68 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08141 sec, avg_samples: 16.00000, ips: 393.05594 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 9200, eta: 155.10 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08125 sec, avg_samples: 16.00000, ips: 393.86758 images/sec
Training: 2023-09-27 06:21:08,999 - loss nan, lr: 0.006250, epoch: 0, step: 9200, eta: 155.10 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08125 sec, avg_samples: 16.00000, ips: 393.86758 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 9300, eta: 154.53 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08109 sec, avg_samples: 16.00000, ips: 394.63616 images/sec
Training: 2023-09-27 06:21:17,112 - loss nan, lr: 0.006250, epoch: 0, step: 9300, eta: 154.53 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08109 sec, avg_samples: 16.00000, ips: 394.63616 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 9400, eta: 153.98 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08133 sec, avg_samples: 16.00000, ips: 393.44780 images/sec
Training: 2023-09-27 06:21:25,250 - loss nan, lr: 0.006250, epoch: 0, step: 9400, eta: 153.98 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08133 sec, avg_samples: 16.00000, ips: 393.44780 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 9500, eta: 153.43 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08134 sec, avg_samples: 16.00000, ips: 393.40580 images/sec
Training: 2023-09-27 06:21:33,389 - loss nan, lr: 0.006250, epoch: 0, step: 9500, eta: 153.43 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08134 sec, avg_samples: 16.00000, ips: 393.40580 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 9600, eta: 152.90 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08138 sec, avg_samples: 16.00000, ips: 393.22643 images/sec
Training: 2023-09-27 06:21:41,532 - loss nan, lr: 0.006250, epoch: 0, step: 9600, eta: 152.90 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08138 sec, avg_samples: 16.00000, ips: 393.22643 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 9700, eta: 152.38 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08123 sec, avg_samples: 16.00000, ips: 393.96170 images/sec
Training: 2023-09-27 06:21:49,660 - loss nan, lr: 0.006250, epoch: 0, step: 9700, eta: 152.38 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08123 sec, avg_samples: 16.00000, ips: 393.96170 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 9800, eta: 151.86 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08106 sec, avg_samples: 16.00000, ips: 394.75481 images/sec
Training: 2023-09-27 06:21:57,772 - loss nan, lr: 0.006250, epoch: 0, step: 9800, eta: 151.86 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08106 sec, avg_samples: 16.00000, ips: 394.75481 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 9900, eta: 151.36 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08106 sec, avg_samples: 16.00000, ips: 394.79218 images/sec
Training: 2023-09-27 06:22:05,882 - loss nan, lr: 0.006250, epoch: 0, step: 9900, eta: 151.36 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08106 sec, avg_samples: 16.00000, ips: 394.79218 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 10000, eta: 150.87 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08130 sec, avg_samples: 16.00000, ips: 393.62428 images/sec
Training: 2023-09-27 06:22:14,017 - loss nan, lr: 0.006250, epoch: 0, step: 10000, eta: 150.87 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08130 sec, avg_samples: 16.00000, ips: 393.62428 images/sec
INFO:root:[lfw][10000]XNorm: 16.675365
Training: 2023-09-27 06:22:44,084 - [lfw][10000]XNorm: 16.675365
INFO:root:[lfw][10000]Accuracy-Flip: 0.78967+-0.01872
Training: 2023-09-27 06:22:44,084 - [lfw][10000]Accuracy-Flip: 0.78967+-0.01872
INFO:root:[lfw][10000]Accuracy-Highest: 0.79050
Training: 2023-09-27 06:22:44,084 - [lfw][10000]Accuracy-Highest: 0.79050
INFO:root:test time: 30.0666
Training: 2023-09-27 06:22:44,084 - test time: 30.0666
INFO:root:[cfp_fp][10000]XNorm: 16.269332
Training: 2023-09-27 06:23:19,046 - [cfp_fp][10000]XNorm: 16.269332
INFO:root:[cfp_fp][10000]Accuracy-Flip: 0.58529+-0.01551
Training: 2023-09-27 06:23:19,047 - [cfp_fp][10000]Accuracy-Flip: 0.58529+-0.01551
INFO:root:[cfp_fp][10000]Accuracy-Highest: 0.58986
Training: 2023-09-27 06:23:19,047 - [cfp_fp][10000]Accuracy-Highest: 0.58986
INFO:root:test time: 34.9626
Training: 2023-09-27 06:23:19,047 - test time: 34.9626
INFO:root:[agedb_30][10000]XNorm: 16.763030
Training: 2023-09-27 06:23:49,115 - [agedb_30][10000]XNorm: 16.763030
INFO:root:[agedb_30][10000]Accuracy-Flip: 0.59267+-0.01786
Training: 2023-09-27 06:23:49,115 - [agedb_30][10000]Accuracy-Flip: 0.59267+-0.01786
INFO:root:[agedb_30][10000]Accuracy-Highest: 0.60133
Training: 2023-09-27 06:23:49,115 - [agedb_30][10000]Accuracy-Highest: 0.60133
INFO:root:test time: 30.0688
Training: 2023-09-27 06:23:49,115 - test time: 30.0688
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 10100, eta: 162.25 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08098 sec, avg_samples: 16.00000, ips: 395.17871 images/sec
Training: 2023-09-27 06:23:57,219 - loss nan, lr: 0.006250, epoch: 0, step: 10100, eta: 162.25 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08098 sec, avg_samples: 16.00000, ips: 395.17871 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 10200, eta: 161.66 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08098 sec, avg_samples: 16.00000, ips: 395.15940 images/sec
Training: 2023-09-27 06:24:05,323 - loss nan, lr: 0.006250, epoch: 0, step: 10200, eta: 161.66 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08098 sec, avg_samples: 16.00000, ips: 395.15940 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 10300, eta: 161.08 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08102 sec, avg_samples: 16.00000, ips: 394.97318 images/sec
Training: 2023-09-27 06:24:13,431 - loss nan, lr: 0.006250, epoch: 0, step: 10300, eta: 161.08 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08102 sec, avg_samples: 16.00000, ips: 394.97318 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 10400, eta: 160.51 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08117 sec, avg_samples: 16.00000, ips: 394.22387 images/sec
Training: 2023-09-27 06:24:21,554 - loss nan, lr: 0.006250, epoch: 0, step: 10400, eta: 160.51 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08117 sec, avg_samples: 16.00000, ips: 394.22387 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 10500, eta: 159.95 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08115 sec, avg_samples: 16.00000, ips: 394.34743 images/sec
Training: 2023-09-27 06:24:29,675 - loss nan, lr: 0.006250, epoch: 0, step: 10500, eta: 159.95 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08115 sec, avg_samples: 16.00000, ips: 394.34743 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 10600, eta: 159.40 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08095 sec, avg_samples: 16.00000, ips: 395.28223 images/sec
Training: 2023-09-27 06:24:37,775 - loss nan, lr: 0.006250, epoch: 0, step: 10600, eta: 159.40 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08095 sec, avg_samples: 16.00000, ips: 395.28223 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 10700, eta: 158.87 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08098 sec, avg_samples: 16.00000, ips: 395.13817 images/sec
Training: 2023-09-27 06:24:45,879 - loss nan, lr: 0.006250, epoch: 0, step: 10700, eta: 158.87 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08098 sec, avg_samples: 16.00000, ips: 395.13817 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 10800, eta: 158.34 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08107 sec, avg_samples: 16.00000, ips: 394.72150 images/sec
Training: 2023-09-27 06:24:53,991 - loss nan, lr: 0.006250, epoch: 0, step: 10800, eta: 158.34 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08107 sec, avg_samples: 16.00000, ips: 394.72150 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 10900, eta: 157.82 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08131 sec, avg_samples: 16.00000, ips: 393.56239 images/sec
Training: 2023-09-27 06:25:02,127 - loss nan, lr: 0.006250, epoch: 0, step: 10900, eta: 157.82 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08131 sec, avg_samples: 16.00000, ips: 393.56239 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 11000, eta: 157.32 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08121 sec, avg_samples: 16.00000, ips: 394.04075 images/sec
Training: 2023-09-27 06:25:10,253 - loss nan, lr: 0.006250, epoch: 0, step: 11000, eta: 157.32 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08121 sec, avg_samples: 16.00000, ips: 394.04075 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 11100, eta: 156.82 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08113 sec, avg_samples: 16.00000, ips: 394.44248 images/sec
Training: 2023-09-27 06:25:18,371 - loss nan, lr: 0.006250, epoch: 0, step: 11100, eta: 156.82 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08113 sec, avg_samples: 16.00000, ips: 394.44248 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 11200, eta: 156.33 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08126 sec, avg_samples: 16.00000, ips: 393.79823 images/sec
Training: 2023-09-27 06:25:26,503 - loss nan, lr: 0.006250, epoch: 0, step: 11200, eta: 156.33 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08126 sec, avg_samples: 16.00000, ips: 393.79823 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 11300, eta: 155.85 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08115 sec, avg_samples: 16.00000, ips: 394.34594 images/sec
Training: 2023-09-27 06:25:34,623 - loss nan, lr: 0.006250, epoch: 0, step: 11300, eta: 155.85 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08115 sec, avg_samples: 16.00000, ips: 394.34594 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 11400, eta: 155.38 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08111 sec, avg_samples: 16.00000, ips: 394.53656 images/sec
Training: 2023-09-27 06:25:42,740 - loss nan, lr: 0.006250, epoch: 0, step: 11400, eta: 155.38 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08111 sec, avg_samples: 16.00000, ips: 394.53656 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 11500, eta: 154.91 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08100 sec, avg_samples: 16.00000, ips: 395.04286 images/sec
Training: 2023-09-27 06:25:50,845 - loss nan, lr: 0.006250, epoch: 0, step: 11500, eta: 154.91 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08100 sec, avg_samples: 16.00000, ips: 395.04286 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 11600, eta: 154.45 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08105 sec, avg_samples: 16.00000, ips: 394.81034 images/sec
Training: 2023-09-27 06:25:58,954 - loss nan, lr: 0.006250, epoch: 0, step: 11600, eta: 154.45 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08105 sec, avg_samples: 16.00000, ips: 394.81034 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 11700, eta: 154.00 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08113 sec, avg_samples: 16.00000, ips: 394.43143 images/sec
Training: 2023-09-27 06:26:07,072 - loss nan, lr: 0.006250, epoch: 0, step: 11700, eta: 154.00 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08113 sec, avg_samples: 16.00000, ips: 394.43143 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 11800, eta: 153.56 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08114 sec, avg_samples: 16.00000, ips: 394.38324 images/sec
Training: 2023-09-27 06:26:15,190 - loss nan, lr: 0.006250, epoch: 0, step: 11800, eta: 153.56 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08114 sec, avg_samples: 16.00000, ips: 394.38324 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 11900, eta: 153.13 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08108 sec, avg_samples: 16.00000, ips: 394.65956 images/sec
Training: 2023-09-27 06:26:23,303 - loss nan, lr: 0.006250, epoch: 0, step: 11900, eta: 153.13 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08108 sec, avg_samples: 16.00000, ips: 394.65956 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 12000, eta: 152.70 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08134 sec, avg_samples: 16.00000, ips: 393.39280 images/sec
Training: 2023-09-27 06:26:31,442 - loss nan, lr: 0.006250, epoch: 0, step: 12000, eta: 152.70 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08134 sec, avg_samples: 16.00000, ips: 393.39280 images/sec
INFO:root:[lfw][12000]XNorm: 15.560614
Training: 2023-09-27 06:27:01,571 - [lfw][12000]XNorm: 15.560614
INFO:root:[lfw][12000]Accuracy-Flip: 0.79150+-0.01974
Training: 2023-09-27 06:27:01,571 - [lfw][12000]Accuracy-Flip: 0.79150+-0.01974
INFO:root:[lfw][12000]Accuracy-Highest: 0.79150
Training: 2023-09-27 06:27:01,571 - [lfw][12000]Accuracy-Highest: 0.79150
INFO:root:test time: 30.1296
Training: 2023-09-27 06:27:01,571 - test time: 30.1296
INFO:root:[cfp_fp][12000]XNorm: 15.190361
Training: 2023-09-27 06:27:36,576 - [cfp_fp][12000]XNorm: 15.190361
INFO:root:[cfp_fp][12000]Accuracy-Flip: 0.57814+-0.01446
Training: 2023-09-27 06:27:36,576 - [cfp_fp][12000]Accuracy-Flip: 0.57814+-0.01446
INFO:root:[cfp_fp][12000]Accuracy-Highest: 0.58986
Training: 2023-09-27 06:27:36,577 - [cfp_fp][12000]Accuracy-Highest: 0.58986
INFO:root:test time: 35.0052
Training: 2023-09-27 06:27:36,577 - test time: 35.0052
INFO:root:[agedb_30][12000]XNorm: 15.637383
Training: 2023-09-27 06:28:06,693 - [agedb_30][12000]XNorm: 15.637383
INFO:root:[agedb_30][12000]Accuracy-Flip: 0.59350+-0.02094
Training: 2023-09-27 06:28:06,693 - [agedb_30][12000]Accuracy-Flip: 0.59350+-0.02094
INFO:root:[agedb_30][12000]Accuracy-Highest: 0.60133
Training: 2023-09-27 06:28:06,694 - [agedb_30][12000]Accuracy-Highest: 0.60133
INFO:root:test time: 30.1169
Training: 2023-09-27 06:28:06,694 - test time: 30.1169
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 12100, eta: 162.20 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08125 sec, avg_samples: 16.00000, ips: 393.85828 images/sec
Training: 2023-09-27 06:28:14,825 - loss nan, lr: 0.006250, epoch: 0, step: 12100, eta: 162.20 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08125 sec, avg_samples: 16.00000, ips: 393.85828 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 12200, eta: 161.71 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08142 sec, avg_samples: 16.00000, ips: 393.01047 images/sec
Training: 2023-09-27 06:28:22,975 - loss nan, lr: 0.006250, epoch: 0, step: 12200, eta: 161.71 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08142 sec, avg_samples: 16.00000, ips: 393.01047 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 12300, eta: 161.23 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08134 sec, avg_samples: 16.00000, ips: 393.41755 images/sec
Training: 2023-09-27 06:28:31,116 - loss nan, lr: 0.006250, epoch: 0, step: 12300, eta: 161.23 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08134 sec, avg_samples: 16.00000, ips: 393.41755 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 12400, eta: 160.75 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08139 sec, avg_samples: 16.00000, ips: 393.18240 images/sec
Training: 2023-09-27 06:28:39,261 - loss nan, lr: 0.006250, epoch: 0, step: 12400, eta: 160.75 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08139 sec, avg_samples: 16.00000, ips: 393.18240 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 12500, eta: 160.28 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08140 sec, avg_samples: 16.00000, ips: 393.12938 images/sec
Training: 2023-09-27 06:28:47,408 - loss nan, lr: 0.006250, epoch: 0, step: 12500, eta: 160.28 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08140 sec, avg_samples: 16.00000, ips: 393.12938 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 12600, eta: 159.82 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08103 sec, avg_samples: 16.00000, ips: 394.90784 images/sec
Training: 2023-09-27 06:28:55,515 - loss nan, lr: 0.006250, epoch: 0, step: 12600, eta: 159.82 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08103 sec, avg_samples: 16.00000, ips: 394.90784 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 12700, eta: 159.37 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08140 sec, avg_samples: 16.00000, ips: 393.11729 images/sec
Training: 2023-09-27 06:29:03,660 - loss nan, lr: 0.006250, epoch: 0, step: 12700, eta: 159.37 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08140 sec, avg_samples: 16.00000, ips: 393.11729 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 12800, eta: 158.92 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08176 sec, avg_samples: 16.00000, ips: 391.37763 images/sec
Training: 2023-09-27 06:29:11,841 - loss nan, lr: 0.006250, epoch: 0, step: 12800, eta: 158.92 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08176 sec, avg_samples: 16.00000, ips: 391.37763 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 12900, eta: 158.49 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08178 sec, avg_samples: 16.00000, ips: 391.29226 images/sec
Training: 2023-09-27 06:29:20,025 - loss nan, lr: 0.006250, epoch: 0, step: 12900, eta: 158.49 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08178 sec, avg_samples: 16.00000, ips: 391.29226 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 13000, eta: 158.06 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08172 sec, avg_samples: 16.00000, ips: 391.55995 images/sec
Training: 2023-09-27 06:29:28,203 - loss nan, lr: 0.006250, epoch: 0, step: 13000, eta: 158.06 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08172 sec, avg_samples: 16.00000, ips: 391.55995 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 13100, eta: 157.63 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08174 sec, avg_samples: 16.00000, ips: 391.50671 images/sec
Training: 2023-09-27 06:29:36,381 - loss nan, lr: 0.006250, epoch: 0, step: 13100, eta: 157.63 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08174 sec, avg_samples: 16.00000, ips: 391.50671 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 13200, eta: 157.22 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08176 sec, avg_samples: 16.00000, ips: 391.39564 images/sec
Training: 2023-09-27 06:29:44,562 - loss nan, lr: 0.006250, epoch: 0, step: 13200, eta: 157.22 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08176 sec, avg_samples: 16.00000, ips: 391.39564 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 13300, eta: 156.80 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08168 sec, avg_samples: 16.00000, ips: 391.78364 images/sec
Training: 2023-09-27 06:29:52,736 - loss nan, lr: 0.006250, epoch: 0, step: 13300, eta: 156.80 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08168 sec, avg_samples: 16.00000, ips: 391.78364 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 13400, eta: 156.40 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08154 sec, avg_samples: 16.00000, ips: 392.43227 images/sec
Training: 2023-09-27 06:30:00,895 - loss nan, lr: 0.006250, epoch: 0, step: 13400, eta: 156.40 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08154 sec, avg_samples: 16.00000, ips: 392.43227 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 13500, eta: 155.99 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08095 sec, avg_samples: 16.00000, ips: 395.28743 images/sec
Training: 2023-09-27 06:30:08,995 - loss nan, lr: 0.006250, epoch: 0, step: 13500, eta: 155.99 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08095 sec, avg_samples: 16.00000, ips: 395.28743 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 13600, eta: 155.59 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08096 sec, avg_samples: 16.00000, ips: 395.27272 images/sec
Training: 2023-09-27 06:30:17,095 - loss nan, lr: 0.006250, epoch: 0, step: 13600, eta: 155.59 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08096 sec, avg_samples: 16.00000, ips: 395.27272 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 13700, eta: 155.20 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08094 sec, avg_samples: 16.00000, ips: 395.33358 images/sec
Training: 2023-09-27 06:30:25,194 - loss nan, lr: 0.006250, epoch: 0, step: 13700, eta: 155.20 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08094 sec, avg_samples: 16.00000, ips: 395.33358 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 13800, eta: 154.81 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08099 sec, avg_samples: 16.00000, ips: 395.10037 images/sec
Training: 2023-09-27 06:30:33,298 - loss nan, lr: 0.006250, epoch: 0, step: 13800, eta: 154.81 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08099 sec, avg_samples: 16.00000, ips: 395.10037 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 13900, eta: 154.43 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08098 sec, avg_samples: 16.00000, ips: 395.14596 images/sec
Training: 2023-09-27 06:30:41,400 - loss nan, lr: 0.006250, epoch: 0, step: 13900, eta: 154.43 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08098 sec, avg_samples: 16.00000, ips: 395.14596 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 14000, eta: 154.05 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08094 sec, avg_samples: 16.00000, ips: 395.33559 images/sec
Training: 2023-09-27 06:30:49,499 - loss nan, lr: 0.006250, epoch: 0, step: 14000, eta: 154.05 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08094 sec, avg_samples: 16.00000, ips: 395.33559 images/sec
INFO:root:[lfw][14000]XNorm: 14.614023
Training: 2023-09-27 06:31:19,581 - [lfw][14000]XNorm: 14.614023
INFO:root:[lfw][14000]Accuracy-Flip: 0.78967+-0.02159
Training: 2023-09-27 06:31:19,581 - [lfw][14000]Accuracy-Flip: 0.78967+-0.02159
INFO:root:[lfw][14000]Accuracy-Highest: 0.79150
Training: 2023-09-27 06:31:19,581 - [lfw][14000]Accuracy-Highest: 0.79150
INFO:root:test time: 30.0819
Training: 2023-09-27 06:31:19,581 - test time: 30.0819
INFO:root:[cfp_fp][14000]XNorm: 14.248203
Training: 2023-09-27 06:31:54,528 - [cfp_fp][14000]XNorm: 14.248203
INFO:root:[cfp_fp][14000]Accuracy-Flip: 0.58300+-0.01554
Training: 2023-09-27 06:31:54,528 - [cfp_fp][14000]Accuracy-Flip: 0.58300+-0.01554
INFO:root:[cfp_fp][14000]Accuracy-Highest: 0.58986
Training: 2023-09-27 06:31:54,528 - [cfp_fp][14000]Accuracy-Highest: 0.58986
INFO:root:test time: 34.9471
Training: 2023-09-27 06:31:54,528 - test time: 34.9471
INFO:root:[agedb_30][14000]XNorm: 14.682876
Training: 2023-09-27 06:32:24,580 - [agedb_30][14000]XNorm: 14.682876
INFO:root:[agedb_30][14000]Accuracy-Flip: 0.59050+-0.01857
Training: 2023-09-27 06:32:24,580 - [agedb_30][14000]Accuracy-Flip: 0.59050+-0.01857
INFO:root:[agedb_30][14000]Accuracy-Highest: 0.60133
Training: 2023-09-27 06:32:24,580 - [agedb_30][14000]Accuracy-Highest: 0.60133
INFO:root:test time: 30.0519
Training: 2023-09-27 06:32:24,580 - test time: 30.0519
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 14100, eta: 162.17 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08095 sec, avg_samples: 16.00000, ips: 395.31829 images/sec
Training: 2023-09-27 06:32:32,680 - loss nan, lr: 0.006250, epoch: 0, step: 14100, eta: 162.17 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08095 sec, avg_samples: 16.00000, ips: 395.31829 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 14200, eta: 161.74 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08074 sec, avg_samples: 16.00000, ips: 396.34988 images/sec
Training: 2023-09-27 06:32:40,758 - loss nan, lr: 0.006250, epoch: 0, step: 14200, eta: 161.74 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08074 sec, avg_samples: 16.00000, ips: 396.34988 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 14300, eta: 161.32 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08084 sec, avg_samples: 16.00000, ips: 395.83234 images/sec
Training: 2023-09-27 06:32:48,848 - loss nan, lr: 0.006250, epoch: 0, step: 14300, eta: 161.32 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08084 sec, avg_samples: 16.00000, ips: 395.83234 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 14400, eta: 160.90 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08096 sec, avg_samples: 16.00000, ips: 395.24994 images/sec
Training: 2023-09-27 06:32:56,948 - loss nan, lr: 0.006250, epoch: 0, step: 14400, eta: 160.90 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08096 sec, avg_samples: 16.00000, ips: 395.24994 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 14500, eta: 160.51 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08301 sec, avg_samples: 16.00000, ips: 385.50138 images/sec
Training: 2023-09-27 06:33:05,254 - loss nan, lr: 0.006250, epoch: 0, step: 14500, eta: 160.51 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08301 sec, avg_samples: 16.00000, ips: 385.50138 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 14600, eta: 160.11 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08102 sec, avg_samples: 16.00000, ips: 394.97456 images/sec
Training: 2023-09-27 06:33:13,361 - loss nan, lr: 0.006250, epoch: 0, step: 14600, eta: 160.11 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08102 sec, avg_samples: 16.00000, ips: 394.97456 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 14700, eta: 159.71 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08116 sec, avg_samples: 16.00000, ips: 394.29042 images/sec
Training: 2023-09-27 06:33:21,482 - loss nan, lr: 0.006250, epoch: 0, step: 14700, eta: 159.71 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08116 sec, avg_samples: 16.00000, ips: 394.29042 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 14800, eta: 159.32 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08140 sec, avg_samples: 16.00000, ips: 393.10091 images/sec
Training: 2023-09-27 06:33:29,629 - loss nan, lr: 0.006250, epoch: 0, step: 14800, eta: 159.32 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08140 sec, avg_samples: 16.00000, ips: 393.10091 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 14900, eta: 158.94 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08147 sec, avg_samples: 16.00000, ips: 392.77221 images/sec
Training: 2023-09-27 06:33:37,783 - loss nan, lr: 0.006250, epoch: 0, step: 14900, eta: 158.94 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08147 sec, avg_samples: 16.00000, ips: 392.77221 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 15000, eta: 158.56 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08124 sec, avg_samples: 16.00000, ips: 393.91318 images/sec
Training: 2023-09-27 06:33:45,912 - loss nan, lr: 0.006250, epoch: 0, step: 15000, eta: 158.56 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08124 sec, avg_samples: 16.00000, ips: 393.91318 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 15100, eta: 158.18 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08110 sec, avg_samples: 16.00000, ips: 394.59294 images/sec
Training: 2023-09-27 06:33:54,026 - loss nan, lr: 0.006250, epoch: 0, step: 15100, eta: 158.18 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08110 sec, avg_samples: 16.00000, ips: 394.59294 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 15200, eta: 157.81 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08109 sec, avg_samples: 16.00000, ips: 394.62514 images/sec
Training: 2023-09-27 06:34:02,140 - loss nan, lr: 0.006250, epoch: 0, step: 15200, eta: 157.81 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08109 sec, avg_samples: 16.00000, ips: 394.62514 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 15300, eta: 157.44 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08112 sec, avg_samples: 16.00000, ips: 394.48259 images/sec
Training: 2023-09-27 06:34:10,256 - loss nan, lr: 0.006250, epoch: 0, step: 15300, eta: 157.44 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08112 sec, avg_samples: 16.00000, ips: 394.48259 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 15400, eta: 157.08 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08111 sec, avg_samples: 16.00000, ips: 394.52259 images/sec
Training: 2023-09-27 06:34:18,372 - loss nan, lr: 0.006250, epoch: 0, step: 15400, eta: 157.08 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08111 sec, avg_samples: 16.00000, ips: 394.52259 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 15500, eta: 156.72 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08110 sec, avg_samples: 16.00000, ips: 394.57774 images/sec
Training: 2023-09-27 06:34:26,487 - loss nan, lr: 0.006250, epoch: 0, step: 15500, eta: 156.72 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08110 sec, avg_samples: 16.00000, ips: 394.57774 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 15600, eta: 156.37 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08120 sec, avg_samples: 16.00000, ips: 394.08504 images/sec
Training: 2023-09-27 06:34:34,612 - loss nan, lr: 0.006250, epoch: 0, step: 15600, eta: 156.37 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08120 sec, avg_samples: 16.00000, ips: 394.08504 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 15700, eta: 156.02 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08107 sec, avg_samples: 16.00000, ips: 394.71703 images/sec
Training: 2023-09-27 06:34:42,723 - loss nan, lr: 0.006250, epoch: 0, step: 15700, eta: 156.02 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08107 sec, avg_samples: 16.00000, ips: 394.71703 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 15800, eta: 155.68 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08107 sec, avg_samples: 16.00000, ips: 394.73305 images/sec
Training: 2023-09-27 06:34:50,835 - loss nan, lr: 0.006250, epoch: 0, step: 15800, eta: 155.68 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08107 sec, avg_samples: 16.00000, ips: 394.73305 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 15900, eta: 155.34 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08115 sec, avg_samples: 16.00000, ips: 394.33797 images/sec
Training: 2023-09-27 06:34:58,955 - loss nan, lr: 0.006250, epoch: 0, step: 15900, eta: 155.34 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08115 sec, avg_samples: 16.00000, ips: 394.33797 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 16000, eta: 155.00 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08113 sec, avg_samples: 16.00000, ips: 394.41551 images/sec
Training: 2023-09-27 06:35:07,072 - loss nan, lr: 0.006250, epoch: 0, step: 16000, eta: 155.00 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08113 sec, avg_samples: 16.00000, ips: 394.41551 images/sec
INFO:root:[lfw][16000]XNorm: 13.833638
Training: 2023-09-27 06:35:37,134 - [lfw][16000]XNorm: 13.833638
INFO:root:[lfw][16000]Accuracy-Flip: 0.79117+-0.02107
Training: 2023-09-27 06:35:37,134 - [lfw][16000]Accuracy-Flip: 0.79117+-0.02107
INFO:root:[lfw][16000]Accuracy-Highest: 0.79150
Training: 2023-09-27 06:35:37,134 - [lfw][16000]Accuracy-Highest: 0.79150
INFO:root:test time: 30.0617
Training: 2023-09-27 06:35:37,134 - test time: 30.0617
INFO:root:[cfp_fp][16000]XNorm: 13.459771
Training: 2023-09-27 06:36:12,066 - [cfp_fp][16000]XNorm: 13.459771
INFO:root:[cfp_fp][16000]Accuracy-Flip: 0.57900+-0.01496
Training: 2023-09-27 06:36:12,066 - [cfp_fp][16000]Accuracy-Flip: 0.57900+-0.01496
INFO:root:[cfp_fp][16000]Accuracy-Highest: 0.58986
Training: 2023-09-27 06:36:12,066 - [cfp_fp][16000]Accuracy-Highest: 0.58986
INFO:root:test time: 34.9315
Training: 2023-09-27 06:36:12,066 - test time: 34.9315
INFO:root:[agedb_30][16000]XNorm: 13.891103
Training: 2023-09-27 06:36:42,132 - [agedb_30][16000]XNorm: 13.891103
INFO:root:[agedb_30][16000]Accuracy-Flip: 0.59933+-0.01774
Training: 2023-09-27 06:36:42,133 - [agedb_30][16000]Accuracy-Flip: 0.59933+-0.01774
INFO:root:[agedb_30][16000]Accuracy-Highest: 0.60133
Training: 2023-09-27 06:36:42,133 - [agedb_30][16000]Accuracy-Highest: 0.60133
INFO:root:test time: 30.0667
Training: 2023-09-27 06:36:42,133 - test time: 30.0667
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 16100, eta: 162.10 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08088 sec, avg_samples: 16.00000, ips: 395.67214 images/sec
Training: 2023-09-27 06:36:50,225 - loss nan, lr: 0.006250, epoch: 0, step: 16100, eta: 162.10 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08088 sec, avg_samples: 16.00000, ips: 395.67214 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 16200, eta: 161.73 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08116 sec, avg_samples: 16.00000, ips: 394.26751 images/sec
Training: 2023-09-27 06:36:58,348 - loss nan, lr: 0.006250, epoch: 0, step: 16200, eta: 161.73 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08116 sec, avg_samples: 16.00000, ips: 394.26751 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 16300, eta: 161.36 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08116 sec, avg_samples: 16.00000, ips: 394.29021 images/sec
Training: 2023-09-27 06:37:06,470 - loss nan, lr: 0.006250, epoch: 0, step: 16300, eta: 161.36 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08116 sec, avg_samples: 16.00000, ips: 394.29021 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 16400, eta: 161.00 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08129 sec, avg_samples: 16.00000, ips: 393.63489 images/sec
Training: 2023-09-27 06:37:14,606 - loss nan, lr: 0.006250, epoch: 0, step: 16400, eta: 161.00 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08129 sec, avg_samples: 16.00000, ips: 393.63489 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 16500, eta: 160.64 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08129 sec, avg_samples: 16.00000, ips: 393.63991 images/sec
Training: 2023-09-27 06:37:22,741 - loss nan, lr: 0.006250, epoch: 0, step: 16500, eta: 160.64 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08129 sec, avg_samples: 16.00000, ips: 393.63991 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 16600, eta: 160.29 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08117 sec, avg_samples: 16.00000, ips: 394.23155 images/sec
Training: 2023-09-27 06:37:30,863 - loss nan, lr: 0.006250, epoch: 0, step: 16600, eta: 160.29 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08117 sec, avg_samples: 16.00000, ips: 394.23155 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 16700, eta: 159.93 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08108 sec, avg_samples: 16.00000, ips: 394.68977 images/sec
Training: 2023-09-27 06:37:38,977 - loss nan, lr: 0.006250, epoch: 0, step: 16700, eta: 159.93 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08108 sec, avg_samples: 16.00000, ips: 394.68977 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 16800, eta: 159.59 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08108 sec, avg_samples: 16.00000, ips: 394.67515 images/sec
Training: 2023-09-27 06:37:47,090 - loss nan, lr: 0.006250, epoch: 0, step: 16800, eta: 159.59 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08108 sec, avg_samples: 16.00000, ips: 394.67515 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 16900, eta: 159.24 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08103 sec, avg_samples: 16.00000, ips: 394.93574 images/sec
Training: 2023-09-27 06:37:55,198 - loss nan, lr: 0.006250, epoch: 0, step: 16900, eta: 159.24 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08103 sec, avg_samples: 16.00000, ips: 394.93574 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 17000, eta: 158.90 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08116 sec, avg_samples: 16.00000, ips: 394.29824 images/sec
Training: 2023-09-27 06:38:03,319 - loss nan, lr: 0.006250, epoch: 0, step: 17000, eta: 158.90 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08116 sec, avg_samples: 16.00000, ips: 394.29824 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 17100, eta: 158.57 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08145 sec, avg_samples: 16.00000, ips: 392.89789 images/sec
Training: 2023-09-27 06:38:11,470 - loss nan, lr: 0.006250, epoch: 0, step: 17100, eta: 158.57 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08145 sec, avg_samples: 16.00000, ips: 392.89789 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 17200, eta: 158.24 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08154 sec, avg_samples: 16.00000, ips: 392.44916 images/sec
Training: 2023-09-27 06:38:19,631 - loss nan, lr: 0.006250, epoch: 0, step: 17200, eta: 158.24 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08154 sec, avg_samples: 16.00000, ips: 392.44916 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 17300, eta: 157.92 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08144 sec, avg_samples: 16.00000, ips: 392.90649 images/sec
Training: 2023-09-27 06:38:27,782 - loss nan, lr: 0.006250, epoch: 0, step: 17300, eta: 157.92 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08144 sec, avg_samples: 16.00000, ips: 392.90649 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 17400, eta: 157.60 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08145 sec, avg_samples: 16.00000, ips: 392.88468 images/sec
Training: 2023-09-27 06:38:35,933 - loss nan, lr: 0.006250, epoch: 0, step: 17400, eta: 157.60 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08145 sec, avg_samples: 16.00000, ips: 392.88468 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 17500, eta: 157.28 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08156 sec, avg_samples: 16.00000, ips: 392.35150 images/sec
Training: 2023-09-27 06:38:44,096 - loss nan, lr: 0.006250, epoch: 0, step: 17500, eta: 157.28 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08156 sec, avg_samples: 16.00000, ips: 392.35150 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 17600, eta: 156.97 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08153 sec, avg_samples: 16.00000, ips: 392.47745 images/sec
Training: 2023-09-27 06:38:52,255 - loss nan, lr: 0.006250, epoch: 0, step: 17600, eta: 156.97 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08153 sec, avg_samples: 16.00000, ips: 392.47745 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 17700, eta: 156.66 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08163 sec, avg_samples: 16.00000, ips: 391.99890 images/sec
Training: 2023-09-27 06:39:00,425 - loss nan, lr: 0.006250, epoch: 0, step: 17700, eta: 156.66 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08163 sec, avg_samples: 16.00000, ips: 391.99890 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 17800, eta: 156.35 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08177 sec, avg_samples: 16.00000, ips: 391.32069 images/sec
Training: 2023-09-27 06:39:08,608 - loss nan, lr: 0.006250, epoch: 0, step: 17800, eta: 156.35 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08177 sec, avg_samples: 16.00000, ips: 391.32069 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 17900, eta: 156.05 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08179 sec, avg_samples: 16.00000, ips: 391.25144 images/sec
Training: 2023-09-27 06:39:16,792 - loss nan, lr: 0.006250, epoch: 0, step: 17900, eta: 156.05 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08179 sec, avg_samples: 16.00000, ips: 391.25144 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 18000, eta: 155.75 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08181 sec, avg_samples: 16.00000, ips: 391.15517 images/sec
Training: 2023-09-27 06:39:24,978 - loss nan, lr: 0.006250, epoch: 0, step: 18000, eta: 155.75 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08181 sec, avg_samples: 16.00000, ips: 391.15517 images/sec
INFO:root:[lfw][18000]XNorm: 12.947855
Training: 2023-09-27 06:39:55,066 - [lfw][18000]XNorm: 12.947855
INFO:root:[lfw][18000]Accuracy-Flip: 0.79017+-0.02165
Training: 2023-09-27 06:39:55,066 - [lfw][18000]Accuracy-Flip: 0.79017+-0.02165
INFO:root:[lfw][18000]Accuracy-Highest: 0.79150
Training: 2023-09-27 06:39:55,066 - [lfw][18000]Accuracy-Highest: 0.79150
INFO:root:test time: 30.0878
Training: 2023-09-27 06:39:55,066 - test time: 30.0878
INFO:root:[cfp_fp][18000]XNorm: 12.626064
Training: 2023-09-27 06:40:30,024 - [cfp_fp][18000]XNorm: 12.626064
INFO:root:[cfp_fp][18000]Accuracy-Flip: 0.58543+-0.01634
Training: 2023-09-27 06:40:30,024 - [cfp_fp][18000]Accuracy-Flip: 0.58543+-0.01634
INFO:root:[cfp_fp][18000]Accuracy-Highest: 0.58986
Training: 2023-09-27 06:40:30,024 - [cfp_fp][18000]Accuracy-Highest: 0.58986
INFO:root:test time: 34.9580
Training: 2023-09-27 06:40:30,024 - test time: 34.9580
INFO:root:[agedb_30][18000]XNorm: 13.009825
Training: 2023-09-27 06:41:00,018 - [agedb_30][18000]XNorm: 13.009825
INFO:root:[agedb_30][18000]Accuracy-Flip: 0.59850+-0.01692
Training: 2023-09-27 06:41:00,019 - [agedb_30][18000]Accuracy-Flip: 0.59850+-0.01692
INFO:root:[agedb_30][18000]Accuracy-Highest: 0.60133
Training: 2023-09-27 06:41:00,019 - [agedb_30][18000]Accuracy-Highest: 0.60133
INFO:root:test time: 29.9945
Training: 2023-09-27 06:41:00,019 - test time: 29.9945
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 18100, eta: 162.06 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08105 sec, avg_samples: 16.00000, ips: 394.82605 images/sec
Training: 2023-09-27 06:41:08,129 - loss nan, lr: 0.006250, epoch: 0, step: 18100, eta: 162.06 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08105 sec, avg_samples: 16.00000, ips: 394.82605 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 18200, eta: 161.73 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08112 sec, avg_samples: 16.00000, ips: 394.48971 images/sec
Training: 2023-09-27 06:41:16,247 - loss nan, lr: 0.006250, epoch: 0, step: 18200, eta: 161.73 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08112 sec, avg_samples: 16.00000, ips: 394.48971 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 18300, eta: 161.40 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08095 sec, avg_samples: 16.00000, ips: 395.30985 images/sec
Training: 2023-09-27 06:41:24,347 - loss nan, lr: 0.006250, epoch: 0, step: 18300, eta: 161.40 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08095 sec, avg_samples: 16.00000, ips: 395.30985 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 18400, eta: 161.07 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08105 sec, avg_samples: 16.00000, ips: 394.80827 images/sec
Training: 2023-09-27 06:41:32,457 - loss nan, lr: 0.006250, epoch: 0, step: 18400, eta: 161.07 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08105 sec, avg_samples: 16.00000, ips: 394.80827 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 18500, eta: 160.76 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08310 sec, avg_samples: 16.00000, ips: 385.09910 images/sec
Training: 2023-09-27 06:41:40,772 - loss nan, lr: 0.006250, epoch: 0, step: 18500, eta: 160.76 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08310 sec, avg_samples: 16.00000, ips: 385.09910 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 18600, eta: 160.45 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08120 sec, avg_samples: 16.00000, ips: 394.09644 images/sec
Training: 2023-09-27 06:41:48,896 - loss nan, lr: 0.006250, epoch: 0, step: 18600, eta: 160.45 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08120 sec, avg_samples: 16.00000, ips: 394.09644 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 18700, eta: 160.13 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08116 sec, avg_samples: 16.00000, ips: 394.30185 images/sec
Training: 2023-09-27 06:41:57,017 - loss nan, lr: 0.006250, epoch: 0, step: 18700, eta: 160.13 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08116 sec, avg_samples: 16.00000, ips: 394.30185 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 18800, eta: 159.82 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08107 sec, avg_samples: 16.00000, ips: 394.71450 images/sec
Training: 2023-09-27 06:42:05,129 - loss nan, lr: 0.006250, epoch: 0, step: 18800, eta: 159.82 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08107 sec, avg_samples: 16.00000, ips: 394.71450 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 18900, eta: 159.51 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08108 sec, avg_samples: 16.00000, ips: 394.68660 images/sec
Training: 2023-09-27 06:42:13,241 - loss nan, lr: 0.006250, epoch: 0, step: 18900, eta: 159.51 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08108 sec, avg_samples: 16.00000, ips: 394.68660 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 19000, eta: 159.21 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08150 sec, avg_samples: 16.00000, ips: 392.62680 images/sec
Training: 2023-09-27 06:42:21,397 - loss nan, lr: 0.006250, epoch: 0, step: 19000, eta: 159.21 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08150 sec, avg_samples: 16.00000, ips: 392.62680 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 19100, eta: 158.91 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08148 sec, avg_samples: 16.00000, ips: 392.75061 images/sec
Training: 2023-09-27 06:42:29,549 - loss nan, lr: 0.006250, epoch: 0, step: 19100, eta: 158.91 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08148 sec, avg_samples: 16.00000, ips: 392.75061 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 19200, eta: 158.61 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08119 sec, avg_samples: 16.00000, ips: 394.12300 images/sec
Training: 2023-09-27 06:42:37,674 - loss nan, lr: 0.006250, epoch: 0, step: 19200, eta: 158.61 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.08119 sec, avg_samples: 16.00000, ips: 394.12300 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 19300, eta: 158.31 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08128 sec, avg_samples: 16.00000, ips: 393.72328 images/sec
Training: 2023-09-27 06:42:45,807 - loss nan, lr: 0.006250, epoch: 0, step: 19300, eta: 158.31 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08128 sec, avg_samples: 16.00000, ips: 393.72328 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 19400, eta: 158.02 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08144 sec, avg_samples: 16.00000, ips: 392.91703 images/sec
Training: 2023-09-27 06:42:53,957 - loss nan, lr: 0.006250, epoch: 0, step: 19400, eta: 158.02 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08144 sec, avg_samples: 16.00000, ips: 392.91703 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 19500, eta: 157.74 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08169 sec, avg_samples: 16.00000, ips: 391.72337 images/sec
Training: 2023-09-27 06:43:02,133 - loss nan, lr: 0.006250, epoch: 0, step: 19500, eta: 157.74 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08169 sec, avg_samples: 16.00000, ips: 391.72337 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 19600, eta: 157.45 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08171 sec, avg_samples: 16.00000, ips: 391.63257 images/sec
Training: 2023-09-27 06:43:10,310 - loss nan, lr: 0.006250, epoch: 0, step: 19600, eta: 157.45 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08171 sec, avg_samples: 16.00000, ips: 391.63257 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 19700, eta: 157.17 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08181 sec, avg_samples: 16.00000, ips: 391.16108 images/sec
Training: 2023-09-27 06:43:18,497 - loss nan, lr: 0.006250, epoch: 0, step: 19700, eta: 157.17 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08181 sec, avg_samples: 16.00000, ips: 391.16108 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 19800, eta: 156.90 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08178 sec, avg_samples: 16.00000, ips: 391.27589 images/sec
Training: 2023-09-27 06:43:26,682 - loss nan, lr: 0.006250, epoch: 0, step: 19800, eta: 156.90 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08178 sec, avg_samples: 16.00000, ips: 391.27589 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 19900, eta: 156.62 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08182 sec, avg_samples: 16.00000, ips: 391.09843 images/sec
Training: 2023-09-27 06:43:34,870 - loss nan, lr: 0.006250, epoch: 0, step: 19900, eta: 156.62 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08182 sec, avg_samples: 16.00000, ips: 391.09843 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 20000, eta: 156.35 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08180 sec, avg_samples: 16.00000, ips: 391.18301 images/sec
Training: 2023-09-27 06:43:43,057 - loss nan, lr: 0.006250, epoch: 0, step: 20000, eta: 156.35 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.08180 sec, avg_samples: 16.00000, ips: 391.18301 images/sec
INFO:root:[lfw][20000]XNorm: 12.189944
Training: 2023-09-27 06:44:13,133 - [lfw][20000]XNorm: 12.189944
INFO:root:[lfw][20000]Accuracy-Flip: 0.79283+-0.02006
Training: 2023-09-27 06:44:13,133 - [lfw][20000]Accuracy-Flip: 0.79283+-0.02006
INFO:root:[lfw][20000]Accuracy-Highest: 0.79283
Training: 2023-09-27 06:44:13,133 - [lfw][20000]Accuracy-Highest: 0.79283
INFO:root:test time: 30.0762
Training: 2023-09-27 06:44:13,133 - test time: 30.0762


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::framework::Executor::RunPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, bool, bool, bool)
1   paddle::framework::Executor::RunPartialPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, long, long, bool, bool, bool)
2   paddle::framework::OperatorBase::Run(paddle::framework::Scope const&, paddle::platform::Place const&)
3   paddle::operators::FetchOp::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&) const
4   paddle::framework::TensorCopySync(paddle::framework::Tensor const&, paddle::platform::Place const&, paddle::framework::Tensor*)
5   void paddle::memory::Copy<paddle::platform::CPUPlace, paddle::platform::CUDAPlace>(paddle::platform::CPUPlace, void*, paddle::platform::CUDAPlace, void const*, unsigned long, CUstream_st*)
6   paddle::platform::GpuMemcpySync(void*, void const*, unsigned long, cudaMemcpyKind)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1695797066 (unix time) try "date -d @1695797066" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x2b62) received by PID 11188 (TID 0x7f24638d3740) from PID 11106 ***]

grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
INFO:root:rank: 0
Training: 2023-09-28 04:38:40,093 - rank: 0
INFO:root:--------args----------
Training: 2023-09-28 04:38:40,093 - --------args----------
INFO:root:config_file: configs/ms1mv3_r50.py
Training: 2023-09-28 04:38:40,093 - config_file: configs/ms1mv3_r50.py
INFO:root:is_static: True
Training: 2023-09-28 04:38:40,093 - is_static: True
INFO:root:backbone: FresResNet50
Training: 2023-09-28 04:38:40,093 - backbone: FresResNet50
INFO:root:classifier: LargeScaleClassifier
Training: 2023-09-28 04:38:40,093 - classifier: LargeScaleClassifier
INFO:root:embedding_size: 512
Training: 2023-09-28 04:38:40,093 - embedding_size: 512
INFO:root:model_parallel: True
Training: 2023-09-28 04:38:40,093 - model_parallel: True
INFO:root:sample_ratio: 0.1
Training: 2023-09-28 04:38:40,093 - sample_ratio: 0.1
INFO:root:loss: ArcFace
Training: 2023-09-28 04:38:40,093 - loss: ArcFace
INFO:root:dropout: 0.0
Training: 2023-09-28 04:38:40,093 - dropout: 0.0
INFO:root:fp16: True
Training: 2023-09-28 04:38:40,093 - fp16: True
INFO:root:init_loss_scaling: 27648.0
Training: 2023-09-28 04:38:40,093 - init_loss_scaling: 27648.0
INFO:root:max_loss_scaling: 128.0
Training: 2023-09-28 04:38:40,093 - max_loss_scaling: 128.0
INFO:root:incr_every_n_steps: 2000
Training: 2023-09-28 04:38:40,093 - incr_every_n_steps: 2000
INFO:root:decr_every_n_nan_or_inf: 1
Training: 2023-09-28 04:38:40,093 - decr_every_n_nan_or_inf: 1
INFO:root:incr_ratio: 2.0
Training: 2023-09-28 04:38:40,093 - incr_ratio: 2.0
INFO:root:decr_ratio: 0.5
Training: 2023-09-28 04:38:40,093 - decr_ratio: 0.5
INFO:root:use_dynamic_loss_scaling: True
Training: 2023-09-28 04:38:40,093 - use_dynamic_loss_scaling: True
INFO:root:custom_white_list: []
Training: 2023-09-28 04:38:40,093 - custom_white_list: []
INFO:root:custom_black_list: []
Training: 2023-09-28 04:38:40,093 - custom_black_list: []
INFO:root:lr: 0.1
Training: 2023-09-28 04:38:40,093 - lr: 0.1
INFO:root:lr_decay: 0.1
Training: 2023-09-28 04:38:40,093 - lr_decay: 0.1
INFO:root:weight_decay: 0.0005
Training: 2023-09-28 04:38:40,094 - weight_decay: 0.0005
INFO:root:momentum: 0.9
Training: 2023-09-28 04:38:40,094 - momentum: 0.9
INFO:root:train_unit: epoch
Training: 2023-09-28 04:38:40,094 - train_unit: epoch
INFO:root:warmup_num: 0
Training: 2023-09-28 04:38:40,094 - warmup_num: 0
INFO:root:train_num: 25
Training: 2023-09-28 04:38:40,094 - train_num: 25
INFO:root:decay_boundaries: [10, 16, 22]
Training: 2023-09-28 04:38:40,094 - decay_boundaries: [10, 16, 22]
INFO:root:use_synthetic_dataset: False
Training: 2023-09-28 04:38:40,094 - use_synthetic_dataset: False
INFO:root:dataset: MS1M_v3
Training: 2023-09-28 04:38:40,094 - dataset: MS1M_v3
INFO:root:data_dir: /paddle/dataset/
Training: 2023-09-28 04:38:40,094 - data_dir: /paddle/dataset/
INFO:root:label_file: /paddle/dataset/label.txt
Training: 2023-09-28 04:38:40,094 - label_file: /paddle/dataset/label.txt
INFO:root:is_bin: False
Training: 2023-09-28 04:38:40,094 - is_bin: False
INFO:root:num_classes: 8277
Training: 2023-09-28 04:38:40,094 - num_classes: 8277
INFO:root:batch_size: 32
Training: 2023-09-28 04:38:40,094 - batch_size: 32
INFO:root:num_workers: 2
Training: 2023-09-28 04:38:40,094 - num_workers: 2
INFO:root:do_validation_while_train: True
Training: 2023-09-28 04:38:40,094 - do_validation_while_train: True
INFO:root:validation_interval_step: 2000
Training: 2023-09-28 04:38:40,094 - validation_interval_step: 2000
INFO:root:val_targets: ['lfw', 'cfp_fp', 'agedb_30']
Training: 2023-09-28 04:38:40,094 - val_targets: ['lfw', 'cfp_fp', 'agedb_30']
INFO:root:logdir: ./log
Training: 2023-09-28 04:38:40,094 - logdir: ./log
INFO:root:log_interval_step: 100
Training: 2023-09-28 04:38:40,094 - log_interval_step: 100
INFO:root:output: MS1M_v3_arcface_static_0.1
Training: 2023-09-28 04:38:40,094 - output: MS1M_v3_arcface_static_0.1
INFO:root:resume: False
Training: 2023-09-28 04:38:40,094 - resume: False
INFO:root:checkpoint_dir: None
Training: 2023-09-28 04:38:40,094 - checkpoint_dir: None
INFO:root:max_num_last_checkpoint: 1
Training: 2023-09-28 04:38:40,094 - max_num_last_checkpoint: 1
INFO:root:------------------------

Training: 2023-09-28 04:38:40,094 - ------------------------

INFO:root:read label file finished, total num: 367888
Training: 2023-09-28 04:38:40,118 - read label file finished, total num: 367888
INFO:root:world_size: 2
Training: 2023-09-28 04:38:40,118 - world_size: 2
INFO:root:total_batch_size: 64
Training: 2023-09-28 04:38:40,118 - total_batch_size: 64
INFO:root:warmup_steps: 0
Training: 2023-09-28 04:38:40,118 - warmup_steps: 0
INFO:root:steps_per_epoch: 5748
Training: 2023-09-28 04:38:40,118 - steps_per_epoch: 5748
INFO:root:total_steps: 143700
Training: 2023-09-28 04:38:40,118 - total_steps: 143700
INFO:root:total_epoch: 25
Training: 2023-09-28 04:38:40,118 - total_epoch: 25
INFO:root:decay_steps: [57480, 91968, 126456]
Training: 2023-09-28 04:38:40,118 - decay_steps: [57480, 91968, 126456]
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in conv2d only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'Out' in guassian_random only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in batch_norm only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in prelu only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in elementwise_add only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'y' in elementwise_add only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in fc only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'Out' in xavier_init only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'dtype' in create_parameter only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'tensor' in all_gather only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in split only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input[0]' in concat only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input[1]' in concat only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in gather only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'X' in norm only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in matmul only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'y' in matmul only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'logits' in margin_cross_entropy only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/framework.py:744: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  elif dtype == np.bool:
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in check_finite_and_unscale only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in update_loss_scaling only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/optimizer/momentum.py:262: UserWarning: Accumulating with FP16 in optimizer can lead to poor accuracy or slow convergence.Consider using multi_precision=True option of the Momentum optimizer.
  "Accumulating with FP16 in optimizer can lead to poor accuracy or slow convergence."
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:42079']
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
(14000, 3, 112, 112)
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
W0928 04:39:00.743917   688 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 12.0, Runtime API Version: 11.2
W0928 04:39:00.745640   688 device_context.cc:465] device: 0, cuDNN Version: 8.1.
I0928 04:39:02.327719   688 gen_comm_id_helper.cc:190] Server listening on: 127.0.0.1:40027 successful.
INFO:root:loss 43.0856, lr: 0.012500, epoch: 0, step: 100, eta: 6.19 hours, avg_reader_cost: 0.00370 sec, avg_batch_cost: 0.13916 sec, avg_samples: 32.00000, ips: 459.91733 images/sec
Training: 2023-09-28 04:39:16,385 - loss 43.0856, lr: 0.012500, epoch: 0, step: 100, eta: 6.19 hours, avg_reader_cost: 0.00370 sec, avg_batch_cost: 0.13916 sec, avg_samples: 32.00000, ips: 459.91733 images/sec
INFO:root:loss 42.1154, lr: 0.012500, epoch: 0, step: 200, eta: 5.46 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11865 sec, avg_samples: 32.00000, ips: 539.38910 images/sec
Training: 2023-09-28 04:39:28,256 - loss 42.1154, lr: 0.012500, epoch: 0, step: 200, eta: 5.46 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11865 sec, avg_samples: 32.00000, ips: 539.38910 images/sec
INFO:root:loss 41.3846, lr: 0.012500, epoch: 0, step: 300, eta: 5.22 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11863 sec, avg_samples: 32.00000, ips: 539.48172 images/sec
Training: 2023-09-28 04:39:40,126 - loss 41.3846, lr: 0.012500, epoch: 0, step: 300, eta: 5.22 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11863 sec, avg_samples: 32.00000, ips: 539.48172 images/sec
INFO:root:loss 40.8338, lr: 0.012500, epoch: 0, step: 400, eta: 5.09 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11863 sec, avg_samples: 32.00000, ips: 539.48704 images/sec
Training: 2023-09-28 04:39:51,995 - loss 40.8338, lr: 0.012500, epoch: 0, step: 400, eta: 5.09 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11863 sec, avg_samples: 32.00000, ips: 539.48704 images/sec
INFO:root:loss 40.3231, lr: 0.012500, epoch: 0, step: 500, eta: 5.01 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.66797 images/sec
Training: 2023-09-28 04:40:03,839 - loss 40.3231, lr: 0.012500, epoch: 0, step: 500, eta: 5.01 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.66797 images/sec
INFO:root:loss 39.7911, lr: 0.012500, epoch: 0, step: 600, eta: 4.96 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.08947 images/sec
Training: 2023-09-28 04:40:15,672 - loss 39.7911, lr: 0.012500, epoch: 0, step: 600, eta: 4.96 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.08947 images/sec
INFO:root:loss 39.3026, lr: 0.012500, epoch: 0, step: 700, eta: 4.92 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11856 sec, avg_samples: 32.00000, ips: 539.80598 images/sec
Training: 2023-09-28 04:40:27,534 - loss 39.3026, lr: 0.012500, epoch: 0, step: 700, eta: 4.92 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11856 sec, avg_samples: 32.00000, ips: 539.80598 images/sec
INFO:root:loss 38.7882, lr: 0.012500, epoch: 0, step: 800, eta: 4.89 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11857 sec, avg_samples: 32.00000, ips: 539.75001 images/sec
Training: 2023-09-28 04:40:39,398 - loss 38.7882, lr: 0.012500, epoch: 0, step: 800, eta: 4.89 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11857 sec, avg_samples: 32.00000, ips: 539.75001 images/sec
INFO:root:loss 38.4132, lr: 0.012500, epoch: 0, step: 900, eta: 4.87 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11870 sec, avg_samples: 32.00000, ips: 539.18705 images/sec
Training: 2023-09-28 04:40:51,274 - loss 38.4132, lr: 0.012500, epoch: 0, step: 900, eta: 4.87 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11870 sec, avg_samples: 32.00000, ips: 539.18705 images/sec
INFO:root:loss 38.0784, lr: 0.012500, epoch: 0, step: 1000, eta: 4.85 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11869 sec, avg_samples: 32.00000, ips: 539.24001 images/sec
Training: 2023-09-28 04:41:03,148 - loss 38.0784, lr: 0.012500, epoch: 0, step: 1000, eta: 4.85 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11869 sec, avg_samples: 32.00000, ips: 539.24001 images/sec
INFO:root:loss 37.7566, lr: 0.012500, epoch: 0, step: 1100, eta: 4.83 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11869 sec, avg_samples: 32.00000, ips: 539.23955 images/sec
Training: 2023-09-28 04:41:15,023 - loss 37.7566, lr: 0.012500, epoch: 0, step: 1100, eta: 4.83 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11869 sec, avg_samples: 32.00000, ips: 539.23955 images/sec
INFO:root:loss 37.4722, lr: 0.012500, epoch: 0, step: 1200, eta: 4.82 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11871 sec, avg_samples: 32.00000, ips: 539.13518 images/sec
Training: 2023-09-28 04:41:26,901 - loss 37.4722, lr: 0.012500, epoch: 0, step: 1200, eta: 4.82 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11871 sec, avg_samples: 32.00000, ips: 539.13518 images/sec
INFO:root:loss 37.0819, lr: 0.012500, epoch: 0, step: 1300, eta: 4.81 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11878 sec, avg_samples: 32.00000, ips: 538.80139 images/sec
Training: 2023-09-28 04:41:38,787 - loss 37.0819, lr: 0.012500, epoch: 0, step: 1300, eta: 4.81 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11878 sec, avg_samples: 32.00000, ips: 538.80139 images/sec
INFO:root:loss 36.6433, lr: 0.012500, epoch: 0, step: 1400, eta: 4.80 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11884 sec, avg_samples: 32.00000, ips: 538.54418 images/sec
Training: 2023-09-28 04:41:50,678 - loss 36.6433, lr: 0.012500, epoch: 0, step: 1400, eta: 4.80 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11884 sec, avg_samples: 32.00000, ips: 538.54418 images/sec
INFO:root:loss 36.4567, lr: 0.012500, epoch: 0, step: 1500, eta: 4.79 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11885 sec, avg_samples: 32.00000, ips: 538.50748 images/sec
Training: 2023-09-28 04:42:02,570 - loss 36.4567, lr: 0.012500, epoch: 0, step: 1500, eta: 4.79 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11885 sec, avg_samples: 32.00000, ips: 538.50748 images/sec
INFO:root:loss 36.1652, lr: 0.012500, epoch: 0, step: 1600, eta: 4.78 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11885 sec, avg_samples: 32.00000, ips: 538.51493 images/sec
Training: 2023-09-28 04:42:14,461 - loss 36.1652, lr: 0.012500, epoch: 0, step: 1600, eta: 4.78 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11885 sec, avg_samples: 32.00000, ips: 538.51493 images/sec
INFO:root:loss 35.7161, lr: 0.012500, epoch: 0, step: 1700, eta: 4.77 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11885 sec, avg_samples: 32.00000, ips: 538.51285 images/sec
Training: 2023-09-28 04:42:26,353 - loss 35.7161, lr: 0.012500, epoch: 0, step: 1700, eta: 4.77 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11885 sec, avg_samples: 32.00000, ips: 538.51285 images/sec
INFO:root:loss 35.5664, lr: 0.012500, epoch: 0, step: 1800, eta: 4.76 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11899 sec, avg_samples: 32.00000, ips: 537.87419 images/sec
Training: 2023-09-28 04:42:38,259 - loss 35.5664, lr: 0.012500, epoch: 0, step: 1800, eta: 4.76 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11899 sec, avg_samples: 32.00000, ips: 537.87419 images/sec
INFO:root:loss 35.3017, lr: 0.012500, epoch: 0, step: 1900, eta: 4.75 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11907 sec, avg_samples: 32.00000, ips: 537.49028 images/sec
Training: 2023-09-28 04:42:50,174 - loss 35.3017, lr: 0.012500, epoch: 0, step: 1900, eta: 4.75 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11907 sec, avg_samples: 32.00000, ips: 537.49028 images/sec
INFO:root:loss 34.9810, lr: 0.012500, epoch: 0, step: 2000, eta: 4.75 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11911 sec, avg_samples: 32.00000, ips: 537.30139 images/sec
Training: 2023-09-28 04:43:02,092 - loss 34.9810, lr: 0.012500, epoch: 0, step: 2000, eta: 4.75 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11911 sec, avg_samples: 32.00000, ips: 537.30139 images/sec
INFO:root:[lfw][2000]XNorm: 19.687113
Training: 2023-09-28 04:43:29,844 - [lfw][2000]XNorm: 19.687113
INFO:root:[lfw][2000]Accuracy-Flip: 0.85217+-0.01336
Training: 2023-09-28 04:43:29,844 - [lfw][2000]Accuracy-Flip: 0.85217+-0.01336
INFO:root:[lfw][2000]Accuracy-Highest: 0.85217
Training: 2023-09-28 04:43:29,844 - [lfw][2000]Accuracy-Highest: 0.85217
INFO:root:test time: 27.7514
Training: 2023-09-28 04:43:29,844 - test time: 27.7514
INFO:root:[cfp_fp][2000]XNorm: 19.215063
Training: 2023-09-28 04:44:02,041 - [cfp_fp][2000]XNorm: 19.215063
INFO:root:[cfp_fp][2000]Accuracy-Flip: 0.61243+-0.01529
Training: 2023-09-28 04:44:02,041 - [cfp_fp][2000]Accuracy-Flip: 0.61243+-0.01529
INFO:root:[cfp_fp][2000]Accuracy-Highest: 0.61243
Training: 2023-09-28 04:44:02,041 - [cfp_fp][2000]Accuracy-Highest: 0.61243
INFO:root:test time: 32.1973
Training: 2023-09-28 04:44:02,041 - test time: 32.1973
INFO:root:[agedb_30][2000]XNorm: 18.647979
Training: 2023-09-28 04:44:29,695 - [agedb_30][2000]XNorm: 18.647979
INFO:root:[agedb_30][2000]Accuracy-Flip: 0.60167+-0.01430
Training: 2023-09-28 04:44:29,695 - [agedb_30][2000]Accuracy-Flip: 0.60167+-0.01430
INFO:root:[agedb_30][2000]Accuracy-Highest: 0.60167
Training: 2023-09-28 04:44:29,695 - [agedb_30][2000]Accuracy-Highest: 0.60167
INFO:root:test time: 27.6540
Training: 2023-09-28 04:44:29,695 - test time: 27.6540
INFO:root:loss 35.0504, lr: 0.012500, epoch: 0, step: 2100, eta: 6.38 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11807 sec, avg_samples: 32.00000, ips: 542.06522 images/sec
Training: 2023-09-28 04:44:41,509 - loss 35.0504, lr: 0.012500, epoch: 0, step: 2100, eta: 6.38 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11807 sec, avg_samples: 32.00000, ips: 542.06522 images/sec
INFO:root:loss 34.9154, lr: 0.012500, epoch: 0, step: 2200, eta: 6.30 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11824 sec, avg_samples: 32.00000, ips: 541.27372 images/sec
Training: 2023-09-28 04:44:53,340 - loss 34.9154, lr: 0.012500, epoch: 0, step: 2200, eta: 6.30 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11824 sec, avg_samples: 32.00000, ips: 541.27372 images/sec
INFO:root:loss 34.5901, lr: 0.012500, epoch: 0, step: 2300, eta: 6.22 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11801 sec, avg_samples: 32.00000, ips: 542.34588 images/sec
Training: 2023-09-28 04:45:05,146 - loss 34.5901, lr: 0.012500, epoch: 0, step: 2300, eta: 6.22 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11801 sec, avg_samples: 32.00000, ips: 542.34588 images/sec
INFO:root:loss 34.3681, lr: 0.012500, epoch: 0, step: 2400, eta: 6.15 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11808 sec, avg_samples: 32.00000, ips: 542.00561 images/sec
Training: 2023-09-28 04:45:16,958 - loss 34.3681, lr: 0.012500, epoch: 0, step: 2400, eta: 6.15 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11808 sec, avg_samples: 32.00000, ips: 542.00561 images/sec
INFO:root:loss 34.0365, lr: 0.012500, epoch: 0, step: 2500, eta: 6.09 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.71237 images/sec
Training: 2023-09-28 04:45:28,777 - loss 34.0365, lr: 0.012500, epoch: 0, step: 2500, eta: 6.09 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.71237 images/sec
INFO:root:loss 33.8175, lr: 0.012500, epoch: 0, step: 2600, eta: 6.03 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.54396 images/sec
Training: 2023-09-28 04:45:40,599 - loss 33.8175, lr: 0.012500, epoch: 0, step: 2600, eta: 6.03 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.54396 images/sec
INFO:root:loss 33.4409, lr: 0.012500, epoch: 0, step: 2700, eta: 5.97 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11860 sec, avg_samples: 32.00000, ips: 539.63762 images/sec
Training: 2023-09-28 04:45:52,466 - loss 33.4409, lr: 0.012500, epoch: 0, step: 2700, eta: 5.97 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11860 sec, avg_samples: 32.00000, ips: 539.63762 images/sec
INFO:root:loss 33.0932, lr: 0.012500, epoch: 0, step: 2800, eta: 5.92 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11836 sec, avg_samples: 32.00000, ips: 540.73168 images/sec
Training: 2023-09-28 04:46:04,308 - loss 33.0932, lr: 0.012500, epoch: 0, step: 2800, eta: 5.92 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11836 sec, avg_samples: 32.00000, ips: 540.73168 images/sec
INFO:root:loss 32.4966, lr: 0.012500, epoch: 0, step: 2900, eta: 5.87 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.71801 images/sec
Training: 2023-09-28 04:46:16,126 - loss 32.4966, lr: 0.012500, epoch: 0, step: 2900, eta: 5.87 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.71801 images/sec
INFO:root:loss 32.4781, lr: 0.012500, epoch: 0, step: 3000, eta: 5.82 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.56158 images/sec
Training: 2023-09-28 04:46:27,948 - loss 32.4781, lr: 0.012500, epoch: 0, step: 3000, eta: 5.82 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.56158 images/sec
INFO:root:loss 32.0773, lr: 0.012500, epoch: 0, step: 3100, eta: 5.78 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.74354 images/sec
Training: 2023-09-28 04:46:39,766 - loss 32.0773, lr: 0.012500, epoch: 0, step: 3100, eta: 5.78 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.74354 images/sec
INFO:root:loss 31.7264, lr: 0.012500, epoch: 0, step: 3200, eta: 5.74 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.68739 images/sec
Training: 2023-09-28 04:46:51,585 - loss 31.7264, lr: 0.012500, epoch: 0, step: 3200, eta: 5.74 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.68739 images/sec
INFO:root:loss 31.4343, lr: 0.012500, epoch: 0, step: 3300, eta: 5.70 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11809 sec, avg_samples: 32.00000, ips: 541.96959 images/sec
Training: 2023-09-28 04:47:03,398 - loss 31.4343, lr: 0.012500, epoch: 0, step: 3300, eta: 5.70 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11809 sec, avg_samples: 32.00000, ips: 541.96959 images/sec
INFO:root:loss 31.0661, lr: 0.012500, epoch: 0, step: 3400, eta: 5.67 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11809 sec, avg_samples: 32.00000, ips: 541.98238 images/sec
Training: 2023-09-28 04:47:15,211 - loss 31.0661, lr: 0.012500, epoch: 0, step: 3400, eta: 5.67 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11809 sec, avg_samples: 32.00000, ips: 541.98238 images/sec
INFO:root:loss 30.9811, lr: 0.012500, epoch: 0, step: 3500, eta: 5.63 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11809 sec, avg_samples: 32.00000, ips: 541.95123 images/sec
Training: 2023-09-28 04:47:27,025 - loss 30.9811, lr: 0.012500, epoch: 0, step: 3500, eta: 5.63 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11809 sec, avg_samples: 32.00000, ips: 541.95123 images/sec
INFO:root:loss 30.7332, lr: 0.012500, epoch: 0, step: 3600, eta: 5.60 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11808 sec, avg_samples: 32.00000, ips: 541.99296 images/sec
Training: 2023-09-28 04:47:38,837 - loss 30.7332, lr: 0.012500, epoch: 0, step: 3600, eta: 5.60 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11808 sec, avg_samples: 32.00000, ips: 541.99296 images/sec
INFO:root:loss 30.4309, lr: 0.012500, epoch: 0, step: 3700, eta: 5.57 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11808 sec, avg_samples: 32.00000, ips: 541.99634 images/sec
Training: 2023-09-28 04:47:50,650 - loss 30.4309, lr: 0.012500, epoch: 0, step: 3700, eta: 5.57 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11808 sec, avg_samples: 32.00000, ips: 541.99634 images/sec
INFO:root:loss 30.2717, lr: 0.012500, epoch: 0, step: 3800, eta: 5.54 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11806 sec, avg_samples: 32.00000, ips: 542.11667 images/sec
Training: 2023-09-28 04:48:02,460 - loss 30.2717, lr: 0.012500, epoch: 0, step: 3800, eta: 5.54 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11806 sec, avg_samples: 32.00000, ips: 542.11667 images/sec
INFO:root:loss 29.8801, lr: 0.012500, epoch: 0, step: 3900, eta: 5.51 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11809 sec, avg_samples: 32.00000, ips: 541.94231 images/sec
Training: 2023-09-28 04:48:14,274 - loss 29.8801, lr: 0.012500, epoch: 0, step: 3900, eta: 5.51 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11809 sec, avg_samples: 32.00000, ips: 541.94231 images/sec
INFO:root:loss 29.8477, lr: 0.012500, epoch: 0, step: 4000, eta: 5.48 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11805 sec, avg_samples: 32.00000, ips: 542.14052 images/sec
Training: 2023-09-28 04:48:26,083 - loss 29.8477, lr: 0.012500, epoch: 0, step: 4000, eta: 5.48 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11805 sec, avg_samples: 32.00000, ips: 542.14052 images/sec
INFO:root:[lfw][4000]XNorm: 19.308154
Training: 2023-09-28 04:48:54,047 - [lfw][4000]XNorm: 19.308154
INFO:root:[lfw][4000]Accuracy-Flip: 0.90150+-0.01514
Training: 2023-09-28 04:48:54,047 - [lfw][4000]Accuracy-Flip: 0.90150+-0.01514
INFO:root:[lfw][4000]Accuracy-Highest: 0.90150
Training: 2023-09-28 04:48:54,047 - [lfw][4000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9637
Training: 2023-09-28 04:48:54,047 - test time: 27.9637
INFO:root:[cfp_fp][4000]XNorm: 17.517870
Training: 2023-09-28 04:49:26,271 - [cfp_fp][4000]XNorm: 17.517870
INFO:root:[cfp_fp][4000]Accuracy-Flip: 0.65071+-0.01505
Training: 2023-09-28 04:49:26,272 - [cfp_fp][4000]Accuracy-Flip: 0.65071+-0.01505
INFO:root:[cfp_fp][4000]Accuracy-Highest: 0.65071
Training: 2023-09-28 04:49:26,272 - [cfp_fp][4000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2248
Training: 2023-09-28 04:49:26,272 - test time: 32.2248
INFO:root:[agedb_30][4000]XNorm: 18.408945
Training: 2023-09-28 04:49:53,966 - [agedb_30][4000]XNorm: 18.408945
INFO:root:[agedb_30][4000]Accuracy-Flip: 0.70083+-0.01632
Training: 2023-09-28 04:49:53,967 - [agedb_30][4000]Accuracy-Flip: 0.70083+-0.01632
INFO:root:[agedb_30][4000]Accuracy-Highest: 0.70083
Training: 2023-09-28 04:49:53,967 - [agedb_30][4000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.6949
Training: 2023-09-28 04:49:53,967 - test time: 27.6949
INFO:root:loss nan, lr: 0.012500, epoch: 0, step: 4100, eta: 6.29 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11762 sec, avg_samples: 32.00000, ips: 544.12691 images/sec
Training: 2023-09-28 04:50:05,733 - loss nan, lr: 0.012500, epoch: 0, step: 4100, eta: 6.29 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11762 sec, avg_samples: 32.00000, ips: 544.12691 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 0, step: 4200, eta: 6.24 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11781 sec, avg_samples: 32.00000, ips: 543.26658 images/sec
Training: 2023-09-28 04:50:17,518 - loss nan, lr: 0.012500, epoch: 0, step: 4200, eta: 6.24 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11781 sec, avg_samples: 32.00000, ips: 543.26658 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 0, step: 4300, eta: 6.20 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11789 sec, avg_samples: 32.00000, ips: 542.87201 images/sec
Training: 2023-09-28 04:50:29,312 - loss nan, lr: 0.012500, epoch: 0, step: 4300, eta: 6.20 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11789 sec, avg_samples: 32.00000, ips: 542.87201 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 0, step: 4400, eta: 6.16 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11785 sec, avg_samples: 32.00000, ips: 543.04706 images/sec
Training: 2023-09-28 04:50:41,102 - loss nan, lr: 0.012500, epoch: 0, step: 4400, eta: 6.16 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11785 sec, avg_samples: 32.00000, ips: 543.04706 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 0, step: 4500, eta: 6.12 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11789 sec, avg_samples: 32.00000, ips: 542.86106 images/sec
Training: 2023-09-28 04:50:52,896 - loss nan, lr: 0.012500, epoch: 0, step: 4500, eta: 6.12 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11789 sec, avg_samples: 32.00000, ips: 542.86106 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 0, step: 4600, eta: 6.08 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11795 sec, avg_samples: 32.00000, ips: 542.62575 images/sec
Training: 2023-09-28 04:51:04,696 - loss nan, lr: 0.012500, epoch: 0, step: 4600, eta: 6.08 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11795 sec, avg_samples: 32.00000, ips: 542.62575 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 0, step: 4700, eta: 6.04 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11794 sec, avg_samples: 32.00000, ips: 542.65892 images/sec
Training: 2023-09-28 04:51:16,494 - loss nan, lr: 0.012500, epoch: 0, step: 4700, eta: 6.04 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11794 sec, avg_samples: 32.00000, ips: 542.65892 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 0, step: 4800, eta: 6.01 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11791 sec, avg_samples: 32.00000, ips: 542.78550 images/sec
Training: 2023-09-28 04:51:28,290 - loss nan, lr: 0.012500, epoch: 0, step: 4800, eta: 6.01 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11791 sec, avg_samples: 32.00000, ips: 542.78550 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 0, step: 4900, eta: 5.97 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11790 sec, avg_samples: 32.00000, ips: 542.84629 images/sec
Training: 2023-09-28 04:51:40,084 - loss nan, lr: 0.012500, epoch: 0, step: 4900, eta: 5.97 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11790 sec, avg_samples: 32.00000, ips: 542.84629 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 0, step: 5000, eta: 5.94 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11787 sec, avg_samples: 32.00000, ips: 542.98348 images/sec
Training: 2023-09-28 04:51:51,876 - loss nan, lr: 0.012500, epoch: 0, step: 5000, eta: 5.94 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11787 sec, avg_samples: 32.00000, ips: 542.98348 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 0, step: 5100, eta: 5.91 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11791 sec, avg_samples: 32.00000, ips: 542.77197 images/sec
Training: 2023-09-28 04:52:03,672 - loss nan, lr: 0.012500, epoch: 0, step: 5100, eta: 5.91 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11791 sec, avg_samples: 32.00000, ips: 542.77197 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 0, step: 5200, eta: 5.88 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11788 sec, avg_samples: 32.00000, ips: 542.90662 images/sec
Training: 2023-09-28 04:52:15,465 - loss nan, lr: 0.012500, epoch: 0, step: 5200, eta: 5.88 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11788 sec, avg_samples: 32.00000, ips: 542.90662 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 0, step: 5300, eta: 5.85 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11798 sec, avg_samples: 32.00000, ips: 542.45293 images/sec
Training: 2023-09-28 04:52:27,268 - loss nan, lr: 0.012500, epoch: 0, step: 5300, eta: 5.85 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11798 sec, avg_samples: 32.00000, ips: 542.45293 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 0, step: 5400, eta: 5.82 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.62768 images/sec
Training: 2023-09-28 04:52:39,091 - loss nan, lr: 0.012500, epoch: 0, step: 5400, eta: 5.82 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.62768 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 0, step: 5500, eta: 5.79 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.59502 images/sec
Training: 2023-09-28 04:52:50,914 - loss nan, lr: 0.012500, epoch: 0, step: 5500, eta: 5.79 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.59502 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 0, step: 5600, eta: 5.77 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.80530 images/sec
Training: 2023-09-28 04:53:02,732 - loss nan, lr: 0.012500, epoch: 0, step: 5600, eta: 5.77 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.80530 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 0, step: 5700, eta: 5.74 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.67327 images/sec
Training: 2023-09-28 04:53:14,554 - loss nan, lr: 0.012500, epoch: 0, step: 5700, eta: 5.74 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.67327 images/sec
/usr/local/lib/python3.7/dist-packages/paddle/framework/io.py:412: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working
  if isinstance(obj, collections.Iterable) and not isinstance(obj, (
INFO:root:Save model to MS1M_v3_arcface_static_0.1/FresResNet50/0.
Training: 2023-09-28 04:53:20,544 - Save model to MS1M_v3_arcface_static_0.1/FresResNet50/0.
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 5800, eta: 5.72 hours, avg_reader_cost: 0.00175 sec, avg_batch_cost: 0.06313 sec, avg_samples: 16.64000, ips: 527.18962 images/sec
Training: 2023-09-28 04:53:26,860 - loss nan, lr: 0.012500, epoch: 1, step: 5800, eta: 5.72 hours, avg_reader_cost: 0.00175 sec, avg_batch_cost: 0.06313 sec, avg_samples: 16.64000, ips: 527.18962 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 5900, eta: 5.70 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.65699 images/sec
Training: 2023-09-28 04:53:38,681 - loss nan, lr: 0.012500, epoch: 1, step: 5900, eta: 5.70 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.65699 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 6000, eta: 5.67 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.13859 images/sec
Training: 2023-09-28 04:53:50,515 - loss nan, lr: 0.012500, epoch: 1, step: 6000, eta: 5.67 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.13859 images/sec
INFO:root:[lfw][6000]XNorm: 16.140554
Training: 2023-09-28 04:54:18,271 - [lfw][6000]XNorm: 16.140554
INFO:root:[lfw][6000]Accuracy-Flip: 0.89933+-0.01711
Training: 2023-09-28 04:54:18,271 - [lfw][6000]Accuracy-Flip: 0.89933+-0.01711
INFO:root:[lfw][6000]Accuracy-Highest: 0.90150
Training: 2023-09-28 04:54:18,271 - [lfw][6000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.7560
Training: 2023-09-28 04:54:18,271 - test time: 27.7560
INFO:root:[cfp_fp][6000]XNorm: 15.114882
Training: 2023-09-28 04:54:50,494 - [cfp_fp][6000]XNorm: 15.114882
INFO:root:[cfp_fp][6000]Accuracy-Flip: 0.64371+-0.01787
Training: 2023-09-28 04:54:50,494 - [cfp_fp][6000]Accuracy-Flip: 0.64371+-0.01787
INFO:root:[cfp_fp][6000]Accuracy-Highest: 0.65071
Training: 2023-09-28 04:54:50,494 - [cfp_fp][6000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2236
Training: 2023-09-28 04:54:50,494 - test time: 32.2236
INFO:root:[agedb_30][6000]XNorm: 15.365849
Training: 2023-09-28 04:55:18,204 - [agedb_30][6000]XNorm: 15.365849
INFO:root:[agedb_30][6000]Accuracy-Flip: 0.68400+-0.02024
Training: 2023-09-28 04:55:18,204 - [agedb_30][6000]Accuracy-Flip: 0.68400+-0.02024
INFO:root:[agedb_30][6000]Accuracy-Highest: 0.70083
Training: 2023-09-28 04:55:18,205 - [agedb_30][6000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7101
Training: 2023-09-28 04:55:18,205 - test time: 27.7101
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 6100, eta: 6.20 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11780 sec, avg_samples: 32.00000, ips: 543.29914 images/sec
Training: 2023-09-28 04:55:29,989 - loss nan, lr: 0.012500, epoch: 1, step: 6100, eta: 6.20 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11780 sec, avg_samples: 32.00000, ips: 543.29914 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 6200, eta: 6.17 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11807 sec, avg_samples: 32.00000, ips: 542.04011 images/sec
Training: 2023-09-28 04:55:41,802 - loss nan, lr: 0.012500, epoch: 1, step: 6200, eta: 6.17 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11807 sec, avg_samples: 32.00000, ips: 542.04011 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 6300, eta: 6.14 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11855 sec, avg_samples: 32.00000, ips: 539.85896 images/sec
Training: 2023-09-28 04:55:53,663 - loss nan, lr: 0.012500, epoch: 1, step: 6300, eta: 6.14 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11855 sec, avg_samples: 32.00000, ips: 539.85896 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 6400, eta: 6.11 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11892 sec, avg_samples: 32.00000, ips: 538.17837 images/sec
Training: 2023-09-28 04:56:05,561 - loss nan, lr: 0.012500, epoch: 1, step: 6400, eta: 6.11 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11892 sec, avg_samples: 32.00000, ips: 538.17837 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 6500, eta: 6.08 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11839 sec, avg_samples: 32.00000, ips: 540.56414 images/sec
Training: 2023-09-28 04:56:17,406 - loss nan, lr: 0.012500, epoch: 1, step: 6500, eta: 6.08 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11839 sec, avg_samples: 32.00000, ips: 540.56414 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 6600, eta: 6.05 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11844 sec, avg_samples: 32.00000, ips: 540.34353 images/sec
Training: 2023-09-28 04:56:29,256 - loss nan, lr: 0.012500, epoch: 1, step: 6600, eta: 6.05 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11844 sec, avg_samples: 32.00000, ips: 540.34353 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 6700, eta: 6.02 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11864 sec, avg_samples: 32.00000, ips: 539.42978 images/sec
Training: 2023-09-28 04:56:41,127 - loss nan, lr: 0.012500, epoch: 1, step: 6700, eta: 6.02 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11864 sec, avg_samples: 32.00000, ips: 539.42978 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 6800, eta: 6.00 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11861 sec, avg_samples: 32.00000, ips: 539.58635 images/sec
Training: 2023-09-28 04:56:52,994 - loss nan, lr: 0.012500, epoch: 1, step: 6800, eta: 6.00 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11861 sec, avg_samples: 32.00000, ips: 539.58635 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 6900, eta: 5.97 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11862 sec, avg_samples: 32.00000, ips: 539.55714 images/sec
Training: 2023-09-28 04:57:04,862 - loss nan, lr: 0.012500, epoch: 1, step: 6900, eta: 5.97 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11862 sec, avg_samples: 32.00000, ips: 539.55714 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 7000, eta: 5.94 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11858 sec, avg_samples: 32.00000, ips: 539.71014 images/sec
Training: 2023-09-28 04:57:16,726 - loss nan, lr: 0.012500, epoch: 1, step: 7000, eta: 5.94 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11858 sec, avg_samples: 32.00000, ips: 539.71014 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 7100, eta: 5.92 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11855 sec, avg_samples: 32.00000, ips: 539.87525 images/sec
Training: 2023-09-28 04:57:28,587 - loss nan, lr: 0.012500, epoch: 1, step: 7100, eta: 5.92 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11855 sec, avg_samples: 32.00000, ips: 539.87525 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 7200, eta: 5.90 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11856 sec, avg_samples: 32.00000, ips: 539.83210 images/sec
Training: 2023-09-28 04:57:40,449 - loss nan, lr: 0.012500, epoch: 1, step: 7200, eta: 5.90 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11856 sec, avg_samples: 32.00000, ips: 539.83210 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 7300, eta: 5.87 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11858 sec, avg_samples: 32.00000, ips: 539.71935 images/sec
Training: 2023-09-28 04:57:52,313 - loss nan, lr: 0.012500, epoch: 1, step: 7300, eta: 5.87 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11858 sec, avg_samples: 32.00000, ips: 539.71935 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 7400, eta: 5.85 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11854 sec, avg_samples: 32.00000, ips: 539.90364 images/sec
Training: 2023-09-28 04:58:04,173 - loss nan, lr: 0.012500, epoch: 1, step: 7400, eta: 5.85 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11854 sec, avg_samples: 32.00000, ips: 539.90364 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 7500, eta: 5.83 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11855 sec, avg_samples: 32.00000, ips: 539.84676 images/sec
Training: 2023-09-28 04:58:16,034 - loss nan, lr: 0.012500, epoch: 1, step: 7500, eta: 5.83 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11855 sec, avg_samples: 32.00000, ips: 539.84676 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 7600, eta: 5.81 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11858 sec, avg_samples: 32.00000, ips: 539.73935 images/sec
Training: 2023-09-28 04:58:27,898 - loss nan, lr: 0.012500, epoch: 1, step: 7600, eta: 5.81 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11858 sec, avg_samples: 32.00000, ips: 539.73935 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 7700, eta: 5.78 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11857 sec, avg_samples: 32.00000, ips: 539.78431 images/sec
Training: 2023-09-28 04:58:39,760 - loss nan, lr: 0.012500, epoch: 1, step: 7700, eta: 5.78 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11857 sec, avg_samples: 32.00000, ips: 539.78431 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 7800, eta: 5.76 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11855 sec, avg_samples: 32.00000, ips: 539.86932 images/sec
Training: 2023-09-28 04:58:51,621 - loss nan, lr: 0.012500, epoch: 1, step: 7800, eta: 5.76 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11855 sec, avg_samples: 32.00000, ips: 539.86932 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 7900, eta: 5.74 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11863 sec, avg_samples: 32.00000, ips: 539.49789 images/sec
Training: 2023-09-28 04:59:03,490 - loss nan, lr: 0.012500, epoch: 1, step: 7900, eta: 5.74 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11863 sec, avg_samples: 32.00000, ips: 539.49789 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 8000, eta: 5.72 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11855 sec, avg_samples: 32.00000, ips: 539.83532 images/sec
Training: 2023-09-28 04:59:15,352 - loss nan, lr: 0.012500, epoch: 1, step: 8000, eta: 5.72 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11855 sec, avg_samples: 32.00000, ips: 539.83532 images/sec
INFO:root:[lfw][8000]XNorm: 14.124764
Training: 2023-09-28 04:59:43,137 - [lfw][8000]XNorm: 14.124764
INFO:root:[lfw][8000]Accuracy-Flip: 0.90133+-0.01891
Training: 2023-09-28 04:59:43,138 - [lfw][8000]Accuracy-Flip: 0.90133+-0.01891
INFO:root:[lfw][8000]Accuracy-Highest: 0.90150
Training: 2023-09-28 04:59:43,138 - [lfw][8000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.7857
Training: 2023-09-28 04:59:43,138 - test time: 27.7857
INFO:root:[cfp_fp][8000]XNorm: 13.224967
Training: 2023-09-28 05:00:15,365 - [cfp_fp][8000]XNorm: 13.224967
INFO:root:[cfp_fp][8000]Accuracy-Flip: 0.64543+-0.01932
Training: 2023-09-28 05:00:15,365 - [cfp_fp][8000]Accuracy-Flip: 0.64543+-0.01932
INFO:root:[cfp_fp][8000]Accuracy-Highest: 0.65071
Training: 2023-09-28 05:00:15,365 - [cfp_fp][8000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2275
Training: 2023-09-28 05:00:15,365 - test time: 32.2275
INFO:root:[agedb_30][8000]XNorm: 13.418239
Training: 2023-09-28 05:00:43,086 - [agedb_30][8000]XNorm: 13.418239
INFO:root:[agedb_30][8000]Accuracy-Flip: 0.68733+-0.02229
Training: 2023-09-28 05:00:43,087 - [agedb_30][8000]Accuracy-Flip: 0.68733+-0.02229
INFO:root:[agedb_30][8000]Accuracy-Highest: 0.70083
Training: 2023-09-28 05:00:43,087 - [agedb_30][8000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7214
Training: 2023-09-28 05:00:43,087 - test time: 27.7214
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 8100, eta: 6.11 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11797 sec, avg_samples: 32.00000, ips: 542.51835 images/sec
Training: 2023-09-28 05:00:54,888 - loss nan, lr: 0.012500, epoch: 1, step: 8100, eta: 6.11 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11797 sec, avg_samples: 32.00000, ips: 542.51835 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 8200, eta: 6.09 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.74568 images/sec
Training: 2023-09-28 05:01:06,706 - loss nan, lr: 0.012500, epoch: 1, step: 8200, eta: 6.09 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.74568 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 8300, eta: 6.06 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.37423 images/sec
Training: 2023-09-28 05:01:18,533 - loss nan, lr: 0.012500, epoch: 1, step: 8300, eta: 6.06 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.37423 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 8400, eta: 6.04 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.32508 images/sec
Training: 2023-09-28 05:01:30,361 - loss nan, lr: 0.012500, epoch: 1, step: 8400, eta: 6.04 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.32508 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 8500, eta: 6.01 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11864 sec, avg_samples: 32.00000, ips: 539.44747 images/sec
Training: 2023-09-28 05:01:42,231 - loss nan, lr: 0.012500, epoch: 1, step: 8500, eta: 6.01 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11864 sec, avg_samples: 32.00000, ips: 539.44747 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 8600, eta: 5.99 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11869 sec, avg_samples: 32.00000, ips: 539.21668 images/sec
Training: 2023-09-28 05:01:54,106 - loss nan, lr: 0.012500, epoch: 1, step: 8600, eta: 5.99 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11869 sec, avg_samples: 32.00000, ips: 539.21668 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 8700, eta: 5.97 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11868 sec, avg_samples: 32.00000, ips: 539.25520 images/sec
Training: 2023-09-28 05:02:05,980 - loss nan, lr: 0.012500, epoch: 1, step: 8700, eta: 5.97 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11868 sec, avg_samples: 32.00000, ips: 539.25520 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 8800, eta: 5.95 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11867 sec, avg_samples: 32.00000, ips: 539.30027 images/sec
Training: 2023-09-28 05:02:17,853 - loss nan, lr: 0.012500, epoch: 1, step: 8800, eta: 5.95 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11867 sec, avg_samples: 32.00000, ips: 539.30027 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 8900, eta: 5.93 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11867 sec, avg_samples: 32.00000, ips: 539.29476 images/sec
Training: 2023-09-28 05:02:29,726 - loss nan, lr: 0.012500, epoch: 1, step: 8900, eta: 5.93 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11867 sec, avg_samples: 32.00000, ips: 539.29476 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 9000, eta: 5.91 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11862 sec, avg_samples: 32.00000, ips: 539.53493 images/sec
Training: 2023-09-28 05:02:41,594 - loss nan, lr: 0.012500, epoch: 1, step: 9000, eta: 5.91 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11862 sec, avg_samples: 32.00000, ips: 539.53493 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 9100, eta: 5.89 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11862 sec, avg_samples: 32.00000, ips: 539.53837 images/sec
Training: 2023-09-28 05:02:53,462 - loss nan, lr: 0.012500, epoch: 1, step: 9100, eta: 5.89 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11862 sec, avg_samples: 32.00000, ips: 539.53837 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 9200, eta: 5.87 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11859 sec, avg_samples: 32.00000, ips: 539.65633 images/sec
Training: 2023-09-28 05:03:05,327 - loss nan, lr: 0.012500, epoch: 1, step: 9200, eta: 5.87 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11859 sec, avg_samples: 32.00000, ips: 539.65633 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 9300, eta: 5.85 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11809 sec, avg_samples: 32.00000, ips: 541.96961 images/sec
Training: 2023-09-28 05:03:17,141 - loss nan, lr: 0.012500, epoch: 1, step: 9300, eta: 5.85 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11809 sec, avg_samples: 32.00000, ips: 541.96961 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 9400, eta: 5.83 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11808 sec, avg_samples: 32.00000, ips: 541.99151 images/sec
Training: 2023-09-28 05:03:28,954 - loss nan, lr: 0.012500, epoch: 1, step: 9400, eta: 5.83 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11808 sec, avg_samples: 32.00000, ips: 541.99151 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 9500, eta: 5.81 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.76421 images/sec
Training: 2023-09-28 05:03:40,795 - loss nan, lr: 0.012500, epoch: 1, step: 9500, eta: 5.81 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.76421 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 9600, eta: 5.79 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11844 sec, avg_samples: 32.00000, ips: 540.37113 images/sec
Training: 2023-09-28 05:03:52,645 - loss nan, lr: 0.012500, epoch: 1, step: 9600, eta: 5.79 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11844 sec, avg_samples: 32.00000, ips: 540.37113 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 9700, eta: 5.77 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11855 sec, avg_samples: 32.00000, ips: 539.87400 images/sec
Training: 2023-09-28 05:04:04,507 - loss nan, lr: 0.012500, epoch: 1, step: 9700, eta: 5.77 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11855 sec, avg_samples: 32.00000, ips: 539.87400 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 9800, eta: 5.75 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11846 sec, avg_samples: 32.00000, ips: 540.27881 images/sec
Training: 2023-09-28 05:04:16,360 - loss nan, lr: 0.012500, epoch: 1, step: 9800, eta: 5.75 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11846 sec, avg_samples: 32.00000, ips: 540.27881 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 9900, eta: 5.73 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.15427 images/sec
Training: 2023-09-28 05:04:28,192 - loss nan, lr: 0.012500, epoch: 1, step: 9900, eta: 5.73 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.15427 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 10000, eta: 5.72 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.03376 images/sec
Training: 2023-09-28 05:04:40,028 - loss nan, lr: 0.012500, epoch: 1, step: 10000, eta: 5.72 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.03376 images/sec
INFO:root:[lfw][10000]XNorm: 12.606652
Training: 2023-09-28 05:05:07,811 - [lfw][10000]XNorm: 12.606652
INFO:root:[lfw][10000]Accuracy-Flip: 0.89800+-0.01552
Training: 2023-09-28 05:05:07,811 - [lfw][10000]Accuracy-Flip: 0.89800+-0.01552
INFO:root:[lfw][10000]Accuracy-Highest: 0.90150
Training: 2023-09-28 05:05:07,811 - [lfw][10000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.7831
Training: 2023-09-28 05:05:07,811 - test time: 27.7831
INFO:root:[cfp_fp][10000]XNorm: 11.760686
Training: 2023-09-28 05:05:40,033 - [cfp_fp][10000]XNorm: 11.760686
INFO:root:[cfp_fp][10000]Accuracy-Flip: 0.64257+-0.01530
Training: 2023-09-28 05:05:40,033 - [cfp_fp][10000]Accuracy-Flip: 0.64257+-0.01530
INFO:root:[cfp_fp][10000]Accuracy-Highest: 0.65071
Training: 2023-09-28 05:05:40,033 - [cfp_fp][10000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2216
Training: 2023-09-28 05:05:40,033 - test time: 32.2216
INFO:root:[agedb_30][10000]XNorm: 11.982495
Training: 2023-09-28 05:06:07,747 - [agedb_30][10000]XNorm: 11.982495
INFO:root:[agedb_30][10000]Accuracy-Flip: 0.68883+-0.02236
Training: 2023-09-28 05:06:07,747 - [agedb_30][10000]Accuracy-Flip: 0.68883+-0.02236
INFO:root:[agedb_30][10000]Accuracy-Highest: 0.70083
Training: 2023-09-28 05:06:07,747 - [agedb_30][10000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7143
Training: 2023-09-28 05:06:07,747 - test time: 27.7143
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 10100, eta: 6.02 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11794 sec, avg_samples: 32.00000, ips: 542.64908 images/sec
Training: 2023-09-28 05:06:19,547 - loss nan, lr: 0.012500, epoch: 1, step: 10100, eta: 6.02 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11794 sec, avg_samples: 32.00000, ips: 542.64908 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 10200, eta: 6.00 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.72467 images/sec
Training: 2023-09-28 05:06:31,368 - loss nan, lr: 0.012500, epoch: 1, step: 10200, eta: 6.00 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.72467 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 10300, eta: 5.98 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11825 sec, avg_samples: 32.00000, ips: 541.23838 images/sec
Training: 2023-09-28 05:06:43,199 - loss nan, lr: 0.012500, epoch: 1, step: 10300, eta: 5.98 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11825 sec, avg_samples: 32.00000, ips: 541.23838 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 10400, eta: 5.96 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11825 sec, avg_samples: 32.00000, ips: 541.20997 images/sec
Training: 2023-09-28 05:06:55,030 - loss nan, lr: 0.012500, epoch: 1, step: 10400, eta: 5.96 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11825 sec, avg_samples: 32.00000, ips: 541.20997 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 10500, eta: 5.94 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.14829 images/sec
Training: 2023-09-28 05:07:06,863 - loss nan, lr: 0.012500, epoch: 1, step: 10500, eta: 5.94 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.14829 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 10600, eta: 5.92 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.11663 images/sec
Training: 2023-09-28 05:07:18,697 - loss nan, lr: 0.012500, epoch: 1, step: 10600, eta: 5.92 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.11663 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 10700, eta: 5.90 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.10349 images/sec
Training: 2023-09-28 05:07:30,531 - loss nan, lr: 0.012500, epoch: 1, step: 10700, eta: 5.90 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.10349 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 10800, eta: 5.88 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.07523 images/sec
Training: 2023-09-28 05:07:42,365 - loss nan, lr: 0.012500, epoch: 1, step: 10800, eta: 5.88 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.07523 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 10900, eta: 5.87 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.51827 images/sec
Training: 2023-09-28 05:07:54,189 - loss nan, lr: 0.012500, epoch: 1, step: 10900, eta: 5.87 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.51827 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 11000, eta: 5.85 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11878 sec, avg_samples: 32.00000, ips: 538.83289 images/sec
Training: 2023-09-28 05:08:06,071 - loss nan, lr: 0.012500, epoch: 1, step: 11000, eta: 5.85 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11878 sec, avg_samples: 32.00000, ips: 538.83289 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 11100, eta: 5.83 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.68565 images/sec
Training: 2023-09-28 05:08:17,891 - loss nan, lr: 0.012500, epoch: 1, step: 11100, eta: 5.83 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.68565 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 11200, eta: 5.81 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11801 sec, avg_samples: 32.00000, ips: 542.34578 images/sec
Training: 2023-09-28 05:08:29,696 - loss nan, lr: 0.012500, epoch: 1, step: 11200, eta: 5.81 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11801 sec, avg_samples: 32.00000, ips: 542.34578 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 11300, eta: 5.80 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11808 sec, avg_samples: 32.00000, ips: 541.99081 images/sec
Training: 2023-09-28 05:08:41,510 - loss nan, lr: 0.012500, epoch: 1, step: 11300, eta: 5.80 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11808 sec, avg_samples: 32.00000, ips: 541.99081 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 1, step: 11400, eta: 5.78 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11824 sec, avg_samples: 32.00000, ips: 541.26593 images/sec
Training: 2023-09-28 05:08:53,340 - loss nan, lr: 0.012500, epoch: 1, step: 11400, eta: 5.78 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11824 sec, avg_samples: 32.00000, ips: 541.26593 images/sec
INFO:root:Save model to MS1M_v3_arcface_static_0.1/FresResNet50/1.
Training: 2023-09-28 05:09:05,037 - Save model to MS1M_v3_arcface_static_0.1/FresResNet50/1.
INFO:root:Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/0.
Training: 2023-09-28 05:09:05,037 - Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/0.
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 11500, eta: 5.76 hours, avg_reader_cost: 0.00168 sec, avg_batch_cost: 0.00643 sec, avg_samples: 1.28000, ips: 398.29287 images/sec
Training: 2023-09-28 05:09:05,709 - loss nan, lr: 0.012500, epoch: 2, step: 11500, eta: 5.76 hours, avg_reader_cost: 0.00168 sec, avg_batch_cost: 0.00643 sec, avg_samples: 1.28000, ips: 398.29287 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 11600, eta: 5.75 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11806 sec, avg_samples: 32.00000, ips: 542.08836 images/sec
Training: 2023-09-28 05:09:17,520 - loss nan, lr: 0.012500, epoch: 2, step: 11600, eta: 5.75 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11806 sec, avg_samples: 32.00000, ips: 542.08836 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 11700, eta: 5.73 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11855 sec, avg_samples: 32.00000, ips: 539.84715 images/sec
Training: 2023-09-28 05:09:29,381 - loss nan, lr: 0.012500, epoch: 2, step: 11700, eta: 5.73 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11855 sec, avg_samples: 32.00000, ips: 539.84715 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 11800, eta: 5.71 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11870 sec, avg_samples: 32.00000, ips: 539.18339 images/sec
Training: 2023-09-28 05:09:41,257 - loss nan, lr: 0.012500, epoch: 2, step: 11800, eta: 5.71 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11870 sec, avg_samples: 32.00000, ips: 539.18339 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 11900, eta: 5.70 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11896 sec, avg_samples: 32.00000, ips: 537.98727 images/sec
Training: 2023-09-28 05:09:53,159 - loss nan, lr: 0.012500, epoch: 2, step: 11900, eta: 5.70 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11896 sec, avg_samples: 32.00000, ips: 537.98727 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 12000, eta: 5.68 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11904 sec, avg_samples: 32.00000, ips: 537.63672 images/sec
Training: 2023-09-28 05:10:05,070 - loss nan, lr: 0.012500, epoch: 2, step: 12000, eta: 5.68 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11904 sec, avg_samples: 32.00000, ips: 537.63672 images/sec
INFO:root:[lfw][12000]XNorm: 11.057322
Training: 2023-09-28 05:10:32,852 - [lfw][12000]XNorm: 11.057322
INFO:root:[lfw][12000]Accuracy-Flip: 0.89867+-0.01531
Training: 2023-09-28 05:10:32,852 - [lfw][12000]Accuracy-Flip: 0.89867+-0.01531
INFO:root:[lfw][12000]Accuracy-Highest: 0.90150
Training: 2023-09-28 05:10:32,852 - [lfw][12000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.7822
Training: 2023-09-28 05:10:32,852 - test time: 27.7822
INFO:root:[cfp_fp][12000]XNorm: 10.340512
Training: 2023-09-28 05:11:05,071 - [cfp_fp][12000]XNorm: 10.340512
INFO:root:[cfp_fp][12000]Accuracy-Flip: 0.64600+-0.01988
Training: 2023-09-28 05:11:05,072 - [cfp_fp][12000]Accuracy-Flip: 0.64600+-0.01988
INFO:root:[cfp_fp][12000]Accuracy-Highest: 0.65071
Training: 2023-09-28 05:11:05,072 - [cfp_fp][12000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2195
Training: 2023-09-28 05:11:05,072 - test time: 32.2195
INFO:root:[agedb_30][12000]XNorm: 10.512045
Training: 2023-09-28 05:11:32,771 - [agedb_30][12000]XNorm: 10.512045
INFO:root:[agedb_30][12000]Accuracy-Flip: 0.68783+-0.02250
Training: 2023-09-28 05:11:32,771 - [agedb_30][12000]Accuracy-Flip: 0.68783+-0.02250
INFO:root:[agedb_30][12000]Accuracy-Highest: 0.70083
Training: 2023-09-28 05:11:32,771 - [agedb_30][12000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.6991
Training: 2023-09-28 05:11:32,771 - test time: 27.6991
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 12100, eta: 5.93 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11861 sec, avg_samples: 32.00000, ips: 539.60075 images/sec
Training: 2023-09-28 05:11:44,637 - loss nan, lr: 0.012500, epoch: 2, step: 12100, eta: 5.93 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11861 sec, avg_samples: 32.00000, ips: 539.60075 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 12200, eta: 5.92 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11882 sec, avg_samples: 32.00000, ips: 538.63006 images/sec
Training: 2023-09-28 05:11:56,525 - loss nan, lr: 0.012500, epoch: 2, step: 12200, eta: 5.92 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11882 sec, avg_samples: 32.00000, ips: 538.63006 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 12300, eta: 5.90 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11898 sec, avg_samples: 32.00000, ips: 537.92558 images/sec
Training: 2023-09-28 05:12:08,429 - loss nan, lr: 0.012500, epoch: 2, step: 12300, eta: 5.90 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11898 sec, avg_samples: 32.00000, ips: 537.92558 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 12400, eta: 5.88 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11843 sec, avg_samples: 32.00000, ips: 540.40244 images/sec
Training: 2023-09-28 05:12:20,278 - loss nan, lr: 0.012500, epoch: 2, step: 12400, eta: 5.88 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11843 sec, avg_samples: 32.00000, ips: 540.40244 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 12500, eta: 5.86 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.47299 images/sec
Training: 2023-09-28 05:12:32,104 - loss nan, lr: 0.012500, epoch: 2, step: 12500, eta: 5.86 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.47299 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 12600, eta: 5.85 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.36121 images/sec
Training: 2023-09-28 05:12:43,931 - loss nan, lr: 0.012500, epoch: 2, step: 12600, eta: 5.85 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.36121 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 12700, eta: 5.83 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11794 sec, avg_samples: 32.00000, ips: 542.66278 images/sec
Training: 2023-09-28 05:12:55,729 - loss nan, lr: 0.012500, epoch: 2, step: 12700, eta: 5.83 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11794 sec, avg_samples: 32.00000, ips: 542.66278 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 12800, eta: 5.81 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11794 sec, avg_samples: 32.00000, ips: 542.64904 images/sec
Training: 2023-09-28 05:13:07,528 - loss nan, lr: 0.012500, epoch: 2, step: 12800, eta: 5.81 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11794 sec, avg_samples: 32.00000, ips: 542.64904 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 12900, eta: 5.80 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11794 sec, avg_samples: 32.00000, ips: 542.66577 images/sec
Training: 2023-09-28 05:13:19,326 - loss nan, lr: 0.012500, epoch: 2, step: 12900, eta: 5.80 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11794 sec, avg_samples: 32.00000, ips: 542.66577 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 13000, eta: 5.78 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11794 sec, avg_samples: 32.00000, ips: 542.65183 images/sec
Training: 2023-09-28 05:13:31,124 - loss nan, lr: 0.012500, epoch: 2, step: 13000, eta: 5.78 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11794 sec, avg_samples: 32.00000, ips: 542.65183 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 13100, eta: 5.77 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11803 sec, avg_samples: 32.00000, ips: 542.25406 images/sec
Training: 2023-09-28 05:13:42,931 - loss nan, lr: 0.012500, epoch: 2, step: 13100, eta: 5.77 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11803 sec, avg_samples: 32.00000, ips: 542.25406 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 13200, eta: 5.75 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11803 sec, avg_samples: 32.00000, ips: 542.24928 images/sec
Training: 2023-09-28 05:13:54,738 - loss nan, lr: 0.012500, epoch: 2, step: 13200, eta: 5.75 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11803 sec, avg_samples: 32.00000, ips: 542.24928 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 13300, eta: 5.73 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11803 sec, avg_samples: 32.00000, ips: 542.25762 images/sec
Training: 2023-09-28 05:14:06,544 - loss nan, lr: 0.012500, epoch: 2, step: 13300, eta: 5.73 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11803 sec, avg_samples: 32.00000, ips: 542.25762 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 13400, eta: 5.72 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11803 sec, avg_samples: 32.00000, ips: 542.24672 images/sec
Training: 2023-09-28 05:14:18,351 - loss nan, lr: 0.012500, epoch: 2, step: 13400, eta: 5.72 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11803 sec, avg_samples: 32.00000, ips: 542.24672 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 13500, eta: 5.70 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.26049 images/sec
Training: 2023-09-28 05:14:30,157 - loss nan, lr: 0.012500, epoch: 2, step: 13500, eta: 5.70 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.26049 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 13600, eta: 5.69 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11805 sec, avg_samples: 32.00000, ips: 542.13510 images/sec
Training: 2023-09-28 05:14:41,967 - loss nan, lr: 0.012500, epoch: 2, step: 13600, eta: 5.69 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11805 sec, avg_samples: 32.00000, ips: 542.13510 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 13700, eta: 5.67 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.82662 images/sec
Training: 2023-09-28 05:14:53,783 - loss nan, lr: 0.012500, epoch: 2, step: 13700, eta: 5.67 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.82662 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 13800, eta: 5.66 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11807 sec, avg_samples: 32.00000, ips: 542.06302 images/sec
Training: 2023-09-28 05:15:05,593 - loss nan, lr: 0.012500, epoch: 2, step: 13800, eta: 5.66 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11807 sec, avg_samples: 32.00000, ips: 542.06302 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 13900, eta: 5.65 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11806 sec, avg_samples: 32.00000, ips: 542.09517 images/sec
Training: 2023-09-28 05:15:17,404 - loss nan, lr: 0.012500, epoch: 2, step: 13900, eta: 5.65 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11806 sec, avg_samples: 32.00000, ips: 542.09517 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 14000, eta: 5.63 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11807 sec, avg_samples: 32.00000, ips: 542.05954 images/sec
Training: 2023-09-28 05:15:29,215 - loss nan, lr: 0.012500, epoch: 2, step: 14000, eta: 5.63 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11807 sec, avg_samples: 32.00000, ips: 542.05954 images/sec
INFO:root:[lfw][14000]XNorm: 9.729306
Training: 2023-09-28 05:15:57,093 - [lfw][14000]XNorm: 9.729306
INFO:root:[lfw][14000]Accuracy-Flip: 0.89867+-0.01690
Training: 2023-09-28 05:15:57,093 - [lfw][14000]Accuracy-Flip: 0.89867+-0.01690
INFO:root:[lfw][14000]Accuracy-Highest: 0.90150
Training: 2023-09-28 05:15:57,093 - [lfw][14000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.8788
Training: 2023-09-28 05:15:57,094 - test time: 27.8788
INFO:root:[cfp_fp][14000]XNorm: 9.127302
Training: 2023-09-28 05:16:29,383 - [cfp_fp][14000]XNorm: 9.127302
INFO:root:[cfp_fp][14000]Accuracy-Flip: 0.64243+-0.01730
Training: 2023-09-28 05:16:29,383 - [cfp_fp][14000]Accuracy-Flip: 0.64243+-0.01730
INFO:root:[cfp_fp][14000]Accuracy-Highest: 0.65071
Training: 2023-09-28 05:16:29,383 - [cfp_fp][14000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2893
Training: 2023-09-28 05:16:29,383 - test time: 32.2893
INFO:root:[agedb_30][14000]XNorm: 9.241956
Training: 2023-09-28 05:16:57,145 - [agedb_30][14000]XNorm: 9.241956
INFO:root:[agedb_30][14000]Accuracy-Flip: 0.69083+-0.02215
Training: 2023-09-28 05:16:57,145 - [agedb_30][14000]Accuracy-Flip: 0.69083+-0.02215
INFO:root:[agedb_30][14000]Accuracy-Highest: 0.70083
Training: 2023-09-28 05:16:57,145 - [agedb_30][14000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7619
Training: 2023-09-28 05:16:57,145 - test time: 27.7619
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 14100, eta: 5.84 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11785 sec, avg_samples: 32.00000, ips: 543.04502 images/sec
Training: 2023-09-28 05:17:08,934 - loss nan, lr: 0.012500, epoch: 2, step: 14100, eta: 5.84 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11785 sec, avg_samples: 32.00000, ips: 543.04502 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 14200, eta: 5.83 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.11788 sec, avg_samples: 32.00000, ips: 542.90740 images/sec
Training: 2023-09-28 05:17:20,727 - loss nan, lr: 0.012500, epoch: 2, step: 14200, eta: 5.83 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.11788 sec, avg_samples: 32.00000, ips: 542.90740 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 14300, eta: 5.81 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11805 sec, avg_samples: 32.00000, ips: 542.14688 images/sec
Training: 2023-09-28 05:17:32,536 - loss nan, lr: 0.012500, epoch: 2, step: 14300, eta: 5.81 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11805 sec, avg_samples: 32.00000, ips: 542.14688 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 14400, eta: 5.80 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.11852 sec, avg_samples: 32.00000, ips: 539.97325 images/sec
Training: 2023-09-28 05:17:44,392 - loss nan, lr: 0.012500, epoch: 2, step: 14400, eta: 5.80 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.11852 sec, avg_samples: 32.00000, ips: 539.97325 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 14500, eta: 5.78 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11808 sec, avg_samples: 32.00000, ips: 541.98927 images/sec
Training: 2023-09-28 05:17:56,205 - loss nan, lr: 0.012500, epoch: 2, step: 14500, eta: 5.78 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11808 sec, avg_samples: 32.00000, ips: 541.98927 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 14600, eta: 5.77 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11836 sec, avg_samples: 32.00000, ips: 540.74223 images/sec
Training: 2023-09-28 05:18:08,046 - loss nan, lr: 0.012500, epoch: 2, step: 14600, eta: 5.77 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11836 sec, avg_samples: 32.00000, ips: 540.74223 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 14700, eta: 5.75 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11801 sec, avg_samples: 32.00000, ips: 542.34065 images/sec
Training: 2023-09-28 05:18:19,851 - loss nan, lr: 0.012500, epoch: 2, step: 14700, eta: 5.75 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11801 sec, avg_samples: 32.00000, ips: 542.34065 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 14800, eta: 5.74 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11797 sec, avg_samples: 32.00000, ips: 542.50041 images/sec
Training: 2023-09-28 05:18:31,652 - loss nan, lr: 0.012500, epoch: 2, step: 14800, eta: 5.74 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11797 sec, avg_samples: 32.00000, ips: 542.50041 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 14900, eta: 5.72 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11795 sec, avg_samples: 32.00000, ips: 542.59384 images/sec
Training: 2023-09-28 05:18:43,452 - loss nan, lr: 0.012500, epoch: 2, step: 14900, eta: 5.72 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11795 sec, avg_samples: 32.00000, ips: 542.59384 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 15000, eta: 5.71 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11795 sec, avg_samples: 32.00000, ips: 542.59920 images/sec
Training: 2023-09-28 05:18:55,251 - loss nan, lr: 0.012500, epoch: 2, step: 15000, eta: 5.71 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11795 sec, avg_samples: 32.00000, ips: 542.59920 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 15100, eta: 5.69 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11800 sec, avg_samples: 32.00000, ips: 542.38667 images/sec
Training: 2023-09-28 05:19:07,054 - loss nan, lr: 0.012500, epoch: 2, step: 15100, eta: 5.69 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11800 sec, avg_samples: 32.00000, ips: 542.38667 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 15200, eta: 5.68 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11796 sec, avg_samples: 32.00000, ips: 542.56598 images/sec
Training: 2023-09-28 05:19:18,854 - loss nan, lr: 0.012500, epoch: 2, step: 15200, eta: 5.68 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11796 sec, avg_samples: 32.00000, ips: 542.56598 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 15300, eta: 5.66 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11796 sec, avg_samples: 32.00000, ips: 542.57784 images/sec
Training: 2023-09-28 05:19:30,654 - loss nan, lr: 0.012500, epoch: 2, step: 15300, eta: 5.66 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11796 sec, avg_samples: 32.00000, ips: 542.57784 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 15400, eta: 5.65 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11798 sec, avg_samples: 32.00000, ips: 542.45294 images/sec
Training: 2023-09-28 05:19:42,456 - loss nan, lr: 0.012500, epoch: 2, step: 15400, eta: 5.65 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11798 sec, avg_samples: 32.00000, ips: 542.45294 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 15500, eta: 5.64 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11796 sec, avg_samples: 32.00000, ips: 542.55171 images/sec
Training: 2023-09-28 05:19:54,256 - loss nan, lr: 0.012500, epoch: 2, step: 15500, eta: 5.64 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11796 sec, avg_samples: 32.00000, ips: 542.55171 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 15600, eta: 5.62 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11793 sec, avg_samples: 32.00000, ips: 542.68265 images/sec
Training: 2023-09-28 05:20:06,053 - loss nan, lr: 0.012500, epoch: 2, step: 15600, eta: 5.62 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11793 sec, avg_samples: 32.00000, ips: 542.68265 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 15700, eta: 5.61 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11795 sec, avg_samples: 32.00000, ips: 542.62232 images/sec
Training: 2023-09-28 05:20:17,852 - loss nan, lr: 0.012500, epoch: 2, step: 15700, eta: 5.61 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11795 sec, avg_samples: 32.00000, ips: 542.62232 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 15800, eta: 5.60 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11795 sec, avg_samples: 32.00000, ips: 542.60877 images/sec
Training: 2023-09-28 05:20:29,651 - loss nan, lr: 0.012500, epoch: 2, step: 15800, eta: 5.60 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11795 sec, avg_samples: 32.00000, ips: 542.60877 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 15900, eta: 5.58 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11798 sec, avg_samples: 32.00000, ips: 542.48686 images/sec
Training: 2023-09-28 05:20:41,452 - loss nan, lr: 0.012500, epoch: 2, step: 15900, eta: 5.58 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11798 sec, avg_samples: 32.00000, ips: 542.48686 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 16000, eta: 5.57 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11797 sec, avg_samples: 32.00000, ips: 542.49157 images/sec
Training: 2023-09-28 05:20:53,254 - loss nan, lr: 0.012500, epoch: 2, step: 16000, eta: 5.57 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11797 sec, avg_samples: 32.00000, ips: 542.49157 images/sec
INFO:root:[lfw][16000]XNorm: 8.609139
Training: 2023-09-28 05:21:21,111 - [lfw][16000]XNorm: 8.609139
INFO:root:[lfw][16000]Accuracy-Flip: 0.89633+-0.01646
Training: 2023-09-28 05:21:21,111 - [lfw][16000]Accuracy-Flip: 0.89633+-0.01646
INFO:root:[lfw][16000]Accuracy-Highest: 0.90150
Training: 2023-09-28 05:21:21,111 - [lfw][16000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.8570
Training: 2023-09-28 05:21:21,111 - test time: 27.8570
INFO:root:[cfp_fp][16000]XNorm: 8.048642
Training: 2023-09-28 05:21:53,375 - [cfp_fp][16000]XNorm: 8.048642
INFO:root:[cfp_fp][16000]Accuracy-Flip: 0.64514+-0.01962
Training: 2023-09-28 05:21:53,375 - [cfp_fp][16000]Accuracy-Flip: 0.64514+-0.01962
INFO:root:[cfp_fp][16000]Accuracy-Highest: 0.65071
Training: 2023-09-28 05:21:53,375 - [cfp_fp][16000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2642
Training: 2023-09-28 05:21:53,375 - test time: 32.2642
INFO:root:[agedb_30][16000]XNorm: 8.185588
Training: 2023-09-28 05:22:21,149 - [agedb_30][16000]XNorm: 8.185588
INFO:root:[agedb_30][16000]Accuracy-Flip: 0.68733+-0.02272
Training: 2023-09-28 05:22:21,149 - [agedb_30][16000]Accuracy-Flip: 0.68733+-0.02272
INFO:root:[agedb_30][16000]Accuracy-Highest: 0.70083
Training: 2023-09-28 05:22:21,149 - [agedb_30][16000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7740
Training: 2023-09-28 05:22:21,149 - test time: 27.7740
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 16100, eta: 5.75 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11779 sec, avg_samples: 32.00000, ips: 543.32239 images/sec
Training: 2023-09-28 05:22:32,933 - loss nan, lr: 0.012500, epoch: 2, step: 16100, eta: 5.75 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11779 sec, avg_samples: 32.00000, ips: 543.32239 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 16200, eta: 5.74 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11789 sec, avg_samples: 32.00000, ips: 542.88634 images/sec
Training: 2023-09-28 05:22:44,727 - loss nan, lr: 0.012500, epoch: 2, step: 16200, eta: 5.74 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11789 sec, avg_samples: 32.00000, ips: 542.88634 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 16300, eta: 5.72 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11801 sec, avg_samples: 32.00000, ips: 542.34219 images/sec
Training: 2023-09-28 05:22:56,532 - loss nan, lr: 0.012500, epoch: 2, step: 16300, eta: 5.72 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11801 sec, avg_samples: 32.00000, ips: 542.34219 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 16400, eta: 5.71 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.47522 images/sec
Training: 2023-09-28 05:23:08,358 - loss nan, lr: 0.012500, epoch: 2, step: 16400, eta: 5.71 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.47522 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 16500, eta: 5.69 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.31274 images/sec
Training: 2023-09-28 05:23:20,187 - loss nan, lr: 0.012500, epoch: 2, step: 16500, eta: 5.69 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.31274 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 16600, eta: 5.68 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11804 sec, avg_samples: 32.00000, ips: 542.16890 images/sec
Training: 2023-09-28 05:23:31,996 - loss nan, lr: 0.012500, epoch: 2, step: 16600, eta: 5.68 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11804 sec, avg_samples: 32.00000, ips: 542.16890 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 16700, eta: 5.67 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11797 sec, avg_samples: 32.00000, ips: 542.49538 images/sec
Training: 2023-09-28 05:23:43,798 - loss nan, lr: 0.012500, epoch: 2, step: 16700, eta: 5.67 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11797 sec, avg_samples: 32.00000, ips: 542.49538 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 16800, eta: 5.65 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.29821 images/sec
Training: 2023-09-28 05:23:55,604 - loss nan, lr: 0.012500, epoch: 2, step: 16800, eta: 5.65 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.29821 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 16900, eta: 5.64 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11795 sec, avg_samples: 32.00000, ips: 542.60433 images/sec
Training: 2023-09-28 05:24:07,403 - loss nan, lr: 0.012500, epoch: 2, step: 16900, eta: 5.64 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11795 sec, avg_samples: 32.00000, ips: 542.60433 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 17000, eta: 5.63 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11799 sec, avg_samples: 32.00000, ips: 542.40356 images/sec
Training: 2023-09-28 05:24:19,207 - loss nan, lr: 0.012500, epoch: 2, step: 17000, eta: 5.63 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11799 sec, avg_samples: 32.00000, ips: 542.40356 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 17100, eta: 5.61 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11865 sec, avg_samples: 32.00000, ips: 539.38384 images/sec
Training: 2023-09-28 05:24:31,077 - loss nan, lr: 0.012500, epoch: 2, step: 17100, eta: 5.61 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11865 sec, avg_samples: 32.00000, ips: 539.38384 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 2, step: 17200, eta: 5.60 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11810 sec, avg_samples: 32.00000, ips: 541.90416 images/sec
Training: 2023-09-28 05:24:42,892 - loss nan, lr: 0.012500, epoch: 2, step: 17200, eta: 5.60 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11810 sec, avg_samples: 32.00000, ips: 541.90416 images/sec
INFO:root:Save model to MS1M_v3_arcface_static_0.1/FresResNet50/2.
Training: 2023-09-28 05:24:48,493 - Save model to MS1M_v3_arcface_static_0.1/FresResNet50/2.
INFO:root:Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/1.
Training: 2023-09-28 05:24:48,493 - Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/1.
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 17300, eta: 5.59 hours, avg_reader_cost: 0.00176 sec, avg_batch_cost: 0.06792 sec, avg_samples: 17.92000, ips: 527.71022 images/sec
Training: 2023-09-28 05:24:55,317 - loss nan, lr: 0.012500, epoch: 3, step: 17300, eta: 5.59 hours, avg_reader_cost: 0.00176 sec, avg_batch_cost: 0.06792 sec, avg_samples: 17.92000, ips: 527.71022 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 17400, eta: 5.58 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.69007 images/sec
Training: 2023-09-28 05:25:07,159 - loss nan, lr: 0.012500, epoch: 3, step: 17400, eta: 5.58 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.69007 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 17500, eta: 5.57 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11850 sec, avg_samples: 32.00000, ips: 540.10305 images/sec
Training: 2023-09-28 05:25:19,014 - loss nan, lr: 0.012500, epoch: 3, step: 17500, eta: 5.57 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11850 sec, avg_samples: 32.00000, ips: 540.10305 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 17600, eta: 5.55 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11843 sec, avg_samples: 32.00000, ips: 540.40239 images/sec
Training: 2023-09-28 05:25:30,863 - loss nan, lr: 0.012500, epoch: 3, step: 17600, eta: 5.55 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11843 sec, avg_samples: 32.00000, ips: 540.40239 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 17700, eta: 5.54 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11859 sec, avg_samples: 32.00000, ips: 539.66057 images/sec
Training: 2023-09-28 05:25:42,727 - loss nan, lr: 0.012500, epoch: 3, step: 17700, eta: 5.54 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11859 sec, avg_samples: 32.00000, ips: 539.66057 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 17800, eta: 5.53 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11862 sec, avg_samples: 32.00000, ips: 539.51991 images/sec
Training: 2023-09-28 05:25:54,595 - loss nan, lr: 0.012500, epoch: 3, step: 17800, eta: 5.53 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11862 sec, avg_samples: 32.00000, ips: 539.51991 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 17900, eta: 5.52 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11864 sec, avg_samples: 32.00000, ips: 539.45093 images/sec
Training: 2023-09-28 05:26:06,465 - loss nan, lr: 0.012500, epoch: 3, step: 17900, eta: 5.52 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11864 sec, avg_samples: 32.00000, ips: 539.45093 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 18000, eta: 5.50 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11863 sec, avg_samples: 32.00000, ips: 539.50999 images/sec
Training: 2023-09-28 05:26:18,333 - loss nan, lr: 0.012500, epoch: 3, step: 18000, eta: 5.50 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11863 sec, avg_samples: 32.00000, ips: 539.50999 images/sec
INFO:root:[lfw][18000]XNorm: 7.651218
Training: 2023-09-28 05:26:46,192 - [lfw][18000]XNorm: 7.651218
INFO:root:[lfw][18000]Accuracy-Flip: 0.89450+-0.01672
Training: 2023-09-28 05:26:46,193 - [lfw][18000]Accuracy-Flip: 0.89450+-0.01672
INFO:root:[lfw][18000]Accuracy-Highest: 0.90150
Training: 2023-09-28 05:26:46,193 - [lfw][18000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.8597
Training: 2023-09-28 05:26:46,193 - test time: 27.8597
INFO:root:[cfp_fp][18000]XNorm: 7.143750
Training: 2023-09-28 05:27:18,414 - [cfp_fp][18000]XNorm: 7.143750
INFO:root:[cfp_fp][18000]Accuracy-Flip: 0.64043+-0.01555
Training: 2023-09-28 05:27:18,414 - [cfp_fp][18000]Accuracy-Flip: 0.64043+-0.01555
INFO:root:[cfp_fp][18000]Accuracy-Highest: 0.65071
Training: 2023-09-28 05:27:18,414 - [cfp_fp][18000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2212
Training: 2023-09-28 05:27:18,414 - test time: 32.2212
INFO:root:[agedb_30][18000]XNorm: 7.247403
Training: 2023-09-28 05:27:46,178 - [agedb_30][18000]XNorm: 7.247403
INFO:root:[agedb_30][18000]Accuracy-Flip: 0.68283+-0.02178
Training: 2023-09-28 05:27:46,179 - [agedb_30][18000]Accuracy-Flip: 0.68283+-0.02178
INFO:root:[agedb_30][18000]Accuracy-Highest: 0.70083
Training: 2023-09-28 05:27:46,179 - [agedb_30][18000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7648
Training: 2023-09-28 05:27:46,179 - test time: 27.7648
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 18100, eta: 5.66 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.45977 images/sec
Training: 2023-09-28 05:27:58,005 - loss nan, lr: 0.012500, epoch: 3, step: 18100, eta: 5.66 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.45977 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 18200, eta: 5.65 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.58763 images/sec
Training: 2023-09-28 05:28:09,828 - loss nan, lr: 0.012500, epoch: 3, step: 18200, eta: 5.65 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.58763 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 18300, eta: 5.64 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.76980 images/sec
Training: 2023-09-28 05:28:21,670 - loss nan, lr: 0.012500, epoch: 3, step: 18300, eta: 5.64 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.76980 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 18400, eta: 5.62 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.05293 images/sec
Training: 2023-09-28 05:28:33,505 - loss nan, lr: 0.012500, epoch: 3, step: 18400, eta: 5.62 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.05293 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 18500, eta: 5.61 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.90045 images/sec
Training: 2023-09-28 05:28:45,344 - loss nan, lr: 0.012500, epoch: 3, step: 18500, eta: 5.61 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.90045 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 18600, eta: 5.60 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11811 sec, avg_samples: 32.00000, ips: 541.85575 images/sec
Training: 2023-09-28 05:28:57,160 - loss nan, lr: 0.012500, epoch: 3, step: 18600, eta: 5.60 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11811 sec, avg_samples: 32.00000, ips: 541.85575 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 18700, eta: 5.59 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11853 sec, avg_samples: 32.00000, ips: 539.94595 images/sec
Training: 2023-09-28 05:29:09,018 - loss nan, lr: 0.012500, epoch: 3, step: 18700, eta: 5.59 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11853 sec, avg_samples: 32.00000, ips: 539.94595 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 18800, eta: 5.57 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11865 sec, avg_samples: 32.00000, ips: 539.39957 images/sec
Training: 2023-09-28 05:29:20,888 - loss nan, lr: 0.012500, epoch: 3, step: 18800, eta: 5.57 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11865 sec, avg_samples: 32.00000, ips: 539.39957 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 18900, eta: 5.56 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11864 sec, avg_samples: 32.00000, ips: 539.44581 images/sec
Training: 2023-09-28 05:29:32,757 - loss nan, lr: 0.012500, epoch: 3, step: 18900, eta: 5.56 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11864 sec, avg_samples: 32.00000, ips: 539.44581 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 19000, eta: 5.55 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11865 sec, avg_samples: 32.00000, ips: 539.42004 images/sec
Training: 2023-09-28 05:29:44,626 - loss nan, lr: 0.012500, epoch: 3, step: 19000, eta: 5.55 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11865 sec, avg_samples: 32.00000, ips: 539.42004 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 19100, eta: 5.54 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11878 sec, avg_samples: 32.00000, ips: 538.79909 images/sec
Training: 2023-09-28 05:29:56,511 - loss nan, lr: 0.012500, epoch: 3, step: 19100, eta: 5.54 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11878 sec, avg_samples: 32.00000, ips: 538.79909 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 19200, eta: 5.53 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11880 sec, avg_samples: 32.00000, ips: 538.70514 images/sec
Training: 2023-09-28 05:30:08,398 - loss nan, lr: 0.012500, epoch: 3, step: 19200, eta: 5.53 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11880 sec, avg_samples: 32.00000, ips: 538.70514 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 19300, eta: 5.51 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11879 sec, avg_samples: 32.00000, ips: 538.78385 images/sec
Training: 2023-09-28 05:30:20,283 - loss nan, lr: 0.012500, epoch: 3, step: 19300, eta: 5.51 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11879 sec, avg_samples: 32.00000, ips: 538.78385 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 19400, eta: 5.50 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11880 sec, avg_samples: 32.00000, ips: 538.72916 images/sec
Training: 2023-09-28 05:30:32,169 - loss nan, lr: 0.012500, epoch: 3, step: 19400, eta: 5.50 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11880 sec, avg_samples: 32.00000, ips: 538.72916 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 19500, eta: 5.49 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11879 sec, avg_samples: 32.00000, ips: 538.77396 images/sec
Training: 2023-09-28 05:30:44,054 - loss nan, lr: 0.012500, epoch: 3, step: 19500, eta: 5.49 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11879 sec, avg_samples: 32.00000, ips: 538.77396 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 19600, eta: 5.48 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11866 sec, avg_samples: 32.00000, ips: 539.37557 images/sec
Training: 2023-09-28 05:30:55,925 - loss nan, lr: 0.012500, epoch: 3, step: 19600, eta: 5.48 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11866 sec, avg_samples: 32.00000, ips: 539.37557 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 19700, eta: 5.47 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11853 sec, avg_samples: 32.00000, ips: 539.93551 images/sec
Training: 2023-09-28 05:31:07,783 - loss nan, lr: 0.012500, epoch: 3, step: 19700, eta: 5.47 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11853 sec, avg_samples: 32.00000, ips: 539.93551 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 19800, eta: 5.46 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11851 sec, avg_samples: 32.00000, ips: 540.04580 images/sec
Training: 2023-09-28 05:31:19,640 - loss nan, lr: 0.012500, epoch: 3, step: 19800, eta: 5.46 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11851 sec, avg_samples: 32.00000, ips: 540.04580 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 19900, eta: 5.44 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11833 sec, avg_samples: 32.00000, ips: 540.84257 images/sec
Training: 2023-09-28 05:31:31,479 - loss nan, lr: 0.012500, epoch: 3, step: 19900, eta: 5.44 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11833 sec, avg_samples: 32.00000, ips: 540.84257 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 20000, eta: 5.43 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.91106 images/sec
Training: 2023-09-28 05:31:43,318 - loss nan, lr: 0.012500, epoch: 3, step: 20000, eta: 5.43 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.91106 images/sec
INFO:root:[lfw][20000]XNorm: 6.680172
Training: 2023-09-28 05:32:11,208 - [lfw][20000]XNorm: 6.680172
INFO:root:[lfw][20000]Accuracy-Flip: 0.89650+-0.01684
Training: 2023-09-28 05:32:11,208 - [lfw][20000]Accuracy-Flip: 0.89650+-0.01684
INFO:root:[lfw][20000]Accuracy-Highest: 0.90150
Training: 2023-09-28 05:32:11,208 - [lfw][20000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.8900
Training: 2023-09-28 05:32:11,208 - test time: 27.8900
INFO:root:[cfp_fp][20000]XNorm: 6.260105
Training: 2023-09-28 05:32:43,426 - [cfp_fp][20000]XNorm: 6.260105
INFO:root:[cfp_fp][20000]Accuracy-Flip: 0.64371+-0.01856
Training: 2023-09-28 05:32:43,427 - [cfp_fp][20000]Accuracy-Flip: 0.64371+-0.01856
INFO:root:[cfp_fp][20000]Accuracy-Highest: 0.65071
Training: 2023-09-28 05:32:43,427 - [cfp_fp][20000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2186
Training: 2023-09-28 05:32:43,427 - test time: 32.2186
INFO:root:[agedb_30][20000]XNorm: 6.359178
Training: 2023-09-28 05:33:11,156 - [agedb_30][20000]XNorm: 6.359178
INFO:root:[agedb_30][20000]Accuracy-Flip: 0.68800+-0.01963
Training: 2023-09-28 05:33:11,156 - [agedb_30][20000]Accuracy-Flip: 0.68800+-0.01963
INFO:root:[agedb_30][20000]Accuracy-Highest: 0.70083
Training: 2023-09-28 05:33:11,156 - [agedb_30][20000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7292
Training: 2023-09-28 05:33:11,156 - test time: 27.7292
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 20100, eta: 5.57 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11788 sec, avg_samples: 32.00000, ips: 542.93646 images/sec
Training: 2023-09-28 05:33:22,949 - loss nan, lr: 0.012500, epoch: 3, step: 20100, eta: 5.57 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11788 sec, avg_samples: 32.00000, ips: 542.93646 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 20200, eta: 5.56 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11800 sec, avg_samples: 32.00000, ips: 542.37706 images/sec
Training: 2023-09-28 05:33:34,754 - loss nan, lr: 0.012500, epoch: 3, step: 20200, eta: 5.56 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11800 sec, avg_samples: 32.00000, ips: 542.37706 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 20300, eta: 5.55 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11830 sec, avg_samples: 32.00000, ips: 541.01242 images/sec
Training: 2023-09-28 05:33:46,590 - loss nan, lr: 0.012500, epoch: 3, step: 20300, eta: 5.55 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11830 sec, avg_samples: 32.00000, ips: 541.01242 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 20400, eta: 5.54 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11825 sec, avg_samples: 32.00000, ips: 541.21637 images/sec
Training: 2023-09-28 05:33:58,422 - loss nan, lr: 0.012500, epoch: 3, step: 20400, eta: 5.54 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11825 sec, avg_samples: 32.00000, ips: 541.21637 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 20500, eta: 5.52 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.07298 images/sec
Training: 2023-09-28 05:34:10,256 - loss nan, lr: 0.012500, epoch: 3, step: 20500, eta: 5.52 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.07298 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 20600, eta: 5.51 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11839 sec, avg_samples: 32.00000, ips: 540.58434 images/sec
Training: 2023-09-28 05:34:22,101 - loss nan, lr: 0.012500, epoch: 3, step: 20600, eta: 5.51 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11839 sec, avg_samples: 32.00000, ips: 540.58434 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 20700, eta: 5.50 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11871 sec, avg_samples: 32.00000, ips: 539.11946 images/sec
Training: 2023-09-28 05:34:33,979 - loss nan, lr: 0.012500, epoch: 3, step: 20700, eta: 5.50 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11871 sec, avg_samples: 32.00000, ips: 539.11946 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 20800, eta: 5.49 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11840 sec, avg_samples: 32.00000, ips: 540.56197 images/sec
Training: 2023-09-28 05:34:45,824 - loss nan, lr: 0.012500, epoch: 3, step: 20800, eta: 5.49 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11840 sec, avg_samples: 32.00000, ips: 540.56197 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 20900, eta: 5.48 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.69372 images/sec
Training: 2023-09-28 05:34:57,643 - loss nan, lr: 0.012500, epoch: 3, step: 20900, eta: 5.48 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.69372 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 21000, eta: 5.47 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.14600 images/sec
Training: 2023-09-28 05:35:09,476 - loss nan, lr: 0.012500, epoch: 3, step: 21000, eta: 5.47 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.14600 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 21100, eta: 5.46 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.05135 images/sec
Training: 2023-09-28 05:35:21,311 - loss nan, lr: 0.012500, epoch: 3, step: 21100, eta: 5.46 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.05135 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 21200, eta: 5.44 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11830 sec, avg_samples: 32.00000, ips: 540.98493 images/sec
Training: 2023-09-28 05:35:33,148 - loss nan, lr: 0.012500, epoch: 3, step: 21200, eta: 5.44 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11830 sec, avg_samples: 32.00000, ips: 540.98493 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 21300, eta: 5.43 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.09435 images/sec
Training: 2023-09-28 05:35:44,982 - loss nan, lr: 0.012500, epoch: 3, step: 21300, eta: 5.43 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.09435 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 21400, eta: 5.42 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.78741 images/sec
Training: 2023-09-28 05:35:56,822 - loss nan, lr: 0.012500, epoch: 3, step: 21400, eta: 5.42 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.78741 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 21500, eta: 5.41 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.44346 images/sec
Training: 2023-09-28 05:36:08,647 - loss nan, lr: 0.012500, epoch: 3, step: 21500, eta: 5.41 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.44346 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 21600, eta: 5.40 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11869 sec, avg_samples: 32.00000, ips: 539.23927 images/sec
Training: 2023-09-28 05:36:20,522 - loss nan, lr: 0.012500, epoch: 3, step: 21600, eta: 5.40 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11869 sec, avg_samples: 32.00000, ips: 539.23927 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 21700, eta: 5.39 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11870 sec, avg_samples: 32.00000, ips: 539.19066 images/sec
Training: 2023-09-28 05:36:32,399 - loss nan, lr: 0.012500, epoch: 3, step: 21700, eta: 5.39 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11870 sec, avg_samples: 32.00000, ips: 539.19066 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 21800, eta: 5.38 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11869 sec, avg_samples: 32.00000, ips: 539.22947 images/sec
Training: 2023-09-28 05:36:44,274 - loss nan, lr: 0.012500, epoch: 3, step: 21800, eta: 5.38 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11869 sec, avg_samples: 32.00000, ips: 539.22947 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 21900, eta: 5.37 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11867 sec, avg_samples: 32.00000, ips: 539.33119 images/sec
Training: 2023-09-28 05:36:56,147 - loss nan, lr: 0.012500, epoch: 3, step: 21900, eta: 5.37 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11867 sec, avg_samples: 32.00000, ips: 539.33119 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 22000, eta: 5.36 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11870 sec, avg_samples: 32.00000, ips: 539.17224 images/sec
Training: 2023-09-28 05:37:08,024 - loss nan, lr: 0.012500, epoch: 3, step: 22000, eta: 5.36 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11870 sec, avg_samples: 32.00000, ips: 539.17224 images/sec
INFO:root:[lfw][22000]XNorm: 5.931161
Training: 2023-09-28 05:37:35,881 - [lfw][22000]XNorm: 5.931161
INFO:root:[lfw][22000]Accuracy-Flip: 0.89600+-0.01504
Training: 2023-09-28 05:37:35,881 - [lfw][22000]Accuracy-Flip: 0.89600+-0.01504
INFO:root:[lfw][22000]Accuracy-Highest: 0.90150
Training: 2023-09-28 05:37:35,881 - [lfw][22000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.8576
Training: 2023-09-28 05:37:35,881 - test time: 27.8576
INFO:root:[cfp_fp][22000]XNorm: 5.540654
Training: 2023-09-28 05:38:08,112 - [cfp_fp][22000]XNorm: 5.540654
INFO:root:[cfp_fp][22000]Accuracy-Flip: 0.64443+-0.01892
Training: 2023-09-28 05:38:08,112 - [cfp_fp][22000]Accuracy-Flip: 0.64443+-0.01892
INFO:root:[cfp_fp][22000]Accuracy-Highest: 0.65071
Training: 2023-09-28 05:38:08,112 - [cfp_fp][22000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2307
Training: 2023-09-28 05:38:08,112 - test time: 32.2307
INFO:root:[agedb_30][22000]XNorm: 5.626011
Training: 2023-09-28 05:38:35,857 - [agedb_30][22000]XNorm: 5.626011
INFO:root:[agedb_30][22000]Accuracy-Flip: 0.68600+-0.02223
Training: 2023-09-28 05:38:35,858 - [agedb_30][22000]Accuracy-Flip: 0.68600+-0.02223
INFO:root:[agedb_30][22000]Accuracy-Highest: 0.70083
Training: 2023-09-28 05:38:35,858 - [agedb_30][22000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7454
Training: 2023-09-28 05:38:35,858 - test time: 27.7454
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 22100, eta: 5.48 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.04453 images/sec
Training: 2023-09-28 05:38:47,693 - loss nan, lr: 0.012500, epoch: 3, step: 22100, eta: 5.48 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.04453 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 22200, eta: 5.47 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.59437 images/sec
Training: 2023-09-28 05:38:59,516 - loss nan, lr: 0.012500, epoch: 3, step: 22200, eta: 5.47 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.59437 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 22300, eta: 5.46 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.08150 images/sec
Training: 2023-09-28 05:39:11,351 - loss nan, lr: 0.012500, epoch: 3, step: 22300, eta: 5.46 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.08150 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 22400, eta: 5.45 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.90720 images/sec
Training: 2023-09-28 05:39:23,189 - loss nan, lr: 0.012500, epoch: 3, step: 22400, eta: 5.45 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.90720 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 22500, eta: 5.44 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.46909 images/sec
Training: 2023-09-28 05:39:35,014 - loss nan, lr: 0.012500, epoch: 3, step: 22500, eta: 5.44 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.46909 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 22600, eta: 5.43 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11806 sec, avg_samples: 32.00000, ips: 542.09099 images/sec
Training: 2023-09-28 05:39:46,825 - loss nan, lr: 0.012500, epoch: 3, step: 22600, eta: 5.43 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11806 sec, avg_samples: 32.00000, ips: 542.09099 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 22700, eta: 5.42 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11842 sec, avg_samples: 32.00000, ips: 540.44428 images/sec
Training: 2023-09-28 05:39:58,672 - loss nan, lr: 0.012500, epoch: 3, step: 22700, eta: 5.42 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11842 sec, avg_samples: 32.00000, ips: 540.44428 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 22800, eta: 5.41 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11858 sec, avg_samples: 32.00000, ips: 539.73138 images/sec
Training: 2023-09-28 05:40:10,535 - loss nan, lr: 0.012500, epoch: 3, step: 22800, eta: 5.41 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11858 sec, avg_samples: 32.00000, ips: 539.73138 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 3, step: 22900, eta: 5.39 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11867 sec, avg_samples: 32.00000, ips: 539.32590 images/sec
Training: 2023-09-28 05:40:22,409 - loss nan, lr: 0.012500, epoch: 3, step: 22900, eta: 5.39 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11867 sec, avg_samples: 32.00000, ips: 539.32590 images/sec
INFO:root:Save model to MS1M_v3_arcface_static_0.1/FresResNet50/3.
Training: 2023-09-28 05:40:33,635 - Save model to MS1M_v3_arcface_static_0.1/FresResNet50/3.
INFO:root:Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/2.
Training: 2023-09-28 05:40:33,636 - Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/2.
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 23000, eta: 5.38 hours, avg_reader_cost: 0.00170 sec, avg_batch_cost: 0.01117 sec, avg_samples: 2.56000, ips: 458.25576 images/sec
Training: 2023-09-28 05:40:34,783 - loss nan, lr: 0.012500, epoch: 4, step: 23000, eta: 5.38 hours, avg_reader_cost: 0.00170 sec, avg_batch_cost: 0.01117 sec, avg_samples: 2.56000, ips: 458.25576 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 23100, eta: 5.37 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11842 sec, avg_samples: 32.00000, ips: 540.46539 images/sec
Training: 2023-09-28 05:40:46,630 - loss nan, lr: 0.012500, epoch: 4, step: 23100, eta: 5.37 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11842 sec, avg_samples: 32.00000, ips: 540.46539 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 23200, eta: 5.36 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.66226 images/sec
Training: 2023-09-28 05:40:58,474 - loss nan, lr: 0.012500, epoch: 4, step: 23200, eta: 5.36 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.66226 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 23300, eta: 5.35 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11840 sec, avg_samples: 32.00000, ips: 540.52328 images/sec
Training: 2023-09-28 05:41:10,320 - loss nan, lr: 0.012500, epoch: 4, step: 23300, eta: 5.35 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11840 sec, avg_samples: 32.00000, ips: 540.52328 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 23400, eta: 5.34 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11881 sec, avg_samples: 32.00000, ips: 538.69627 images/sec
Training: 2023-09-28 05:41:22,208 - loss nan, lr: 0.012500, epoch: 4, step: 23400, eta: 5.34 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11881 sec, avg_samples: 32.00000, ips: 538.69627 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 23500, eta: 5.33 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11883 sec, avg_samples: 32.00000, ips: 538.59718 images/sec
Training: 2023-09-28 05:41:34,098 - loss nan, lr: 0.012500, epoch: 4, step: 23500, eta: 5.33 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11883 sec, avg_samples: 32.00000, ips: 538.59718 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 23600, eta: 5.32 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11878 sec, avg_samples: 32.00000, ips: 538.80702 images/sec
Training: 2023-09-28 05:41:45,983 - loss nan, lr: 0.012500, epoch: 4, step: 23600, eta: 5.32 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11878 sec, avg_samples: 32.00000, ips: 538.80702 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 23700, eta: 5.31 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11842 sec, avg_samples: 32.00000, ips: 540.43273 images/sec
Training: 2023-09-28 05:41:57,833 - loss nan, lr: 0.012500, epoch: 4, step: 23700, eta: 5.31 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11842 sec, avg_samples: 32.00000, ips: 540.43273 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 23800, eta: 5.30 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11848 sec, avg_samples: 32.00000, ips: 540.17296 images/sec
Training: 2023-09-28 05:42:09,688 - loss nan, lr: 0.012500, epoch: 4, step: 23800, eta: 5.30 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11848 sec, avg_samples: 32.00000, ips: 540.17296 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 23900, eta: 5.29 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11844 sec, avg_samples: 32.00000, ips: 540.34443 images/sec
Training: 2023-09-28 05:42:21,539 - loss nan, lr: 0.012500, epoch: 4, step: 23900, eta: 5.29 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11844 sec, avg_samples: 32.00000, ips: 540.34443 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 24000, eta: 5.28 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11843 sec, avg_samples: 32.00000, ips: 540.40282 images/sec
Training: 2023-09-28 05:42:33,389 - loss nan, lr: 0.012500, epoch: 4, step: 24000, eta: 5.28 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11843 sec, avg_samples: 32.00000, ips: 540.40282 images/sec
INFO:root:[lfw][24000]XNorm: 5.242456
Training: 2023-09-28 05:43:01,244 - [lfw][24000]XNorm: 5.242456
INFO:root:[lfw][24000]Accuracy-Flip: 0.90033+-0.01871
Training: 2023-09-28 05:43:01,244 - [lfw][24000]Accuracy-Flip: 0.90033+-0.01871
INFO:root:[lfw][24000]Accuracy-Highest: 0.90150
Training: 2023-09-28 05:43:01,244 - [lfw][24000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.8549
Training: 2023-09-28 05:43:01,244 - test time: 27.8549
INFO:root:[cfp_fp][24000]XNorm: 4.892076
Training: 2023-09-28 05:43:33,480 - [cfp_fp][24000]XNorm: 4.892076
INFO:root:[cfp_fp][24000]Accuracy-Flip: 0.63729+-0.01748
Training: 2023-09-28 05:43:33,480 - [cfp_fp][24000]Accuracy-Flip: 0.63729+-0.01748
INFO:root:[cfp_fp][24000]Accuracy-Highest: 0.65071
Training: 2023-09-28 05:43:33,480 - [cfp_fp][24000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2361
Training: 2023-09-28 05:43:33,480 - test time: 32.2361
INFO:root:[agedb_30][24000]XNorm: 4.973765
Training: 2023-09-28 05:44:01,211 - [agedb_30][24000]XNorm: 4.973765
INFO:root:[agedb_30][24000]Accuracy-Flip: 0.69183+-0.02034
Training: 2023-09-28 05:44:01,211 - [agedb_30][24000]Accuracy-Flip: 0.69183+-0.02034
INFO:root:[agedb_30][24000]Accuracy-Highest: 0.70083
Training: 2023-09-28 05:44:01,211 - [agedb_30][24000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7311
Training: 2023-09-28 05:44:01,211 - test time: 27.7311
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 24100, eta: 5.39 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11809 sec, avg_samples: 32.00000, ips: 541.94633 images/sec
Training: 2023-09-28 05:44:13,027 - loss nan, lr: 0.012500, epoch: 4, step: 24100, eta: 5.39 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11809 sec, avg_samples: 32.00000, ips: 541.94633 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 24200, eta: 5.38 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11809 sec, avg_samples: 32.00000, ips: 541.93740 images/sec
Training: 2023-09-28 05:44:24,842 - loss nan, lr: 0.012500, epoch: 4, step: 24200, eta: 5.38 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11809 sec, avg_samples: 32.00000, ips: 541.93740 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 24300, eta: 5.37 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11800 sec, avg_samples: 32.00000, ips: 542.38087 images/sec
Training: 2023-09-28 05:44:36,647 - loss nan, lr: 0.012500, epoch: 4, step: 24300, eta: 5.37 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11800 sec, avg_samples: 32.00000, ips: 542.38087 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 24400, eta: 5.36 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11800 sec, avg_samples: 32.00000, ips: 542.37869 images/sec
Training: 2023-09-28 05:44:48,451 - loss nan, lr: 0.012500, epoch: 4, step: 24400, eta: 5.36 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11800 sec, avg_samples: 32.00000, ips: 542.37869 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 24500, eta: 5.35 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11799 sec, avg_samples: 32.00000, ips: 542.42978 images/sec
Training: 2023-09-28 05:45:00,254 - loss nan, lr: 0.012500, epoch: 4, step: 24500, eta: 5.35 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11799 sec, avg_samples: 32.00000, ips: 542.42978 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 24600, eta: 5.34 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11809 sec, avg_samples: 32.00000, ips: 541.98092 images/sec
Training: 2023-09-28 05:45:12,067 - loss nan, lr: 0.012500, epoch: 4, step: 24600, eta: 5.34 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11809 sec, avg_samples: 32.00000, ips: 541.98092 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 24700, eta: 5.33 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.25924 images/sec
Training: 2023-09-28 05:45:23,874 - loss nan, lr: 0.012500, epoch: 4, step: 24700, eta: 5.33 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.25924 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 24800, eta: 5.32 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.29202 images/sec
Training: 2023-09-28 05:45:35,680 - loss nan, lr: 0.012500, epoch: 4, step: 24800, eta: 5.32 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.29202 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 24900, eta: 5.31 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11797 sec, avg_samples: 32.00000, ips: 542.50390 images/sec
Training: 2023-09-28 05:45:47,482 - loss nan, lr: 0.012500, epoch: 4, step: 24900, eta: 5.31 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11797 sec, avg_samples: 32.00000, ips: 542.50390 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 25000, eta: 5.30 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11800 sec, avg_samples: 32.00000, ips: 542.35596 images/sec
Training: 2023-09-28 05:45:59,287 - loss nan, lr: 0.012500, epoch: 4, step: 25000, eta: 5.30 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11800 sec, avg_samples: 32.00000, ips: 542.35596 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 25100, eta: 5.29 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11801 sec, avg_samples: 32.00000, ips: 542.30911 images/sec
Training: 2023-09-28 05:46:11,093 - loss nan, lr: 0.012500, epoch: 4, step: 25100, eta: 5.29 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11801 sec, avg_samples: 32.00000, ips: 542.30911 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 25200, eta: 5.28 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11799 sec, avg_samples: 32.00000, ips: 542.41300 images/sec
Training: 2023-09-28 05:46:22,897 - loss nan, lr: 0.012500, epoch: 4, step: 25200, eta: 5.28 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11799 sec, avg_samples: 32.00000, ips: 542.41300 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 25300, eta: 5.27 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11803 sec, avg_samples: 32.00000, ips: 542.25059 images/sec
Training: 2023-09-28 05:46:34,704 - loss nan, lr: 0.012500, epoch: 4, step: 25300, eta: 5.27 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11803 sec, avg_samples: 32.00000, ips: 542.25059 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 25400, eta: 5.26 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11809 sec, avg_samples: 32.00000, ips: 541.97393 images/sec
Training: 2023-09-28 05:46:46,517 - loss nan, lr: 0.012500, epoch: 4, step: 25400, eta: 5.26 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11809 sec, avg_samples: 32.00000, ips: 541.97393 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 25500, eta: 5.25 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11805 sec, avg_samples: 32.00000, ips: 542.12039 images/sec
Training: 2023-09-28 05:46:58,327 - loss nan, lr: 0.012500, epoch: 4, step: 25500, eta: 5.25 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11805 sec, avg_samples: 32.00000, ips: 542.12039 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 25600, eta: 5.24 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11807 sec, avg_samples: 32.00000, ips: 542.07214 images/sec
Training: 2023-09-28 05:47:10,138 - loss nan, lr: 0.012500, epoch: 4, step: 25600, eta: 5.24 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11807 sec, avg_samples: 32.00000, ips: 542.07214 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 25700, eta: 5.23 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.11808 sec, avg_samples: 32.00000, ips: 541.98702 images/sec
Training: 2023-09-28 05:47:21,951 - loss nan, lr: 0.012500, epoch: 4, step: 25700, eta: 5.23 hours, avg_reader_cost: 0.00004 sec, avg_batch_cost: 0.11808 sec, avg_samples: 32.00000, ips: 541.98702 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 25800, eta: 5.22 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.76034 images/sec
Training: 2023-09-28 05:47:33,770 - loss nan, lr: 0.012500, epoch: 4, step: 25800, eta: 5.22 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.76034 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 25900, eta: 5.21 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11809 sec, avg_samples: 32.00000, ips: 541.96461 images/sec
Training: 2023-09-28 05:47:45,584 - loss nan, lr: 0.012500, epoch: 4, step: 25900, eta: 5.21 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11809 sec, avg_samples: 32.00000, ips: 541.96461 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 26000, eta: 5.20 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11806 sec, avg_samples: 32.00000, ips: 542.09299 images/sec
Training: 2023-09-28 05:47:57,395 - loss nan, lr: 0.012500, epoch: 4, step: 26000, eta: 5.20 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11806 sec, avg_samples: 32.00000, ips: 542.09299 images/sec
INFO:root:[lfw][26000]XNorm: 4.608163
Training: 2023-09-28 05:48:25,270 - [lfw][26000]XNorm: 4.608163
INFO:root:[lfw][26000]Accuracy-Flip: 0.89700+-0.01641
Training: 2023-09-28 05:48:25,270 - [lfw][26000]Accuracy-Flip: 0.89700+-0.01641
INFO:root:[lfw][26000]Accuracy-Highest: 0.90150
Training: 2023-09-28 05:48:25,270 - [lfw][26000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.8753
Training: 2023-09-28 05:48:25,270 - test time: 27.8753
INFO:root:[cfp_fp][26000]XNorm: 4.303696
Training: 2023-09-28 05:48:57,509 - [cfp_fp][26000]XNorm: 4.303696
INFO:root:[cfp_fp][26000]Accuracy-Flip: 0.63871+-0.01846
Training: 2023-09-28 05:48:57,509 - [cfp_fp][26000]Accuracy-Flip: 0.63871+-0.01846
INFO:root:[cfp_fp][26000]Accuracy-Highest: 0.65071
Training: 2023-09-28 05:48:57,509 - [cfp_fp][26000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2387
Training: 2023-09-28 05:48:57,509 - test time: 32.2387
INFO:root:[agedb_30][26000]XNorm: 4.375364
Training: 2023-09-28 05:49:25,241 - [agedb_30][26000]XNorm: 4.375364
INFO:root:[agedb_30][26000]Accuracy-Flip: 0.68950+-0.02155
Training: 2023-09-28 05:49:25,241 - [agedb_30][26000]Accuracy-Flip: 0.68950+-0.02155
INFO:root:[agedb_30][26000]Accuracy-Highest: 0.70083
Training: 2023-09-28 05:49:25,241 - [agedb_30][26000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7325
Training: 2023-09-28 05:49:25,241 - test time: 27.7325
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 26100, eta: 5.30 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11807 sec, avg_samples: 32.00000, ips: 542.03314 images/sec
Training: 2023-09-28 05:49:37,055 - loss nan, lr: 0.012500, epoch: 4, step: 26100, eta: 5.30 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11807 sec, avg_samples: 32.00000, ips: 542.03314 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 26200, eta: 5.29 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11810 sec, avg_samples: 32.00000, ips: 541.90224 images/sec
Training: 2023-09-28 05:49:48,871 - loss nan, lr: 0.012500, epoch: 4, step: 26200, eta: 5.29 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11810 sec, avg_samples: 32.00000, ips: 541.90224 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 26300, eta: 5.28 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.19809 images/sec
Training: 2023-09-28 05:50:00,703 - loss nan, lr: 0.012500, epoch: 4, step: 26300, eta: 5.28 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.19809 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 26400, eta: 5.27 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.06424 images/sec
Training: 2023-09-28 05:50:12,538 - loss nan, lr: 0.012500, epoch: 4, step: 26400, eta: 5.27 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.06424 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 26500, eta: 5.26 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.08777 images/sec
Training: 2023-09-28 05:50:24,372 - loss nan, lr: 0.012500, epoch: 4, step: 26500, eta: 5.26 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.08777 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 26600, eta: 5.25 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.07498 images/sec
Training: 2023-09-28 05:50:36,207 - loss nan, lr: 0.012500, epoch: 4, step: 26600, eta: 5.25 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.07498 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 26700, eta: 5.24 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11830 sec, avg_samples: 32.00000, ips: 540.99776 images/sec
Training: 2023-09-28 05:50:48,043 - loss nan, lr: 0.012500, epoch: 4, step: 26700, eta: 5.24 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11830 sec, avg_samples: 32.00000, ips: 540.99776 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 26800, eta: 5.23 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11830 sec, avg_samples: 32.00000, ips: 541.00699 images/sec
Training: 2023-09-28 05:50:59,879 - loss nan, lr: 0.012500, epoch: 4, step: 26800, eta: 5.23 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11830 sec, avg_samples: 32.00000, ips: 541.00699 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 26900, eta: 5.22 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.13920 images/sec
Training: 2023-09-28 05:51:11,712 - loss nan, lr: 0.012500, epoch: 4, step: 26900, eta: 5.22 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.13920 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 27000, eta: 5.21 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.65621 images/sec
Training: 2023-09-28 05:51:23,533 - loss nan, lr: 0.012500, epoch: 4, step: 27000, eta: 5.21 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.65621 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 27100, eta: 5.20 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11807 sec, avg_samples: 32.00000, ips: 542.06724 images/sec
Training: 2023-09-28 05:51:35,345 - loss nan, lr: 0.012500, epoch: 4, step: 27100, eta: 5.20 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11807 sec, avg_samples: 32.00000, ips: 542.06724 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 27200, eta: 5.19 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11807 sec, avg_samples: 32.00000, ips: 542.05849 images/sec
Training: 2023-09-28 05:51:47,157 - loss nan, lr: 0.012500, epoch: 4, step: 27200, eta: 5.19 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11807 sec, avg_samples: 32.00000, ips: 542.05849 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 27300, eta: 5.19 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.64693 images/sec
Training: 2023-09-28 05:51:58,978 - loss nan, lr: 0.012500, epoch: 4, step: 27300, eta: 5.19 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.64693 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 27400, eta: 5.18 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.91866 images/sec
Training: 2023-09-28 05:52:10,816 - loss nan, lr: 0.012500, epoch: 4, step: 27400, eta: 5.18 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.91866 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 27500, eta: 5.17 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.06729 images/sec
Training: 2023-09-28 05:52:22,651 - loss nan, lr: 0.012500, epoch: 4, step: 27500, eta: 5.17 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.06729 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 27600, eta: 5.16 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.06284 images/sec
Training: 2023-09-28 05:52:34,486 - loss nan, lr: 0.012500, epoch: 4, step: 27600, eta: 5.16 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.06284 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 27700, eta: 5.15 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.03595 images/sec
Training: 2023-09-28 05:52:46,322 - loss nan, lr: 0.012500, epoch: 4, step: 27700, eta: 5.15 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.03595 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 27800, eta: 5.14 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11840 sec, avg_samples: 32.00000, ips: 540.52717 images/sec
Training: 2023-09-28 05:52:58,168 - loss nan, lr: 0.012500, epoch: 4, step: 27800, eta: 5.14 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11840 sec, avg_samples: 32.00000, ips: 540.52717 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 27900, eta: 5.13 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.05133 images/sec
Training: 2023-09-28 05:53:10,004 - loss nan, lr: 0.012500, epoch: 4, step: 27900, eta: 5.13 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.05133 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 28000, eta: 5.12 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.07409 images/sec
Training: 2023-09-28 05:53:21,838 - loss nan, lr: 0.012500, epoch: 4, step: 28000, eta: 5.12 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.07409 images/sec
INFO:root:[lfw][28000]XNorm: 4.100103
Training: 2023-09-28 05:53:49,743 - [lfw][28000]XNorm: 4.100103
INFO:root:[lfw][28000]Accuracy-Flip: 0.89700+-0.01676
Training: 2023-09-28 05:53:49,743 - [lfw][28000]Accuracy-Flip: 0.89700+-0.01676
INFO:root:[lfw][28000]Accuracy-Highest: 0.90150
Training: 2023-09-28 05:53:49,743 - [lfw][28000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9049
Training: 2023-09-28 05:53:49,743 - test time: 27.9049
INFO:root:[cfp_fp][28000]XNorm: 3.837603
Training: 2023-09-28 05:54:22,029 - [cfp_fp][28000]XNorm: 3.837603
INFO:root:[cfp_fp][28000]Accuracy-Flip: 0.63900+-0.01851
Training: 2023-09-28 05:54:22,029 - [cfp_fp][28000]Accuracy-Flip: 0.63900+-0.01851
INFO:root:[cfp_fp][28000]Accuracy-Highest: 0.65071
Training: 2023-09-28 05:54:22,029 - [cfp_fp][28000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2857
Training: 2023-09-28 05:54:22,029 - test time: 32.2857
INFO:root:[agedb_30][28000]XNorm: 3.895937
Training: 2023-09-28 05:54:49,761 - [agedb_30][28000]XNorm: 3.895937
INFO:root:[agedb_30][28000]Accuracy-Flip: 0.68917+-0.02488
Training: 2023-09-28 05:54:49,761 - [agedb_30][28000]Accuracy-Flip: 0.68917+-0.02488
INFO:root:[agedb_30][28000]Accuracy-Highest: 0.70083
Training: 2023-09-28 05:54:49,761 - [agedb_30][28000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7320
Training: 2023-09-28 05:54:49,761 - test time: 27.7320
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 28100, eta: 5.21 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.20208 images/sec
Training: 2023-09-28 05:55:01,593 - loss nan, lr: 0.012500, epoch: 4, step: 28100, eta: 5.21 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.20208 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 28200, eta: 5.20 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11808 sec, avg_samples: 32.00000, ips: 542.00350 images/sec
Training: 2023-09-28 05:55:13,407 - loss nan, lr: 0.012500, epoch: 4, step: 28200, eta: 5.20 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11808 sec, avg_samples: 32.00000, ips: 542.00350 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 28300, eta: 5.19 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.06973 images/sec
Training: 2023-09-28 05:55:25,242 - loss nan, lr: 0.012500, epoch: 4, step: 28300, eta: 5.19 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.06973 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 28400, eta: 5.18 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11892 sec, avg_samples: 32.00000, ips: 538.19003 images/sec
Training: 2023-09-28 05:55:37,140 - loss nan, lr: 0.012500, epoch: 4, step: 28400, eta: 5.18 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11892 sec, avg_samples: 32.00000, ips: 538.19003 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 28500, eta: 5.17 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11893 sec, avg_samples: 32.00000, ips: 538.13604 images/sec
Training: 2023-09-28 05:55:49,039 - loss nan, lr: 0.012500, epoch: 4, step: 28500, eta: 5.17 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11893 sec, avg_samples: 32.00000, ips: 538.13604 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 28600, eta: 5.16 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11870 sec, avg_samples: 32.00000, ips: 539.15382 images/sec
Training: 2023-09-28 05:56:00,916 - loss nan, lr: 0.012500, epoch: 4, step: 28600, eta: 5.16 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11870 sec, avg_samples: 32.00000, ips: 539.15382 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 4, step: 28700, eta: 5.16 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11846 sec, avg_samples: 32.00000, ips: 540.28844 images/sec
Training: 2023-09-28 05:56:12,768 - loss nan, lr: 0.012500, epoch: 4, step: 28700, eta: 5.16 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11846 sec, avg_samples: 32.00000, ips: 540.28844 images/sec
INFO:root:Save model to MS1M_v3_arcface_static_0.1/FresResNet50/4.
Training: 2023-09-28 05:56:17,848 - Save model to MS1M_v3_arcface_static_0.1/FresResNet50/4.
INFO:root:Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/3.
Training: 2023-09-28 05:56:17,848 - Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/3.
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 28800, eta: 5.15 hours, avg_reader_cost: 0.00172 sec, avg_batch_cost: 0.07264 sec, avg_samples: 19.20000, ips: 528.63588 images/sec
Training: 2023-09-28 05:56:25,144 - loss nan, lr: 0.012500, epoch: 5, step: 28800, eta: 5.15 hours, avg_reader_cost: 0.00172 sec, avg_batch_cost: 0.07264 sec, avg_samples: 19.20000, ips: 528.63588 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 28900, eta: 5.14 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11836 sec, avg_samples: 32.00000, ips: 540.72239 images/sec
Training: 2023-09-28 05:56:36,985 - loss nan, lr: 0.012500, epoch: 5, step: 28900, eta: 5.14 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11836 sec, avg_samples: 32.00000, ips: 540.72239 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 29000, eta: 5.13 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11854 sec, avg_samples: 32.00000, ips: 539.91576 images/sec
Training: 2023-09-28 05:56:48,845 - loss nan, lr: 0.012500, epoch: 5, step: 29000, eta: 5.13 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11854 sec, avg_samples: 32.00000, ips: 539.91576 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 29100, eta: 5.12 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11847 sec, avg_samples: 32.00000, ips: 540.24107 images/sec
Training: 2023-09-28 05:57:00,698 - loss nan, lr: 0.012500, epoch: 5, step: 29100, eta: 5.12 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11847 sec, avg_samples: 32.00000, ips: 540.24107 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 29200, eta: 5.11 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11848 sec, avg_samples: 32.00000, ips: 540.18057 images/sec
Training: 2023-09-28 05:57:12,552 - loss nan, lr: 0.012500, epoch: 5, step: 29200, eta: 5.11 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11848 sec, avg_samples: 32.00000, ips: 540.18057 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 29300, eta: 5.10 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11839 sec, avg_samples: 32.00000, ips: 540.60336 images/sec
Training: 2023-09-28 05:57:24,396 - loss nan, lr: 0.012500, epoch: 5, step: 29300, eta: 5.10 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11839 sec, avg_samples: 32.00000, ips: 540.60336 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 29400, eta: 5.09 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.56306 images/sec
Training: 2023-09-28 05:57:36,218 - loss nan, lr: 0.012500, epoch: 5, step: 29400, eta: 5.09 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.56306 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 29500, eta: 5.08 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11852 sec, avg_samples: 32.00000, ips: 539.99652 images/sec
Training: 2023-09-28 05:57:48,075 - loss nan, lr: 0.012500, epoch: 5, step: 29500, eta: 5.08 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11852 sec, avg_samples: 32.00000, ips: 539.99652 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 29600, eta: 5.07 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11893 sec, avg_samples: 32.00000, ips: 538.14515 images/sec
Training: 2023-09-28 05:57:59,973 - loss nan, lr: 0.012500, epoch: 5, step: 29600, eta: 5.07 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11893 sec, avg_samples: 32.00000, ips: 538.14515 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 29700, eta: 5.07 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11882 sec, avg_samples: 32.00000, ips: 538.64671 images/sec
Training: 2023-09-28 05:58:11,860 - loss nan, lr: 0.012500, epoch: 5, step: 29700, eta: 5.07 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11882 sec, avg_samples: 32.00000, ips: 538.64671 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 29800, eta: 5.06 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.55284 images/sec
Training: 2023-09-28 05:58:23,682 - loss nan, lr: 0.012500, epoch: 5, step: 29800, eta: 5.06 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.55284 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 29900, eta: 5.05 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.52222 images/sec
Training: 2023-09-28 05:58:35,506 - loss nan, lr: 0.012500, epoch: 5, step: 29900, eta: 5.05 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.52222 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 30000, eta: 5.04 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.32928 images/sec
Training: 2023-09-28 05:58:47,333 - loss nan, lr: 0.012500, epoch: 5, step: 30000, eta: 5.04 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.32928 images/sec
INFO:root:[lfw][30000]XNorm: 3.591997
Training: 2023-09-28 05:59:15,258 - [lfw][30000]XNorm: 3.591997
INFO:root:[lfw][30000]Accuracy-Flip: 0.89800+-0.01703
Training: 2023-09-28 05:59:15,258 - [lfw][30000]Accuracy-Flip: 0.89800+-0.01703
INFO:root:[lfw][30000]Accuracy-Highest: 0.90150
Training: 2023-09-28 05:59:15,258 - [lfw][30000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9241
Training: 2023-09-28 05:59:15,258 - test time: 27.9241
INFO:root:[cfp_fp][30000]XNorm: 3.359054
Training: 2023-09-28 05:59:47,477 - [cfp_fp][30000]XNorm: 3.359054
INFO:root:[cfp_fp][30000]Accuracy-Flip: 0.64071+-0.02031
Training: 2023-09-28 05:59:47,478 - [cfp_fp][30000]Accuracy-Flip: 0.64071+-0.02031
INFO:root:[cfp_fp][30000]Accuracy-Highest: 0.65071
Training: 2023-09-28 05:59:47,478 - [cfp_fp][30000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2198
Training: 2023-09-28 05:59:47,478 - test time: 32.2198
INFO:root:[agedb_30][30000]XNorm: 3.412434
Training: 2023-09-28 06:00:15,202 - [agedb_30][30000]XNorm: 3.412434
INFO:root:[agedb_30][30000]Accuracy-Flip: 0.68867+-0.02011
Training: 2023-09-28 06:00:15,202 - [agedb_30][30000]Accuracy-Flip: 0.68867+-0.02011
INFO:root:[agedb_30][30000]Accuracy-Highest: 0.70083
Training: 2023-09-28 06:00:15,202 - [agedb_30][30000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7242
Training: 2023-09-28 06:00:15,202 - test time: 27.7242
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 30100, eta: 5.12 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11791 sec, avg_samples: 32.00000, ips: 542.80608 images/sec
Training: 2023-09-28 06:00:26,998 - loss nan, lr: 0.012500, epoch: 5, step: 30100, eta: 5.12 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11791 sec, avg_samples: 32.00000, ips: 542.80608 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 30200, eta: 5.11 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11795 sec, avg_samples: 32.00000, ips: 542.59582 images/sec
Training: 2023-09-28 06:00:38,798 - loss nan, lr: 0.012500, epoch: 5, step: 30200, eta: 5.11 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11795 sec, avg_samples: 32.00000, ips: 542.59582 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 30300, eta: 5.10 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.68964 images/sec
Training: 2023-09-28 06:00:50,619 - loss nan, lr: 0.012500, epoch: 5, step: 30300, eta: 5.10 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.68964 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 30400, eta: 5.10 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11842 sec, avg_samples: 32.00000, ips: 540.42654 images/sec
Training: 2023-09-28 06:01:02,468 - loss nan, lr: 0.012500, epoch: 5, step: 30400, eta: 5.10 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11842 sec, avg_samples: 32.00000, ips: 540.42654 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 30500, eta: 5.09 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11848 sec, avg_samples: 32.00000, ips: 540.18387 images/sec
Training: 2023-09-28 06:01:14,323 - loss nan, lr: 0.012500, epoch: 5, step: 30500, eta: 5.09 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11848 sec, avg_samples: 32.00000, ips: 540.18387 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 30600, eta: 5.08 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11850 sec, avg_samples: 32.00000, ips: 540.06441 images/sec
Training: 2023-09-28 06:01:26,179 - loss nan, lr: 0.012500, epoch: 5, step: 30600, eta: 5.08 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11850 sec, avg_samples: 32.00000, ips: 540.06441 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 30700, eta: 5.07 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11854 sec, avg_samples: 32.00000, ips: 539.88954 images/sec
Training: 2023-09-28 06:01:38,039 - loss nan, lr: 0.012500, epoch: 5, step: 30700, eta: 5.07 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11854 sec, avg_samples: 32.00000, ips: 539.88954 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 30800, eta: 5.06 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11852 sec, avg_samples: 32.00000, ips: 539.97267 images/sec
Training: 2023-09-28 06:01:49,898 - loss nan, lr: 0.012500, epoch: 5, step: 30800, eta: 5.06 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11852 sec, avg_samples: 32.00000, ips: 539.97267 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 30900, eta: 5.05 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11849 sec, avg_samples: 32.00000, ips: 540.12093 images/sec
Training: 2023-09-28 06:02:01,753 - loss nan, lr: 0.012500, epoch: 5, step: 30900, eta: 5.05 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11849 sec, avg_samples: 32.00000, ips: 540.12093 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 31000, eta: 5.04 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11831 sec, avg_samples: 32.00000, ips: 540.95328 images/sec
Training: 2023-09-28 06:02:13,589 - loss nan, lr: 0.012500, epoch: 5, step: 31000, eta: 5.04 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11831 sec, avg_samples: 32.00000, ips: 540.95328 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 31100, eta: 5.03 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.32263 images/sec
Training: 2023-09-28 06:02:25,416 - loss nan, lr: 0.012500, epoch: 5, step: 31100, eta: 5.03 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.32263 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 31200, eta: 5.02 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11882 sec, avg_samples: 32.00000, ips: 538.63825 images/sec
Training: 2023-09-28 06:02:37,303 - loss nan, lr: 0.012500, epoch: 5, step: 31200, eta: 5.02 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11882 sec, avg_samples: 32.00000, ips: 538.63825 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 31300, eta: 5.02 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11905 sec, avg_samples: 32.00000, ips: 537.57560 images/sec
Training: 2023-09-28 06:02:49,213 - loss nan, lr: 0.012500, epoch: 5, step: 31300, eta: 5.02 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11905 sec, avg_samples: 32.00000, ips: 537.57560 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 31400, eta: 5.01 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11898 sec, avg_samples: 32.00000, ips: 537.91200 images/sec
Training: 2023-09-28 06:03:01,115 - loss nan, lr: 0.012500, epoch: 5, step: 31400, eta: 5.01 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11898 sec, avg_samples: 32.00000, ips: 537.91200 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 31500, eta: 5.00 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11867 sec, avg_samples: 32.00000, ips: 539.29087 images/sec
Training: 2023-09-28 06:03:12,987 - loss nan, lr: 0.012500, epoch: 5, step: 31500, eta: 5.00 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11867 sec, avg_samples: 32.00000, ips: 539.29087 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 31600, eta: 4.99 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11873 sec, avg_samples: 32.00000, ips: 539.04478 images/sec
Training: 2023-09-28 06:03:24,866 - loss nan, lr: 0.012500, epoch: 5, step: 31600, eta: 4.99 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11873 sec, avg_samples: 32.00000, ips: 539.04478 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 31700, eta: 4.98 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11868 sec, avg_samples: 32.00000, ips: 539.25235 images/sec
Training: 2023-09-28 06:03:36,740 - loss nan, lr: 0.012500, epoch: 5, step: 31700, eta: 4.98 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11868 sec, avg_samples: 32.00000, ips: 539.25235 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 31800, eta: 4.97 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11866 sec, avg_samples: 32.00000, ips: 539.37041 images/sec
Training: 2023-09-28 06:03:48,612 - loss nan, lr: 0.012500, epoch: 5, step: 31800, eta: 4.97 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11866 sec, avg_samples: 32.00000, ips: 539.37041 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 31900, eta: 4.96 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11865 sec, avg_samples: 32.00000, ips: 539.41798 images/sec
Training: 2023-09-28 06:04:00,483 - loss nan, lr: 0.012500, epoch: 5, step: 31900, eta: 4.96 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11865 sec, avg_samples: 32.00000, ips: 539.41798 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 32000, eta: 4.96 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11862 sec, avg_samples: 32.00000, ips: 539.51716 images/sec
Training: 2023-09-28 06:04:12,352 - loss nan, lr: 0.012500, epoch: 5, step: 32000, eta: 4.96 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11862 sec, avg_samples: 32.00000, ips: 539.51716 images/sec
INFO:root:[lfw][32000]XNorm: 3.167355
Training: 2023-09-28 06:04:40,228 - [lfw][32000]XNorm: 3.167355
INFO:root:[lfw][32000]Accuracy-Flip: 0.89567+-0.01690
Training: 2023-09-28 06:04:40,228 - [lfw][32000]Accuracy-Flip: 0.89567+-0.01690
INFO:root:[lfw][32000]Accuracy-Highest: 0.90150
Training: 2023-09-28 06:04:40,228 - [lfw][32000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.8757
Training: 2023-09-28 06:04:40,228 - test time: 27.8757
INFO:root:[cfp_fp][32000]XNorm: 2.955982
Training: 2023-09-28 06:05:12,477 - [cfp_fp][32000]XNorm: 2.955982
INFO:root:[cfp_fp][32000]Accuracy-Flip: 0.64571+-0.02028
Training: 2023-09-28 06:05:12,477 - [cfp_fp][32000]Accuracy-Flip: 0.64571+-0.02028
INFO:root:[cfp_fp][32000]Accuracy-Highest: 0.65071
Training: 2023-09-28 06:05:12,477 - [cfp_fp][32000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2490
Training: 2023-09-28 06:05:12,477 - test time: 32.2490
INFO:root:[agedb_30][32000]XNorm: 3.001698
Training: 2023-09-28 06:05:40,231 - [agedb_30][32000]XNorm: 3.001698
INFO:root:[agedb_30][32000]Accuracy-Flip: 0.68567+-0.02140
Training: 2023-09-28 06:05:40,231 - [agedb_30][32000]Accuracy-Flip: 0.68567+-0.02140
INFO:root:[agedb_30][32000]Accuracy-Highest: 0.70083
Training: 2023-09-28 06:05:40,231 - [agedb_30][32000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7538
Training: 2023-09-28 06:05:40,231 - test time: 27.7538
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 32100, eta: 5.03 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11833 sec, avg_samples: 32.00000, ips: 540.87787 images/sec
Training: 2023-09-28 06:05:52,070 - loss nan, lr: 0.012500, epoch: 5, step: 32100, eta: 5.03 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11833 sec, avg_samples: 32.00000, ips: 540.87787 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 32200, eta: 5.02 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11843 sec, avg_samples: 32.00000, ips: 540.38588 images/sec
Training: 2023-09-28 06:06:03,920 - loss nan, lr: 0.012500, epoch: 5, step: 32200, eta: 5.02 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11843 sec, avg_samples: 32.00000, ips: 540.38588 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 32300, eta: 5.02 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11841 sec, avg_samples: 32.00000, ips: 540.51722 images/sec
Training: 2023-09-28 06:06:15,767 - loss nan, lr: 0.012500, epoch: 5, step: 32300, eta: 5.02 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11841 sec, avg_samples: 32.00000, ips: 540.51722 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 32400, eta: 5.01 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11856 sec, avg_samples: 32.00000, ips: 539.83362 images/sec
Training: 2023-09-28 06:06:27,629 - loss nan, lr: 0.012500, epoch: 5, step: 32400, eta: 5.01 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11856 sec, avg_samples: 32.00000, ips: 539.83362 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 32500, eta: 5.00 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11871 sec, avg_samples: 32.00000, ips: 539.14831 images/sec
Training: 2023-09-28 06:06:39,505 - loss nan, lr: 0.012500, epoch: 5, step: 32500, eta: 5.00 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11871 sec, avg_samples: 32.00000, ips: 539.14831 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 32600, eta: 4.99 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11869 sec, avg_samples: 32.00000, ips: 539.23880 images/sec
Training: 2023-09-28 06:06:51,378 - loss nan, lr: 0.012500, epoch: 5, step: 32600, eta: 4.99 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11869 sec, avg_samples: 32.00000, ips: 539.23880 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 32700, eta: 4.98 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11867 sec, avg_samples: 32.00000, ips: 539.31106 images/sec
Training: 2023-09-28 06:07:03,249 - loss nan, lr: 0.012500, epoch: 5, step: 32700, eta: 4.98 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11867 sec, avg_samples: 32.00000, ips: 539.31106 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 32800, eta: 4.97 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11844 sec, avg_samples: 32.00000, ips: 540.37494 images/sec
Training: 2023-09-28 06:07:15,098 - loss nan, lr: 0.012500, epoch: 5, step: 32800, eta: 4.97 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11844 sec, avg_samples: 32.00000, ips: 540.37494 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 32900, eta: 4.96 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11857 sec, avg_samples: 32.00000, ips: 539.77395 images/sec
Training: 2023-09-28 06:07:26,961 - loss nan, lr: 0.012500, epoch: 5, step: 32900, eta: 4.96 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11857 sec, avg_samples: 32.00000, ips: 539.77395 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 33000, eta: 4.96 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11860 sec, avg_samples: 32.00000, ips: 539.61889 images/sec
Training: 2023-09-28 06:07:38,828 - loss nan, lr: 0.012500, epoch: 5, step: 33000, eta: 4.96 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11860 sec, avg_samples: 32.00000, ips: 539.61889 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 33100, eta: 4.95 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11859 sec, avg_samples: 32.00000, ips: 539.66453 images/sec
Training: 2023-09-28 06:07:50,693 - loss nan, lr: 0.012500, epoch: 5, step: 33100, eta: 4.95 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11859 sec, avg_samples: 32.00000, ips: 539.66453 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 33200, eta: 4.94 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11862 sec, avg_samples: 32.00000, ips: 539.55668 images/sec
Training: 2023-09-28 06:08:02,561 - loss nan, lr: 0.012500, epoch: 5, step: 33200, eta: 4.94 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11862 sec, avg_samples: 32.00000, ips: 539.55668 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 33300, eta: 4.93 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11855 sec, avg_samples: 32.00000, ips: 539.86729 images/sec
Training: 2023-09-28 06:08:14,422 - loss nan, lr: 0.012500, epoch: 5, step: 33300, eta: 4.93 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11855 sec, avg_samples: 32.00000, ips: 539.86729 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 33400, eta: 4.92 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11855 sec, avg_samples: 32.00000, ips: 539.84723 images/sec
Training: 2023-09-28 06:08:26,284 - loss nan, lr: 0.012500, epoch: 5, step: 33400, eta: 4.92 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11855 sec, avg_samples: 32.00000, ips: 539.84723 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 33500, eta: 4.91 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11856 sec, avg_samples: 32.00000, ips: 539.80402 images/sec
Training: 2023-09-28 06:08:38,146 - loss nan, lr: 0.012500, epoch: 5, step: 33500, eta: 4.91 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11856 sec, avg_samples: 32.00000, ips: 539.80402 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 33600, eta: 4.91 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11846 sec, avg_samples: 32.00000, ips: 540.26919 images/sec
Training: 2023-09-28 06:08:49,998 - loss nan, lr: 0.012500, epoch: 5, step: 33600, eta: 4.91 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11846 sec, avg_samples: 32.00000, ips: 540.26919 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 33700, eta: 4.90 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.90087 images/sec
Training: 2023-09-28 06:09:01,835 - loss nan, lr: 0.012500, epoch: 5, step: 33700, eta: 4.90 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.90087 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 33800, eta: 4.89 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.92552 images/sec
Training: 2023-09-28 06:09:13,671 - loss nan, lr: 0.012500, epoch: 5, step: 33800, eta: 4.89 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.92552 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 33900, eta: 4.88 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11834 sec, avg_samples: 32.00000, ips: 540.82258 images/sec
Training: 2023-09-28 06:09:25,509 - loss nan, lr: 0.012500, epoch: 5, step: 33900, eta: 4.88 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11834 sec, avg_samples: 32.00000, ips: 540.82258 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 34000, eta: 4.87 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11830 sec, avg_samples: 32.00000, ips: 541.01751 images/sec
Training: 2023-09-28 06:09:37,343 - loss nan, lr: 0.012500, epoch: 5, step: 34000, eta: 4.87 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11830 sec, avg_samples: 32.00000, ips: 541.01751 images/sec
INFO:root:[lfw][34000]XNorm: 2.807848
Training: 2023-09-28 06:10:05,253 - [lfw][34000]XNorm: 2.807848
INFO:root:[lfw][34000]Accuracy-Flip: 0.89633+-0.01862
Training: 2023-09-28 06:10:05,253 - [lfw][34000]Accuracy-Flip: 0.89633+-0.01862
INFO:root:[lfw][34000]Accuracy-Highest: 0.90150
Training: 2023-09-28 06:10:05,253 - [lfw][34000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9101
Training: 2023-09-28 06:10:05,253 - test time: 27.9101
INFO:root:[cfp_fp][34000]XNorm: 2.627101
Training: 2023-09-28 06:10:37,546 - [cfp_fp][34000]XNorm: 2.627101
INFO:root:[cfp_fp][34000]Accuracy-Flip: 0.64686+-0.02152
Training: 2023-09-28 06:10:37,546 - [cfp_fp][34000]Accuracy-Flip: 0.64686+-0.02152
INFO:root:[cfp_fp][34000]Accuracy-Highest: 0.65071
Training: 2023-09-28 06:10:37,546 - [cfp_fp][34000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2929
Training: 2023-09-28 06:10:37,546 - test time: 32.2929
INFO:root:[agedb_30][34000]XNorm: 2.669731
Training: 2023-09-28 06:11:05,327 - [agedb_30][34000]XNorm: 2.669731
INFO:root:[agedb_30][34000]Accuracy-Flip: 0.68367+-0.01930
Training: 2023-09-28 06:11:05,328 - [agedb_30][34000]Accuracy-Flip: 0.68367+-0.01930
INFO:root:[agedb_30][34000]Accuracy-Highest: 0.70083
Training: 2023-09-28 06:11:05,328 - [agedb_30][34000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7815
Training: 2023-09-28 06:11:05,328 - test time: 27.7815
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 34100, eta: 4.94 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11779 sec, avg_samples: 32.00000, ips: 543.34073 images/sec
Training: 2023-09-28 06:11:17,112 - loss nan, lr: 0.012500, epoch: 5, step: 34100, eta: 4.94 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11779 sec, avg_samples: 32.00000, ips: 543.34073 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 34200, eta: 4.93 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11785 sec, avg_samples: 32.00000, ips: 543.07135 images/sec
Training: 2023-09-28 06:11:28,901 - loss nan, lr: 0.012500, epoch: 5, step: 34200, eta: 4.93 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11785 sec, avg_samples: 32.00000, ips: 543.07135 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 34300, eta: 4.93 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11788 sec, avg_samples: 32.00000, ips: 542.92803 images/sec
Training: 2023-09-28 06:11:40,694 - loss nan, lr: 0.012500, epoch: 5, step: 34300, eta: 4.93 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11788 sec, avg_samples: 32.00000, ips: 542.92803 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 5, step: 34400, eta: 4.92 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11810 sec, avg_samples: 32.00000, ips: 541.90521 images/sec
Training: 2023-09-28 06:11:52,509 - loss nan, lr: 0.012500, epoch: 5, step: 34400, eta: 4.92 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11810 sec, avg_samples: 32.00000, ips: 541.90521 images/sec
INFO:root:Save model to MS1M_v3_arcface_static_0.1/FresResNet50/5.
Training: 2023-09-28 06:12:03,259 - Save model to MS1M_v3_arcface_static_0.1/FresResNet50/5.
INFO:root:Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/4.
Training: 2023-09-28 06:12:03,259 - Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/4.
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 34500, eta: 4.91 hours, avg_reader_cost: 0.00174 sec, avg_batch_cost: 0.01591 sec, avg_samples: 3.84000, ips: 482.56917 images/sec
Training: 2023-09-28 06:12:04,881 - loss nan, lr: 0.012500, epoch: 6, step: 34500, eta: 4.91 hours, avg_reader_cost: 0.00174 sec, avg_batch_cost: 0.01591 sec, avg_samples: 3.84000, ips: 482.56917 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 34600, eta: 4.90 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11831 sec, avg_samples: 32.00000, ips: 540.93561 images/sec
Training: 2023-09-28 06:12:16,717 - loss nan, lr: 0.012500, epoch: 6, step: 34600, eta: 4.90 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11831 sec, avg_samples: 32.00000, ips: 540.93561 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 34700, eta: 4.89 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11856 sec, avg_samples: 32.00000, ips: 539.79569 images/sec
Training: 2023-09-28 06:12:28,579 - loss nan, lr: 0.012500, epoch: 6, step: 34700, eta: 4.89 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11856 sec, avg_samples: 32.00000, ips: 539.79569 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 34800, eta: 4.88 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11850 sec, avg_samples: 32.00000, ips: 540.08934 images/sec
Training: 2023-09-28 06:12:40,435 - loss nan, lr: 0.012500, epoch: 6, step: 34800, eta: 4.88 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11850 sec, avg_samples: 32.00000, ips: 540.08934 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 34900, eta: 4.88 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11854 sec, avg_samples: 32.00000, ips: 539.92163 images/sec
Training: 2023-09-28 06:12:52,295 - loss nan, lr: 0.012500, epoch: 6, step: 34900, eta: 4.88 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11854 sec, avg_samples: 32.00000, ips: 539.92163 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 35000, eta: 4.87 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11853 sec, avg_samples: 32.00000, ips: 539.95762 images/sec
Training: 2023-09-28 06:13:04,154 - loss nan, lr: 0.012500, epoch: 6, step: 35000, eta: 4.87 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11853 sec, avg_samples: 32.00000, ips: 539.95762 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 35100, eta: 4.86 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11850 sec, avg_samples: 32.00000, ips: 540.07259 images/sec
Training: 2023-09-28 06:13:16,010 - loss nan, lr: 0.012500, epoch: 6, step: 35100, eta: 4.86 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11850 sec, avg_samples: 32.00000, ips: 540.07259 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 35200, eta: 4.85 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11851 sec, avg_samples: 32.00000, ips: 540.05767 images/sec
Training: 2023-09-28 06:13:27,867 - loss nan, lr: 0.012500, epoch: 6, step: 35200, eta: 4.85 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11851 sec, avg_samples: 32.00000, ips: 540.05767 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 35300, eta: 4.84 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11856 sec, avg_samples: 32.00000, ips: 539.82033 images/sec
Training: 2023-09-28 06:13:39,729 - loss nan, lr: 0.012500, epoch: 6, step: 35300, eta: 4.84 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11856 sec, avg_samples: 32.00000, ips: 539.82033 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 35400, eta: 4.84 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11848 sec, avg_samples: 32.00000, ips: 540.18763 images/sec
Training: 2023-09-28 06:13:51,583 - loss nan, lr: 0.012500, epoch: 6, step: 35400, eta: 4.84 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11848 sec, avg_samples: 32.00000, ips: 540.18763 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 35500, eta: 4.83 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11851 sec, avg_samples: 32.00000, ips: 540.01628 images/sec
Training: 2023-09-28 06:14:03,439 - loss nan, lr: 0.012500, epoch: 6, step: 35500, eta: 4.83 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11851 sec, avg_samples: 32.00000, ips: 540.01628 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 35600, eta: 4.82 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.18891 images/sec
Training: 2023-09-28 06:14:15,269 - loss nan, lr: 0.012500, epoch: 6, step: 35600, eta: 4.82 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.18891 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 35700, eta: 4.81 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.34724 images/sec
Training: 2023-09-28 06:14:27,096 - loss nan, lr: 0.012500, epoch: 6, step: 35700, eta: 4.81 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.34724 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 35800, eta: 4.80 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.20027 images/sec
Training: 2023-09-28 06:14:38,927 - loss nan, lr: 0.012500, epoch: 6, step: 35800, eta: 4.80 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.20027 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 35900, eta: 4.80 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11824 sec, avg_samples: 32.00000, ips: 541.26536 images/sec
Training: 2023-09-28 06:14:50,755 - loss nan, lr: 0.012500, epoch: 6, step: 35900, eta: 4.80 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11824 sec, avg_samples: 32.00000, ips: 541.26536 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 36000, eta: 4.79 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.16464 images/sec
Training: 2023-09-28 06:15:02,586 - loss nan, lr: 0.012500, epoch: 6, step: 36000, eta: 4.79 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.16464 images/sec
INFO:root:[lfw][36000]XNorm: 2.465104
Training: 2023-09-28 06:15:30,514 - [lfw][36000]XNorm: 2.465104
INFO:root:[lfw][36000]Accuracy-Flip: 0.89717+-0.01897
Training: 2023-09-28 06:15:30,515 - [lfw][36000]Accuracy-Flip: 0.89717+-0.01897
INFO:root:[lfw][36000]Accuracy-Highest: 0.90150
Training: 2023-09-28 06:15:30,515 - [lfw][36000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9283
Training: 2023-09-28 06:15:30,515 - test time: 27.9283
INFO:root:[cfp_fp][36000]XNorm: 2.305180
Training: 2023-09-28 06:16:02,770 - [cfp_fp][36000]XNorm: 2.305180
INFO:root:[cfp_fp][36000]Accuracy-Flip: 0.64243+-0.01982
Training: 2023-09-28 06:16:02,770 - [cfp_fp][36000]Accuracy-Flip: 0.64243+-0.01982
INFO:root:[cfp_fp][36000]Accuracy-Highest: 0.65071
Training: 2023-09-28 06:16:02,771 - [cfp_fp][36000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2558
Training: 2023-09-28 06:16:02,771 - test time: 32.2558
INFO:root:[agedb_30][36000]XNorm: 2.347426
Training: 2023-09-28 06:16:30,499 - [agedb_30][36000]XNorm: 2.347426
INFO:root:[agedb_30][36000]Accuracy-Flip: 0.68400+-0.01981
Training: 2023-09-28 06:16:30,499 - [agedb_30][36000]Accuracy-Flip: 0.68400+-0.01981
INFO:root:[agedb_30][36000]Accuracy-Highest: 0.70083
Training: 2023-09-28 06:16:30,499 - [agedb_30][36000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7286
Training: 2023-09-28 06:16:30,499 - test time: 27.7286
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 36100, eta: 4.85 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.35883 images/sec
Training: 2023-09-28 06:16:42,326 - loss nan, lr: 0.012500, epoch: 6, step: 36100, eta: 4.85 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.35883 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 36200, eta: 4.84 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11798 sec, avg_samples: 32.00000, ips: 542.47214 images/sec
Training: 2023-09-28 06:16:54,129 - loss nan, lr: 0.012500, epoch: 6, step: 36200, eta: 4.84 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11798 sec, avg_samples: 32.00000, ips: 542.47214 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 36300, eta: 4.84 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11799 sec, avg_samples: 32.00000, ips: 542.40084 images/sec
Training: 2023-09-28 06:17:05,933 - loss nan, lr: 0.012500, epoch: 6, step: 36300, eta: 4.84 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11799 sec, avg_samples: 32.00000, ips: 542.40084 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 36400, eta: 4.83 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11811 sec, avg_samples: 32.00000, ips: 541.88043 images/sec
Training: 2023-09-28 06:17:17,749 - loss nan, lr: 0.012500, epoch: 6, step: 36400, eta: 4.83 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11811 sec, avg_samples: 32.00000, ips: 541.88043 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 36500, eta: 4.82 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11831 sec, avg_samples: 32.00000, ips: 540.93538 images/sec
Training: 2023-09-28 06:17:29,585 - loss nan, lr: 0.012500, epoch: 6, step: 36500, eta: 4.82 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11831 sec, avg_samples: 32.00000, ips: 540.93538 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 36600, eta: 4.81 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11859 sec, avg_samples: 32.00000, ips: 539.67642 images/sec
Training: 2023-09-28 06:17:41,449 - loss nan, lr: 0.012500, epoch: 6, step: 36600, eta: 4.81 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11859 sec, avg_samples: 32.00000, ips: 539.67642 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 36700, eta: 4.80 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11886 sec, avg_samples: 32.00000, ips: 538.46304 images/sec
Training: 2023-09-28 06:17:53,339 - loss nan, lr: 0.012500, epoch: 6, step: 36700, eta: 4.80 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11886 sec, avg_samples: 32.00000, ips: 538.46304 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 36800, eta: 4.80 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11889 sec, avg_samples: 32.00000, ips: 538.33382 images/sec
Training: 2023-09-28 06:18:05,233 - loss nan, lr: 0.012500, epoch: 6, step: 36800, eta: 4.80 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11889 sec, avg_samples: 32.00000, ips: 538.33382 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 36900, eta: 4.79 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11836 sec, avg_samples: 32.00000, ips: 540.74440 images/sec
Training: 2023-09-28 06:18:17,073 - loss nan, lr: 0.012500, epoch: 6, step: 36900, eta: 4.79 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11836 sec, avg_samples: 32.00000, ips: 540.74440 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 37000, eta: 4.78 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11833 sec, avg_samples: 32.00000, ips: 540.86482 images/sec
Training: 2023-09-28 06:18:28,911 - loss nan, lr: 0.012500, epoch: 6, step: 37000, eta: 4.78 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11833 sec, avg_samples: 32.00000, ips: 540.86482 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 37100, eta: 4.77 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11893 sec, avg_samples: 32.00000, ips: 538.11743 images/sec
Training: 2023-09-28 06:18:40,809 - loss nan, lr: 0.012500, epoch: 6, step: 37100, eta: 4.77 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11893 sec, avg_samples: 32.00000, ips: 538.11743 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 37200, eta: 4.76 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11881 sec, avg_samples: 32.00000, ips: 538.68009 images/sec
Training: 2023-09-28 06:18:52,695 - loss nan, lr: 0.012500, epoch: 6, step: 37200, eta: 4.76 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11881 sec, avg_samples: 32.00000, ips: 538.68009 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 37300, eta: 4.76 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11880 sec, avg_samples: 32.00000, ips: 538.69960 images/sec
Training: 2023-09-28 06:19:04,580 - loss nan, lr: 0.012500, epoch: 6, step: 37300, eta: 4.76 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11880 sec, avg_samples: 32.00000, ips: 538.69960 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 37400, eta: 4.75 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11877 sec, avg_samples: 32.00000, ips: 538.83553 images/sec
Training: 2023-09-28 06:19:16,463 - loss nan, lr: 0.012500, epoch: 6, step: 37400, eta: 4.75 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11877 sec, avg_samples: 32.00000, ips: 538.83553 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 37500, eta: 4.74 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11876 sec, avg_samples: 32.00000, ips: 538.89434 images/sec
Training: 2023-09-28 06:19:28,344 - loss nan, lr: 0.012500, epoch: 6, step: 37500, eta: 4.74 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11876 sec, avg_samples: 32.00000, ips: 538.89434 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 37600, eta: 4.73 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11875 sec, avg_samples: 32.00000, ips: 538.92814 images/sec
Training: 2023-09-28 06:19:40,224 - loss nan, lr: 0.012500, epoch: 6, step: 37600, eta: 4.73 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11875 sec, avg_samples: 32.00000, ips: 538.92814 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 37700, eta: 4.73 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11875 sec, avg_samples: 32.00000, ips: 538.93459 images/sec
Training: 2023-09-28 06:19:52,104 - loss nan, lr: 0.012500, epoch: 6, step: 37700, eta: 4.73 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11875 sec, avg_samples: 32.00000, ips: 538.93459 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 37800, eta: 4.72 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11876 sec, avg_samples: 32.00000, ips: 538.89685 images/sec
Training: 2023-09-28 06:20:03,985 - loss nan, lr: 0.012500, epoch: 6, step: 37800, eta: 4.72 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11876 sec, avg_samples: 32.00000, ips: 538.89685 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 37900, eta: 4.71 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11852 sec, avg_samples: 32.00000, ips: 539.97768 images/sec
Training: 2023-09-28 06:20:15,843 - loss nan, lr: 0.012500, epoch: 6, step: 37900, eta: 4.71 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11852 sec, avg_samples: 32.00000, ips: 539.97768 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 38000, eta: 4.70 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.66721 images/sec
Training: 2023-09-28 06:20:27,686 - loss nan, lr: 0.012500, epoch: 6, step: 38000, eta: 4.70 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.66721 images/sec
INFO:root:[lfw][38000]XNorm: 2.197112
Training: 2023-09-28 06:20:55,562 - [lfw][38000]XNorm: 2.197112
INFO:root:[lfw][38000]Accuracy-Flip: 0.90050+-0.01713
Training: 2023-09-28 06:20:55,562 - [lfw][38000]Accuracy-Flip: 0.90050+-0.01713
INFO:root:[lfw][38000]Accuracy-Highest: 0.90150
Training: 2023-09-28 06:20:55,562 - [lfw][38000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.8756
Training: 2023-09-28 06:20:55,562 - test time: 27.8756
INFO:root:[cfp_fp][38000]XNorm: 2.051218
Training: 2023-09-28 06:21:27,795 - [cfp_fp][38000]XNorm: 2.051218
INFO:root:[cfp_fp][38000]Accuracy-Flip: 0.64443+-0.01844
Training: 2023-09-28 06:21:27,795 - [cfp_fp][38000]Accuracy-Flip: 0.64443+-0.01844
INFO:root:[cfp_fp][38000]Accuracy-Highest: 0.65071
Training: 2023-09-28 06:21:27,795 - [cfp_fp][38000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2333
Training: 2023-09-28 06:21:27,795 - test time: 32.2333
INFO:root:[agedb_30][38000]XNorm: 2.086239
Training: 2023-09-28 06:21:55,531 - [agedb_30][38000]XNorm: 2.086239
INFO:root:[agedb_30][38000]Accuracy-Flip: 0.68333+-0.02051
Training: 2023-09-28 06:21:55,531 - [agedb_30][38000]Accuracy-Flip: 0.68333+-0.02051
INFO:root:[agedb_30][38000]Accuracy-Highest: 0.70083
Training: 2023-09-28 06:21:55,531 - [agedb_30][38000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7358
Training: 2023-09-28 06:21:55,531 - test time: 27.7358
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 38100, eta: 4.76 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11808 sec, avg_samples: 32.00000, ips: 541.98890 images/sec
Training: 2023-09-28 06:22:07,344 - loss nan, lr: 0.012500, epoch: 6, step: 38100, eta: 4.76 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11808 sec, avg_samples: 32.00000, ips: 541.98890 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 38200, eta: 4.76 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.45501 images/sec
Training: 2023-09-28 06:22:19,169 - loss nan, lr: 0.012500, epoch: 6, step: 38200, eta: 4.76 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.45501 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 38300, eta: 4.75 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.37469 images/sec
Training: 2023-09-28 06:22:30,996 - loss nan, lr: 0.012500, epoch: 6, step: 38300, eta: 4.75 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.37469 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 38400, eta: 4.74 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.58958 images/sec
Training: 2023-09-28 06:22:42,818 - loss nan, lr: 0.012500, epoch: 6, step: 38400, eta: 4.74 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.58958 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 38500, eta: 4.73 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.76478 images/sec
Training: 2023-09-28 06:22:54,658 - loss nan, lr: 0.012500, epoch: 6, step: 38500, eta: 4.73 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.76478 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 38600, eta: 4.72 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11865 sec, avg_samples: 32.00000, ips: 539.39607 images/sec
Training: 2023-09-28 06:23:06,530 - loss nan, lr: 0.012500, epoch: 6, step: 38600, eta: 4.72 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11865 sec, avg_samples: 32.00000, ips: 539.39607 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 38700, eta: 4.72 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11871 sec, avg_samples: 32.00000, ips: 539.10948 images/sec
Training: 2023-09-28 06:23:18,406 - loss nan, lr: 0.012500, epoch: 6, step: 38700, eta: 4.72 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11871 sec, avg_samples: 32.00000, ips: 539.10948 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 38800, eta: 4.71 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11877 sec, avg_samples: 32.00000, ips: 538.83407 images/sec
Training: 2023-09-28 06:23:30,288 - loss nan, lr: 0.012500, epoch: 6, step: 38800, eta: 4.71 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11877 sec, avg_samples: 32.00000, ips: 538.83407 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 38900, eta: 4.70 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11836 sec, avg_samples: 32.00000, ips: 540.71167 images/sec
Training: 2023-09-28 06:23:42,130 - loss nan, lr: 0.012500, epoch: 6, step: 38900, eta: 4.70 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11836 sec, avg_samples: 32.00000, ips: 540.71167 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 39000, eta: 4.69 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11840 sec, avg_samples: 32.00000, ips: 540.53498 images/sec
Training: 2023-09-28 06:23:53,975 - loss nan, lr: 0.012500, epoch: 6, step: 39000, eta: 4.69 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11840 sec, avg_samples: 32.00000, ips: 540.53498 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 39100, eta: 4.69 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.06919 images/sec
Training: 2023-09-28 06:24:05,809 - loss nan, lr: 0.012500, epoch: 6, step: 39100, eta: 4.69 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.06919 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 39200, eta: 4.68 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11847 sec, avg_samples: 32.00000, ips: 540.21300 images/sec
Training: 2023-09-28 06:24:17,662 - loss nan, lr: 0.012500, epoch: 6, step: 39200, eta: 4.68 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11847 sec, avg_samples: 32.00000, ips: 540.21300 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 39300, eta: 4.67 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11838 sec, avg_samples: 32.00000, ips: 540.63913 images/sec
Training: 2023-09-28 06:24:29,506 - loss nan, lr: 0.012500, epoch: 6, step: 39300, eta: 4.67 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11838 sec, avg_samples: 32.00000, ips: 540.63913 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 39400, eta: 4.66 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11841 sec, avg_samples: 32.00000, ips: 540.47993 images/sec
Training: 2023-09-28 06:24:41,353 - loss nan, lr: 0.012500, epoch: 6, step: 39400, eta: 4.66 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11841 sec, avg_samples: 32.00000, ips: 540.47993 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 39500, eta: 4.65 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.06030 images/sec
Training: 2023-09-28 06:24:53,186 - loss nan, lr: 0.012500, epoch: 6, step: 39500, eta: 4.65 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.06030 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 39600, eta: 4.65 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11830 sec, avg_samples: 32.00000, ips: 541.00764 images/sec
Training: 2023-09-28 06:25:05,021 - loss nan, lr: 0.012500, epoch: 6, step: 39600, eta: 4.65 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11830 sec, avg_samples: 32.00000, ips: 541.00764 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 39700, eta: 4.64 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.75206 images/sec
Training: 2023-09-28 06:25:16,861 - loss nan, lr: 0.012500, epoch: 6, step: 39700, eta: 4.64 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.75206 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 39800, eta: 4.63 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.75967 images/sec
Training: 2023-09-28 06:25:28,701 - loss nan, lr: 0.012500, epoch: 6, step: 39800, eta: 4.63 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.75967 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 39900, eta: 4.62 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11856 sec, avg_samples: 32.00000, ips: 539.79144 images/sec
Training: 2023-09-28 06:25:40,562 - loss nan, lr: 0.012500, epoch: 6, step: 39900, eta: 4.62 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11856 sec, avg_samples: 32.00000, ips: 539.79144 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 40000, eta: 4.62 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11866 sec, avg_samples: 32.00000, ips: 539.35324 images/sec
Training: 2023-09-28 06:25:52,432 - loss nan, lr: 0.012500, epoch: 6, step: 40000, eta: 4.62 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11866 sec, avg_samples: 32.00000, ips: 539.35324 images/sec
INFO:root:[lfw][40000]XNorm: 1.937428
Training: 2023-09-28 06:26:20,291 - [lfw][40000]XNorm: 1.937428
INFO:root:[lfw][40000]Accuracy-Flip: 0.89867+-0.01841
Training: 2023-09-28 06:26:20,291 - [lfw][40000]Accuracy-Flip: 0.89867+-0.01841
INFO:root:[lfw][40000]Accuracy-Highest: 0.90150
Training: 2023-09-28 06:26:20,291 - [lfw][40000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.8582
Training: 2023-09-28 06:26:20,291 - test time: 27.8582
INFO:root:[cfp_fp][40000]XNorm: 1.808060
Training: 2023-09-28 06:26:52,471 - [cfp_fp][40000]XNorm: 1.808060
INFO:root:[cfp_fp][40000]Accuracy-Flip: 0.64400+-0.01920
Training: 2023-09-28 06:26:52,471 - [cfp_fp][40000]Accuracy-Flip: 0.64400+-0.01920
INFO:root:[cfp_fp][40000]Accuracy-Highest: 0.65071
Training: 2023-09-28 06:26:52,471 - [cfp_fp][40000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.1800
Training: 2023-09-28 06:26:52,471 - test time: 32.1800
INFO:root:[agedb_30][40000]XNorm: 1.838633
Training: 2023-09-28 06:27:20,162 - [agedb_30][40000]XNorm: 1.838633
INFO:root:[agedb_30][40000]Accuracy-Flip: 0.68867+-0.02272
Training: 2023-09-28 06:27:20,162 - [agedb_30][40000]Accuracy-Flip: 0.68867+-0.02272
INFO:root:[agedb_30][40000]Accuracy-Highest: 0.70083
Training: 2023-09-28 06:27:20,162 - [agedb_30][40000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.6910
Training: 2023-09-28 06:27:20,162 - test time: 27.6910
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 40100, eta: 4.67 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.76387 images/sec
Training: 2023-09-28 06:27:31,981 - loss nan, lr: 0.012500, epoch: 6, step: 40100, eta: 4.67 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.76387 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 6, step: 40200, eta: 4.67 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11830 sec, avg_samples: 32.00000, ips: 540.97965 images/sec
Training: 2023-09-28 06:27:43,817 - loss nan, lr: 0.012500, epoch: 6, step: 40200, eta: 4.67 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11830 sec, avg_samples: 32.00000, ips: 540.97965 images/sec
INFO:root:Save model to MS1M_v3_arcface_static_0.1/FresResNet50/6.
Training: 2023-09-28 06:27:48,419 - Save model to MS1M_v3_arcface_static_0.1/FresResNet50/6.
INFO:root:Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/5.
Training: 2023-09-28 06:27:48,419 - Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/5.
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 40300, eta: 4.66 hours, avg_reader_cost: 0.00177 sec, avg_batch_cost: 0.07735 sec, avg_samples: 20.48000, ips: 529.54177 images/sec
Training: 2023-09-28 06:27:56,187 - loss nan, lr: 0.012500, epoch: 7, step: 40300, eta: 4.66 hours, avg_reader_cost: 0.00177 sec, avg_batch_cost: 0.07735 sec, avg_samples: 20.48000, ips: 529.54177 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 40400, eta: 4.65 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11808 sec, avg_samples: 32.00000, ips: 541.98513 images/sec
Training: 2023-09-28 06:28:08,001 - loss nan, lr: 0.012500, epoch: 7, step: 40400, eta: 4.65 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11808 sec, avg_samples: 32.00000, ips: 541.98513 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 40500, eta: 4.64 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11809 sec, avg_samples: 32.00000, ips: 541.97863 images/sec
Training: 2023-09-28 06:28:19,815 - loss nan, lr: 0.012500, epoch: 7, step: 40500, eta: 4.64 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11809 sec, avg_samples: 32.00000, ips: 541.97863 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 40600, eta: 4.63 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.35247 images/sec
Training: 2023-09-28 06:28:31,642 - loss nan, lr: 0.012500, epoch: 7, step: 40600, eta: 4.63 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.35247 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 40700, eta: 4.63 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11859 sec, avg_samples: 32.00000, ips: 539.68101 images/sec
Training: 2023-09-28 06:28:43,507 - loss nan, lr: 0.012500, epoch: 7, step: 40700, eta: 4.63 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11859 sec, avg_samples: 32.00000, ips: 539.68101 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 40800, eta: 4.62 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.05189 images/sec
Training: 2023-09-28 06:28:55,341 - loss nan, lr: 0.012500, epoch: 7, step: 40800, eta: 4.62 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.05189 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 40900, eta: 4.61 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.03610 images/sec
Training: 2023-09-28 06:29:07,175 - loss nan, lr: 0.012500, epoch: 7, step: 40900, eta: 4.61 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.03610 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 41000, eta: 4.60 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11836 sec, avg_samples: 32.00000, ips: 540.70957 images/sec
Training: 2023-09-28 06:29:19,016 - loss nan, lr: 0.012500, epoch: 7, step: 41000, eta: 4.60 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11836 sec, avg_samples: 32.00000, ips: 540.70957 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 41100, eta: 4.60 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11838 sec, avg_samples: 32.00000, ips: 540.65333 images/sec
Training: 2023-09-28 06:29:30,859 - loss nan, lr: 0.012500, epoch: 7, step: 41100, eta: 4.60 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11838 sec, avg_samples: 32.00000, ips: 540.65333 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 41200, eta: 4.59 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11839 sec, avg_samples: 32.00000, ips: 540.58224 images/sec
Training: 2023-09-28 06:29:42,704 - loss nan, lr: 0.012500, epoch: 7, step: 41200, eta: 4.59 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11839 sec, avg_samples: 32.00000, ips: 540.58224 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 41300, eta: 4.58 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11831 sec, avg_samples: 32.00000, ips: 540.94711 images/sec
Training: 2023-09-28 06:29:54,540 - loss nan, lr: 0.012500, epoch: 7, step: 41300, eta: 4.58 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11831 sec, avg_samples: 32.00000, ips: 540.94711 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 41400, eta: 4.58 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11834 sec, avg_samples: 32.00000, ips: 540.83287 images/sec
Training: 2023-09-28 06:30:06,379 - loss nan, lr: 0.012500, epoch: 7, step: 41400, eta: 4.58 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11834 sec, avg_samples: 32.00000, ips: 540.83287 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 41500, eta: 4.57 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.91370 images/sec
Training: 2023-09-28 06:30:18,216 - loss nan, lr: 0.012500, epoch: 7, step: 41500, eta: 4.57 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.91370 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 41600, eta: 4.56 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11836 sec, avg_samples: 32.00000, ips: 540.73781 images/sec
Training: 2023-09-28 06:30:30,057 - loss nan, lr: 0.012500, epoch: 7, step: 41600, eta: 4.56 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11836 sec, avg_samples: 32.00000, ips: 540.73781 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 41700, eta: 4.55 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11891 sec, avg_samples: 32.00000, ips: 538.21742 images/sec
Training: 2023-09-28 06:30:41,953 - loss nan, lr: 0.012500, epoch: 7, step: 41700, eta: 4.55 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11891 sec, avg_samples: 32.00000, ips: 538.21742 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 41800, eta: 4.55 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11884 sec, avg_samples: 32.00000, ips: 538.52685 images/sec
Training: 2023-09-28 06:30:53,842 - loss nan, lr: 0.012500, epoch: 7, step: 41800, eta: 4.55 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11884 sec, avg_samples: 32.00000, ips: 538.52685 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 41900, eta: 4.54 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11884 sec, avg_samples: 32.00000, ips: 538.51904 images/sec
Training: 2023-09-28 06:31:05,732 - loss nan, lr: 0.012500, epoch: 7, step: 41900, eta: 4.54 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11884 sec, avg_samples: 32.00000, ips: 538.51904 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 42000, eta: 4.53 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11870 sec, avg_samples: 32.00000, ips: 539.19001 images/sec
Training: 2023-09-28 06:31:17,607 - loss nan, lr: 0.012500, epoch: 7, step: 42000, eta: 4.53 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11870 sec, avg_samples: 32.00000, ips: 539.19001 images/sec
INFO:root:[lfw][42000]XNorm: 1.693229
Training: 2023-09-28 06:31:45,543 - [lfw][42000]XNorm: 1.693229
INFO:root:[lfw][42000]Accuracy-Flip: 0.89933+-0.01808
Training: 2023-09-28 06:31:45,543 - [lfw][42000]Accuracy-Flip: 0.89933+-0.01808
INFO:root:[lfw][42000]Accuracy-Highest: 0.90150
Training: 2023-09-28 06:31:45,543 - [lfw][42000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9362
Training: 2023-09-28 06:31:45,543 - test time: 27.9362
INFO:root:[cfp_fp][42000]XNorm: 1.585641
Training: 2023-09-28 06:32:17,775 - [cfp_fp][42000]XNorm: 1.585641
INFO:root:[cfp_fp][42000]Accuracy-Flip: 0.64214+-0.01924
Training: 2023-09-28 06:32:17,775 - [cfp_fp][42000]Accuracy-Flip: 0.64214+-0.01924
INFO:root:[cfp_fp][42000]Accuracy-Highest: 0.65071
Training: 2023-09-28 06:32:17,775 - [cfp_fp][42000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2320
Training: 2023-09-28 06:32:17,775 - test time: 32.2320
INFO:root:[agedb_30][42000]XNorm: 1.608887
Training: 2023-09-28 06:32:45,505 - [agedb_30][42000]XNorm: 1.608887
INFO:root:[agedb_30][42000]Accuracy-Flip: 0.68283+-0.02138
Training: 2023-09-28 06:32:45,505 - [agedb_30][42000]Accuracy-Flip: 0.68283+-0.02138
INFO:root:[agedb_30][42000]Accuracy-Highest: 0.70083
Training: 2023-09-28 06:32:45,505 - [agedb_30][42000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7299
Training: 2023-09-28 06:32:45,505 - test time: 27.7299
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 42100, eta: 4.58 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11784 sec, avg_samples: 32.00000, ips: 543.10535 images/sec
Training: 2023-09-28 06:32:57,294 - loss nan, lr: 0.012500, epoch: 7, step: 42100, eta: 4.58 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11784 sec, avg_samples: 32.00000, ips: 543.10535 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 42200, eta: 4.58 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.89784 images/sec
Training: 2023-09-28 06:33:09,133 - loss nan, lr: 0.012500, epoch: 7, step: 42200, eta: 4.58 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.89784 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 42300, eta: 4.57 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11842 sec, avg_samples: 32.00000, ips: 540.44943 images/sec
Training: 2023-09-28 06:33:20,982 - loss nan, lr: 0.012500, epoch: 7, step: 42300, eta: 4.57 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11842 sec, avg_samples: 32.00000, ips: 540.44943 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 42400, eta: 4.56 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11841 sec, avg_samples: 32.00000, ips: 540.48002 images/sec
Training: 2023-09-28 06:33:32,831 - loss nan, lr: 0.012500, epoch: 7, step: 42400, eta: 4.56 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11841 sec, avg_samples: 32.00000, ips: 540.48002 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 42500, eta: 4.55 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11863 sec, avg_samples: 32.00000, ips: 539.47732 images/sec
Training: 2023-09-28 06:33:44,702 - loss nan, lr: 0.012500, epoch: 7, step: 42500, eta: 4.55 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11863 sec, avg_samples: 32.00000, ips: 539.47732 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 42600, eta: 4.55 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.40235 images/sec
Training: 2023-09-28 06:33:56,527 - loss nan, lr: 0.012500, epoch: 7, step: 42600, eta: 4.55 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.40235 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 42700, eta: 4.54 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11830 sec, avg_samples: 32.00000, ips: 540.97892 images/sec
Training: 2023-09-28 06:34:08,362 - loss nan, lr: 0.012500, epoch: 7, step: 42700, eta: 4.54 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11830 sec, avg_samples: 32.00000, ips: 540.97892 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 42800, eta: 4.53 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.92514 images/sec
Training: 2023-09-28 06:34:20,199 - loss nan, lr: 0.012500, epoch: 7, step: 42800, eta: 4.53 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.92514 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 42900, eta: 4.52 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.89165 images/sec
Training: 2023-09-28 06:34:32,035 - loss nan, lr: 0.012500, epoch: 7, step: 42900, eta: 4.52 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.89165 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 43000, eta: 4.52 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11833 sec, avg_samples: 32.00000, ips: 540.85883 images/sec
Training: 2023-09-28 06:34:43,873 - loss nan, lr: 0.012500, epoch: 7, step: 43000, eta: 4.52 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11833 sec, avg_samples: 32.00000, ips: 540.85883 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 43100, eta: 4.51 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.65722 images/sec
Training: 2023-09-28 06:34:55,715 - loss nan, lr: 0.012500, epoch: 7, step: 43100, eta: 4.51 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.65722 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 43200, eta: 4.50 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11834 sec, avg_samples: 32.00000, ips: 540.81512 images/sec
Training: 2023-09-28 06:35:07,554 - loss nan, lr: 0.012500, epoch: 7, step: 43200, eta: 4.50 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11834 sec, avg_samples: 32.00000, ips: 540.81512 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 43300, eta: 4.49 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11834 sec, avg_samples: 32.00000, ips: 540.80724 images/sec
Training: 2023-09-28 06:35:19,394 - loss nan, lr: 0.012500, epoch: 7, step: 43300, eta: 4.49 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11834 sec, avg_samples: 32.00000, ips: 540.80724 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 43400, eta: 4.49 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11841 sec, avg_samples: 32.00000, ips: 540.48231 images/sec
Training: 2023-09-28 06:35:31,240 - loss nan, lr: 0.012500, epoch: 7, step: 43400, eta: 4.49 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11841 sec, avg_samples: 32.00000, ips: 540.48231 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 43500, eta: 4.48 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11843 sec, avg_samples: 32.00000, ips: 540.39513 images/sec
Training: 2023-09-28 06:35:43,088 - loss nan, lr: 0.012500, epoch: 7, step: 43500, eta: 4.48 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11843 sec, avg_samples: 32.00000, ips: 540.39513 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 43600, eta: 4.47 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11840 sec, avg_samples: 32.00000, ips: 540.52752 images/sec
Training: 2023-09-28 06:35:54,934 - loss nan, lr: 0.012500, epoch: 7, step: 43600, eta: 4.47 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11840 sec, avg_samples: 32.00000, ips: 540.52752 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 43700, eta: 4.47 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11843 sec, avg_samples: 32.00000, ips: 540.38455 images/sec
Training: 2023-09-28 06:36:06,782 - loss nan, lr: 0.012500, epoch: 7, step: 43700, eta: 4.47 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11843 sec, avg_samples: 32.00000, ips: 540.38455 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 43800, eta: 4.46 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11842 sec, avg_samples: 32.00000, ips: 540.43074 images/sec
Training: 2023-09-28 06:36:18,629 - loss nan, lr: 0.012500, epoch: 7, step: 43800, eta: 4.46 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11842 sec, avg_samples: 32.00000, ips: 540.43074 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 43900, eta: 4.45 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11843 sec, avg_samples: 32.00000, ips: 540.39015 images/sec
Training: 2023-09-28 06:36:30,478 - loss nan, lr: 0.012500, epoch: 7, step: 43900, eta: 4.45 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11843 sec, avg_samples: 32.00000, ips: 540.39015 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 44000, eta: 4.44 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11844 sec, avg_samples: 32.00000, ips: 540.36647 images/sec
Training: 2023-09-28 06:36:42,327 - loss nan, lr: 0.012500, epoch: 7, step: 44000, eta: 4.44 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11844 sec, avg_samples: 32.00000, ips: 540.36647 images/sec
INFO:root:[lfw][44000]XNorm: 1.492912
Training: 2023-09-28 06:37:10,266 - [lfw][44000]XNorm: 1.492912
INFO:root:[lfw][44000]Accuracy-Flip: 0.89850+-0.02003
Training: 2023-09-28 06:37:10,266 - [lfw][44000]Accuracy-Flip: 0.89850+-0.02003
INFO:root:[lfw][44000]Accuracy-Highest: 0.90150
Training: 2023-09-28 06:37:10,266 - [lfw][44000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9393
Training: 2023-09-28 06:37:10,266 - test time: 27.9393
INFO:root:[cfp_fp][44000]XNorm: 1.399477
Training: 2023-09-28 06:37:42,554 - [cfp_fp][44000]XNorm: 1.399477
INFO:root:[cfp_fp][44000]Accuracy-Flip: 0.64214+-0.01998
Training: 2023-09-28 06:37:42,554 - [cfp_fp][44000]Accuracy-Flip: 0.64214+-0.01998
INFO:root:[cfp_fp][44000]Accuracy-Highest: 0.65071
Training: 2023-09-28 06:37:42,554 - [cfp_fp][44000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2878
Training: 2023-09-28 06:37:42,554 - test time: 32.2878
INFO:root:[agedb_30][44000]XNorm: 1.419194
Training: 2023-09-28 06:38:10,319 - [agedb_30][44000]XNorm: 1.419194
INFO:root:[agedb_30][44000]Accuracy-Flip: 0.69083+-0.02265
Training: 2023-09-28 06:38:10,319 - [agedb_30][44000]Accuracy-Flip: 0.69083+-0.02265
INFO:root:[agedb_30][44000]Accuracy-Highest: 0.70083
Training: 2023-09-28 06:38:10,319 - [agedb_30][44000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7650
Training: 2023-09-28 06:38:10,319 - test time: 27.7650
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 44100, eta: 4.49 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11793 sec, avg_samples: 32.00000, ips: 542.71683 images/sec
Training: 2023-09-28 06:38:22,117 - loss nan, lr: 0.012500, epoch: 7, step: 44100, eta: 4.49 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11793 sec, avg_samples: 32.00000, ips: 542.71683 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 44200, eta: 4.49 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.52632 images/sec
Training: 2023-09-28 06:38:33,940 - loss nan, lr: 0.012500, epoch: 7, step: 44200, eta: 4.49 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.52632 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 44300, eta: 4.48 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.50962 images/sec
Training: 2023-09-28 06:38:45,765 - loss nan, lr: 0.012500, epoch: 7, step: 44300, eta: 4.48 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.50962 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 44400, eta: 4.47 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.54956 images/sec
Training: 2023-09-28 06:38:57,588 - loss nan, lr: 0.012500, epoch: 7, step: 44400, eta: 4.47 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.54956 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 44500, eta: 4.46 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.16711 images/sec
Training: 2023-09-28 06:39:09,419 - loss nan, lr: 0.012500, epoch: 7, step: 44500, eta: 4.46 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.16711 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 44600, eta: 4.46 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.68995 images/sec
Training: 2023-09-28 06:39:21,261 - loss nan, lr: 0.012500, epoch: 7, step: 44600, eta: 4.46 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.68995 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 44700, eta: 4.45 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11838 sec, avg_samples: 32.00000, ips: 540.62096 images/sec
Training: 2023-09-28 06:39:33,105 - loss nan, lr: 0.012500, epoch: 7, step: 44700, eta: 4.45 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11838 sec, avg_samples: 32.00000, ips: 540.62096 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 44800, eta: 4.44 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.75179 images/sec
Training: 2023-09-28 06:39:44,945 - loss nan, lr: 0.012500, epoch: 7, step: 44800, eta: 4.44 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.75179 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 44900, eta: 4.44 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11839 sec, avg_samples: 32.00000, ips: 540.57158 images/sec
Training: 2023-09-28 06:39:56,790 - loss nan, lr: 0.012500, epoch: 7, step: 44900, eta: 4.44 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11839 sec, avg_samples: 32.00000, ips: 540.57158 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 45000, eta: 4.43 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11839 sec, avg_samples: 32.00000, ips: 540.57211 images/sec
Training: 2023-09-28 06:40:08,634 - loss nan, lr: 0.012500, epoch: 7, step: 45000, eta: 4.43 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11839 sec, avg_samples: 32.00000, ips: 540.57211 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 45100, eta: 4.42 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11841 sec, avg_samples: 32.00000, ips: 540.51154 images/sec
Training: 2023-09-28 06:40:20,480 - loss nan, lr: 0.012500, epoch: 7, step: 45100, eta: 4.42 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11841 sec, avg_samples: 32.00000, ips: 540.51154 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 45200, eta: 4.41 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11839 sec, avg_samples: 32.00000, ips: 540.57348 images/sec
Training: 2023-09-28 06:40:32,325 - loss nan, lr: 0.012500, epoch: 7, step: 45200, eta: 4.41 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11839 sec, avg_samples: 32.00000, ips: 540.57348 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 45300, eta: 4.41 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11841 sec, avg_samples: 32.00000, ips: 540.49812 images/sec
Training: 2023-09-28 06:40:44,171 - loss nan, lr: 0.012500, epoch: 7, step: 45300, eta: 4.41 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11841 sec, avg_samples: 32.00000, ips: 540.49812 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 45400, eta: 4.40 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11839 sec, avg_samples: 32.00000, ips: 540.57279 images/sec
Training: 2023-09-28 06:40:56,015 - loss nan, lr: 0.012500, epoch: 7, step: 45400, eta: 4.40 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11839 sec, avg_samples: 32.00000, ips: 540.57279 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 45500, eta: 4.39 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11841 sec, avg_samples: 32.00000, ips: 540.51301 images/sec
Training: 2023-09-28 06:41:07,861 - loss nan, lr: 0.012500, epoch: 7, step: 45500, eta: 4.39 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11841 sec, avg_samples: 32.00000, ips: 540.51301 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 45600, eta: 4.39 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11840 sec, avg_samples: 32.00000, ips: 540.56250 images/sec
Training: 2023-09-28 06:41:19,706 - loss nan, lr: 0.012500, epoch: 7, step: 45600, eta: 4.39 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11840 sec, avg_samples: 32.00000, ips: 540.56250 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 45700, eta: 4.38 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.69255 images/sec
Training: 2023-09-28 06:41:31,548 - loss nan, lr: 0.012500, epoch: 7, step: 45700, eta: 4.38 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.69255 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 45800, eta: 4.37 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11836 sec, avg_samples: 32.00000, ips: 540.72221 images/sec
Training: 2023-09-28 06:41:43,389 - loss nan, lr: 0.012500, epoch: 7, step: 45800, eta: 4.37 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11836 sec, avg_samples: 32.00000, ips: 540.72221 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 7, step: 45900, eta: 4.36 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.68225 images/sec
Training: 2023-09-28 06:41:55,231 - loss nan, lr: 0.012500, epoch: 7, step: 45900, eta: 4.36 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.68225 images/sec
INFO:root:Save model to MS1M_v3_arcface_static_0.1/FresResNet50/7.
Training: 2023-09-28 06:42:05,519 - Save model to MS1M_v3_arcface_static_0.1/FresResNet50/7.
INFO:root:Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/6.
Training: 2023-09-28 06:42:05,520 - Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/6.
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 46000, eta: 4.36 hours, avg_reader_cost: 0.00173 sec, avg_batch_cost: 0.02067 sec, avg_samples: 5.12000, ips: 495.36302 images/sec
Training: 2023-09-28 06:42:07,617 - loss nan, lr: 0.012500, epoch: 8, step: 46000, eta: 4.36 hours, avg_reader_cost: 0.00173 sec, avg_batch_cost: 0.02067 sec, avg_samples: 5.12000, ips: 495.36302 images/sec
INFO:root:[lfw][46000]XNorm: 1.328504
Training: 2023-09-28 06:42:35,538 - [lfw][46000]XNorm: 1.328504
INFO:root:[lfw][46000]Accuracy-Flip: 0.90050+-0.01767
Training: 2023-09-28 06:42:35,538 - [lfw][46000]Accuracy-Flip: 0.90050+-0.01767
INFO:root:[lfw][46000]Accuracy-Highest: 0.90150
Training: 2023-09-28 06:42:35,538 - [lfw][46000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9203
Training: 2023-09-28 06:42:35,538 - test time: 27.9203
INFO:root:[cfp_fp][46000]XNorm: 1.238306
Training: 2023-09-28 06:43:07,858 - [cfp_fp][46000]XNorm: 1.238306
INFO:root:[cfp_fp][46000]Accuracy-Flip: 0.64614+-0.01928
Training: 2023-09-28 06:43:07,858 - [cfp_fp][46000]Accuracy-Flip: 0.64614+-0.01928
INFO:root:[cfp_fp][46000]Accuracy-Highest: 0.65071
Training: 2023-09-28 06:43:07,858 - [cfp_fp][46000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.3198
Training: 2023-09-28 06:43:07,858 - test time: 32.3198
INFO:root:[agedb_30][46000]XNorm: 1.259965
Training: 2023-09-28 06:43:35,629 - [agedb_30][46000]XNorm: 1.259965
INFO:root:[agedb_30][46000]Accuracy-Flip: 0.69150+-0.01769
Training: 2023-09-28 06:43:35,629 - [agedb_30][46000]Accuracy-Flip: 0.69150+-0.01769
INFO:root:[agedb_30][46000]Accuracy-Highest: 0.70083
Training: 2023-09-28 06:43:35,629 - [agedb_30][46000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7710
Training: 2023-09-28 06:43:35,629 - test time: 27.7710
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 46100, eta: 4.40 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.88782 images/sec
Training: 2023-09-28 06:43:47,466 - loss nan, lr: 0.012500, epoch: 8, step: 46100, eta: 4.40 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.88782 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 46200, eta: 4.40 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11856 sec, avg_samples: 32.00000, ips: 539.79371 images/sec
Training: 2023-09-28 06:43:59,328 - loss nan, lr: 0.012500, epoch: 8, step: 46200, eta: 4.40 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11856 sec, avg_samples: 32.00000, ips: 539.79371 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 46300, eta: 4.39 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11882 sec, avg_samples: 32.00000, ips: 538.63919 images/sec
Training: 2023-09-28 06:44:11,215 - loss nan, lr: 0.012500, epoch: 8, step: 46300, eta: 4.39 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11882 sec, avg_samples: 32.00000, ips: 538.63919 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 46400, eta: 4.38 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11888 sec, avg_samples: 32.00000, ips: 538.33991 images/sec
Training: 2023-09-28 06:44:23,108 - loss nan, lr: 0.012500, epoch: 8, step: 46400, eta: 4.38 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11888 sec, avg_samples: 32.00000, ips: 538.33991 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 46500, eta: 4.37 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11912 sec, avg_samples: 32.00000, ips: 537.27411 images/sec
Training: 2023-09-28 06:44:35,025 - loss nan, lr: 0.012500, epoch: 8, step: 46500, eta: 4.37 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11912 sec, avg_samples: 32.00000, ips: 537.27411 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 46600, eta: 4.37 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11913 sec, avg_samples: 32.00000, ips: 537.25068 images/sec
Training: 2023-09-28 06:44:46,943 - loss nan, lr: 0.012500, epoch: 8, step: 46600, eta: 4.37 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11913 sec, avg_samples: 32.00000, ips: 537.25068 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 46700, eta: 4.36 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11904 sec, avg_samples: 32.00000, ips: 537.62556 images/sec
Training: 2023-09-28 06:44:58,853 - loss nan, lr: 0.012500, epoch: 8, step: 46700, eta: 4.36 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11904 sec, avg_samples: 32.00000, ips: 537.62556 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 46800, eta: 4.35 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11825 sec, avg_samples: 32.00000, ips: 541.20707 images/sec
Training: 2023-09-28 06:45:10,684 - loss nan, lr: 0.012500, epoch: 8, step: 46800, eta: 4.35 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11825 sec, avg_samples: 32.00000, ips: 541.20707 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 46900, eta: 4.35 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.68478 images/sec
Training: 2023-09-28 06:45:22,526 - loss nan, lr: 0.012500, epoch: 8, step: 46900, eta: 4.35 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.68478 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 47000, eta: 4.34 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11857 sec, avg_samples: 32.00000, ips: 539.78050 images/sec
Training: 2023-09-28 06:45:34,390 - loss nan, lr: 0.012500, epoch: 8, step: 47000, eta: 4.34 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11857 sec, avg_samples: 32.00000, ips: 539.78050 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 47100, eta: 4.33 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11868 sec, avg_samples: 32.00000, ips: 539.26653 images/sec
Training: 2023-09-28 06:45:46,265 - loss nan, lr: 0.012500, epoch: 8, step: 47100, eta: 4.33 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11868 sec, avg_samples: 32.00000, ips: 539.26653 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 47200, eta: 4.33 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11866 sec, avg_samples: 32.00000, ips: 539.33554 images/sec
Training: 2023-09-28 06:45:58,139 - loss nan, lr: 0.012500, epoch: 8, step: 47200, eta: 4.33 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11866 sec, avg_samples: 32.00000, ips: 539.33554 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 47300, eta: 4.32 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11864 sec, avg_samples: 32.00000, ips: 539.43245 images/sec
Training: 2023-09-28 06:46:10,010 - loss nan, lr: 0.012500, epoch: 8, step: 47300, eta: 4.32 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11864 sec, avg_samples: 32.00000, ips: 539.43245 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 47400, eta: 4.31 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11838 sec, avg_samples: 32.00000, ips: 540.63621 images/sec
Training: 2023-09-28 06:46:21,854 - loss nan, lr: 0.012500, epoch: 8, step: 47400, eta: 4.31 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11838 sec, avg_samples: 32.00000, ips: 540.63621 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 47500, eta: 4.31 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.36450 images/sec
Training: 2023-09-28 06:46:33,680 - loss nan, lr: 0.012500, epoch: 8, step: 47500, eta: 4.31 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.36450 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 47600, eta: 4.30 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.39852 images/sec
Training: 2023-09-28 06:46:45,506 - loss nan, lr: 0.012500, epoch: 8, step: 47600, eta: 4.30 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.39852 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 47700, eta: 4.29 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11824 sec, avg_samples: 32.00000, ips: 541.27917 images/sec
Training: 2023-09-28 06:46:57,334 - loss nan, lr: 0.012500, epoch: 8, step: 47700, eta: 4.29 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11824 sec, avg_samples: 32.00000, ips: 541.27917 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 47800, eta: 4.28 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11860 sec, avg_samples: 32.00000, ips: 539.61021 images/sec
Training: 2023-09-28 06:47:09,200 - loss nan, lr: 0.012500, epoch: 8, step: 47800, eta: 4.28 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11860 sec, avg_samples: 32.00000, ips: 539.61021 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 47900, eta: 4.28 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11855 sec, avg_samples: 32.00000, ips: 539.84468 images/sec
Training: 2023-09-28 06:47:21,060 - loss nan, lr: 0.012500, epoch: 8, step: 47900, eta: 4.28 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11855 sec, avg_samples: 32.00000, ips: 539.84468 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 48000, eta: 4.27 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11824 sec, avg_samples: 32.00000, ips: 541.25012 images/sec
Training: 2023-09-28 06:47:32,889 - loss nan, lr: 0.012500, epoch: 8, step: 48000, eta: 4.27 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11824 sec, avg_samples: 32.00000, ips: 541.25012 images/sec
INFO:root:[lfw][48000]XNorm: 1.171629
Training: 2023-09-28 06:48:00,831 - [lfw][48000]XNorm: 1.171629
INFO:root:[lfw][48000]Accuracy-Flip: 0.89717+-0.01814
Training: 2023-09-28 06:48:00,831 - [lfw][48000]Accuracy-Flip: 0.89717+-0.01814
INFO:root:[lfw][48000]Accuracy-Highest: 0.90150
Training: 2023-09-28 06:48:00,831 - [lfw][48000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9419
Training: 2023-09-28 06:48:00,831 - test time: 27.9419
INFO:root:[cfp_fp][48000]XNorm: 1.096214
Training: 2023-09-28 06:48:33,107 - [cfp_fp][48000]XNorm: 1.096214
INFO:root:[cfp_fp][48000]Accuracy-Flip: 0.64429+-0.01723
Training: 2023-09-28 06:48:33,107 - [cfp_fp][48000]Accuracy-Flip: 0.64429+-0.01723
INFO:root:[cfp_fp][48000]Accuracy-Highest: 0.65071
Training: 2023-09-28 06:48:33,107 - [cfp_fp][48000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2762
Training: 2023-09-28 06:48:33,107 - test time: 32.2762
INFO:root:[agedb_30][48000]XNorm: 1.113013
Training: 2023-09-28 06:49:00,847 - [agedb_30][48000]XNorm: 1.113013
INFO:root:[agedb_30][48000]Accuracy-Flip: 0.68983+-0.02376
Training: 2023-09-28 06:49:00,847 - [agedb_30][48000]Accuracy-Flip: 0.68983+-0.02376
INFO:root:[agedb_30][48000]Accuracy-Highest: 0.70083
Training: 2023-09-28 06:49:00,847 - [agedb_30][48000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7399
Training: 2023-09-28 06:49:00,847 - test time: 27.7399
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 48100, eta: 4.31 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11790 sec, avg_samples: 32.00000, ips: 542.83298 images/sec
Training: 2023-09-28 06:49:12,642 - loss nan, lr: 0.012500, epoch: 8, step: 48100, eta: 4.31 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11790 sec, avg_samples: 32.00000, ips: 542.83298 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 48200, eta: 4.31 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.29710 images/sec
Training: 2023-09-28 06:49:24,449 - loss nan, lr: 0.012500, epoch: 8, step: 48200, eta: 4.31 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.29710 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 48300, eta: 4.30 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11806 sec, avg_samples: 32.00000, ips: 542.11548 images/sec
Training: 2023-09-28 06:49:36,260 - loss nan, lr: 0.012500, epoch: 8, step: 48300, eta: 4.30 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11806 sec, avg_samples: 32.00000, ips: 542.11548 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 48400, eta: 4.29 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11806 sec, avg_samples: 32.00000, ips: 542.11791 images/sec
Training: 2023-09-28 06:49:48,070 - loss nan, lr: 0.012500, epoch: 8, step: 48400, eta: 4.29 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11806 sec, avg_samples: 32.00000, ips: 542.11791 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 48500, eta: 4.29 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11809 sec, avg_samples: 32.00000, ips: 541.94561 images/sec
Training: 2023-09-28 06:49:59,884 - loss nan, lr: 0.012500, epoch: 8, step: 48500, eta: 4.29 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11809 sec, avg_samples: 32.00000, ips: 541.94561 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 48600, eta: 4.28 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.02383 images/sec
Training: 2023-09-28 06:50:11,719 - loss nan, lr: 0.012500, epoch: 8, step: 48600, eta: 4.28 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.02383 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 48700, eta: 4.27 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.09292 images/sec
Training: 2023-09-28 06:50:23,551 - loss nan, lr: 0.012500, epoch: 8, step: 48700, eta: 4.27 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.09292 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 48800, eta: 4.26 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11833 sec, avg_samples: 32.00000, ips: 540.87479 images/sec
Training: 2023-09-28 06:50:35,389 - loss nan, lr: 0.012500, epoch: 8, step: 48800, eta: 4.26 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11833 sec, avg_samples: 32.00000, ips: 540.87479 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 48900, eta: 4.26 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.12691 images/sec
Training: 2023-09-28 06:50:47,221 - loss nan, lr: 0.012500, epoch: 8, step: 48900, eta: 4.26 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.12691 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 49000, eta: 4.25 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.19548 images/sec
Training: 2023-09-28 06:50:59,052 - loss nan, lr: 0.012500, epoch: 8, step: 49000, eta: 4.25 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.19548 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 49100, eta: 4.24 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.08567 images/sec
Training: 2023-09-28 06:51:10,885 - loss nan, lr: 0.012500, epoch: 8, step: 49100, eta: 4.24 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.08567 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 49200, eta: 4.24 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.03865 images/sec
Training: 2023-09-28 06:51:22,719 - loss nan, lr: 0.012500, epoch: 8, step: 49200, eta: 4.24 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.03865 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 49300, eta: 4.23 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11830 sec, avg_samples: 32.00000, ips: 541.01074 images/sec
Training: 2023-09-28 06:51:34,553 - loss nan, lr: 0.012500, epoch: 8, step: 49300, eta: 4.23 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11830 sec, avg_samples: 32.00000, ips: 541.01074 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 49400, eta: 4.22 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11833 sec, avg_samples: 32.00000, ips: 540.87684 images/sec
Training: 2023-09-28 06:51:46,390 - loss nan, lr: 0.012500, epoch: 8, step: 49400, eta: 4.22 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11833 sec, avg_samples: 32.00000, ips: 540.87684 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 49500, eta: 4.22 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11911 sec, avg_samples: 32.00000, ips: 537.31806 images/sec
Training: 2023-09-28 06:51:58,306 - loss nan, lr: 0.012500, epoch: 8, step: 49500, eta: 4.22 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11911 sec, avg_samples: 32.00000, ips: 537.31806 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 49600, eta: 4.21 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11911 sec, avg_samples: 32.00000, ips: 537.32031 images/sec
Training: 2023-09-28 06:52:10,222 - loss nan, lr: 0.012500, epoch: 8, step: 49600, eta: 4.21 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11911 sec, avg_samples: 32.00000, ips: 537.32031 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 49700, eta: 4.20 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11885 sec, avg_samples: 32.00000, ips: 538.50758 images/sec
Training: 2023-09-28 06:52:22,111 - loss nan, lr: 0.012500, epoch: 8, step: 49700, eta: 4.20 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11885 sec, avg_samples: 32.00000, ips: 538.50758 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 49800, eta: 4.20 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11877 sec, avg_samples: 32.00000, ips: 538.85776 images/sec
Training: 2023-09-28 06:52:33,993 - loss nan, lr: 0.012500, epoch: 8, step: 49800, eta: 4.20 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11877 sec, avg_samples: 32.00000, ips: 538.85776 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 49900, eta: 4.19 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11878 sec, avg_samples: 32.00000, ips: 538.79186 images/sec
Training: 2023-09-28 06:52:45,876 - loss nan, lr: 0.012500, epoch: 8, step: 49900, eta: 4.19 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11878 sec, avg_samples: 32.00000, ips: 538.79186 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 50000, eta: 4.18 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11877 sec, avg_samples: 32.00000, ips: 538.84418 images/sec
Training: 2023-09-28 06:52:57,759 - loss nan, lr: 0.012500, epoch: 8, step: 50000, eta: 4.18 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11877 sec, avg_samples: 32.00000, ips: 538.84418 images/sec
INFO:root:[lfw][50000]XNorm: 1.027140
Training: 2023-09-28 06:53:25,725 - [lfw][50000]XNorm: 1.027140
INFO:root:[lfw][50000]Accuracy-Flip: 0.89617+-0.01817
Training: 2023-09-28 06:53:25,725 - [lfw][50000]Accuracy-Flip: 0.89617+-0.01817
INFO:root:[lfw][50000]Accuracy-Highest: 0.90150
Training: 2023-09-28 06:53:25,725 - [lfw][50000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9665
Training: 2023-09-28 06:53:25,725 - test time: 27.9665
INFO:root:[cfp_fp][50000]XNorm: 0.959868
Training: 2023-09-28 06:53:58,066 - [cfp_fp][50000]XNorm: 0.959868
INFO:root:[cfp_fp][50000]Accuracy-Flip: 0.64357+-0.01723
Training: 2023-09-28 06:53:58,067 - [cfp_fp][50000]Accuracy-Flip: 0.64357+-0.01723
INFO:root:[cfp_fp][50000]Accuracy-Highest: 0.65071
Training: 2023-09-28 06:53:58,067 - [cfp_fp][50000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.3414
Training: 2023-09-28 06:53:58,067 - test time: 32.3414
INFO:root:[agedb_30][50000]XNorm: 0.975401
Training: 2023-09-28 06:54:25,860 - [agedb_30][50000]XNorm: 0.975401
INFO:root:[agedb_30][50000]Accuracy-Flip: 0.68817+-0.01923
Training: 2023-09-28 06:54:25,861 - [agedb_30][50000]Accuracy-Flip: 0.68817+-0.01923
INFO:root:[agedb_30][50000]Accuracy-Highest: 0.70083
Training: 2023-09-28 06:54:25,861 - [agedb_30][50000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7941
Training: 2023-09-28 06:54:25,861 - test time: 27.7941
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 50100, eta: 4.22 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11831 sec, avg_samples: 32.00000, ips: 540.97096 images/sec
Training: 2023-09-28 06:54:37,697 - loss nan, lr: 0.012500, epoch: 8, step: 50100, eta: 4.22 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11831 sec, avg_samples: 32.00000, ips: 540.97096 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 50200, eta: 4.22 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11811 sec, avg_samples: 32.00000, ips: 541.88943 images/sec
Training: 2023-09-28 06:54:49,512 - loss nan, lr: 0.012500, epoch: 8, step: 50200, eta: 4.22 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11811 sec, avg_samples: 32.00000, ips: 541.88943 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 50300, eta: 4.21 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11831 sec, avg_samples: 32.00000, ips: 540.96134 images/sec
Training: 2023-09-28 06:55:01,349 - loss nan, lr: 0.012500, epoch: 8, step: 50300, eta: 4.21 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11831 sec, avg_samples: 32.00000, ips: 540.96134 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 50400, eta: 4.20 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11857 sec, avg_samples: 32.00000, ips: 539.75023 images/sec
Training: 2023-09-28 06:55:13,211 - loss nan, lr: 0.012500, epoch: 8, step: 50400, eta: 4.20 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11857 sec, avg_samples: 32.00000, ips: 539.75023 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 50500, eta: 4.20 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11860 sec, avg_samples: 32.00000, ips: 539.62490 images/sec
Training: 2023-09-28 06:55:25,077 - loss nan, lr: 0.012500, epoch: 8, step: 50500, eta: 4.20 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11860 sec, avg_samples: 32.00000, ips: 539.62490 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 50600, eta: 4.19 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11894 sec, avg_samples: 32.00000, ips: 538.08489 images/sec
Training: 2023-09-28 06:55:36,976 - loss nan, lr: 0.012500, epoch: 8, step: 50600, eta: 4.19 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11894 sec, avg_samples: 32.00000, ips: 538.08489 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 50700, eta: 4.18 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11892 sec, avg_samples: 32.00000, ips: 538.19045 images/sec
Training: 2023-09-28 06:55:48,874 - loss nan, lr: 0.012500, epoch: 8, step: 50700, eta: 4.18 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11892 sec, avg_samples: 32.00000, ips: 538.19045 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 50800, eta: 4.18 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11848 sec, avg_samples: 32.00000, ips: 540.17829 images/sec
Training: 2023-09-28 06:56:00,727 - loss nan, lr: 0.012500, epoch: 8, step: 50800, eta: 4.18 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11848 sec, avg_samples: 32.00000, ips: 540.17829 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 50900, eta: 4.17 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11831 sec, avg_samples: 32.00000, ips: 540.93523 images/sec
Training: 2023-09-28 06:56:12,564 - loss nan, lr: 0.012500, epoch: 8, step: 50900, eta: 4.17 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11831 sec, avg_samples: 32.00000, ips: 540.93523 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 51000, eta: 4.16 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11839 sec, avg_samples: 32.00000, ips: 540.57414 images/sec
Training: 2023-09-28 06:56:24,408 - loss nan, lr: 0.012500, epoch: 8, step: 51000, eta: 4.16 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11839 sec, avg_samples: 32.00000, ips: 540.57414 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 51100, eta: 4.16 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.02113 images/sec
Training: 2023-09-28 06:56:36,243 - loss nan, lr: 0.012500, epoch: 8, step: 51100, eta: 4.16 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.02113 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 51200, eta: 4.15 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11831 sec, avg_samples: 32.00000, ips: 540.97333 images/sec
Training: 2023-09-28 06:56:48,079 - loss nan, lr: 0.012500, epoch: 8, step: 51200, eta: 4.15 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11831 sec, avg_samples: 32.00000, ips: 540.97333 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 51300, eta: 4.14 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11825 sec, avg_samples: 32.00000, ips: 541.23701 images/sec
Training: 2023-09-28 06:56:59,909 - loss nan, lr: 0.012500, epoch: 8, step: 51300, eta: 4.14 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11825 sec, avg_samples: 32.00000, ips: 541.23701 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 51400, eta: 4.14 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11825 sec, avg_samples: 32.00000, ips: 541.22031 images/sec
Training: 2023-09-28 06:57:11,738 - loss nan, lr: 0.012500, epoch: 8, step: 51400, eta: 4.14 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11825 sec, avg_samples: 32.00000, ips: 541.22031 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 51500, eta: 4.13 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.08183 images/sec
Training: 2023-09-28 06:57:23,571 - loss nan, lr: 0.012500, epoch: 8, step: 51500, eta: 4.13 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.08183 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 51600, eta: 4.12 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11850 sec, avg_samples: 32.00000, ips: 540.08783 images/sec
Training: 2023-09-28 06:57:35,426 - loss nan, lr: 0.012500, epoch: 8, step: 51600, eta: 4.12 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11850 sec, avg_samples: 32.00000, ips: 540.08783 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 8, step: 51700, eta: 4.12 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11839 sec, avg_samples: 32.00000, ips: 540.58979 images/sec
Training: 2023-09-28 06:57:47,271 - loss nan, lr: 0.012500, epoch: 8, step: 51700, eta: 4.12 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11839 sec, avg_samples: 32.00000, ips: 540.58979 images/sec
INFO:root:Save model to MS1M_v3_arcface_static_0.1/FresResNet50/8.
Training: 2023-09-28 06:57:51,398 - Save model to MS1M_v3_arcface_static_0.1/FresResNet50/8.
INFO:root:Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/7.
Training: 2023-09-28 06:57:51,399 - Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/7.
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 51800, eta: 4.11 hours, avg_reader_cost: 0.00175 sec, avg_batch_cost: 0.08221 sec, avg_samples: 21.76000, ips: 529.39141 images/sec
Training: 2023-09-28 06:57:59,653 - loss nan, lr: 0.012500, epoch: 9, step: 51800, eta: 4.11 hours, avg_reader_cost: 0.00175 sec, avg_batch_cost: 0.08221 sec, avg_samples: 21.76000, ips: 529.39141 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 51900, eta: 4.10 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11855 sec, avg_samples: 32.00000, ips: 539.84743 images/sec
Training: 2023-09-28 06:58:11,513 - loss nan, lr: 0.012500, epoch: 9, step: 51900, eta: 4.10 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11855 sec, avg_samples: 32.00000, ips: 539.84743 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 52000, eta: 4.10 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11900 sec, avg_samples: 32.00000, ips: 537.83008 images/sec
Training: 2023-09-28 06:58:23,418 - loss nan, lr: 0.012500, epoch: 9, step: 52000, eta: 4.10 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11900 sec, avg_samples: 32.00000, ips: 537.83008 images/sec
INFO:root:[lfw][52000]XNorm: 0.915069
Training: 2023-09-28 06:58:51,303 - [lfw][52000]XNorm: 0.915069
INFO:root:[lfw][52000]Accuracy-Flip: 0.90033+-0.01868
Training: 2023-09-28 06:58:51,303 - [lfw][52000]Accuracy-Flip: 0.90033+-0.01868
INFO:root:[lfw][52000]Accuracy-Highest: 0.90150
Training: 2023-09-28 06:58:51,303 - [lfw][52000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.8850
Training: 2023-09-28 06:58:51,303 - test time: 27.8850
INFO:root:[cfp_fp][52000]XNorm: 0.855213
Training: 2023-09-28 06:59:23,567 - [cfp_fp][52000]XNorm: 0.855213
INFO:root:[cfp_fp][52000]Accuracy-Flip: 0.64257+-0.01755
Training: 2023-09-28 06:59:23,567 - [cfp_fp][52000]Accuracy-Flip: 0.64257+-0.01755
INFO:root:[cfp_fp][52000]Accuracy-Highest: 0.65071
Training: 2023-09-28 06:59:23,567 - [cfp_fp][52000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2638
Training: 2023-09-28 06:59:23,567 - test time: 32.2638
INFO:root:[agedb_30][52000]XNorm: 0.867756
Training: 2023-09-28 06:59:51,344 - [agedb_30][52000]XNorm: 0.867756
INFO:root:[agedb_30][52000]Accuracy-Flip: 0.69417+-0.02248
Training: 2023-09-28 06:59:51,345 - [agedb_30][52000]Accuracy-Flip: 0.69417+-0.02248
INFO:root:[agedb_30][52000]Accuracy-Highest: 0.70083
Training: 2023-09-28 06:59:51,345 - [agedb_30][52000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7775
Training: 2023-09-28 06:59:51,345 - test time: 27.7775
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 52100, eta: 4.13 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11847 sec, avg_samples: 32.00000, ips: 540.20110 images/sec
Training: 2023-09-28 07:00:03,197 - loss nan, lr: 0.012500, epoch: 9, step: 52100, eta: 4.13 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11847 sec, avg_samples: 32.00000, ips: 540.20110 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 52200, eta: 4.13 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.68912 images/sec
Training: 2023-09-28 07:00:15,017 - loss nan, lr: 0.012500, epoch: 9, step: 52200, eta: 4.13 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.68912 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 52300, eta: 4.12 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.74512 images/sec
Training: 2023-09-28 07:00:26,836 - loss nan, lr: 0.012500, epoch: 9, step: 52300, eta: 4.12 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.74512 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 52400, eta: 4.11 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.16830 images/sec
Training: 2023-09-28 07:00:38,668 - loss nan, lr: 0.012500, epoch: 9, step: 52400, eta: 4.11 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.16830 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 52500, eta: 4.11 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11841 sec, avg_samples: 32.00000, ips: 540.49323 images/sec
Training: 2023-09-28 07:00:50,515 - loss nan, lr: 0.012500, epoch: 9, step: 52500, eta: 4.11 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11841 sec, avg_samples: 32.00000, ips: 540.49323 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 52600, eta: 4.10 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11882 sec, avg_samples: 32.00000, ips: 538.62351 images/sec
Training: 2023-09-28 07:01:02,404 - loss nan, lr: 0.012500, epoch: 9, step: 52600, eta: 4.10 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11882 sec, avg_samples: 32.00000, ips: 538.62351 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 52700, eta: 4.09 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11870 sec, avg_samples: 32.00000, ips: 539.15245 images/sec
Training: 2023-09-28 07:01:14,281 - loss nan, lr: 0.012500, epoch: 9, step: 52700, eta: 4.09 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11870 sec, avg_samples: 32.00000, ips: 539.15245 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 52800, eta: 4.09 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.30329 images/sec
Training: 2023-09-28 07:01:26,108 - loss nan, lr: 0.012500, epoch: 9, step: 52800, eta: 4.09 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.30329 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 52900, eta: 4.08 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.66667 images/sec
Training: 2023-09-28 07:01:37,950 - loss nan, lr: 0.012500, epoch: 9, step: 52900, eta: 4.08 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.66667 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 53000, eta: 4.07 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.18093 images/sec
Training: 2023-09-28 07:01:49,781 - loss nan, lr: 0.012500, epoch: 9, step: 53000, eta: 4.07 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.18093 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 53100, eta: 4.07 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.39487 images/sec
Training: 2023-09-28 07:02:01,606 - loss nan, lr: 0.012500, epoch: 9, step: 53100, eta: 4.07 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.39487 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 53200, eta: 4.06 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.14501 images/sec
Training: 2023-09-28 07:02:13,438 - loss nan, lr: 0.012500, epoch: 9, step: 53200, eta: 4.06 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.14501 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 53300, eta: 4.05 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.31893 images/sec
Training: 2023-09-28 07:02:25,266 - loss nan, lr: 0.012500, epoch: 9, step: 53300, eta: 4.05 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.31893 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 53400, eta: 4.05 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.31094 images/sec
Training: 2023-09-28 07:02:37,093 - loss nan, lr: 0.012500, epoch: 9, step: 53400, eta: 4.05 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.31094 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 53500, eta: 4.04 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.73502 images/sec
Training: 2023-09-28 07:02:48,912 - loss nan, lr: 0.012500, epoch: 9, step: 53500, eta: 4.04 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.73502 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 53600, eta: 4.03 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.73657 images/sec
Training: 2023-09-28 07:03:00,730 - loss nan, lr: 0.012500, epoch: 9, step: 53600, eta: 4.03 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.73657 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 53700, eta: 4.03 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.59975 images/sec
Training: 2023-09-28 07:03:12,552 - loss nan, lr: 0.012500, epoch: 9, step: 53700, eta: 4.03 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.59975 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 53800, eta: 4.02 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.56911 images/sec
Training: 2023-09-28 07:03:24,374 - loss nan, lr: 0.012500, epoch: 9, step: 53800, eta: 4.02 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.56911 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 53900, eta: 4.01 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.43821 images/sec
Training: 2023-09-28 07:03:36,199 - loss nan, lr: 0.012500, epoch: 9, step: 53900, eta: 4.01 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.43821 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 54000, eta: 4.01 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.50114 images/sec
Training: 2023-09-28 07:03:48,023 - loss nan, lr: 0.012500, epoch: 9, step: 54000, eta: 4.01 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.50114 images/sec
INFO:root:[lfw][54000]XNorm: 0.799028
Training: 2023-09-28 07:04:15,949 - [lfw][54000]XNorm: 0.799028
INFO:root:[lfw][54000]Accuracy-Flip: 0.89600+-0.01919
Training: 2023-09-28 07:04:15,949 - [lfw][54000]Accuracy-Flip: 0.89600+-0.01919
INFO:root:[lfw][54000]Accuracy-Highest: 0.90150
Training: 2023-09-28 07:04:15,949 - [lfw][54000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9264
Training: 2023-09-28 07:04:15,949 - test time: 27.9264
INFO:root:[cfp_fp][54000]XNorm: 0.749330
Training: 2023-09-28 07:04:48,204 - [cfp_fp][54000]XNorm: 0.749330
INFO:root:[cfp_fp][54000]Accuracy-Flip: 0.64571+-0.01855
Training: 2023-09-28 07:04:48,204 - [cfp_fp][54000]Accuracy-Flip: 0.64571+-0.01855
INFO:root:[cfp_fp][54000]Accuracy-Highest: 0.65071
Training: 2023-09-28 07:04:48,205 - [cfp_fp][54000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2555
Training: 2023-09-28 07:04:48,205 - test time: 32.2555
INFO:root:[agedb_30][54000]XNorm: 0.758834
Training: 2023-09-28 07:05:15,928 - [agedb_30][54000]XNorm: 0.758834
INFO:root:[agedb_30][54000]Accuracy-Flip: 0.68783+-0.02341
Training: 2023-09-28 07:05:15,928 - [agedb_30][54000]Accuracy-Flip: 0.68783+-0.02341
INFO:root:[agedb_30][54000]Accuracy-Highest: 0.70083
Training: 2023-09-28 07:05:15,928 - [agedb_30][54000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7237
Training: 2023-09-28 07:05:15,928 - test time: 27.7237
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 54100, eta: 4.04 hours, avg_reader_cost: 0.00008 sec, avg_batch_cost: 0.11825 sec, avg_samples: 32.00000, ips: 541.24758 images/sec
Training: 2023-09-28 07:05:27,760 - loss nan, lr: 0.012500, epoch: 9, step: 54100, eta: 4.04 hours, avg_reader_cost: 0.00008 sec, avg_batch_cost: 0.11825 sec, avg_samples: 32.00000, ips: 541.24758 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 54200, eta: 4.04 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11849 sec, avg_samples: 32.00000, ips: 540.11220 images/sec
Training: 2023-09-28 07:05:39,617 - loss nan, lr: 0.012500, epoch: 9, step: 54200, eta: 4.04 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11849 sec, avg_samples: 32.00000, ips: 540.11220 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 54300, eta: 4.03 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11847 sec, avg_samples: 32.00000, ips: 540.22999 images/sec
Training: 2023-09-28 07:05:51,471 - loss nan, lr: 0.012500, epoch: 9, step: 54300, eta: 4.03 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11847 sec, avg_samples: 32.00000, ips: 540.22999 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 54400, eta: 4.02 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11845 sec, avg_samples: 32.00000, ips: 540.30817 images/sec
Training: 2023-09-28 07:06:03,324 - loss nan, lr: 0.012500, epoch: 9, step: 54400, eta: 4.02 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11845 sec, avg_samples: 32.00000, ips: 540.30817 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 54500, eta: 4.02 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11849 sec, avg_samples: 32.00000, ips: 540.12911 images/sec
Training: 2023-09-28 07:06:15,180 - loss nan, lr: 0.012500, epoch: 9, step: 54500, eta: 4.02 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11849 sec, avg_samples: 32.00000, ips: 540.12911 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 54600, eta: 4.01 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11867 sec, avg_samples: 32.00000, ips: 539.32409 images/sec
Training: 2023-09-28 07:06:27,055 - loss nan, lr: 0.012500, epoch: 9, step: 54600, eta: 4.01 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11867 sec, avg_samples: 32.00000, ips: 539.32409 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 54700, eta: 4.00 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11872 sec, avg_samples: 32.00000, ips: 539.10061 images/sec
Training: 2023-09-28 07:06:38,934 - loss nan, lr: 0.012500, epoch: 9, step: 54700, eta: 4.00 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11872 sec, avg_samples: 32.00000, ips: 539.10061 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 54800, eta: 4.00 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11875 sec, avg_samples: 32.00000, ips: 538.94973 images/sec
Training: 2023-09-28 07:06:50,816 - loss nan, lr: 0.012500, epoch: 9, step: 54800, eta: 4.00 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11875 sec, avg_samples: 32.00000, ips: 538.94973 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 54900, eta: 3.99 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11861 sec, avg_samples: 32.00000, ips: 539.56228 images/sec
Training: 2023-09-28 07:07:02,684 - loss nan, lr: 0.012500, epoch: 9, step: 54900, eta: 3.99 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11861 sec, avg_samples: 32.00000, ips: 539.56228 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 55000, eta: 3.98 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.29581 images/sec
Training: 2023-09-28 07:07:14,512 - loss nan, lr: 0.012500, epoch: 9, step: 55000, eta: 3.98 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.29581 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 55100, eta: 3.98 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11830 sec, avg_samples: 32.00000, ips: 540.99325 images/sec
Training: 2023-09-28 07:07:26,347 - loss nan, lr: 0.012500, epoch: 9, step: 55100, eta: 3.98 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11830 sec, avg_samples: 32.00000, ips: 540.99325 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 55200, eta: 3.97 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.37510 images/sec
Training: 2023-09-28 07:07:38,174 - loss nan, lr: 0.012500, epoch: 9, step: 55200, eta: 3.97 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.37510 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 55300, eta: 3.96 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.34316 images/sec
Training: 2023-09-28 07:07:50,001 - loss nan, lr: 0.012500, epoch: 9, step: 55300, eta: 3.96 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.34316 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 55400, eta: 3.96 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.37542 images/sec
Training: 2023-09-28 07:08:01,827 - loss nan, lr: 0.012500, epoch: 9, step: 55400, eta: 3.96 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.37542 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 55500, eta: 3.95 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.29733 images/sec
Training: 2023-09-28 07:08:13,655 - loss nan, lr: 0.012500, epoch: 9, step: 55500, eta: 3.95 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.29733 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 55600, eta: 3.95 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.41314 images/sec
Training: 2023-09-28 07:08:25,480 - loss nan, lr: 0.012500, epoch: 9, step: 55600, eta: 3.95 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.41314 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 55700, eta: 3.94 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.33645 images/sec
Training: 2023-09-28 07:08:37,308 - loss nan, lr: 0.012500, epoch: 9, step: 55700, eta: 3.94 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.33645 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 55800, eta: 3.93 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.37918 images/sec
Training: 2023-09-28 07:08:49,134 - loss nan, lr: 0.012500, epoch: 9, step: 55800, eta: 3.93 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.37918 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 55900, eta: 3.93 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11824 sec, avg_samples: 32.00000, ips: 541.27207 images/sec
Training: 2023-09-28 07:09:00,962 - loss nan, lr: 0.012500, epoch: 9, step: 55900, eta: 3.93 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11824 sec, avg_samples: 32.00000, ips: 541.27207 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 56000, eta: 3.92 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11833 sec, avg_samples: 32.00000, ips: 540.88205 images/sec
Training: 2023-09-28 07:09:12,800 - loss nan, lr: 0.012500, epoch: 9, step: 56000, eta: 3.92 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11833 sec, avg_samples: 32.00000, ips: 540.88205 images/sec
INFO:root:[lfw][56000]XNorm: 0.706647
Training: 2023-09-28 07:09:40,704 - [lfw][56000]XNorm: 0.706647
INFO:root:[lfw][56000]Accuracy-Flip: 0.89567+-0.01595
Training: 2023-09-28 07:09:40,704 - [lfw][56000]Accuracy-Flip: 0.89567+-0.01595
INFO:root:[lfw][56000]Accuracy-Highest: 0.90150
Training: 2023-09-28 07:09:40,704 - [lfw][56000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9041
Training: 2023-09-28 07:09:40,704 - test time: 27.9041
INFO:root:[cfp_fp][56000]XNorm: 0.662699
Training: 2023-09-28 07:10:12,933 - [cfp_fp][56000]XNorm: 0.662699
INFO:root:[cfp_fp][56000]Accuracy-Flip: 0.63929+-0.02008
Training: 2023-09-28 07:10:12,933 - [cfp_fp][56000]Accuracy-Flip: 0.63929+-0.02008
INFO:root:[cfp_fp][56000]Accuracy-Highest: 0.65071
Training: 2023-09-28 07:10:12,933 - [cfp_fp][56000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2288
Training: 2023-09-28 07:10:12,933 - test time: 32.2288
INFO:root:[agedb_30][56000]XNorm: 0.671517
Training: 2023-09-28 07:10:40,673 - [agedb_30][56000]XNorm: 0.671517
INFO:root:[agedb_30][56000]Accuracy-Flip: 0.68700+-0.02252
Training: 2023-09-28 07:10:40,673 - [agedb_30][56000]Accuracy-Flip: 0.68700+-0.02252
INFO:root:[agedb_30][56000]Accuracy-Highest: 0.70083
Training: 2023-09-28 07:10:40,673 - [agedb_30][56000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7397
Training: 2023-09-28 07:10:40,673 - test time: 27.7397
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 56100, eta: 3.95 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11788 sec, avg_samples: 32.00000, ips: 542.91695 images/sec
Training: 2023-09-28 07:10:52,466 - loss nan, lr: 0.012500, epoch: 9, step: 56100, eta: 3.95 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11788 sec, avg_samples: 32.00000, ips: 542.91695 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 56200, eta: 3.95 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11798 sec, avg_samples: 32.00000, ips: 542.47673 images/sec
Training: 2023-09-28 07:11:04,268 - loss nan, lr: 0.012500, epoch: 9, step: 56200, eta: 3.95 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11798 sec, avg_samples: 32.00000, ips: 542.47673 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 56300, eta: 3.94 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11796 sec, avg_samples: 32.00000, ips: 542.53585 images/sec
Training: 2023-09-28 07:11:16,069 - loss nan, lr: 0.012500, epoch: 9, step: 56300, eta: 3.94 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11796 sec, avg_samples: 32.00000, ips: 542.53585 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 56400, eta: 3.93 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11798 sec, avg_samples: 32.00000, ips: 542.47054 images/sec
Training: 2023-09-28 07:11:27,872 - loss nan, lr: 0.012500, epoch: 9, step: 56400, eta: 3.93 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11798 sec, avg_samples: 32.00000, ips: 542.47054 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 56500, eta: 3.93 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11797 sec, avg_samples: 32.00000, ips: 542.51101 images/sec
Training: 2023-09-28 07:11:39,673 - loss nan, lr: 0.012500, epoch: 9, step: 56500, eta: 3.93 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11797 sec, avg_samples: 32.00000, ips: 542.51101 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 56600, eta: 3.92 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11800 sec, avg_samples: 32.00000, ips: 542.36785 images/sec
Training: 2023-09-28 07:11:51,478 - loss nan, lr: 0.012500, epoch: 9, step: 56600, eta: 3.92 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11800 sec, avg_samples: 32.00000, ips: 542.36785 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 56700, eta: 3.91 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.77419 images/sec
Training: 2023-09-28 07:12:03,295 - loss nan, lr: 0.012500, epoch: 9, step: 56700, eta: 3.91 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.77419 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 56800, eta: 3.91 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.58114 images/sec
Training: 2023-09-28 07:12:15,117 - loss nan, lr: 0.012500, epoch: 9, step: 56800, eta: 3.91 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.58114 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 56900, eta: 3.90 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.14813 images/sec
Training: 2023-09-28 07:12:26,948 - loss nan, lr: 0.012500, epoch: 9, step: 56900, eta: 3.90 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.14813 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 57000, eta: 3.89 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.02971 images/sec
Training: 2023-09-28 07:12:38,782 - loss nan, lr: 0.012500, epoch: 9, step: 57000, eta: 3.89 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.02971 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 57100, eta: 3.89 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.41408 images/sec
Training: 2023-09-28 07:12:50,608 - loss nan, lr: 0.012500, epoch: 9, step: 57100, eta: 3.89 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.41408 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 57200, eta: 3.88 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.29959 images/sec
Training: 2023-09-28 07:13:02,436 - loss nan, lr: 0.012500, epoch: 9, step: 57200, eta: 3.88 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.29959 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 57300, eta: 3.88 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.54645 images/sec
Training: 2023-09-28 07:13:14,258 - loss nan, lr: 0.012500, epoch: 9, step: 57300, eta: 3.88 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.54645 images/sec
INFO:root:loss nan, lr: 0.012500, epoch: 9, step: 57400, eta: 3.87 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.38157 images/sec
Training: 2023-09-28 07:13:26,084 - loss nan, lr: 0.012500, epoch: 9, step: 57400, eta: 3.87 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.38157 images/sec
INFO:root:Save model to MS1M_v3_arcface_static_0.1/FresResNet50/9.
Training: 2023-09-28 07:13:35,890 - Save model to MS1M_v3_arcface_static_0.1/FresResNet50/9.
INFO:root:Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/8.
Training: 2023-09-28 07:13:35,890 - Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/8.
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 57500, eta: 3.86 hours, avg_reader_cost: 0.00175 sec, avg_batch_cost: 0.02539 sec, avg_samples: 6.40000, ips: 504.18325 images/sec
Training: 2023-09-28 07:13:38,461 - loss nan, lr: 0.001250, epoch: 10, step: 57500, eta: 3.86 hours, avg_reader_cost: 0.00175 sec, avg_batch_cost: 0.02539 sec, avg_samples: 6.40000, ips: 504.18325 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 57600, eta: 3.86 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11844 sec, avg_samples: 32.00000, ips: 540.35273 images/sec
Training: 2023-09-28 07:13:50,311 - loss nan, lr: 0.001250, epoch: 10, step: 57600, eta: 3.86 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11844 sec, avg_samples: 32.00000, ips: 540.35273 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 57700, eta: 3.85 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11846 sec, avg_samples: 32.00000, ips: 540.26477 images/sec
Training: 2023-09-28 07:14:02,162 - loss nan, lr: 0.001250, epoch: 10, step: 57700, eta: 3.85 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11846 sec, avg_samples: 32.00000, ips: 540.26477 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 57800, eta: 3.84 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11847 sec, avg_samples: 32.00000, ips: 540.21090 images/sec
Training: 2023-09-28 07:14:14,014 - loss nan, lr: 0.001250, epoch: 10, step: 57800, eta: 3.84 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11847 sec, avg_samples: 32.00000, ips: 540.21090 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 57900, eta: 3.84 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11858 sec, avg_samples: 32.00000, ips: 539.74128 images/sec
Training: 2023-09-28 07:14:25,876 - loss nan, lr: 0.001250, epoch: 10, step: 57900, eta: 3.84 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11858 sec, avg_samples: 32.00000, ips: 539.74128 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 58000, eta: 3.83 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11836 sec, avg_samples: 32.00000, ips: 540.71704 images/sec
Training: 2023-09-28 07:14:37,717 - loss nan, lr: 0.001250, epoch: 10, step: 58000, eta: 3.83 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11836 sec, avg_samples: 32.00000, ips: 540.71704 images/sec
INFO:root:[lfw][58000]XNorm: 0.645923
Training: 2023-09-28 07:15:05,700 - [lfw][58000]XNorm: 0.645923
INFO:root:[lfw][58000]Accuracy-Flip: 0.89983+-0.01864
Training: 2023-09-28 07:15:05,700 - [lfw][58000]Accuracy-Flip: 0.89983+-0.01864
INFO:root:[lfw][58000]Accuracy-Highest: 0.90150
Training: 2023-09-28 07:15:05,700 - [lfw][58000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9831
Training: 2023-09-28 07:15:05,700 - test time: 27.9831
INFO:root:[cfp_fp][58000]XNorm: 0.602874
Training: 2023-09-28 07:15:37,943 - [cfp_fp][58000]XNorm: 0.602874
INFO:root:[cfp_fp][58000]Accuracy-Flip: 0.64014+-0.01896
Training: 2023-09-28 07:15:37,943 - [cfp_fp][58000]Accuracy-Flip: 0.64014+-0.01896
INFO:root:[cfp_fp][58000]Accuracy-Highest: 0.65071
Training: 2023-09-28 07:15:37,943 - [cfp_fp][58000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2432
Training: 2023-09-28 07:15:37,943 - test time: 32.2432
INFO:root:[agedb_30][58000]XNorm: 0.611999
Training: 2023-09-28 07:16:05,682 - [agedb_30][58000]XNorm: 0.611999
INFO:root:[agedb_30][58000]Accuracy-Flip: 0.69217+-0.02106
Training: 2023-09-28 07:16:05,682 - [agedb_30][58000]Accuracy-Flip: 0.69217+-0.02106
INFO:root:[agedb_30][58000]Accuracy-Highest: 0.70083
Training: 2023-09-28 07:16:05,682 - [agedb_30][58000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7390
Training: 2023-09-28 07:16:05,682 - test time: 27.7390
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 58100, eta: 3.86 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11797 sec, avg_samples: 32.00000, ips: 542.50808 images/sec
Training: 2023-09-28 07:16:17,485 - loss nan, lr: 0.001250, epoch: 10, step: 58100, eta: 3.86 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11797 sec, avg_samples: 32.00000, ips: 542.50808 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 58200, eta: 3.86 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11811 sec, avg_samples: 32.00000, ips: 541.88531 images/sec
Training: 2023-09-28 07:16:29,301 - loss nan, lr: 0.001250, epoch: 10, step: 58200, eta: 3.86 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11811 sec, avg_samples: 32.00000, ips: 541.88531 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 58300, eta: 3.85 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.06473 images/sec
Training: 2023-09-28 07:16:41,134 - loss nan, lr: 0.001250, epoch: 10, step: 58300, eta: 3.85 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.06473 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 58400, eta: 3.84 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11854 sec, avg_samples: 32.00000, ips: 539.92177 images/sec
Training: 2023-09-28 07:16:52,994 - loss nan, lr: 0.001250, epoch: 10, step: 58400, eta: 3.84 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11854 sec, avg_samples: 32.00000, ips: 539.92177 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 58500, eta: 3.84 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11866 sec, avg_samples: 32.00000, ips: 539.37105 images/sec
Training: 2023-09-28 07:17:04,866 - loss nan, lr: 0.001250, epoch: 10, step: 58500, eta: 3.84 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11866 sec, avg_samples: 32.00000, ips: 539.37105 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 58600, eta: 3.83 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11881 sec, avg_samples: 32.00000, ips: 538.67587 images/sec
Training: 2023-09-28 07:17:16,753 - loss nan, lr: 0.001250, epoch: 10, step: 58600, eta: 3.83 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11881 sec, avg_samples: 32.00000, ips: 538.67587 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 58700, eta: 3.82 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11831 sec, avg_samples: 32.00000, ips: 540.96839 images/sec
Training: 2023-09-28 07:17:28,589 - loss nan, lr: 0.001250, epoch: 10, step: 58700, eta: 3.82 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11831 sec, avg_samples: 32.00000, ips: 540.96839 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 58800, eta: 3.82 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.12237 images/sec
Training: 2023-09-28 07:17:40,421 - loss nan, lr: 0.001250, epoch: 10, step: 58800, eta: 3.82 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.12237 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 58900, eta: 3.81 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.07092 images/sec
Training: 2023-09-28 07:17:52,254 - loss nan, lr: 0.001250, epoch: 10, step: 58900, eta: 3.81 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.07092 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 59000, eta: 3.81 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11855 sec, avg_samples: 32.00000, ips: 539.83401 images/sec
Training: 2023-09-28 07:18:04,115 - loss nan, lr: 0.001250, epoch: 10, step: 59000, eta: 3.81 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11855 sec, avg_samples: 32.00000, ips: 539.83401 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 59100, eta: 3.80 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11875 sec, avg_samples: 32.00000, ips: 538.94753 images/sec
Training: 2023-09-28 07:18:15,995 - loss nan, lr: 0.001250, epoch: 10, step: 59100, eta: 3.80 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11875 sec, avg_samples: 32.00000, ips: 538.94753 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 59200, eta: 3.79 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11877 sec, avg_samples: 32.00000, ips: 538.87929 images/sec
Training: 2023-09-28 07:18:27,876 - loss nan, lr: 0.001250, epoch: 10, step: 59200, eta: 3.79 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11877 sec, avg_samples: 32.00000, ips: 538.87929 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 59300, eta: 3.79 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11842 sec, avg_samples: 32.00000, ips: 540.46067 images/sec
Training: 2023-09-28 07:18:39,723 - loss nan, lr: 0.001250, epoch: 10, step: 59300, eta: 3.79 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11842 sec, avg_samples: 32.00000, ips: 540.46067 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 59400, eta: 3.78 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.30152 images/sec
Training: 2023-09-28 07:18:51,551 - loss nan, lr: 0.001250, epoch: 10, step: 59400, eta: 3.78 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.30152 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 59500, eta: 3.77 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.10247 images/sec
Training: 2023-09-28 07:19:03,383 - loss nan, lr: 0.001250, epoch: 10, step: 59500, eta: 3.77 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.10247 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 59600, eta: 3.77 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11847 sec, avg_samples: 32.00000, ips: 540.22457 images/sec
Training: 2023-09-28 07:19:15,235 - loss nan, lr: 0.001250, epoch: 10, step: 59600, eta: 3.77 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11847 sec, avg_samples: 32.00000, ips: 540.22457 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 59700, eta: 3.76 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11886 sec, avg_samples: 32.00000, ips: 538.45752 images/sec
Training: 2023-09-28 07:19:27,126 - loss nan, lr: 0.001250, epoch: 10, step: 59700, eta: 3.76 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11886 sec, avg_samples: 32.00000, ips: 538.45752 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 59800, eta: 3.76 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11840 sec, avg_samples: 32.00000, ips: 540.52715 images/sec
Training: 2023-09-28 07:19:38,971 - loss nan, lr: 0.001250, epoch: 10, step: 59800, eta: 3.76 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11840 sec, avg_samples: 32.00000, ips: 540.52715 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 59900, eta: 3.75 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.17915 images/sec
Training: 2023-09-28 07:19:50,802 - loss nan, lr: 0.001250, epoch: 10, step: 59900, eta: 3.75 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.17915 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 60000, eta: 3.74 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.43339 images/sec
Training: 2023-09-28 07:20:02,628 - loss nan, lr: 0.001250, epoch: 10, step: 60000, eta: 3.74 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.43339 images/sec
INFO:root:[lfw][60000]XNorm: 0.638258
Training: 2023-09-28 07:20:30,590 - [lfw][60000]XNorm: 0.638258
INFO:root:[lfw][60000]Accuracy-Flip: 0.89967+-0.01922
Training: 2023-09-28 07:20:30,590 - [lfw][60000]Accuracy-Flip: 0.89967+-0.01922
INFO:root:[lfw][60000]Accuracy-Highest: 0.90150
Training: 2023-09-28 07:20:30,590 - [lfw][60000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9625
Training: 2023-09-28 07:20:30,590 - test time: 27.9625
INFO:root:[cfp_fp][60000]XNorm: 0.595535
Training: 2023-09-28 07:21:02,854 - [cfp_fp][60000]XNorm: 0.595535
INFO:root:[cfp_fp][60000]Accuracy-Flip: 0.63929+-0.01802
Training: 2023-09-28 07:21:02,854 - [cfp_fp][60000]Accuracy-Flip: 0.63929+-0.01802
INFO:root:[cfp_fp][60000]Accuracy-Highest: 0.65071
Training: 2023-09-28 07:21:02,854 - [cfp_fp][60000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2636
Training: 2023-09-28 07:21:02,854 - test time: 32.2636
INFO:root:[agedb_30][60000]XNorm: 0.605159
Training: 2023-09-28 07:21:30,624 - [agedb_30][60000]XNorm: 0.605159
INFO:root:[agedb_30][60000]Accuracy-Flip: 0.68717+-0.02195
Training: 2023-09-28 07:21:30,624 - [agedb_30][60000]Accuracy-Flip: 0.68717+-0.02195
INFO:root:[agedb_30][60000]Accuracy-Highest: 0.70083
Training: 2023-09-28 07:21:30,624 - [agedb_30][60000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7699
Training: 2023-09-28 07:21:30,624 - test time: 27.7699
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 60100, eta: 3.77 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11776 sec, avg_samples: 32.00000, ips: 543.50107 images/sec
Training: 2023-09-28 07:21:42,404 - loss nan, lr: 0.001250, epoch: 10, step: 60100, eta: 3.77 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11776 sec, avg_samples: 32.00000, ips: 543.50107 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 60200, eta: 3.77 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11789 sec, avg_samples: 32.00000, ips: 542.89078 images/sec
Training: 2023-09-28 07:21:54,198 - loss nan, lr: 0.001250, epoch: 10, step: 60200, eta: 3.77 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11789 sec, avg_samples: 32.00000, ips: 542.89078 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 60300, eta: 3.76 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.39819 images/sec
Training: 2023-09-28 07:22:06,025 - loss nan, lr: 0.001250, epoch: 10, step: 60300, eta: 3.76 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.39819 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 60400, eta: 3.75 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.65288 images/sec
Training: 2023-09-28 07:22:17,847 - loss nan, lr: 0.001250, epoch: 10, step: 60400, eta: 3.75 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.65288 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 60500, eta: 3.75 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.20328 images/sec
Training: 2023-09-28 07:22:29,678 - loss nan, lr: 0.001250, epoch: 10, step: 60500, eta: 3.75 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.20328 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 60600, eta: 3.74 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11831 sec, avg_samples: 32.00000, ips: 540.97334 images/sec
Training: 2023-09-28 07:22:41,515 - loss nan, lr: 0.001250, epoch: 10, step: 60600, eta: 3.74 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11831 sec, avg_samples: 32.00000, ips: 540.97334 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 60700, eta: 3.73 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.77027 images/sec
Training: 2023-09-28 07:22:53,332 - loss nan, lr: 0.001250, epoch: 10, step: 60700, eta: 3.73 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.77027 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 60800, eta: 3.73 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.69864 images/sec
Training: 2023-09-28 07:23:05,151 - loss nan, lr: 0.001250, epoch: 10, step: 60800, eta: 3.73 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.69864 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 60900, eta: 3.72 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.82144 images/sec
Training: 2023-09-28 07:23:16,968 - loss nan, lr: 0.001250, epoch: 10, step: 60900, eta: 3.72 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.82144 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 61000, eta: 3.72 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.79672 images/sec
Training: 2023-09-28 07:23:28,785 - loss nan, lr: 0.001250, epoch: 10, step: 61000, eta: 3.72 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.79672 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 61100, eta: 3.71 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.77575 images/sec
Training: 2023-09-28 07:23:40,602 - loss nan, lr: 0.001250, epoch: 10, step: 61100, eta: 3.71 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.77575 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 61200, eta: 3.70 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.69460 images/sec
Training: 2023-09-28 07:23:52,445 - loss nan, lr: 0.001250, epoch: 10, step: 61200, eta: 3.70 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.69460 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 61300, eta: 3.70 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.75728 images/sec
Training: 2023-09-28 07:24:04,285 - loss nan, lr: 0.001250, epoch: 10, step: 61300, eta: 3.70 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.75728 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 61400, eta: 3.69 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.54630 images/sec
Training: 2023-09-28 07:24:16,108 - loss nan, lr: 0.001250, epoch: 10, step: 61400, eta: 3.69 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.54630 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 61500, eta: 3.69 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.56262 images/sec
Training: 2023-09-28 07:24:27,930 - loss nan, lr: 0.001250, epoch: 10, step: 61500, eta: 3.69 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.56262 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 61600, eta: 3.68 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.58233 images/sec
Training: 2023-09-28 07:24:39,752 - loss nan, lr: 0.001250, epoch: 10, step: 61600, eta: 3.68 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.58233 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 61700, eta: 3.67 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.59446 images/sec
Training: 2023-09-28 07:24:51,573 - loss nan, lr: 0.001250, epoch: 10, step: 61700, eta: 3.67 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.59446 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 61800, eta: 3.67 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.63342 images/sec
Training: 2023-09-28 07:25:03,394 - loss nan, lr: 0.001250, epoch: 10, step: 61800, eta: 3.67 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.63342 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 61900, eta: 3.66 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.53046 images/sec
Training: 2023-09-28 07:25:15,217 - loss nan, lr: 0.001250, epoch: 10, step: 61900, eta: 3.66 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.53046 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 62000, eta: 3.66 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.56274 images/sec
Training: 2023-09-28 07:25:27,039 - loss nan, lr: 0.001250, epoch: 10, step: 62000, eta: 3.66 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.56274 images/sec
INFO:root:[lfw][62000]XNorm: 0.627972
Training: 2023-09-28 07:25:54,966 - [lfw][62000]XNorm: 0.627972
INFO:root:[lfw][62000]Accuracy-Flip: 0.89400+-0.01528
Training: 2023-09-28 07:25:54,966 - [lfw][62000]Accuracy-Flip: 0.89400+-0.01528
INFO:root:[lfw][62000]Accuracy-Highest: 0.90150
Training: 2023-09-28 07:25:54,967 - [lfw][62000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9277
Training: 2023-09-28 07:25:54,967 - test time: 27.9277
INFO:root:[cfp_fp][62000]XNorm: 0.587290
Training: 2023-09-28 07:26:27,210 - [cfp_fp][62000]XNorm: 0.587290
INFO:root:[cfp_fp][62000]Accuracy-Flip: 0.64429+-0.02042
Training: 2023-09-28 07:26:27,210 - [cfp_fp][62000]Accuracy-Flip: 0.64429+-0.02042
INFO:root:[cfp_fp][62000]Accuracy-Highest: 0.65071
Training: 2023-09-28 07:26:27,210 - [cfp_fp][62000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2437
Training: 2023-09-28 07:26:27,210 - test time: 32.2437
INFO:root:[agedb_30][62000]XNorm: 0.596931
Training: 2023-09-28 07:26:54,943 - [agedb_30][62000]XNorm: 0.596931
INFO:root:[agedb_30][62000]Accuracy-Flip: 0.68950+-0.02246
Training: 2023-09-28 07:26:54,943 - [agedb_30][62000]Accuracy-Flip: 0.68950+-0.02246
INFO:root:[agedb_30][62000]Accuracy-Highest: 0.70083
Training: 2023-09-28 07:26:54,943 - [agedb_30][62000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7325
Training: 2023-09-28 07:26:54,943 - test time: 27.7325
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 62100, eta: 3.68 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11794 sec, avg_samples: 32.00000, ips: 542.64954 images/sec
Training: 2023-09-28 07:27:06,743 - loss nan, lr: 0.001250, epoch: 10, step: 62100, eta: 3.68 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11794 sec, avg_samples: 32.00000, ips: 542.64954 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 62200, eta: 3.68 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.29469 images/sec
Training: 2023-09-28 07:27:18,551 - loss nan, lr: 0.001250, epoch: 10, step: 62200, eta: 3.68 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.29469 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 62300, eta: 3.67 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11805 sec, avg_samples: 32.00000, ips: 542.15431 images/sec
Training: 2023-09-28 07:27:30,363 - loss nan, lr: 0.001250, epoch: 10, step: 62300, eta: 3.67 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11805 sec, avg_samples: 32.00000, ips: 542.15431 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 62400, eta: 3.66 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11809 sec, avg_samples: 32.00000, ips: 541.95240 images/sec
Training: 2023-09-28 07:27:42,178 - loss nan, lr: 0.001250, epoch: 10, step: 62400, eta: 3.66 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11809 sec, avg_samples: 32.00000, ips: 541.95240 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 62500, eta: 3.66 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.68539 images/sec
Training: 2023-09-28 07:27:54,021 - loss nan, lr: 0.001250, epoch: 10, step: 62500, eta: 3.66 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.68539 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 62600, eta: 3.65 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11836 sec, avg_samples: 32.00000, ips: 540.71921 images/sec
Training: 2023-09-28 07:28:05,863 - loss nan, lr: 0.001250, epoch: 10, step: 62600, eta: 3.65 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11836 sec, avg_samples: 32.00000, ips: 540.71921 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 62700, eta: 3.64 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.65610 images/sec
Training: 2023-09-28 07:28:17,707 - loss nan, lr: 0.001250, epoch: 10, step: 62700, eta: 3.64 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.65610 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 62800, eta: 3.64 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11838 sec, avg_samples: 32.00000, ips: 540.63452 images/sec
Training: 2023-09-28 07:28:29,551 - loss nan, lr: 0.001250, epoch: 10, step: 62800, eta: 3.64 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11838 sec, avg_samples: 32.00000, ips: 540.63452 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 62900, eta: 3.63 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11840 sec, avg_samples: 32.00000, ips: 540.53186 images/sec
Training: 2023-09-28 07:28:41,398 - loss nan, lr: 0.001250, epoch: 10, step: 62900, eta: 3.63 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11840 sec, avg_samples: 32.00000, ips: 540.53186 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 63000, eta: 3.63 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11838 sec, avg_samples: 32.00000, ips: 540.63306 images/sec
Training: 2023-09-28 07:28:53,242 - loss nan, lr: 0.001250, epoch: 10, step: 63000, eta: 3.63 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11838 sec, avg_samples: 32.00000, ips: 540.63306 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 63100, eta: 3.62 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11836 sec, avg_samples: 32.00000, ips: 540.74341 images/sec
Training: 2023-09-28 07:29:05,084 - loss nan, lr: 0.001250, epoch: 10, step: 63100, eta: 3.62 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11836 sec, avg_samples: 32.00000, ips: 540.74341 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 10, step: 63200, eta: 3.61 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11838 sec, avg_samples: 32.00000, ips: 540.62765 images/sec
Training: 2023-09-28 07:29:16,928 - loss nan, lr: 0.001250, epoch: 10, step: 63200, eta: 3.61 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11838 sec, avg_samples: 32.00000, ips: 540.62765 images/sec
INFO:root:Save model to MS1M_v3_arcface_static_0.1/FresResNet50/10.
Training: 2023-09-28 07:29:20,594 - Save model to MS1M_v3_arcface_static_0.1/FresResNet50/10.
INFO:root:Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/9.
Training: 2023-09-28 07:29:20,595 - Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/9.
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 63300, eta: 3.61 hours, avg_reader_cost: 0.00161 sec, avg_batch_cost: 0.08678 sec, avg_samples: 23.04000, ips: 530.98315 images/sec
Training: 2023-09-28 07:29:29,308 - loss nan, lr: 0.001250, epoch: 11, step: 63300, eta: 3.61 hours, avg_reader_cost: 0.00161 sec, avg_batch_cost: 0.08678 sec, avg_samples: 23.04000, ips: 530.98315 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 63400, eta: 3.60 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.11563 images/sec
Training: 2023-09-28 07:29:41,141 - loss nan, lr: 0.001250, epoch: 11, step: 63400, eta: 3.60 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.11563 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 63500, eta: 3.60 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.20174 images/sec
Training: 2023-09-28 07:29:52,972 - loss nan, lr: 0.001250, epoch: 11, step: 63500, eta: 3.60 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.20174 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 63600, eta: 3.59 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11825 sec, avg_samples: 32.00000, ips: 541.21674 images/sec
Training: 2023-09-28 07:30:04,802 - loss nan, lr: 0.001250, epoch: 11, step: 63600, eta: 3.59 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11825 sec, avg_samples: 32.00000, ips: 541.21674 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 63700, eta: 3.58 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.18859 images/sec
Training: 2023-09-28 07:30:16,634 - loss nan, lr: 0.001250, epoch: 11, step: 63700, eta: 3.58 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.18859 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 63800, eta: 3.58 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.05432 images/sec
Training: 2023-09-28 07:30:28,468 - loss nan, lr: 0.001250, epoch: 11, step: 63800, eta: 3.58 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.05432 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 63900, eta: 3.57 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.02294 images/sec
Training: 2023-09-28 07:30:40,303 - loss nan, lr: 0.001250, epoch: 11, step: 63900, eta: 3.57 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.02294 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 64000, eta: 3.57 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11833 sec, avg_samples: 32.00000, ips: 540.86787 images/sec
Training: 2023-09-28 07:30:52,141 - loss nan, lr: 0.001250, epoch: 11, step: 64000, eta: 3.57 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11833 sec, avg_samples: 32.00000, ips: 540.86787 images/sec
INFO:root:[lfw][64000]XNorm: 0.619987
Training: 2023-09-28 07:31:20,071 - [lfw][64000]XNorm: 0.619987
INFO:root:[lfw][64000]Accuracy-Flip: 0.89850+-0.01497
Training: 2023-09-28 07:31:20,072 - [lfw][64000]Accuracy-Flip: 0.89850+-0.01497
INFO:root:[lfw][64000]Accuracy-Highest: 0.90150
Training: 2023-09-28 07:31:20,072 - [lfw][64000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9307
Training: 2023-09-28 07:31:20,072 - test time: 27.9307
INFO:root:[cfp_fp][64000]XNorm: 0.579207
Training: 2023-09-28 07:31:52,336 - [cfp_fp][64000]XNorm: 0.579207
INFO:root:[cfp_fp][64000]Accuracy-Flip: 0.64171+-0.02084
Training: 2023-09-28 07:31:52,336 - [cfp_fp][64000]Accuracy-Flip: 0.64171+-0.02084
INFO:root:[cfp_fp][64000]Accuracy-Highest: 0.65071
Training: 2023-09-28 07:31:52,336 - [cfp_fp][64000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2648
Training: 2023-09-28 07:31:52,337 - test time: 32.2648
INFO:root:[agedb_30][64000]XNorm: 0.589272
Training: 2023-09-28 07:32:20,091 - [agedb_30][64000]XNorm: 0.589272
INFO:root:[agedb_30][64000]Accuracy-Flip: 0.68617+-0.02530
Training: 2023-09-28 07:32:20,091 - [agedb_30][64000]Accuracy-Flip: 0.68617+-0.02530
INFO:root:[agedb_30][64000]Accuracy-Highest: 0.70083
Training: 2023-09-28 07:32:20,091 - [agedb_30][64000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7549
Training: 2023-09-28 07:32:20,092 - test time: 27.7549
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 64100, eta: 3.59 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11778 sec, avg_samples: 32.00000, ips: 543.38155 images/sec
Training: 2023-09-28 07:32:31,875 - loss nan, lr: 0.001250, epoch: 11, step: 64100, eta: 3.59 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11778 sec, avg_samples: 32.00000, ips: 543.38155 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 64200, eta: 3.59 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11783 sec, avg_samples: 32.00000, ips: 543.14559 images/sec
Training: 2023-09-28 07:32:43,663 - loss nan, lr: 0.001250, epoch: 11, step: 64200, eta: 3.59 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11783 sec, avg_samples: 32.00000, ips: 543.14559 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 64300, eta: 3.58 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11786 sec, avg_samples: 32.00000, ips: 543.02309 images/sec
Training: 2023-09-28 07:32:55,455 - loss nan, lr: 0.001250, epoch: 11, step: 64300, eta: 3.58 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11786 sec, avg_samples: 32.00000, ips: 543.02309 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 64400, eta: 3.57 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11798 sec, avg_samples: 32.00000, ips: 542.44570 images/sec
Training: 2023-09-28 07:33:07,258 - loss nan, lr: 0.001250, epoch: 11, step: 64400, eta: 3.57 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11798 sec, avg_samples: 32.00000, ips: 542.44570 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 64500, eta: 3.57 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.61991 images/sec
Training: 2023-09-28 07:33:19,080 - loss nan, lr: 0.001250, epoch: 11, step: 64500, eta: 3.57 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.61991 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 64600, eta: 3.56 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.66780 images/sec
Training: 2023-09-28 07:33:30,901 - loss nan, lr: 0.001250, epoch: 11, step: 64600, eta: 3.56 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.66780 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 64700, eta: 3.56 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.49229 images/sec
Training: 2023-09-28 07:33:42,725 - loss nan, lr: 0.001250, epoch: 11, step: 64700, eta: 3.56 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.49229 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 64800, eta: 3.55 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.54866 images/sec
Training: 2023-09-28 07:33:54,549 - loss nan, lr: 0.001250, epoch: 11, step: 64800, eta: 3.55 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.54866 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 64900, eta: 3.54 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.64926 images/sec
Training: 2023-09-28 07:34:06,370 - loss nan, lr: 0.001250, epoch: 11, step: 64900, eta: 3.54 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.64926 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 65000, eta: 3.54 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.40530 images/sec
Training: 2023-09-28 07:34:18,196 - loss nan, lr: 0.001250, epoch: 11, step: 65000, eta: 3.54 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.40530 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 65100, eta: 3.53 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.49846 images/sec
Training: 2023-09-28 07:34:30,021 - loss nan, lr: 0.001250, epoch: 11, step: 65100, eta: 3.53 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.49846 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 65200, eta: 3.53 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.45407 images/sec
Training: 2023-09-28 07:34:41,846 - loss nan, lr: 0.001250, epoch: 11, step: 65200, eta: 3.53 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.45407 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 65300, eta: 3.52 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.46683 images/sec
Training: 2023-09-28 07:34:53,671 - loss nan, lr: 0.001250, epoch: 11, step: 65300, eta: 3.52 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.46683 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 65400, eta: 3.51 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.45073 images/sec
Training: 2023-09-28 07:35:05,497 - loss nan, lr: 0.001250, epoch: 11, step: 65400, eta: 3.51 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.45073 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 65500, eta: 3.51 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.59437 images/sec
Training: 2023-09-28 07:35:17,319 - loss nan, lr: 0.001250, epoch: 11, step: 65500, eta: 3.51 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.59437 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 65600, eta: 3.50 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.50650 images/sec
Training: 2023-09-28 07:35:29,143 - loss nan, lr: 0.001250, epoch: 11, step: 65600, eta: 3.50 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.50650 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 65700, eta: 3.50 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.60058 images/sec
Training: 2023-09-28 07:35:40,965 - loss nan, lr: 0.001250, epoch: 11, step: 65700, eta: 3.50 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.60058 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 65800, eta: 3.49 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.56034 images/sec
Training: 2023-09-28 07:35:52,788 - loss nan, lr: 0.001250, epoch: 11, step: 65800, eta: 3.49 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.56034 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 65900, eta: 3.48 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.54428 images/sec
Training: 2023-09-28 07:36:04,612 - loss nan, lr: 0.001250, epoch: 11, step: 65900, eta: 3.48 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.54428 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 66000, eta: 3.48 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.54356 images/sec
Training: 2023-09-28 07:36:16,435 - loss nan, lr: 0.001250, epoch: 11, step: 66000, eta: 3.48 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.54356 images/sec
INFO:root:[lfw][66000]XNorm: 0.613180
Training: 2023-09-28 07:36:44,346 - [lfw][66000]XNorm: 0.613180
INFO:root:[lfw][66000]Accuracy-Flip: 0.89783+-0.01714
Training: 2023-09-28 07:36:44,346 - [lfw][66000]Accuracy-Flip: 0.89783+-0.01714
INFO:root:[lfw][66000]Accuracy-Highest: 0.90150
Training: 2023-09-28 07:36:44,346 - [lfw][66000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9111
Training: 2023-09-28 07:36:44,346 - test time: 27.9111
INFO:root:[cfp_fp][66000]XNorm: 0.573391
Training: 2023-09-28 07:37:16,570 - [cfp_fp][66000]XNorm: 0.573391
INFO:root:[cfp_fp][66000]Accuracy-Flip: 0.64186+-0.02029
Training: 2023-09-28 07:37:16,571 - [cfp_fp][66000]Accuracy-Flip: 0.64186+-0.02029
INFO:root:[cfp_fp][66000]Accuracy-Highest: 0.65071
Training: 2023-09-28 07:37:16,571 - [cfp_fp][66000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2244
Training: 2023-09-28 07:37:16,571 - test time: 32.2244
INFO:root:[agedb_30][66000]XNorm: 0.581867
Training: 2023-09-28 07:37:44,297 - [agedb_30][66000]XNorm: 0.581867
INFO:root:[agedb_30][66000]Accuracy-Flip: 0.68767+-0.02087
Training: 2023-09-28 07:37:44,298 - [agedb_30][66000]Accuracy-Flip: 0.68767+-0.02087
INFO:root:[agedb_30][66000]Accuracy-Highest: 0.70083
Training: 2023-09-28 07:37:44,298 - [agedb_30][66000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7269
Training: 2023-09-28 07:37:44,298 - test time: 27.7269
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 66100, eta: 3.50 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11777 sec, avg_samples: 32.00000, ips: 543.42553 images/sec
Training: 2023-09-28 07:37:56,080 - loss nan, lr: 0.001250, epoch: 11, step: 66100, eta: 3.50 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11777 sec, avg_samples: 32.00000, ips: 543.42553 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 66200, eta: 3.49 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11794 sec, avg_samples: 32.00000, ips: 542.65896 images/sec
Training: 2023-09-28 07:38:07,879 - loss nan, lr: 0.001250, epoch: 11, step: 66200, eta: 3.49 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11794 sec, avg_samples: 32.00000, ips: 542.65896 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 66300, eta: 3.49 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11792 sec, avg_samples: 32.00000, ips: 542.72511 images/sec
Training: 2023-09-28 07:38:19,677 - loss nan, lr: 0.001250, epoch: 11, step: 66300, eta: 3.49 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11792 sec, avg_samples: 32.00000, ips: 542.72511 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 66400, eta: 3.48 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11794 sec, avg_samples: 32.00000, ips: 542.64048 images/sec
Training: 2023-09-28 07:38:31,476 - loss nan, lr: 0.001250, epoch: 11, step: 66400, eta: 3.48 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11794 sec, avg_samples: 32.00000, ips: 542.64048 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 66500, eta: 3.48 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.81291 images/sec
Training: 2023-09-28 07:38:43,294 - loss nan, lr: 0.001250, epoch: 11, step: 66500, eta: 3.48 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.81291 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 66600, eta: 3.47 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.74407 images/sec
Training: 2023-09-28 07:38:55,113 - loss nan, lr: 0.001250, epoch: 11, step: 66600, eta: 3.47 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.74407 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 66700, eta: 3.47 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.68066 images/sec
Training: 2023-09-28 07:39:06,933 - loss nan, lr: 0.001250, epoch: 11, step: 66700, eta: 3.47 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.68066 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 66800, eta: 3.46 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.62545 images/sec
Training: 2023-09-28 07:39:18,755 - loss nan, lr: 0.001250, epoch: 11, step: 66800, eta: 3.46 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.62545 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 66900, eta: 3.45 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.62928 images/sec
Training: 2023-09-28 07:39:30,576 - loss nan, lr: 0.001250, epoch: 11, step: 66900, eta: 3.45 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.62928 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 67000, eta: 3.45 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.71593 images/sec
Training: 2023-09-28 07:39:42,396 - loss nan, lr: 0.001250, epoch: 11, step: 67000, eta: 3.45 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.71593 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 67100, eta: 3.44 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.69283 images/sec
Training: 2023-09-28 07:39:54,216 - loss nan, lr: 0.001250, epoch: 11, step: 67100, eta: 3.44 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.69283 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 67200, eta: 3.44 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.58384 images/sec
Training: 2023-09-28 07:40:06,038 - loss nan, lr: 0.001250, epoch: 11, step: 67200, eta: 3.44 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.58384 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 67300, eta: 3.43 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.59499 images/sec
Training: 2023-09-28 07:40:17,861 - loss nan, lr: 0.001250, epoch: 11, step: 67300, eta: 3.43 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.59499 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 67400, eta: 3.42 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.58535 images/sec
Training: 2023-09-28 07:40:29,683 - loss nan, lr: 0.001250, epoch: 11, step: 67400, eta: 3.42 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.58535 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 67500, eta: 3.42 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.20063 images/sec
Training: 2023-09-28 07:40:41,514 - loss nan, lr: 0.001250, epoch: 11, step: 67500, eta: 3.42 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.20063 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 67600, eta: 3.41 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.12168 images/sec
Training: 2023-09-28 07:40:53,346 - loss nan, lr: 0.001250, epoch: 11, step: 67600, eta: 3.41 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.12168 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 67700, eta: 3.41 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11825 sec, avg_samples: 32.00000, ips: 541.22108 images/sec
Training: 2023-09-28 07:41:05,177 - loss nan, lr: 0.001250, epoch: 11, step: 67700, eta: 3.41 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11825 sec, avg_samples: 32.00000, ips: 541.22108 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 67800, eta: 3.40 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.06306 images/sec
Training: 2023-09-28 07:41:17,010 - loss nan, lr: 0.001250, epoch: 11, step: 67800, eta: 3.40 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.06306 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 67900, eta: 3.39 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.08208 images/sec
Training: 2023-09-28 07:41:28,844 - loss nan, lr: 0.001250, epoch: 11, step: 67900, eta: 3.39 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.08208 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 68000, eta: 3.39 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.12591 images/sec
Training: 2023-09-28 07:41:40,676 - loss nan, lr: 0.001250, epoch: 11, step: 68000, eta: 3.39 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.12591 images/sec
INFO:root:[lfw][68000]XNorm: 0.607373
Training: 2023-09-28 07:42:08,521 - [lfw][68000]XNorm: 0.607373
INFO:root:[lfw][68000]Accuracy-Flip: 0.89417+-0.01718
Training: 2023-09-28 07:42:08,521 - [lfw][68000]Accuracy-Flip: 0.89417+-0.01718
INFO:root:[lfw][68000]Accuracy-Highest: 0.90150
Training: 2023-09-28 07:42:08,521 - [lfw][68000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.8448
Training: 2023-09-28 07:42:08,521 - test time: 27.8448
INFO:root:[cfp_fp][68000]XNorm: 0.568399
Training: 2023-09-28 07:42:40,696 - [cfp_fp][68000]XNorm: 0.568399
INFO:root:[cfp_fp][68000]Accuracy-Flip: 0.63800+-0.01724
Training: 2023-09-28 07:42:40,696 - [cfp_fp][68000]Accuracy-Flip: 0.63800+-0.01724
INFO:root:[cfp_fp][68000]Accuracy-Highest: 0.65071
Training: 2023-09-28 07:42:40,696 - [cfp_fp][68000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.1753
Training: 2023-09-28 07:42:40,696 - test time: 32.1753
INFO:root:[agedb_30][68000]XNorm: 0.575985
Training: 2023-09-28 07:43:08,375 - [agedb_30][68000]XNorm: 0.575985
INFO:root:[agedb_30][68000]Accuracy-Flip: 0.68800+-0.02308
Training: 2023-09-28 07:43:08,375 - [agedb_30][68000]Accuracy-Flip: 0.68800+-0.02308
INFO:root:[agedb_30][68000]Accuracy-Highest: 0.70083
Training: 2023-09-28 07:43:08,375 - [agedb_30][68000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.6791
Training: 2023-09-28 07:43:08,376 - test time: 27.6791
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 68100, eta: 3.41 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11777 sec, avg_samples: 32.00000, ips: 543.42198 images/sec
Training: 2023-09-28 07:43:20,158 - loss nan, lr: 0.001250, epoch: 11, step: 68100, eta: 3.41 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11777 sec, avg_samples: 32.00000, ips: 543.42198 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 68200, eta: 3.40 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11795 sec, avg_samples: 32.00000, ips: 542.59972 images/sec
Training: 2023-09-28 07:43:31,958 - loss nan, lr: 0.001250, epoch: 11, step: 68200, eta: 3.40 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11795 sec, avg_samples: 32.00000, ips: 542.59972 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 68300, eta: 3.40 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11790 sec, avg_samples: 32.00000, ips: 542.83505 images/sec
Training: 2023-09-28 07:43:43,753 - loss nan, lr: 0.001250, epoch: 11, step: 68300, eta: 3.40 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11790 sec, avg_samples: 32.00000, ips: 542.83505 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 68400, eta: 3.39 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11796 sec, avg_samples: 32.00000, ips: 542.56108 images/sec
Training: 2023-09-28 07:43:55,554 - loss nan, lr: 0.001250, epoch: 11, step: 68400, eta: 3.39 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11796 sec, avg_samples: 32.00000, ips: 542.56108 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 68500, eta: 3.39 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11811 sec, avg_samples: 32.00000, ips: 541.86551 images/sec
Training: 2023-09-28 07:44:07,371 - loss nan, lr: 0.001250, epoch: 11, step: 68500, eta: 3.39 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11811 sec, avg_samples: 32.00000, ips: 541.86551 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 68600, eta: 3.38 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.53307 images/sec
Training: 2023-09-28 07:44:19,194 - loss nan, lr: 0.001250, epoch: 11, step: 68600, eta: 3.38 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.53307 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 68700, eta: 3.38 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.61995 images/sec
Training: 2023-09-28 07:44:31,016 - loss nan, lr: 0.001250, epoch: 11, step: 68700, eta: 3.38 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.61995 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 68800, eta: 3.37 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.54595 images/sec
Training: 2023-09-28 07:44:42,839 - loss nan, lr: 0.001250, epoch: 11, step: 68800, eta: 3.37 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.54595 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 11, step: 68900, eta: 3.36 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11824 sec, avg_samples: 32.00000, ips: 541.26164 images/sec
Training: 2023-09-28 07:44:54,668 - loss nan, lr: 0.001250, epoch: 11, step: 68900, eta: 3.36 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11824 sec, avg_samples: 32.00000, ips: 541.26164 images/sec
INFO:root:Save model to MS1M_v3_arcface_static_0.1/FresResNet50/11.
Training: 2023-09-28 07:45:03,996 - Save model to MS1M_v3_arcface_static_0.1/FresResNet50/11.
INFO:root:Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/10.
Training: 2023-09-28 07:45:03,997 - Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/10.
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 69000, eta: 3.36 hours, avg_reader_cost: 0.00178 sec, avg_batch_cost: 0.03016 sec, avg_samples: 7.68000, ips: 509.22818 images/sec
Training: 2023-09-28 07:45:07,045 - loss nan, lr: 0.001250, epoch: 12, step: 69000, eta: 3.36 hours, avg_reader_cost: 0.00178 sec, avg_batch_cost: 0.03016 sec, avg_samples: 7.68000, ips: 509.22818 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 69100, eta: 3.35 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.48434 images/sec
Training: 2023-09-28 07:45:18,870 - loss nan, lr: 0.001250, epoch: 12, step: 69100, eta: 3.35 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.48434 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 69200, eta: 3.35 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.18719 images/sec
Training: 2023-09-28 07:45:30,701 - loss nan, lr: 0.001250, epoch: 12, step: 69200, eta: 3.35 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.18719 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 69300, eta: 3.34 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.19785 images/sec
Training: 2023-09-28 07:45:42,532 - loss nan, lr: 0.001250, epoch: 12, step: 69300, eta: 3.34 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.19785 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 69400, eta: 3.33 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11825 sec, avg_samples: 32.00000, ips: 541.24591 images/sec
Training: 2023-09-28 07:45:54,361 - loss nan, lr: 0.001250, epoch: 12, step: 69400, eta: 3.33 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11825 sec, avg_samples: 32.00000, ips: 541.24591 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 69500, eta: 3.33 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11824 sec, avg_samples: 32.00000, ips: 541.29306 images/sec
Training: 2023-09-28 07:46:06,190 - loss nan, lr: 0.001250, epoch: 12, step: 69500, eta: 3.33 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11824 sec, avg_samples: 32.00000, ips: 541.29306 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 69600, eta: 3.32 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.14121 images/sec
Training: 2023-09-28 07:46:18,022 - loss nan, lr: 0.001250, epoch: 12, step: 69600, eta: 3.32 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.14121 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 69700, eta: 3.32 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11825 sec, avg_samples: 32.00000, ips: 541.22094 images/sec
Training: 2023-09-28 07:46:29,852 - loss nan, lr: 0.001250, epoch: 12, step: 69700, eta: 3.32 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11825 sec, avg_samples: 32.00000, ips: 541.22094 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 69800, eta: 3.31 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11831 sec, avg_samples: 32.00000, ips: 540.93826 images/sec
Training: 2023-09-28 07:46:41,689 - loss nan, lr: 0.001250, epoch: 12, step: 69800, eta: 3.31 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11831 sec, avg_samples: 32.00000, ips: 540.93826 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 69900, eta: 3.31 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.16280 images/sec
Training: 2023-09-28 07:46:53,520 - loss nan, lr: 0.001250, epoch: 12, step: 69900, eta: 3.31 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.16280 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 70000, eta: 3.30 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.08359 images/sec
Training: 2023-09-28 07:47:05,353 - loss nan, lr: 0.001250, epoch: 12, step: 70000, eta: 3.30 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.08359 images/sec
INFO:root:[lfw][70000]XNorm: 0.600516
Training: 2023-09-28 07:47:33,261 - [lfw][70000]XNorm: 0.600516
INFO:root:[lfw][70000]Accuracy-Flip: 0.89817+-0.01834
Training: 2023-09-28 07:47:33,262 - [lfw][70000]Accuracy-Flip: 0.89817+-0.01834
INFO:root:[lfw][70000]Accuracy-Highest: 0.90150
Training: 2023-09-28 07:47:33,262 - [lfw][70000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9082
Training: 2023-09-28 07:47:33,262 - test time: 27.9082
INFO:root:[cfp_fp][70000]XNorm: 0.560406
Training: 2023-09-28 07:48:05,463 - [cfp_fp][70000]XNorm: 0.560406
INFO:root:[cfp_fp][70000]Accuracy-Flip: 0.64143+-0.01793
Training: 2023-09-28 07:48:05,463 - [cfp_fp][70000]Accuracy-Flip: 0.64143+-0.01793
INFO:root:[cfp_fp][70000]Accuracy-Highest: 0.65071
Training: 2023-09-28 07:48:05,463 - [cfp_fp][70000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2012
Training: 2023-09-28 07:48:05,463 - test time: 32.2012
INFO:root:[agedb_30][70000]XNorm: 0.569539
Training: 2023-09-28 07:48:33,168 - [agedb_30][70000]XNorm: 0.569539
INFO:root:[agedb_30][70000]Accuracy-Flip: 0.68883+-0.02124
Training: 2023-09-28 07:48:33,168 - [agedb_30][70000]Accuracy-Flip: 0.68883+-0.02124
INFO:root:[agedb_30][70000]Accuracy-Highest: 0.70083
Training: 2023-09-28 07:48:33,168 - [agedb_30][70000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7050
Training: 2023-09-28 07:48:33,168 - test time: 27.7050
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 70100, eta: 3.32 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11781 sec, avg_samples: 32.00000, ips: 543.25583 images/sec
Training: 2023-09-28 07:48:44,954 - loss nan, lr: 0.001250, epoch: 12, step: 70100, eta: 3.32 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11781 sec, avg_samples: 32.00000, ips: 543.25583 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 70200, eta: 3.31 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11788 sec, avg_samples: 32.00000, ips: 542.94285 images/sec
Training: 2023-09-28 07:48:56,747 - loss nan, lr: 0.001250, epoch: 12, step: 70200, eta: 3.31 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11788 sec, avg_samples: 32.00000, ips: 542.94285 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 70300, eta: 3.31 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11785 sec, avg_samples: 32.00000, ips: 543.06000 images/sec
Training: 2023-09-28 07:49:08,537 - loss nan, lr: 0.001250, epoch: 12, step: 70300, eta: 3.31 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11785 sec, avg_samples: 32.00000, ips: 543.06000 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 70400, eta: 3.30 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11811 sec, avg_samples: 32.00000, ips: 541.86297 images/sec
Training: 2023-09-28 07:49:20,353 - loss nan, lr: 0.001250, epoch: 12, step: 70400, eta: 3.30 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11811 sec, avg_samples: 32.00000, ips: 541.86297 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 70500, eta: 3.30 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.36544 images/sec
Training: 2023-09-28 07:49:32,180 - loss nan, lr: 0.001250, epoch: 12, step: 70500, eta: 3.30 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.36544 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 70600, eta: 3.29 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.40347 images/sec
Training: 2023-09-28 07:49:44,006 - loss nan, lr: 0.001250, epoch: 12, step: 70600, eta: 3.29 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.40347 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 70700, eta: 3.29 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.34958 images/sec
Training: 2023-09-28 07:49:55,834 - loss nan, lr: 0.001250, epoch: 12, step: 70700, eta: 3.29 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.34958 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 70800, eta: 3.28 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.33158 images/sec
Training: 2023-09-28 07:50:07,662 - loss nan, lr: 0.001250, epoch: 12, step: 70800, eta: 3.28 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.33158 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 70900, eta: 3.27 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.15983 images/sec
Training: 2023-09-28 07:50:19,493 - loss nan, lr: 0.001250, epoch: 12, step: 70900, eta: 3.27 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.15983 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 71000, eta: 3.27 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.30055 images/sec
Training: 2023-09-28 07:50:31,322 - loss nan, lr: 0.001250, epoch: 12, step: 71000, eta: 3.27 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.30055 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 71100, eta: 3.26 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.39769 images/sec
Training: 2023-09-28 07:50:43,148 - loss nan, lr: 0.001250, epoch: 12, step: 71100, eta: 3.26 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.39769 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 71200, eta: 3.26 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.46211 images/sec
Training: 2023-09-28 07:50:54,973 - loss nan, lr: 0.001250, epoch: 12, step: 71200, eta: 3.26 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.46211 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 71300, eta: 3.25 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.32673 images/sec
Training: 2023-09-28 07:51:06,801 - loss nan, lr: 0.001250, epoch: 12, step: 71300, eta: 3.25 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.32673 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 71400, eta: 3.25 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11824 sec, avg_samples: 32.00000, ips: 541.28616 images/sec
Training: 2023-09-28 07:51:18,630 - loss nan, lr: 0.001250, epoch: 12, step: 71400, eta: 3.25 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11824 sec, avg_samples: 32.00000, ips: 541.28616 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 71500, eta: 3.24 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.41703 images/sec
Training: 2023-09-28 07:51:30,456 - loss nan, lr: 0.001250, epoch: 12, step: 71500, eta: 3.24 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.41703 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 71600, eta: 3.23 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11824 sec, avg_samples: 32.00000, ips: 541.29089 images/sec
Training: 2023-09-28 07:51:42,284 - loss nan, lr: 0.001250, epoch: 12, step: 71600, eta: 3.23 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11824 sec, avg_samples: 32.00000, ips: 541.29089 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 71700, eta: 3.23 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.34252 images/sec
Training: 2023-09-28 07:51:54,112 - loss nan, lr: 0.001250, epoch: 12, step: 71700, eta: 3.23 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.34252 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 71800, eta: 3.22 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.36795 images/sec
Training: 2023-09-28 07:52:05,939 - loss nan, lr: 0.001250, epoch: 12, step: 71800, eta: 3.22 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.36795 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 71900, eta: 3.22 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11824 sec, avg_samples: 32.00000, ips: 541.26465 images/sec
Training: 2023-09-28 07:52:17,768 - loss nan, lr: 0.001250, epoch: 12, step: 71900, eta: 3.22 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11824 sec, avg_samples: 32.00000, ips: 541.26465 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 72000, eta: 3.21 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.31236 images/sec
Training: 2023-09-28 07:52:29,597 - loss nan, lr: 0.001250, epoch: 12, step: 72000, eta: 3.21 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.31236 images/sec
INFO:root:[lfw][72000]XNorm: 0.589552
Training: 2023-09-28 07:52:57,534 - [lfw][72000]XNorm: 0.589552
INFO:root:[lfw][72000]Accuracy-Flip: 0.89983+-0.01831
Training: 2023-09-28 07:52:57,534 - [lfw][72000]Accuracy-Flip: 0.89983+-0.01831
INFO:root:[lfw][72000]Accuracy-Highest: 0.90150
Training: 2023-09-28 07:52:57,534 - [lfw][72000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9370
Training: 2023-09-28 07:52:57,534 - test time: 27.9370
INFO:root:[cfp_fp][72000]XNorm: 0.550924
Training: 2023-09-28 07:53:29,739 - [cfp_fp][72000]XNorm: 0.550924
INFO:root:[cfp_fp][72000]Accuracy-Flip: 0.64443+-0.01837
Training: 2023-09-28 07:53:29,740 - [cfp_fp][72000]Accuracy-Flip: 0.64443+-0.01837
INFO:root:[cfp_fp][72000]Accuracy-Highest: 0.65071
Training: 2023-09-28 07:53:29,740 - [cfp_fp][72000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2059
Training: 2023-09-28 07:53:29,740 - test time: 32.2059
INFO:root:[agedb_30][72000]XNorm: 0.560631
Training: 2023-09-28 07:53:57,435 - [agedb_30][72000]XNorm: 0.560631
INFO:root:[agedb_30][72000]Accuracy-Flip: 0.68933+-0.02055
Training: 2023-09-28 07:53:57,436 - [agedb_30][72000]Accuracy-Flip: 0.68933+-0.02055
INFO:root:[agedb_30][72000]Accuracy-Highest: 0.70083
Training: 2023-09-28 07:53:57,436 - [agedb_30][72000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.6960
Training: 2023-09-28 07:53:57,436 - test time: 27.6960
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 72100, eta: 3.23 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11775 sec, avg_samples: 32.00000, ips: 543.50228 images/sec
Training: 2023-09-28 07:54:09,216 - loss nan, lr: 0.001250, epoch: 12, step: 72100, eta: 3.23 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11775 sec, avg_samples: 32.00000, ips: 543.50228 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 72200, eta: 3.22 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11781 sec, avg_samples: 32.00000, ips: 543.25614 images/sec
Training: 2023-09-28 07:54:21,002 - loss nan, lr: 0.001250, epoch: 12, step: 72200, eta: 3.22 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11781 sec, avg_samples: 32.00000, ips: 543.25614 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 72300, eta: 3.22 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11778 sec, avg_samples: 32.00000, ips: 543.39789 images/sec
Training: 2023-09-28 07:54:32,785 - loss nan, lr: 0.001250, epoch: 12, step: 72300, eta: 3.22 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11778 sec, avg_samples: 32.00000, ips: 543.39789 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 72400, eta: 3.21 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11794 sec, avg_samples: 32.00000, ips: 542.63638 images/sec
Training: 2023-09-28 07:54:44,584 - loss nan, lr: 0.001250, epoch: 12, step: 72400, eta: 3.21 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11794 sec, avg_samples: 32.00000, ips: 542.63638 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 72500, eta: 3.21 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.65927 images/sec
Training: 2023-09-28 07:54:56,405 - loss nan, lr: 0.001250, epoch: 12, step: 72500, eta: 3.21 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.65927 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 72600, eta: 3.20 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.73220 images/sec
Training: 2023-09-28 07:55:08,224 - loss nan, lr: 0.001250, epoch: 12, step: 72600, eta: 3.20 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.73220 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 72700, eta: 3.20 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.66803 images/sec
Training: 2023-09-28 07:55:20,045 - loss nan, lr: 0.001250, epoch: 12, step: 72700, eta: 3.20 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.66803 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 72800, eta: 3.19 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.56266 images/sec
Training: 2023-09-28 07:55:31,867 - loss nan, lr: 0.001250, epoch: 12, step: 72800, eta: 3.19 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.56266 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 72900, eta: 3.18 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.65084 images/sec
Training: 2023-09-28 07:55:43,688 - loss nan, lr: 0.001250, epoch: 12, step: 72900, eta: 3.18 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.65084 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 73000, eta: 3.18 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.60286 images/sec
Training: 2023-09-28 07:55:55,510 - loss nan, lr: 0.001250, epoch: 12, step: 73000, eta: 3.18 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.60286 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 73100, eta: 3.17 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.69816 images/sec
Training: 2023-09-28 07:56:07,330 - loss nan, lr: 0.001250, epoch: 12, step: 73100, eta: 3.17 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.69816 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 73200, eta: 3.17 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.60348 images/sec
Training: 2023-09-28 07:56:19,152 - loss nan, lr: 0.001250, epoch: 12, step: 73200, eta: 3.17 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.60348 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 73300, eta: 3.16 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.70163 images/sec
Training: 2023-09-28 07:56:30,972 - loss nan, lr: 0.001250, epoch: 12, step: 73300, eta: 3.16 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.70163 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 73400, eta: 3.16 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11824 sec, avg_samples: 32.00000, ips: 541.26109 images/sec
Training: 2023-09-28 07:56:42,801 - loss nan, lr: 0.001250, epoch: 12, step: 73400, eta: 3.16 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11824 sec, avg_samples: 32.00000, ips: 541.26109 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 73500, eta: 3.15 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.02585 images/sec
Training: 2023-09-28 07:56:54,636 - loss nan, lr: 0.001250, epoch: 12, step: 73500, eta: 3.15 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.02585 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 73600, eta: 3.14 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.14805 images/sec
Training: 2023-09-28 07:57:06,467 - loss nan, lr: 0.001250, epoch: 12, step: 73600, eta: 3.14 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.14805 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 73700, eta: 3.14 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.10448 images/sec
Training: 2023-09-28 07:57:18,300 - loss nan, lr: 0.001250, epoch: 12, step: 73700, eta: 3.14 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.10448 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 73800, eta: 3.13 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.05537 images/sec
Training: 2023-09-28 07:57:30,134 - loss nan, lr: 0.001250, epoch: 12, step: 73800, eta: 3.13 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.05537 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 73900, eta: 3.13 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.06654 images/sec
Training: 2023-09-28 07:57:41,967 - loss nan, lr: 0.001250, epoch: 12, step: 73900, eta: 3.13 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.06654 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 74000, eta: 3.12 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11830 sec, avg_samples: 32.00000, ips: 541.00354 images/sec
Training: 2023-09-28 07:57:53,802 - loss nan, lr: 0.001250, epoch: 12, step: 74000, eta: 3.12 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11830 sec, avg_samples: 32.00000, ips: 541.00354 images/sec
INFO:root:[lfw][74000]XNorm: 0.580916
Training: 2023-09-28 07:58:21,713 - [lfw][74000]XNorm: 0.580916
INFO:root:[lfw][74000]Accuracy-Flip: 0.89500+-0.01653
Training: 2023-09-28 07:58:21,713 - [lfw][74000]Accuracy-Flip: 0.89500+-0.01653
INFO:root:[lfw][74000]Accuracy-Highest: 0.90150
Training: 2023-09-28 07:58:21,713 - [lfw][74000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9106
Training: 2023-09-28 07:58:21,713 - test time: 27.9106
INFO:root:[cfp_fp][74000]XNorm: 0.544863
Training: 2023-09-28 07:58:53,956 - [cfp_fp][74000]XNorm: 0.544863
INFO:root:[cfp_fp][74000]Accuracy-Flip: 0.64200+-0.01757
Training: 2023-09-28 07:58:53,956 - [cfp_fp][74000]Accuracy-Flip: 0.64200+-0.01757
INFO:root:[cfp_fp][74000]Accuracy-Highest: 0.65071
Training: 2023-09-28 07:58:53,956 - [cfp_fp][74000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2433
Training: 2023-09-28 07:58:53,957 - test time: 32.2433
INFO:root:[agedb_30][74000]XNorm: 0.552481
Training: 2023-09-28 07:59:21,689 - [agedb_30][74000]XNorm: 0.552481
INFO:root:[agedb_30][74000]Accuracy-Flip: 0.69183+-0.02160
Training: 2023-09-28 07:59:21,689 - [agedb_30][74000]Accuracy-Flip: 0.69183+-0.02160
INFO:root:[agedb_30][74000]Accuracy-Highest: 0.70083
Training: 2023-09-28 07:59:21,689 - [agedb_30][74000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7324
Training: 2023-09-28 07:59:21,689 - test time: 27.7324
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 74100, eta: 3.14 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11771 sec, avg_samples: 32.00000, ips: 543.71409 images/sec
Training: 2023-09-28 07:59:33,465 - loss nan, lr: 0.001250, epoch: 12, step: 74100, eta: 3.14 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11771 sec, avg_samples: 32.00000, ips: 543.71409 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 74200, eta: 3.13 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11782 sec, avg_samples: 32.00000, ips: 543.19060 images/sec
Training: 2023-09-28 07:59:45,252 - loss nan, lr: 0.001250, epoch: 12, step: 74200, eta: 3.13 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11782 sec, avg_samples: 32.00000, ips: 543.19060 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 74300, eta: 3.13 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11788 sec, avg_samples: 32.00000, ips: 542.93611 images/sec
Training: 2023-09-28 07:59:57,045 - loss nan, lr: 0.001250, epoch: 12, step: 74300, eta: 3.13 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11788 sec, avg_samples: 32.00000, ips: 542.93611 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 74400, eta: 3.12 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11842 sec, avg_samples: 32.00000, ips: 540.44680 images/sec
Training: 2023-09-28 08:00:08,894 - loss nan, lr: 0.001250, epoch: 12, step: 74400, eta: 3.12 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11842 sec, avg_samples: 32.00000, ips: 540.44680 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 74500, eta: 3.12 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.44319 images/sec
Training: 2023-09-28 08:00:20,719 - loss nan, lr: 0.001250, epoch: 12, step: 74500, eta: 3.12 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.44319 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 74600, eta: 3.11 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.36250 images/sec
Training: 2023-09-28 08:00:32,545 - loss nan, lr: 0.001250, epoch: 12, step: 74600, eta: 3.11 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.36250 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 12, step: 74700, eta: 3.11 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.64352 images/sec
Training: 2023-09-28 08:00:44,365 - loss nan, lr: 0.001250, epoch: 12, step: 74700, eta: 3.11 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.64352 images/sec
INFO:root:Save model to MS1M_v3_arcface_static_0.1/FresResNet50/12.
Training: 2023-09-28 08:00:47,546 - Save model to MS1M_v3_arcface_static_0.1/FresResNet50/12.
INFO:root:Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/11.
Training: 2023-09-28 08:00:47,546 - Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/11.
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 74800, eta: 3.10 hours, avg_reader_cost: 0.00186 sec, avg_batch_cost: 0.09166 sec, avg_samples: 24.32000, ips: 530.63510 images/sec
Training: 2023-09-28 08:00:56,747 - loss nan, lr: 0.001250, epoch: 13, step: 74800, eta: 3.10 hours, avg_reader_cost: 0.00186 sec, avg_batch_cost: 0.09166 sec, avg_samples: 24.32000, ips: 530.63510 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 74900, eta: 3.09 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.49403 images/sec
Training: 2023-09-28 08:01:08,572 - loss nan, lr: 0.001250, epoch: 13, step: 74900, eta: 3.09 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.49403 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 75000, eta: 3.09 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.55779 images/sec
Training: 2023-09-28 08:01:20,395 - loss nan, lr: 0.001250, epoch: 13, step: 75000, eta: 3.09 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.55779 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 75100, eta: 3.08 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.56771 images/sec
Training: 2023-09-28 08:01:32,217 - loss nan, lr: 0.001250, epoch: 13, step: 75100, eta: 3.08 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.56771 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 75200, eta: 3.08 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.49672 images/sec
Training: 2023-09-28 08:01:44,042 - loss nan, lr: 0.001250, epoch: 13, step: 75200, eta: 3.08 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.49672 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 75300, eta: 3.07 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.56981 images/sec
Training: 2023-09-28 08:01:55,864 - loss nan, lr: 0.001250, epoch: 13, step: 75300, eta: 3.07 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.56981 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 75400, eta: 3.07 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.66416 images/sec
Training: 2023-09-28 08:02:07,685 - loss nan, lr: 0.001250, epoch: 13, step: 75400, eta: 3.07 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.66416 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 75500, eta: 3.06 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.30859 images/sec
Training: 2023-09-28 08:02:19,513 - loss nan, lr: 0.001250, epoch: 13, step: 75500, eta: 3.06 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.30859 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 75600, eta: 3.06 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.64077 images/sec
Training: 2023-09-28 08:02:31,334 - loss nan, lr: 0.001250, epoch: 13, step: 75600, eta: 3.06 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.64077 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 75700, eta: 3.05 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.50246 images/sec
Training: 2023-09-28 08:02:43,158 - loss nan, lr: 0.001250, epoch: 13, step: 75700, eta: 3.05 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.50246 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 75800, eta: 3.04 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.59139 images/sec
Training: 2023-09-28 08:02:54,981 - loss nan, lr: 0.001250, epoch: 13, step: 75800, eta: 3.04 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.59139 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 75900, eta: 3.04 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.62270 images/sec
Training: 2023-09-28 08:03:06,802 - loss nan, lr: 0.001250, epoch: 13, step: 75900, eta: 3.04 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.62270 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 76000, eta: 3.03 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.50489 images/sec
Training: 2023-09-28 08:03:18,626 - loss nan, lr: 0.001250, epoch: 13, step: 76000, eta: 3.03 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.50489 images/sec
INFO:root:[lfw][76000]XNorm: 0.576173
Training: 2023-09-28 08:03:46,599 - [lfw][76000]XNorm: 0.576173
INFO:root:[lfw][76000]Accuracy-Flip: 0.90133+-0.01939
Training: 2023-09-28 08:03:46,599 - [lfw][76000]Accuracy-Flip: 0.90133+-0.01939
INFO:root:[lfw][76000]Accuracy-Highest: 0.90150
Training: 2023-09-28 08:03:46,599 - [lfw][76000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9724
Training: 2023-09-28 08:03:46,599 - test time: 27.9724
INFO:root:[cfp_fp][76000]XNorm: 0.540150
Training: 2023-09-28 08:04:18,884 - [cfp_fp][76000]XNorm: 0.540150
INFO:root:[cfp_fp][76000]Accuracy-Flip: 0.64300+-0.02022
Training: 2023-09-28 08:04:18,885 - [cfp_fp][76000]Accuracy-Flip: 0.64300+-0.02022
INFO:root:[cfp_fp][76000]Accuracy-Highest: 0.65071
Training: 2023-09-28 08:04:18,885 - [cfp_fp][76000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2858
Training: 2023-09-28 08:04:18,885 - test time: 32.2858
INFO:root:[agedb_30][76000]XNorm: 0.547569
Training: 2023-09-28 08:04:46,636 - [agedb_30][76000]XNorm: 0.547569
INFO:root:[agedb_30][76000]Accuracy-Flip: 0.68683+-0.02209
Training: 2023-09-28 08:04:46,636 - [agedb_30][76000]Accuracy-Flip: 0.68683+-0.02209
INFO:root:[agedb_30][76000]Accuracy-Highest: 0.70083
Training: 2023-09-28 08:04:46,636 - [agedb_30][76000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7517
Training: 2023-09-28 08:04:46,636 - test time: 27.7517
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 76100, eta: 3.05 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11778 sec, avg_samples: 32.00000, ips: 543.36965 images/sec
Training: 2023-09-28 08:04:58,420 - loss nan, lr: 0.001250, epoch: 13, step: 76100, eta: 3.05 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11778 sec, avg_samples: 32.00000, ips: 543.36965 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 76200, eta: 3.04 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11780 sec, avg_samples: 32.00000, ips: 543.28470 images/sec
Training: 2023-09-28 08:05:10,205 - loss nan, lr: 0.001250, epoch: 13, step: 76200, eta: 3.04 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11780 sec, avg_samples: 32.00000, ips: 543.28470 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 76300, eta: 3.04 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11781 sec, avg_samples: 32.00000, ips: 543.25641 images/sec
Training: 2023-09-28 08:05:21,991 - loss nan, lr: 0.001250, epoch: 13, step: 76300, eta: 3.04 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11781 sec, avg_samples: 32.00000, ips: 543.25641 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 76400, eta: 3.03 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11807 sec, avg_samples: 32.00000, ips: 542.03237 images/sec
Training: 2023-09-28 08:05:33,804 - loss nan, lr: 0.001250, epoch: 13, step: 76400, eta: 3.03 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11807 sec, avg_samples: 32.00000, ips: 542.03237 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 76500, eta: 3.03 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.76509 images/sec
Training: 2023-09-28 08:05:45,622 - loss nan, lr: 0.001250, epoch: 13, step: 76500, eta: 3.03 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.76509 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 76600, eta: 3.02 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.80067 images/sec
Training: 2023-09-28 08:05:57,439 - loss nan, lr: 0.001250, epoch: 13, step: 76600, eta: 3.02 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.80067 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 76700, eta: 3.02 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.81500 images/sec
Training: 2023-09-28 08:06:09,257 - loss nan, lr: 0.001250, epoch: 13, step: 76700, eta: 3.02 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.81500 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 76800, eta: 3.01 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.77186 images/sec
Training: 2023-09-28 08:06:21,075 - loss nan, lr: 0.001250, epoch: 13, step: 76800, eta: 3.01 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.77186 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 76900, eta: 3.00 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.68541 images/sec
Training: 2023-09-28 08:06:32,895 - loss nan, lr: 0.001250, epoch: 13, step: 76900, eta: 3.00 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.68541 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 77000, eta: 3.00 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.70955 images/sec
Training: 2023-09-28 08:06:44,715 - loss nan, lr: 0.001250, epoch: 13, step: 77000, eta: 3.00 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.70955 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 77100, eta: 2.99 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.64545 images/sec
Training: 2023-09-28 08:06:56,536 - loss nan, lr: 0.001250, epoch: 13, step: 77100, eta: 2.99 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.64545 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 77200, eta: 2.99 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.70871 images/sec
Training: 2023-09-28 08:07:08,355 - loss nan, lr: 0.001250, epoch: 13, step: 77200, eta: 2.99 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.70871 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 77300, eta: 2.98 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.66978 images/sec
Training: 2023-09-28 08:07:20,176 - loss nan, lr: 0.001250, epoch: 13, step: 77300, eta: 2.98 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.66978 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 77400, eta: 2.98 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11811 sec, avg_samples: 32.00000, ips: 541.85908 images/sec
Training: 2023-09-28 08:07:31,992 - loss nan, lr: 0.001250, epoch: 13, step: 77400, eta: 2.98 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11811 sec, avg_samples: 32.00000, ips: 541.85908 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 77500, eta: 2.97 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.65932 images/sec
Training: 2023-09-28 08:07:43,813 - loss nan, lr: 0.001250, epoch: 13, step: 77500, eta: 2.97 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.65932 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 77600, eta: 2.97 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.42724 images/sec
Training: 2023-09-28 08:07:55,638 - loss nan, lr: 0.001250, epoch: 13, step: 77600, eta: 2.97 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.42724 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 77700, eta: 2.96 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.68865 images/sec
Training: 2023-09-28 08:08:07,458 - loss nan, lr: 0.001250, epoch: 13, step: 77700, eta: 2.96 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.68865 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 77800, eta: 2.95 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.72870 images/sec
Training: 2023-09-28 08:08:19,278 - loss nan, lr: 0.001250, epoch: 13, step: 77800, eta: 2.95 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.72870 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 77900, eta: 2.95 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.72331 images/sec
Training: 2023-09-28 08:08:31,097 - loss nan, lr: 0.001250, epoch: 13, step: 77900, eta: 2.95 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.72331 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 78000, eta: 2.94 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.68751 images/sec
Training: 2023-09-28 08:08:42,917 - loss nan, lr: 0.001250, epoch: 13, step: 78000, eta: 2.94 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.68751 images/sec
INFO:root:[lfw][78000]XNorm: 0.568425
Training: 2023-09-28 08:09:10,821 - [lfw][78000]XNorm: 0.568425
INFO:root:[lfw][78000]Accuracy-Flip: 0.90150+-0.01852
Training: 2023-09-28 08:09:10,821 - [lfw][78000]Accuracy-Flip: 0.90150+-0.01852
INFO:root:[lfw][78000]Accuracy-Highest: 0.90150
Training: 2023-09-28 08:09:10,821 - [lfw][78000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9039
Training: 2023-09-28 08:09:10,821 - test time: 27.9039
INFO:root:[cfp_fp][78000]XNorm: 0.531938
Training: 2023-09-28 08:09:43,075 - [cfp_fp][78000]XNorm: 0.531938
INFO:root:[cfp_fp][78000]Accuracy-Flip: 0.64300+-0.01984
Training: 2023-09-28 08:09:43,075 - [cfp_fp][78000]Accuracy-Flip: 0.64300+-0.01984
INFO:root:[cfp_fp][78000]Accuracy-Highest: 0.65071
Training: 2023-09-28 08:09:43,075 - [cfp_fp][78000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2540
Training: 2023-09-28 08:09:43,075 - test time: 32.2540
INFO:root:[agedb_30][78000]XNorm: 0.540894
Training: 2023-09-28 08:10:10,806 - [agedb_30][78000]XNorm: 0.540894
INFO:root:[agedb_30][78000]Accuracy-Flip: 0.68917+-0.02005
Training: 2023-09-28 08:10:10,806 - [agedb_30][78000]Accuracy-Flip: 0.68917+-0.02005
INFO:root:[agedb_30][78000]Accuracy-Highest: 0.70083
Training: 2023-09-28 08:10:10,806 - [agedb_30][78000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7314
Training: 2023-09-28 08:10:10,806 - test time: 27.7314
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 78100, eta: 2.96 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11777 sec, avg_samples: 32.00000, ips: 543.41794 images/sec
Training: 2023-09-28 08:10:22,589 - loss nan, lr: 0.001250, epoch: 13, step: 78100, eta: 2.96 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11777 sec, avg_samples: 32.00000, ips: 543.41794 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 78200, eta: 2.95 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11783 sec, avg_samples: 32.00000, ips: 543.16521 images/sec
Training: 2023-09-28 08:10:34,377 - loss nan, lr: 0.001250, epoch: 13, step: 78200, eta: 2.95 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11783 sec, avg_samples: 32.00000, ips: 543.16521 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 78300, eta: 2.95 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11785 sec, avg_samples: 32.00000, ips: 543.04449 images/sec
Training: 2023-09-28 08:10:46,167 - loss nan, lr: 0.001250, epoch: 13, step: 78300, eta: 2.95 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11785 sec, avg_samples: 32.00000, ips: 543.04449 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 78400, eta: 2.94 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.30255 images/sec
Training: 2023-09-28 08:10:57,974 - loss nan, lr: 0.001250, epoch: 13, step: 78400, eta: 2.94 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.30255 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 78500, eta: 2.94 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.54453 images/sec
Training: 2023-09-28 08:11:09,797 - loss nan, lr: 0.001250, epoch: 13, step: 78500, eta: 2.94 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.54453 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 78600, eta: 2.93 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.50044 images/sec
Training: 2023-09-28 08:11:21,621 - loss nan, lr: 0.001250, epoch: 13, step: 78600, eta: 2.93 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.50044 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 78700, eta: 2.93 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.48947 images/sec
Training: 2023-09-28 08:11:33,446 - loss nan, lr: 0.001250, epoch: 13, step: 78700, eta: 2.93 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.48947 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 78800, eta: 2.92 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.54357 images/sec
Training: 2023-09-28 08:11:45,269 - loss nan, lr: 0.001250, epoch: 13, step: 78800, eta: 2.92 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.54357 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 78900, eta: 2.91 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.46816 images/sec
Training: 2023-09-28 08:11:57,094 - loss nan, lr: 0.001250, epoch: 13, step: 78900, eta: 2.91 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.46816 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 79000, eta: 2.91 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.51960 images/sec
Training: 2023-09-28 08:12:08,917 - loss nan, lr: 0.001250, epoch: 13, step: 79000, eta: 2.91 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.51960 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 79100, eta: 2.90 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.49671 images/sec
Training: 2023-09-28 08:12:20,741 - loss nan, lr: 0.001250, epoch: 13, step: 79100, eta: 2.90 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.49671 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 79200, eta: 2.90 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.43743 images/sec
Training: 2023-09-28 08:12:32,567 - loss nan, lr: 0.001250, epoch: 13, step: 79200, eta: 2.90 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.43743 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 79300, eta: 2.89 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.47245 images/sec
Training: 2023-09-28 08:12:44,392 - loss nan, lr: 0.001250, epoch: 13, step: 79300, eta: 2.89 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.47245 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 79400, eta: 2.89 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.40140 images/sec
Training: 2023-09-28 08:12:56,218 - loss nan, lr: 0.001250, epoch: 13, step: 79400, eta: 2.89 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.40140 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 79500, eta: 2.88 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.40839 images/sec
Training: 2023-09-28 08:13:08,044 - loss nan, lr: 0.001250, epoch: 13, step: 79500, eta: 2.88 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.40839 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 79600, eta: 2.88 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.45945 images/sec
Training: 2023-09-28 08:13:19,869 - loss nan, lr: 0.001250, epoch: 13, step: 79600, eta: 2.88 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.45945 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 79700, eta: 2.87 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.45602 images/sec
Training: 2023-09-28 08:13:31,694 - loss nan, lr: 0.001250, epoch: 13, step: 79700, eta: 2.87 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.45602 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 79800, eta: 2.87 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.47193 images/sec
Training: 2023-09-28 08:13:43,519 - loss nan, lr: 0.001250, epoch: 13, step: 79800, eta: 2.87 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.47193 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 79900, eta: 2.86 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.41097 images/sec
Training: 2023-09-28 08:13:55,345 - loss nan, lr: 0.001250, epoch: 13, step: 79900, eta: 2.86 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.41097 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 80000, eta: 2.85 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.42568 images/sec
Training: 2023-09-28 08:14:07,171 - loss nan, lr: 0.001250, epoch: 13, step: 80000, eta: 2.85 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.42568 images/sec
INFO:root:[lfw][80000]XNorm: 0.564396
Training: 2023-09-28 08:14:35,121 - [lfw][80000]XNorm: 0.564396
INFO:root:[lfw][80000]Accuracy-Flip: 0.89417+-0.01620
Training: 2023-09-28 08:14:35,121 - [lfw][80000]Accuracy-Flip: 0.89417+-0.01620
INFO:root:[lfw][80000]Accuracy-Highest: 0.90150
Training: 2023-09-28 08:14:35,121 - [lfw][80000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9497
Training: 2023-09-28 08:14:35,121 - test time: 27.9497
INFO:root:[cfp_fp][80000]XNorm: 0.527704
Training: 2023-09-28 08:15:07,359 - [cfp_fp][80000]XNorm: 0.527704
INFO:root:[cfp_fp][80000]Accuracy-Flip: 0.63786+-0.01823
Training: 2023-09-28 08:15:07,360 - [cfp_fp][80000]Accuracy-Flip: 0.63786+-0.01823
INFO:root:[cfp_fp][80000]Accuracy-Highest: 0.65071
Training: 2023-09-28 08:15:07,360 - [cfp_fp][80000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2386
Training: 2023-09-28 08:15:07,360 - test time: 32.2386
INFO:root:[agedb_30][80000]XNorm: 0.535996
Training: 2023-09-28 08:15:35,085 - [agedb_30][80000]XNorm: 0.535996
INFO:root:[agedb_30][80000]Accuracy-Flip: 0.68100+-0.02319
Training: 2023-09-28 08:15:35,085 - [agedb_30][80000]Accuracy-Flip: 0.68100+-0.02319
INFO:root:[agedb_30][80000]Accuracy-Highest: 0.70083
Training: 2023-09-28 08:15:35,085 - [agedb_30][80000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7253
Training: 2023-09-28 08:15:35,085 - test time: 27.7253
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 80100, eta: 2.87 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11783 sec, avg_samples: 32.00000, ips: 543.15090 images/sec
Training: 2023-09-28 08:15:46,873 - loss nan, lr: 0.001250, epoch: 13, step: 80100, eta: 2.87 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11783 sec, avg_samples: 32.00000, ips: 543.15090 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 80200, eta: 2.86 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11788 sec, avg_samples: 32.00000, ips: 542.91331 images/sec
Training: 2023-09-28 08:15:58,667 - loss nan, lr: 0.001250, epoch: 13, step: 80200, eta: 2.86 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11788 sec, avg_samples: 32.00000, ips: 542.91331 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 80300, eta: 2.86 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11787 sec, avg_samples: 32.00000, ips: 542.96932 images/sec
Training: 2023-09-28 08:16:10,459 - loss nan, lr: 0.001250, epoch: 13, step: 80300, eta: 2.86 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11787 sec, avg_samples: 32.00000, ips: 542.96932 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 13, step: 80400, eta: 2.85 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11809 sec, avg_samples: 32.00000, ips: 541.95622 images/sec
Training: 2023-09-28 08:16:22,273 - loss nan, lr: 0.001250, epoch: 13, step: 80400, eta: 2.85 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11809 sec, avg_samples: 32.00000, ips: 541.95622 images/sec
INFO:root:Save model to MS1M_v3_arcface_static_0.1/FresResNet50/13.
Training: 2023-09-28 08:16:31,138 - Save model to MS1M_v3_arcface_static_0.1/FresResNet50/13.
INFO:root:Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/12.
Training: 2023-09-28 08:16:31,138 - Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/12.
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 80500, eta: 2.85 hours, avg_reader_cost: 0.00180 sec, avg_batch_cost: 0.03488 sec, avg_samples: 8.96000, ips: 513.70168 images/sec
Training: 2023-09-28 08:16:34,659 - loss nan, lr: 0.001250, epoch: 14, step: 80500, eta: 2.85 hours, avg_reader_cost: 0.00180 sec, avg_batch_cost: 0.03488 sec, avg_samples: 8.96000, ips: 513.70168 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 80600, eta: 2.84 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.50260 images/sec
Training: 2023-09-28 08:16:46,483 - loss nan, lr: 0.001250, epoch: 14, step: 80600, eta: 2.84 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.50260 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 80700, eta: 2.84 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.48252 images/sec
Training: 2023-09-28 08:16:58,308 - loss nan, lr: 0.001250, epoch: 14, step: 80700, eta: 2.84 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.48252 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 80800, eta: 2.83 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.47590 images/sec
Training: 2023-09-28 08:17:10,133 - loss nan, lr: 0.001250, epoch: 14, step: 80800, eta: 2.83 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.47590 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 80900, eta: 2.82 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.65503 images/sec
Training: 2023-09-28 08:17:21,954 - loss nan, lr: 0.001250, epoch: 14, step: 80900, eta: 2.82 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.65503 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 81000, eta: 2.82 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.55607 images/sec
Training: 2023-09-28 08:17:33,777 - loss nan, lr: 0.001250, epoch: 14, step: 81000, eta: 2.82 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.55607 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 81100, eta: 2.81 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.54287 images/sec
Training: 2023-09-28 08:17:45,600 - loss nan, lr: 0.001250, epoch: 14, step: 81100, eta: 2.81 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.54287 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 81200, eta: 2.81 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.38291 images/sec
Training: 2023-09-28 08:17:57,427 - loss nan, lr: 0.001250, epoch: 14, step: 81200, eta: 2.81 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.38291 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 81300, eta: 2.80 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.58774 images/sec
Training: 2023-09-28 08:18:09,250 - loss nan, lr: 0.001250, epoch: 14, step: 81300, eta: 2.80 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.58774 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 81400, eta: 2.80 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.62552 images/sec
Training: 2023-09-28 08:18:21,071 - loss nan, lr: 0.001250, epoch: 14, step: 81400, eta: 2.80 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.62552 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 81500, eta: 2.79 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.53839 images/sec
Training: 2023-09-28 08:18:32,895 - loss nan, lr: 0.001250, epoch: 14, step: 81500, eta: 2.79 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.53839 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 81600, eta: 2.79 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.56329 images/sec
Training: 2023-09-28 08:18:44,718 - loss nan, lr: 0.001250, epoch: 14, step: 81600, eta: 2.79 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.56329 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 81700, eta: 2.78 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.69451 images/sec
Training: 2023-09-28 08:18:56,538 - loss nan, lr: 0.001250, epoch: 14, step: 81700, eta: 2.78 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.69451 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 81800, eta: 2.78 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.56110 images/sec
Training: 2023-09-28 08:19:08,361 - loss nan, lr: 0.001250, epoch: 14, step: 81800, eta: 2.78 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.56110 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 81900, eta: 2.77 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.45206 images/sec
Training: 2023-09-28 08:19:20,186 - loss nan, lr: 0.001250, epoch: 14, step: 81900, eta: 2.77 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.45206 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 82000, eta: 2.77 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.60350 images/sec
Training: 2023-09-28 08:19:32,008 - loss nan, lr: 0.001250, epoch: 14, step: 82000, eta: 2.77 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.60350 images/sec
INFO:root:[lfw][82000]XNorm: 0.555326
Training: 2023-09-28 08:19:59,984 - [lfw][82000]XNorm: 0.555326
INFO:root:[lfw][82000]Accuracy-Flip: 0.89833+-0.01783
Training: 2023-09-28 08:19:59,984 - [lfw][82000]Accuracy-Flip: 0.89833+-0.01783
INFO:root:[lfw][82000]Accuracy-Highest: 0.90150
Training: 2023-09-28 08:19:59,985 - [lfw][82000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9764
Training: 2023-09-28 08:19:59,985 - test time: 27.9764
INFO:root:[cfp_fp][82000]XNorm: 0.519679
Training: 2023-09-28 08:20:32,251 - [cfp_fp][82000]XNorm: 0.519679
INFO:root:[cfp_fp][82000]Accuracy-Flip: 0.63914+-0.01624
Training: 2023-09-28 08:20:32,252 - [cfp_fp][82000]Accuracy-Flip: 0.63914+-0.01624
INFO:root:[cfp_fp][82000]Accuracy-Highest: 0.65071
Training: 2023-09-28 08:20:32,252 - [cfp_fp][82000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2671
Training: 2023-09-28 08:20:32,252 - test time: 32.2671
INFO:root:[agedb_30][82000]XNorm: 0.527674
Training: 2023-09-28 08:21:00,006 - [agedb_30][82000]XNorm: 0.527674
INFO:root:[agedb_30][82000]Accuracy-Flip: 0.68667+-0.02072
Training: 2023-09-28 08:21:00,006 - [agedb_30][82000]Accuracy-Flip: 0.68667+-0.02072
INFO:root:[agedb_30][82000]Accuracy-Highest: 0.70083
Training: 2023-09-28 08:21:00,006 - [agedb_30][82000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7547
Training: 2023-09-28 08:21:00,006 - test time: 27.7547
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 82100, eta: 2.78 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11774 sec, avg_samples: 32.00000, ips: 543.57726 images/sec
Training: 2023-09-28 08:21:11,785 - loss nan, lr: 0.001250, epoch: 14, step: 82100, eta: 2.78 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11774 sec, avg_samples: 32.00000, ips: 543.57726 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 82200, eta: 2.77 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11793 sec, avg_samples: 32.00000, ips: 542.68464 images/sec
Training: 2023-09-28 08:21:23,584 - loss nan, lr: 0.001250, epoch: 14, step: 82200, eta: 2.77 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11793 sec, avg_samples: 32.00000, ips: 542.68464 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 82300, eta: 2.77 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11794 sec, avg_samples: 32.00000, ips: 542.63251 images/sec
Training: 2023-09-28 08:21:35,384 - loss nan, lr: 0.001250, epoch: 14, step: 82300, eta: 2.77 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11794 sec, avg_samples: 32.00000, ips: 542.63251 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 82400, eta: 2.76 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11794 sec, avg_samples: 32.00000, ips: 542.64263 images/sec
Training: 2023-09-28 08:21:47,183 - loss nan, lr: 0.001250, epoch: 14, step: 82400, eta: 2.76 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11794 sec, avg_samples: 32.00000, ips: 542.64263 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 82500, eta: 2.76 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11792 sec, avg_samples: 32.00000, ips: 542.74640 images/sec
Training: 2023-09-28 08:21:58,980 - loss nan, lr: 0.001250, epoch: 14, step: 82500, eta: 2.76 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11792 sec, avg_samples: 32.00000, ips: 542.74640 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 82600, eta: 2.75 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.79096 images/sec
Training: 2023-09-28 08:22:10,798 - loss nan, lr: 0.001250, epoch: 14, step: 82600, eta: 2.75 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.79096 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 82700, eta: 2.75 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.56958 images/sec
Training: 2023-09-28 08:22:22,621 - loss nan, lr: 0.001250, epoch: 14, step: 82700, eta: 2.75 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.56958 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 82800, eta: 2.74 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.65159 images/sec
Training: 2023-09-28 08:22:34,442 - loss nan, lr: 0.001250, epoch: 14, step: 82800, eta: 2.74 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.65159 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 82900, eta: 2.74 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.57346 images/sec
Training: 2023-09-28 08:22:46,264 - loss nan, lr: 0.001250, epoch: 14, step: 82900, eta: 2.74 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.57346 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 83000, eta: 2.73 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.57747 images/sec
Training: 2023-09-28 08:22:58,087 - loss nan, lr: 0.001250, epoch: 14, step: 83000, eta: 2.73 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.57747 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 83100, eta: 2.72 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.61318 images/sec
Training: 2023-09-28 08:23:09,909 - loss nan, lr: 0.001250, epoch: 14, step: 83100, eta: 2.72 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.61318 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 83200, eta: 2.72 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.52874 images/sec
Training: 2023-09-28 08:23:21,732 - loss nan, lr: 0.001250, epoch: 14, step: 83200, eta: 2.72 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.52874 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 83300, eta: 2.71 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11825 sec, avg_samples: 32.00000, ips: 541.22604 images/sec
Training: 2023-09-28 08:23:33,562 - loss nan, lr: 0.001250, epoch: 14, step: 83300, eta: 2.71 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11825 sec, avg_samples: 32.00000, ips: 541.22604 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 83400, eta: 2.71 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.39372 images/sec
Training: 2023-09-28 08:23:45,389 - loss nan, lr: 0.001250, epoch: 14, step: 83400, eta: 2.71 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.39372 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 83500, eta: 2.70 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.44038 images/sec
Training: 2023-09-28 08:23:57,214 - loss nan, lr: 0.001250, epoch: 14, step: 83500, eta: 2.70 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.44038 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 83600, eta: 2.70 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.52521 images/sec
Training: 2023-09-28 08:24:09,038 - loss nan, lr: 0.001250, epoch: 14, step: 83600, eta: 2.70 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.52521 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 83700, eta: 2.69 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.53059 images/sec
Training: 2023-09-28 08:24:20,862 - loss nan, lr: 0.001250, epoch: 14, step: 83700, eta: 2.69 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.53059 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 83800, eta: 2.69 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.46534 images/sec
Training: 2023-09-28 08:24:32,687 - loss nan, lr: 0.001250, epoch: 14, step: 83800, eta: 2.69 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.46534 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 83900, eta: 2.68 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.42535 images/sec
Training: 2023-09-28 08:24:44,512 - loss nan, lr: 0.001250, epoch: 14, step: 83900, eta: 2.68 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.42535 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 84000, eta: 2.68 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.50732 images/sec
Training: 2023-09-28 08:24:56,337 - loss nan, lr: 0.001250, epoch: 14, step: 84000, eta: 2.68 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.50732 images/sec
INFO:root:[lfw][84000]XNorm: 0.549223
Training: 2023-09-28 08:25:24,261 - [lfw][84000]XNorm: 0.549223
INFO:root:[lfw][84000]Accuracy-Flip: 0.89667+-0.01483
Training: 2023-09-28 08:25:24,261 - [lfw][84000]Accuracy-Flip: 0.89667+-0.01483
INFO:root:[lfw][84000]Accuracy-Highest: 0.90150
Training: 2023-09-28 08:25:24,261 - [lfw][84000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9241
Training: 2023-09-28 08:25:24,261 - test time: 27.9241
INFO:root:[cfp_fp][84000]XNorm: 0.513924
Training: 2023-09-28 08:25:56,498 - [cfp_fp][84000]XNorm: 0.513924
INFO:root:[cfp_fp][84000]Accuracy-Flip: 0.64129+-0.01647
Training: 2023-09-28 08:25:56,498 - [cfp_fp][84000]Accuracy-Flip: 0.64129+-0.01647
INFO:root:[cfp_fp][84000]Accuracy-Highest: 0.65071
Training: 2023-09-28 08:25:56,498 - [cfp_fp][84000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2371
Training: 2023-09-28 08:25:56,498 - test time: 32.2371
INFO:root:[agedb_30][84000]XNorm: 0.520258
Training: 2023-09-28 08:26:24,199 - [agedb_30][84000]XNorm: 0.520258
INFO:root:[agedb_30][84000]Accuracy-Flip: 0.68650+-0.02176
Training: 2023-09-28 08:26:24,199 - [agedb_30][84000]Accuracy-Flip: 0.68650+-0.02176
INFO:root:[agedb_30][84000]Accuracy-Highest: 0.70083
Training: 2023-09-28 08:26:24,200 - [agedb_30][84000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7015
Training: 2023-09-28 08:26:24,200 - test time: 27.7015
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 84100, eta: 2.69 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11772 sec, avg_samples: 32.00000, ips: 543.64635 images/sec
Training: 2023-09-28 08:26:35,977 - loss nan, lr: 0.001250, epoch: 14, step: 84100, eta: 2.69 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11772 sec, avg_samples: 32.00000, ips: 543.64635 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 84200, eta: 2.68 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11790 sec, avg_samples: 32.00000, ips: 542.83416 images/sec
Training: 2023-09-28 08:26:47,772 - loss nan, lr: 0.001250, epoch: 14, step: 84200, eta: 2.68 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11790 sec, avg_samples: 32.00000, ips: 542.83416 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 84300, eta: 2.68 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11790 sec, avg_samples: 32.00000, ips: 542.83810 images/sec
Training: 2023-09-28 08:26:59,567 - loss nan, lr: 0.001250, epoch: 14, step: 84300, eta: 2.68 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11790 sec, avg_samples: 32.00000, ips: 542.83810 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 84400, eta: 2.67 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11794 sec, avg_samples: 32.00000, ips: 542.66474 images/sec
Training: 2023-09-28 08:27:11,366 - loss nan, lr: 0.001250, epoch: 14, step: 84400, eta: 2.67 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11794 sec, avg_samples: 32.00000, ips: 542.66474 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 84500, eta: 2.67 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11790 sec, avg_samples: 32.00000, ips: 542.82000 images/sec
Training: 2023-09-28 08:27:23,162 - loss nan, lr: 0.001250, epoch: 14, step: 84500, eta: 2.67 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11790 sec, avg_samples: 32.00000, ips: 542.82000 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 84600, eta: 2.66 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.64860 images/sec
Training: 2023-09-28 08:27:34,983 - loss nan, lr: 0.001250, epoch: 14, step: 84600, eta: 2.66 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.64860 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 84700, eta: 2.66 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.48117 images/sec
Training: 2023-09-28 08:27:46,807 - loss nan, lr: 0.001250, epoch: 14, step: 84700, eta: 2.66 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.48117 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 84800, eta: 2.65 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.42443 images/sec
Training: 2023-09-28 08:27:58,633 - loss nan, lr: 0.001250, epoch: 14, step: 84800, eta: 2.65 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.42443 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 84900, eta: 2.65 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11825 sec, avg_samples: 32.00000, ips: 541.23108 images/sec
Training: 2023-09-28 08:28:10,463 - loss nan, lr: 0.001250, epoch: 14, step: 84900, eta: 2.65 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11825 sec, avg_samples: 32.00000, ips: 541.23108 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 85000, eta: 2.64 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11824 sec, avg_samples: 32.00000, ips: 541.25673 images/sec
Training: 2023-09-28 08:28:22,293 - loss nan, lr: 0.001250, epoch: 14, step: 85000, eta: 2.64 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11824 sec, avg_samples: 32.00000, ips: 541.25673 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 85100, eta: 2.63 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.32807 images/sec
Training: 2023-09-28 08:28:34,121 - loss nan, lr: 0.001250, epoch: 14, step: 85100, eta: 2.63 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.32807 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 85200, eta: 2.63 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11825 sec, avg_samples: 32.00000, ips: 541.21364 images/sec
Training: 2023-09-28 08:28:45,951 - loss nan, lr: 0.001250, epoch: 14, step: 85200, eta: 2.63 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11825 sec, avg_samples: 32.00000, ips: 541.21364 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 85300, eta: 2.62 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.17614 images/sec
Training: 2023-09-28 08:28:57,782 - loss nan, lr: 0.001250, epoch: 14, step: 85300, eta: 2.62 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.17614 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 85400, eta: 2.62 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.18787 images/sec
Training: 2023-09-28 08:29:09,613 - loss nan, lr: 0.001250, epoch: 14, step: 85400, eta: 2.62 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.18787 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 85500, eta: 2.61 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11824 sec, avg_samples: 32.00000, ips: 541.28772 images/sec
Training: 2023-09-28 08:29:21,442 - loss nan, lr: 0.001250, epoch: 14, step: 85500, eta: 2.61 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11824 sec, avg_samples: 32.00000, ips: 541.28772 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 85600, eta: 2.61 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11825 sec, avg_samples: 32.00000, ips: 541.23830 images/sec
Training: 2023-09-28 08:29:33,272 - loss nan, lr: 0.001250, epoch: 14, step: 85600, eta: 2.61 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11825 sec, avg_samples: 32.00000, ips: 541.23830 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 85700, eta: 2.60 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.16066 images/sec
Training: 2023-09-28 08:29:45,104 - loss nan, lr: 0.001250, epoch: 14, step: 85700, eta: 2.60 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.16066 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 85800, eta: 2.60 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.13305 images/sec
Training: 2023-09-28 08:29:56,936 - loss nan, lr: 0.001250, epoch: 14, step: 85800, eta: 2.60 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.13305 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 85900, eta: 2.59 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.19515 images/sec
Training: 2023-09-28 08:30:08,767 - loss nan, lr: 0.001250, epoch: 14, step: 85900, eta: 2.59 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.19515 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 86000, eta: 2.59 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.17275 images/sec
Training: 2023-09-28 08:30:20,599 - loss nan, lr: 0.001250, epoch: 14, step: 86000, eta: 2.59 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.17275 images/sec
INFO:root:[lfw][86000]XNorm: 0.539480
Training: 2023-09-28 08:30:48,503 - [lfw][86000]XNorm: 0.539480
INFO:root:[lfw][86000]Accuracy-Flip: 0.89733+-0.01583
Training: 2023-09-28 08:30:48,503 - [lfw][86000]Accuracy-Flip: 0.89733+-0.01583
INFO:root:[lfw][86000]Accuracy-Highest: 0.90150
Training: 2023-09-28 08:30:48,503 - [lfw][86000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9046
Training: 2023-09-28 08:30:48,503 - test time: 27.9046
INFO:root:[cfp_fp][86000]XNorm: 0.504404
Training: 2023-09-28 08:31:20,698 - [cfp_fp][86000]XNorm: 0.504404
INFO:root:[cfp_fp][86000]Accuracy-Flip: 0.63900+-0.01678
Training: 2023-09-28 08:31:20,698 - [cfp_fp][86000]Accuracy-Flip: 0.63900+-0.01678
INFO:root:[cfp_fp][86000]Accuracy-Highest: 0.65071
Training: 2023-09-28 08:31:20,698 - [cfp_fp][86000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.1946
Training: 2023-09-28 08:31:20,698 - test time: 32.1946
INFO:root:[agedb_30][86000]XNorm: 0.512373
Training: 2023-09-28 08:31:48,409 - [agedb_30][86000]XNorm: 0.512373
INFO:root:[agedb_30][86000]Accuracy-Flip: 0.68717+-0.02365
Training: 2023-09-28 08:31:48,409 - [agedb_30][86000]Accuracy-Flip: 0.68717+-0.02365
INFO:root:[agedb_30][86000]Accuracy-Highest: 0.70083
Training: 2023-09-28 08:31:48,409 - [agedb_30][86000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7113
Training: 2023-09-28 08:31:48,409 - test time: 27.7113
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 86100, eta: 2.60 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11772 sec, avg_samples: 32.00000, ips: 543.64740 images/sec
Training: 2023-09-28 08:32:00,187 - loss nan, lr: 0.001250, epoch: 14, step: 86100, eta: 2.60 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11772 sec, avg_samples: 32.00000, ips: 543.64740 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 14, step: 86200, eta: 2.59 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11792 sec, avg_samples: 32.00000, ips: 542.75508 images/sec
Training: 2023-09-28 08:32:11,984 - loss nan, lr: 0.001250, epoch: 14, step: 86200, eta: 2.59 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11792 sec, avg_samples: 32.00000, ips: 542.75508 images/sec
INFO:root:Save model to MS1M_v3_arcface_static_0.1/FresResNet50/14.
Training: 2023-09-28 08:32:14,720 - Save model to MS1M_v3_arcface_static_0.1/FresResNet50/14.
INFO:root:Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/13.
Training: 2023-09-28 08:32:14,720 - Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/13.
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 86300, eta: 2.59 hours, avg_reader_cost: 0.00184 sec, avg_batch_cost: 0.09624 sec, avg_samples: 25.60000, ips: 532.02969 images/sec
Training: 2023-09-28 08:32:24,379 - loss nan, lr: 0.001250, epoch: 15, step: 86300, eta: 2.59 hours, avg_reader_cost: 0.00184 sec, avg_batch_cost: 0.09624 sec, avg_samples: 25.60000, ips: 532.02969 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 86400, eta: 2.58 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11801 sec, avg_samples: 32.00000, ips: 542.32045 images/sec
Training: 2023-09-28 08:32:36,186 - loss nan, lr: 0.001250, epoch: 15, step: 86400, eta: 2.58 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11801 sec, avg_samples: 32.00000, ips: 542.32045 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 86500, eta: 2.58 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11803 sec, avg_samples: 32.00000, ips: 542.22420 images/sec
Training: 2023-09-28 08:32:47,994 - loss nan, lr: 0.001250, epoch: 15, step: 86500, eta: 2.58 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11803 sec, avg_samples: 32.00000, ips: 542.22420 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 86600, eta: 2.57 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.28412 images/sec
Training: 2023-09-28 08:32:59,801 - loss nan, lr: 0.001250, epoch: 15, step: 86600, eta: 2.57 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.28412 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 86700, eta: 2.57 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11803 sec, avg_samples: 32.00000, ips: 542.24123 images/sec
Training: 2023-09-28 08:33:11,609 - loss nan, lr: 0.001250, epoch: 15, step: 86700, eta: 2.57 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11803 sec, avg_samples: 32.00000, ips: 542.24123 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 86800, eta: 2.56 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11807 sec, avg_samples: 32.00000, ips: 542.06758 images/sec
Training: 2023-09-28 08:33:23,420 - loss nan, lr: 0.001250, epoch: 15, step: 86800, eta: 2.56 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11807 sec, avg_samples: 32.00000, ips: 542.06758 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 86900, eta: 2.56 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11807 sec, avg_samples: 32.00000, ips: 542.02977 images/sec
Training: 2023-09-28 08:33:35,232 - loss nan, lr: 0.001250, epoch: 15, step: 86900, eta: 2.56 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11807 sec, avg_samples: 32.00000, ips: 542.02977 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 87000, eta: 2.55 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11811 sec, avg_samples: 32.00000, ips: 541.86409 images/sec
Training: 2023-09-28 08:33:47,048 - loss nan, lr: 0.001250, epoch: 15, step: 87000, eta: 2.55 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11811 sec, avg_samples: 32.00000, ips: 541.86409 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 87100, eta: 2.54 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11808 sec, avg_samples: 32.00000, ips: 542.02123 images/sec
Training: 2023-09-28 08:33:58,861 - loss nan, lr: 0.001250, epoch: 15, step: 87100, eta: 2.54 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11808 sec, avg_samples: 32.00000, ips: 542.02123 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 87200, eta: 2.54 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11807 sec, avg_samples: 32.00000, ips: 542.04165 images/sec
Training: 2023-09-28 08:34:10,673 - loss nan, lr: 0.001250, epoch: 15, step: 87200, eta: 2.54 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11807 sec, avg_samples: 32.00000, ips: 542.04165 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 87300, eta: 2.53 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11807 sec, avg_samples: 32.00000, ips: 542.04575 images/sec
Training: 2023-09-28 08:34:22,485 - loss nan, lr: 0.001250, epoch: 15, step: 87300, eta: 2.53 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11807 sec, avg_samples: 32.00000, ips: 542.04575 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 87400, eta: 2.53 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11807 sec, avg_samples: 32.00000, ips: 542.03817 images/sec
Training: 2023-09-28 08:34:34,298 - loss nan, lr: 0.001250, epoch: 15, step: 87400, eta: 2.53 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11807 sec, avg_samples: 32.00000, ips: 542.03817 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 87500, eta: 2.52 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11810 sec, avg_samples: 32.00000, ips: 541.92628 images/sec
Training: 2023-09-28 08:34:46,112 - loss nan, lr: 0.001250, epoch: 15, step: 87500, eta: 2.52 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11810 sec, avg_samples: 32.00000, ips: 541.92628 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 87600, eta: 2.52 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.67647 images/sec
Training: 2023-09-28 08:34:57,933 - loss nan, lr: 0.001250, epoch: 15, step: 87600, eta: 2.52 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.67647 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 87700, eta: 2.51 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.06797 images/sec
Training: 2023-09-28 08:35:09,766 - loss nan, lr: 0.001250, epoch: 15, step: 87700, eta: 2.51 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.06797 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 87800, eta: 2.51 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11831 sec, avg_samples: 32.00000, ips: 540.97308 images/sec
Training: 2023-09-28 08:35:21,602 - loss nan, lr: 0.001250, epoch: 15, step: 87800, eta: 2.51 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11831 sec, avg_samples: 32.00000, ips: 540.97308 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 87900, eta: 2.50 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11833 sec, avg_samples: 32.00000, ips: 540.85660 images/sec
Training: 2023-09-28 08:35:33,440 - loss nan, lr: 0.001250, epoch: 15, step: 87900, eta: 2.50 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11833 sec, avg_samples: 32.00000, ips: 540.85660 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 88000, eta: 2.50 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11830 sec, avg_samples: 32.00000, ips: 540.97923 images/sec
Training: 2023-09-28 08:35:45,275 - loss nan, lr: 0.001250, epoch: 15, step: 88000, eta: 2.50 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11830 sec, avg_samples: 32.00000, ips: 540.97923 images/sec
INFO:root:[lfw][88000]XNorm: 0.535201
Training: 2023-09-28 08:36:13,265 - [lfw][88000]XNorm: 0.535201
INFO:root:[lfw][88000]Accuracy-Flip: 0.90050+-0.01688
Training: 2023-09-28 08:36:13,265 - [lfw][88000]Accuracy-Flip: 0.90050+-0.01688
INFO:root:[lfw][88000]Accuracy-Highest: 0.90150
Training: 2023-09-28 08:36:13,265 - [lfw][88000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9893
Training: 2023-09-28 08:36:13,265 - test time: 27.9893
INFO:root:[cfp_fp][88000]XNorm: 0.499824
Training: 2023-09-28 08:36:45,559 - [cfp_fp][88000]XNorm: 0.499824
INFO:root:[cfp_fp][88000]Accuracy-Flip: 0.63671+-0.01616
Training: 2023-09-28 08:36:45,559 - [cfp_fp][88000]Accuracy-Flip: 0.63671+-0.01616
INFO:root:[cfp_fp][88000]Accuracy-Highest: 0.65071
Training: 2023-09-28 08:36:45,559 - [cfp_fp][88000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2945
Training: 2023-09-28 08:36:45,559 - test time: 32.2945
INFO:root:[agedb_30][88000]XNorm: 0.508874
Training: 2023-09-28 08:37:13,405 - [agedb_30][88000]XNorm: 0.508874
INFO:root:[agedb_30][88000]Accuracy-Flip: 0.68283+-0.01979
Training: 2023-09-28 08:37:13,405 - [agedb_30][88000]Accuracy-Flip: 0.68283+-0.01979
INFO:root:[agedb_30][88000]Accuracy-Highest: 0.70083
Training: 2023-09-28 08:37:13,405 - [agedb_30][88000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.8460
Training: 2023-09-28 08:37:13,405 - test time: 27.8460
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 88100, eta: 2.51 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11780 sec, avg_samples: 32.00000, ips: 543.30928 images/sec
Training: 2023-09-28 08:37:25,190 - loss nan, lr: 0.001250, epoch: 15, step: 88100, eta: 2.51 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11780 sec, avg_samples: 32.00000, ips: 543.30928 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 88200, eta: 2.50 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11801 sec, avg_samples: 32.00000, ips: 542.31938 images/sec
Training: 2023-09-28 08:37:36,997 - loss nan, lr: 0.001250, epoch: 15, step: 88200, eta: 2.50 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11801 sec, avg_samples: 32.00000, ips: 542.31938 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 88300, eta: 2.50 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11808 sec, avg_samples: 32.00000, ips: 541.98627 images/sec
Training: 2023-09-28 08:37:48,810 - loss nan, lr: 0.001250, epoch: 15, step: 88300, eta: 2.50 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11808 sec, avg_samples: 32.00000, ips: 541.98627 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 88400, eta: 2.49 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11810 sec, avg_samples: 32.00000, ips: 541.93395 images/sec
Training: 2023-09-28 08:38:00,625 - loss nan, lr: 0.001250, epoch: 15, step: 88400, eta: 2.49 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11810 sec, avg_samples: 32.00000, ips: 541.93395 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 88500, eta: 2.49 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11811 sec, avg_samples: 32.00000, ips: 541.85624 images/sec
Training: 2023-09-28 08:38:12,442 - loss nan, lr: 0.001250, epoch: 15, step: 88500, eta: 2.49 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11811 sec, avg_samples: 32.00000, ips: 541.85624 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 88600, eta: 2.48 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11811 sec, avg_samples: 32.00000, ips: 541.87138 images/sec
Training: 2023-09-28 08:38:24,258 - loss nan, lr: 0.001250, epoch: 15, step: 88600, eta: 2.48 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11811 sec, avg_samples: 32.00000, ips: 541.87138 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 88700, eta: 2.48 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.74122 images/sec
Training: 2023-09-28 08:38:36,077 - loss nan, lr: 0.001250, epoch: 15, step: 88700, eta: 2.48 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.74122 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 88800, eta: 2.47 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.80324 images/sec
Training: 2023-09-28 08:38:47,894 - loss nan, lr: 0.001250, epoch: 15, step: 88800, eta: 2.47 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.80324 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 88900, eta: 2.47 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.80719 images/sec
Training: 2023-09-28 08:38:59,712 - loss nan, lr: 0.001250, epoch: 15, step: 88900, eta: 2.47 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.80719 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 89000, eta: 2.46 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11808 sec, avg_samples: 32.00000, ips: 541.99700 images/sec
Training: 2023-09-28 08:39:11,525 - loss nan, lr: 0.001250, epoch: 15, step: 89000, eta: 2.46 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11808 sec, avg_samples: 32.00000, ips: 541.99700 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 89100, eta: 2.45 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11811 sec, avg_samples: 32.00000, ips: 541.84960 images/sec
Training: 2023-09-28 08:39:23,342 - loss nan, lr: 0.001250, epoch: 15, step: 89100, eta: 2.45 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11811 sec, avg_samples: 32.00000, ips: 541.84960 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 89200, eta: 2.45 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11810 sec, avg_samples: 32.00000, ips: 541.93231 images/sec
Training: 2023-09-28 08:39:35,157 - loss nan, lr: 0.001250, epoch: 15, step: 89200, eta: 2.45 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11810 sec, avg_samples: 32.00000, ips: 541.93231 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 89300, eta: 2.44 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.84440 images/sec
Training: 2023-09-28 08:39:46,974 - loss nan, lr: 0.001250, epoch: 15, step: 89300, eta: 2.44 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.84440 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 89400, eta: 2.44 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.52992 images/sec
Training: 2023-09-28 08:39:58,797 - loss nan, lr: 0.001250, epoch: 15, step: 89400, eta: 2.44 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.52992 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 89500, eta: 2.43 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.39779 images/sec
Training: 2023-09-28 08:40:10,623 - loss nan, lr: 0.001250, epoch: 15, step: 89500, eta: 2.43 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.39779 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 89600, eta: 2.43 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.36071 images/sec
Training: 2023-09-28 08:40:22,451 - loss nan, lr: 0.001250, epoch: 15, step: 89600, eta: 2.43 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.36071 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 89700, eta: 2.42 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.34772 images/sec
Training: 2023-09-28 08:40:34,278 - loss nan, lr: 0.001250, epoch: 15, step: 89700, eta: 2.42 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.34772 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 89800, eta: 2.42 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.30003 images/sec
Training: 2023-09-28 08:40:46,107 - loss nan, lr: 0.001250, epoch: 15, step: 89800, eta: 2.42 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.30003 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 89900, eta: 2.41 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.48000 images/sec
Training: 2023-09-28 08:40:57,932 - loss nan, lr: 0.001250, epoch: 15, step: 89900, eta: 2.41 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.48000 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 90000, eta: 2.41 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.41336 images/sec
Training: 2023-09-28 08:41:09,758 - loss nan, lr: 0.001250, epoch: 15, step: 90000, eta: 2.41 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.41336 images/sec
INFO:root:[lfw][90000]XNorm: 0.527434
Training: 2023-09-28 08:41:37,712 - [lfw][90000]XNorm: 0.527434
INFO:root:[lfw][90000]Accuracy-Flip: 0.89733+-0.01772
Training: 2023-09-28 08:41:37,712 - [lfw][90000]Accuracy-Flip: 0.89733+-0.01772
INFO:root:[lfw][90000]Accuracy-Highest: 0.90150
Training: 2023-09-28 08:41:37,712 - [lfw][90000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9540
Training: 2023-09-28 08:41:37,712 - test time: 27.9540
INFO:root:[cfp_fp][90000]XNorm: 0.493739
Training: 2023-09-28 08:42:09,996 - [cfp_fp][90000]XNorm: 0.493739
INFO:root:[cfp_fp][90000]Accuracy-Flip: 0.64029+-0.01854
Training: 2023-09-28 08:42:09,996 - [cfp_fp][90000]Accuracy-Flip: 0.64029+-0.01854
INFO:root:[cfp_fp][90000]Accuracy-Highest: 0.65071
Training: 2023-09-28 08:42:09,996 - [cfp_fp][90000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2839
Training: 2023-09-28 08:42:09,996 - test time: 32.2839
INFO:root:[agedb_30][90000]XNorm: 0.501155
Training: 2023-09-28 08:42:37,788 - [agedb_30][90000]XNorm: 0.501155
INFO:root:[agedb_30][90000]Accuracy-Flip: 0.68650+-0.02128
Training: 2023-09-28 08:42:37,788 - [agedb_30][90000]Accuracy-Flip: 0.68650+-0.02128
INFO:root:[agedb_30][90000]Accuracy-Highest: 0.70083
Training: 2023-09-28 08:42:37,788 - [agedb_30][90000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7923
Training: 2023-09-28 08:42:37,788 - test time: 27.7923
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 90100, eta: 2.42 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11780 sec, avg_samples: 32.00000, ips: 543.27270 images/sec
Training: 2023-09-28 08:42:49,574 - loss nan, lr: 0.001250, epoch: 15, step: 90100, eta: 2.42 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11780 sec, avg_samples: 32.00000, ips: 543.27270 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 90200, eta: 2.41 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.27973 images/sec
Training: 2023-09-28 08:43:01,381 - loss nan, lr: 0.001250, epoch: 15, step: 90200, eta: 2.41 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.27973 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 90300, eta: 2.41 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.77312 images/sec
Training: 2023-09-28 08:43:13,199 - loss nan, lr: 0.001250, epoch: 15, step: 90300, eta: 2.41 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.77312 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 90400, eta: 2.40 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11811 sec, avg_samples: 32.00000, ips: 541.87579 images/sec
Training: 2023-09-28 08:43:25,015 - loss nan, lr: 0.001250, epoch: 15, step: 90400, eta: 2.40 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11811 sec, avg_samples: 32.00000, ips: 541.87579 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 90500, eta: 2.40 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11809 sec, avg_samples: 32.00000, ips: 541.93873 images/sec
Training: 2023-09-28 08:43:36,830 - loss nan, lr: 0.001250, epoch: 15, step: 90500, eta: 2.40 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11809 sec, avg_samples: 32.00000, ips: 541.93873 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 90600, eta: 2.39 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.82309 images/sec
Training: 2023-09-28 08:43:48,647 - loss nan, lr: 0.001250, epoch: 15, step: 90600, eta: 2.39 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.82309 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 90700, eta: 2.39 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.78968 images/sec
Training: 2023-09-28 08:44:00,465 - loss nan, lr: 0.001250, epoch: 15, step: 90700, eta: 2.39 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.78968 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 90800, eta: 2.38 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.73857 images/sec
Training: 2023-09-28 08:44:12,284 - loss nan, lr: 0.001250, epoch: 15, step: 90800, eta: 2.38 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.73857 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 90900, eta: 2.38 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.77438 images/sec
Training: 2023-09-28 08:44:24,102 - loss nan, lr: 0.001250, epoch: 15, step: 90900, eta: 2.38 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.77438 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 91000, eta: 2.37 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.81921 images/sec
Training: 2023-09-28 08:44:35,920 - loss nan, lr: 0.001250, epoch: 15, step: 91000, eta: 2.37 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.81921 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 91100, eta: 2.37 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.80010 images/sec
Training: 2023-09-28 08:44:47,737 - loss nan, lr: 0.001250, epoch: 15, step: 91100, eta: 2.37 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.80010 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 91200, eta: 2.36 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.77728 images/sec
Training: 2023-09-28 08:44:59,556 - loss nan, lr: 0.001250, epoch: 15, step: 91200, eta: 2.36 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.77728 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 91300, eta: 2.35 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11811 sec, avg_samples: 32.00000, ips: 541.87833 images/sec
Training: 2023-09-28 08:45:11,372 - loss nan, lr: 0.001250, epoch: 15, step: 91300, eta: 2.35 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11811 sec, avg_samples: 32.00000, ips: 541.87833 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 91400, eta: 2.35 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11810 sec, avg_samples: 32.00000, ips: 541.90045 images/sec
Training: 2023-09-28 08:45:23,187 - loss nan, lr: 0.001250, epoch: 15, step: 91400, eta: 2.35 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11810 sec, avg_samples: 32.00000, ips: 541.90045 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 91500, eta: 2.34 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.78211 images/sec
Training: 2023-09-28 08:45:35,005 - loss nan, lr: 0.001250, epoch: 15, step: 91500, eta: 2.34 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.78211 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 91600, eta: 2.34 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11810 sec, avg_samples: 32.00000, ips: 541.91681 images/sec
Training: 2023-09-28 08:45:46,820 - loss nan, lr: 0.001250, epoch: 15, step: 91600, eta: 2.34 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11810 sec, avg_samples: 32.00000, ips: 541.91681 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 91700, eta: 2.33 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.80160 images/sec
Training: 2023-09-28 08:45:58,638 - loss nan, lr: 0.001250, epoch: 15, step: 91700, eta: 2.33 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.80160 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 91800, eta: 2.33 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.46337 images/sec
Training: 2023-09-28 08:46:10,463 - loss nan, lr: 0.001250, epoch: 15, step: 91800, eta: 2.33 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.46337 images/sec
INFO:root:loss nan, lr: 0.001250, epoch: 15, step: 91900, eta: 2.32 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.67780 images/sec
Training: 2023-09-28 08:46:22,284 - loss nan, lr: 0.001250, epoch: 15, step: 91900, eta: 2.32 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.67780 images/sec
INFO:root:Save model to MS1M_v3_arcface_static_0.1/FresResNet50/15.
Training: 2023-09-28 08:46:30,674 - Save model to MS1M_v3_arcface_static_0.1/FresResNet50/15.
INFO:root:Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/14.
Training: 2023-09-28 08:46:30,674 - Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/14.
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 92000, eta: 2.32 hours, avg_reader_cost: 0.00164 sec, avg_batch_cost: 0.03948 sec, avg_samples: 10.24000, ips: 518.76250 images/sec
Training: 2023-09-28 08:46:34,654 - loss nan, lr: 0.000125, epoch: 16, step: 92000, eta: 2.32 hours, avg_reader_cost: 0.00164 sec, avg_batch_cost: 0.03948 sec, avg_samples: 10.24000, ips: 518.76250 images/sec
INFO:root:[lfw][92000]XNorm: 0.519762
Training: 2023-09-28 08:47:02,604 - [lfw][92000]XNorm: 0.519762
INFO:root:[lfw][92000]Accuracy-Flip: 0.89867+-0.01809
Training: 2023-09-28 08:47:02,604 - [lfw][92000]Accuracy-Flip: 0.89867+-0.01809
INFO:root:[lfw][92000]Accuracy-Highest: 0.90150
Training: 2023-09-28 08:47:02,604 - [lfw][92000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9501
Training: 2023-09-28 08:47:02,604 - test time: 27.9501
INFO:root:[cfp_fp][92000]XNorm: 0.486303
Training: 2023-09-28 08:47:34,887 - [cfp_fp][92000]XNorm: 0.486303
INFO:root:[cfp_fp][92000]Accuracy-Flip: 0.64129+-0.01718
Training: 2023-09-28 08:47:34,887 - [cfp_fp][92000]Accuracy-Flip: 0.64129+-0.01718
INFO:root:[cfp_fp][92000]Accuracy-Highest: 0.65071
Training: 2023-09-28 08:47:34,887 - [cfp_fp][92000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2824
Training: 2023-09-28 08:47:34,887 - test time: 32.2824
INFO:root:[agedb_30][92000]XNorm: 0.493367
Training: 2023-09-28 08:48:02,667 - [agedb_30][92000]XNorm: 0.493367
INFO:root:[agedb_30][92000]Accuracy-Flip: 0.68067+-0.02243
Training: 2023-09-28 08:48:02,667 - [agedb_30][92000]Accuracy-Flip: 0.68067+-0.02243
INFO:root:[agedb_30][92000]Accuracy-Highest: 0.70083
Training: 2023-09-28 08:48:02,667 - [agedb_30][92000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7805
Training: 2023-09-28 08:48:02,667 - test time: 27.7805
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 92100, eta: 2.33 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11783 sec, avg_samples: 32.00000, ips: 543.14483 images/sec
Training: 2023-09-28 08:48:14,456 - loss nan, lr: 0.000125, epoch: 16, step: 92100, eta: 2.33 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11783 sec, avg_samples: 32.00000, ips: 543.14483 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 92200, eta: 2.32 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11799 sec, avg_samples: 32.00000, ips: 542.41723 images/sec
Training: 2023-09-28 08:48:26,260 - loss nan, lr: 0.000125, epoch: 16, step: 92200, eta: 2.32 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11799 sec, avg_samples: 32.00000, ips: 542.41723 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 92300, eta: 2.32 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.83245 images/sec
Training: 2023-09-28 08:48:38,078 - loss nan, lr: 0.000125, epoch: 16, step: 92300, eta: 2.32 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.83245 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 92400, eta: 2.31 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11809 sec, avg_samples: 32.00000, ips: 541.94040 images/sec
Training: 2023-09-28 08:48:49,892 - loss nan, lr: 0.000125, epoch: 16, step: 92400, eta: 2.31 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11809 sec, avg_samples: 32.00000, ips: 541.94040 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 92500, eta: 2.31 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.80785 images/sec
Training: 2023-09-28 08:49:01,710 - loss nan, lr: 0.000125, epoch: 16, step: 92500, eta: 2.31 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.80785 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 92600, eta: 2.30 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11811 sec, avg_samples: 32.00000, ips: 541.87703 images/sec
Training: 2023-09-28 08:49:13,526 - loss nan, lr: 0.000125, epoch: 16, step: 92600, eta: 2.30 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11811 sec, avg_samples: 32.00000, ips: 541.87703 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 92700, eta: 2.30 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.54739 images/sec
Training: 2023-09-28 08:49:25,350 - loss nan, lr: 0.000125, epoch: 16, step: 92700, eta: 2.30 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.54739 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 92800, eta: 2.29 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11811 sec, avg_samples: 32.00000, ips: 541.87841 images/sec
Training: 2023-09-28 08:49:37,166 - loss nan, lr: 0.000125, epoch: 16, step: 92800, eta: 2.29 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11811 sec, avg_samples: 32.00000, ips: 541.87841 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 92900, eta: 2.29 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11810 sec, avg_samples: 32.00000, ips: 541.89945 images/sec
Training: 2023-09-28 08:49:48,982 - loss nan, lr: 0.000125, epoch: 16, step: 92900, eta: 2.29 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11810 sec, avg_samples: 32.00000, ips: 541.89945 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 93000, eta: 2.28 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11810 sec, avg_samples: 32.00000, ips: 541.91157 images/sec
Training: 2023-09-28 08:50:00,797 - loss nan, lr: 0.000125, epoch: 16, step: 93000, eta: 2.28 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11810 sec, avg_samples: 32.00000, ips: 541.91157 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 93100, eta: 2.28 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11809 sec, avg_samples: 32.00000, ips: 541.93703 images/sec
Training: 2023-09-28 08:50:12,612 - loss nan, lr: 0.000125, epoch: 16, step: 93100, eta: 2.28 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11809 sec, avg_samples: 32.00000, ips: 541.93703 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 93200, eta: 2.27 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.54673 images/sec
Training: 2023-09-28 08:50:24,436 - loss nan, lr: 0.000125, epoch: 16, step: 93200, eta: 2.27 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.54673 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 93300, eta: 2.27 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.90034 images/sec
Training: 2023-09-28 08:50:36,274 - loss nan, lr: 0.000125, epoch: 16, step: 93300, eta: 2.27 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.90034 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 93400, eta: 2.26 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.53960 images/sec
Training: 2023-09-28 08:50:48,098 - loss nan, lr: 0.000125, epoch: 16, step: 93400, eta: 2.26 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.53960 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 93500, eta: 2.25 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11806 sec, avg_samples: 32.00000, ips: 542.10853 images/sec
Training: 2023-09-28 08:50:59,909 - loss nan, lr: 0.000125, epoch: 16, step: 93500, eta: 2.25 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11806 sec, avg_samples: 32.00000, ips: 542.10853 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 93600, eta: 2.25 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11800 sec, avg_samples: 32.00000, ips: 542.36305 images/sec
Training: 2023-09-28 08:51:11,714 - loss nan, lr: 0.000125, epoch: 16, step: 93600, eta: 2.25 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11800 sec, avg_samples: 32.00000, ips: 542.36305 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 93700, eta: 2.24 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.26161 images/sec
Training: 2023-09-28 08:51:23,522 - loss nan, lr: 0.000125, epoch: 16, step: 93700, eta: 2.24 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.26161 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 93800, eta: 2.24 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11803 sec, avg_samples: 32.00000, ips: 542.25655 images/sec
Training: 2023-09-28 08:51:35,329 - loss nan, lr: 0.000125, epoch: 16, step: 93800, eta: 2.24 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11803 sec, avg_samples: 32.00000, ips: 542.25655 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 93900, eta: 2.23 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11804 sec, avg_samples: 32.00000, ips: 542.18899 images/sec
Training: 2023-09-28 08:51:47,138 - loss nan, lr: 0.000125, epoch: 16, step: 93900, eta: 2.23 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11804 sec, avg_samples: 32.00000, ips: 542.18899 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 94000, eta: 2.23 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11804 sec, avg_samples: 32.00000, ips: 542.17441 images/sec
Training: 2023-09-28 08:51:58,948 - loss nan, lr: 0.000125, epoch: 16, step: 94000, eta: 2.23 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11804 sec, avg_samples: 32.00000, ips: 542.17441 images/sec
INFO:root:[lfw][94000]XNorm: 0.524153
Training: 2023-09-28 08:52:26,870 - [lfw][94000]XNorm: 0.524153
INFO:root:[lfw][94000]Accuracy-Flip: 0.89717+-0.01830
Training: 2023-09-28 08:52:26,870 - [lfw][94000]Accuracy-Flip: 0.89717+-0.01830
INFO:root:[lfw][94000]Accuracy-Highest: 0.90150
Training: 2023-09-28 08:52:26,870 - [lfw][94000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9225
Training: 2023-09-28 08:52:26,870 - test time: 27.9225
INFO:root:[cfp_fp][94000]XNorm: 0.489198
Training: 2023-09-28 08:52:59,133 - [cfp_fp][94000]XNorm: 0.489198
INFO:root:[cfp_fp][94000]Accuracy-Flip: 0.64086+-0.01874
Training: 2023-09-28 08:52:59,133 - [cfp_fp][94000]Accuracy-Flip: 0.64086+-0.01874
INFO:root:[cfp_fp][94000]Accuracy-Highest: 0.65071
Training: 2023-09-28 08:52:59,134 - [cfp_fp][94000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2632
Training: 2023-09-28 08:52:59,134 - test time: 32.2632
INFO:root:[agedb_30][94000]XNorm: 0.497542
Training: 2023-09-28 08:53:26,872 - [agedb_30][94000]XNorm: 0.497542
INFO:root:[agedb_30][94000]Accuracy-Flip: 0.68817+-0.02113
Training: 2023-09-28 08:53:26,872 - [agedb_30][94000]Accuracy-Flip: 0.68817+-0.02113
INFO:root:[agedb_30][94000]Accuracy-Highest: 0.70083
Training: 2023-09-28 08:53:26,872 - [agedb_30][94000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7387
Training: 2023-09-28 08:53:26,872 - test time: 27.7387
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 94100, eta: 2.24 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11781 sec, avg_samples: 32.00000, ips: 543.25142 images/sec
Training: 2023-09-28 08:53:38,658 - loss nan, lr: 0.000125, epoch: 16, step: 94100, eta: 2.24 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11781 sec, avg_samples: 32.00000, ips: 543.25142 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 94200, eta: 2.23 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11792 sec, avg_samples: 32.00000, ips: 542.72051 images/sec
Training: 2023-09-28 08:53:50,456 - loss nan, lr: 0.000125, epoch: 16, step: 94200, eta: 2.23 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11792 sec, avg_samples: 32.00000, ips: 542.72051 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 94300, eta: 2.23 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.27098 images/sec
Training: 2023-09-28 08:54:02,264 - loss nan, lr: 0.000125, epoch: 16, step: 94300, eta: 2.23 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.27098 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 94400, eta: 2.22 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.27480 images/sec
Training: 2023-09-28 08:54:14,071 - loss nan, lr: 0.000125, epoch: 16, step: 94400, eta: 2.22 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.27480 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 94500, eta: 2.22 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11805 sec, avg_samples: 32.00000, ips: 542.16162 images/sec
Training: 2023-09-28 08:54:25,881 - loss nan, lr: 0.000125, epoch: 16, step: 94500, eta: 2.22 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11805 sec, avg_samples: 32.00000, ips: 542.16162 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 94600, eta: 2.21 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11801 sec, avg_samples: 32.00000, ips: 542.32731 images/sec
Training: 2023-09-28 08:54:37,687 - loss nan, lr: 0.000125, epoch: 16, step: 94600, eta: 2.21 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11801 sec, avg_samples: 32.00000, ips: 542.32731 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 94700, eta: 2.21 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11805 sec, avg_samples: 32.00000, ips: 542.13341 images/sec
Training: 2023-09-28 08:54:49,498 - loss nan, lr: 0.000125, epoch: 16, step: 94700, eta: 2.21 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11805 sec, avg_samples: 32.00000, ips: 542.13341 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 94800, eta: 2.20 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11809 sec, avg_samples: 32.00000, ips: 541.94449 images/sec
Training: 2023-09-28 08:55:01,313 - loss nan, lr: 0.000125, epoch: 16, step: 94800, eta: 2.20 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11809 sec, avg_samples: 32.00000, ips: 541.94449 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 94900, eta: 2.20 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11808 sec, avg_samples: 32.00000, ips: 542.02672 images/sec
Training: 2023-09-28 08:55:13,126 - loss nan, lr: 0.000125, epoch: 16, step: 94900, eta: 2.20 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11808 sec, avg_samples: 32.00000, ips: 542.02672 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 95000, eta: 2.19 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.27021 images/sec
Training: 2023-09-28 08:55:24,933 - loss nan, lr: 0.000125, epoch: 16, step: 95000, eta: 2.19 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.27021 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 95100, eta: 2.19 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11804 sec, avg_samples: 32.00000, ips: 542.16762 images/sec
Training: 2023-09-28 08:55:36,743 - loss nan, lr: 0.000125, epoch: 16, step: 95100, eta: 2.19 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11804 sec, avg_samples: 32.00000, ips: 542.16762 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 95200, eta: 2.18 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11804 sec, avg_samples: 32.00000, ips: 542.17476 images/sec
Training: 2023-09-28 08:55:48,552 - loss nan, lr: 0.000125, epoch: 16, step: 95200, eta: 2.18 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11804 sec, avg_samples: 32.00000, ips: 542.17476 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 95300, eta: 2.18 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11804 sec, avg_samples: 32.00000, ips: 542.20505 images/sec
Training: 2023-09-28 08:56:00,361 - loss nan, lr: 0.000125, epoch: 16, step: 95300, eta: 2.18 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11804 sec, avg_samples: 32.00000, ips: 542.20505 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 95400, eta: 2.17 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11801 sec, avg_samples: 32.00000, ips: 542.31182 images/sec
Training: 2023-09-28 08:56:12,168 - loss nan, lr: 0.000125, epoch: 16, step: 95400, eta: 2.17 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11801 sec, avg_samples: 32.00000, ips: 542.31182 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 95500, eta: 2.17 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.29772 images/sec
Training: 2023-09-28 08:56:23,975 - loss nan, lr: 0.000125, epoch: 16, step: 95500, eta: 2.17 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.29772 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 95600, eta: 2.16 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.84357 images/sec
Training: 2023-09-28 08:56:35,792 - loss nan, lr: 0.000125, epoch: 16, step: 95600, eta: 2.16 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.84357 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 95700, eta: 2.15 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.69329 images/sec
Training: 2023-09-28 08:56:47,612 - loss nan, lr: 0.000125, epoch: 16, step: 95700, eta: 2.15 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.69329 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 95800, eta: 2.15 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.67540 images/sec
Training: 2023-09-28 08:56:59,432 - loss nan, lr: 0.000125, epoch: 16, step: 95800, eta: 2.15 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.67540 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 95900, eta: 2.14 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.70919 images/sec
Training: 2023-09-28 08:57:11,252 - loss nan, lr: 0.000125, epoch: 16, step: 95900, eta: 2.14 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.70919 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 96000, eta: 2.14 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.83716 images/sec
Training: 2023-09-28 08:57:23,069 - loss nan, lr: 0.000125, epoch: 16, step: 96000, eta: 2.14 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.83716 images/sec
INFO:root:[lfw][96000]XNorm: 0.519149
Training: 2023-09-28 08:57:50,996 - [lfw][96000]XNorm: 0.519149
INFO:root:[lfw][96000]Accuracy-Flip: 0.89733+-0.01750
Training: 2023-09-28 08:57:50,996 - [lfw][96000]Accuracy-Flip: 0.89733+-0.01750
INFO:root:[lfw][96000]Accuracy-Highest: 0.90150
Training: 2023-09-28 08:57:50,996 - [lfw][96000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9273
Training: 2023-09-28 08:57:50,997 - test time: 27.9273
INFO:root:[cfp_fp][96000]XNorm: 0.485422
Training: 2023-09-28 08:58:23,218 - [cfp_fp][96000]XNorm: 0.485422
INFO:root:[cfp_fp][96000]Accuracy-Flip: 0.64586+-0.02065
Training: 2023-09-28 08:58:23,219 - [cfp_fp][96000]Accuracy-Flip: 0.64586+-0.02065
INFO:root:[cfp_fp][96000]Accuracy-Highest: 0.65071
Training: 2023-09-28 08:58:23,219 - [cfp_fp][96000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2221
Training: 2023-09-28 08:58:23,219 - test time: 32.2221
INFO:root:[agedb_30][96000]XNorm: 0.492447
Training: 2023-09-28 08:58:50,955 - [agedb_30][96000]XNorm: 0.492447
INFO:root:[agedb_30][96000]Accuracy-Flip: 0.68783+-0.02282
Training: 2023-09-28 08:58:50,955 - [agedb_30][96000]Accuracy-Flip: 0.68783+-0.02282
INFO:root:[agedb_30][96000]Accuracy-Highest: 0.70083
Training: 2023-09-28 08:58:50,955 - [agedb_30][96000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7363
Training: 2023-09-28 08:58:50,955 - test time: 27.7363
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 96100, eta: 2.15 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11786 sec, avg_samples: 32.00000, ips: 543.03384 images/sec
Training: 2023-09-28 08:59:02,746 - loss nan, lr: 0.000125, epoch: 16, step: 96100, eta: 2.15 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11786 sec, avg_samples: 32.00000, ips: 543.03384 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 96200, eta: 2.14 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11789 sec, avg_samples: 32.00000, ips: 542.86555 images/sec
Training: 2023-09-28 08:59:14,541 - loss nan, lr: 0.000125, epoch: 16, step: 96200, eta: 2.14 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11789 sec, avg_samples: 32.00000, ips: 542.86555 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 96300, eta: 2.14 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11804 sec, avg_samples: 32.00000, ips: 542.20104 images/sec
Training: 2023-09-28 08:59:26,350 - loss nan, lr: 0.000125, epoch: 16, step: 96300, eta: 2.14 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11804 sec, avg_samples: 32.00000, ips: 542.20104 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 96400, eta: 2.13 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11803 sec, avg_samples: 32.00000, ips: 542.21216 images/sec
Training: 2023-09-28 08:59:38,158 - loss nan, lr: 0.000125, epoch: 16, step: 96400, eta: 2.13 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11803 sec, avg_samples: 32.00000, ips: 542.21216 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 96500, eta: 2.13 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11800 sec, avg_samples: 32.00000, ips: 542.36249 images/sec
Training: 2023-09-28 08:59:49,964 - loss nan, lr: 0.000125, epoch: 16, step: 96500, eta: 2.13 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11800 sec, avg_samples: 32.00000, ips: 542.36249 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 96600, eta: 2.12 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.29166 images/sec
Training: 2023-09-28 09:00:01,771 - loss nan, lr: 0.000125, epoch: 16, step: 96600, eta: 2.12 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.29166 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 96700, eta: 2.12 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.27535 images/sec
Training: 2023-09-28 09:00:13,579 - loss nan, lr: 0.000125, epoch: 16, step: 96700, eta: 2.12 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.27535 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 96800, eta: 2.11 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.28681 images/sec
Training: 2023-09-28 09:00:25,386 - loss nan, lr: 0.000125, epoch: 16, step: 96800, eta: 2.11 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.28681 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 96900, eta: 2.11 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.29765 images/sec
Training: 2023-09-28 09:00:37,193 - loss nan, lr: 0.000125, epoch: 16, step: 96900, eta: 2.11 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.29765 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 97000, eta: 2.10 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11800 sec, avg_samples: 32.00000, ips: 542.38615 images/sec
Training: 2023-09-28 09:00:48,998 - loss nan, lr: 0.000125, epoch: 16, step: 97000, eta: 2.10 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11800 sec, avg_samples: 32.00000, ips: 542.38615 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 97100, eta: 2.10 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11805 sec, avg_samples: 32.00000, ips: 542.13992 images/sec
Training: 2023-09-28 09:01:00,808 - loss nan, lr: 0.000125, epoch: 16, step: 97100, eta: 2.10 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11805 sec, avg_samples: 32.00000, ips: 542.13992 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 97200, eta: 2.09 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11806 sec, avg_samples: 32.00000, ips: 542.09557 images/sec
Training: 2023-09-28 09:01:12,619 - loss nan, lr: 0.000125, epoch: 16, step: 97200, eta: 2.09 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11806 sec, avg_samples: 32.00000, ips: 542.09557 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 97300, eta: 2.09 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11805 sec, avg_samples: 32.00000, ips: 542.15115 images/sec
Training: 2023-09-28 09:01:24,429 - loss nan, lr: 0.000125, epoch: 16, step: 97300, eta: 2.09 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11805 sec, avg_samples: 32.00000, ips: 542.15115 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 97400, eta: 2.08 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11804 sec, avg_samples: 32.00000, ips: 542.19601 images/sec
Training: 2023-09-28 09:01:36,239 - loss nan, lr: 0.000125, epoch: 16, step: 97400, eta: 2.08 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11804 sec, avg_samples: 32.00000, ips: 542.19601 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 97500, eta: 2.08 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11808 sec, avg_samples: 32.00000, ips: 541.99971 images/sec
Training: 2023-09-28 09:01:48,052 - loss nan, lr: 0.000125, epoch: 16, step: 97500, eta: 2.08 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11808 sec, avg_samples: 32.00000, ips: 541.99971 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 97600, eta: 2.07 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.71624 images/sec
Training: 2023-09-28 09:01:59,872 - loss nan, lr: 0.000125, epoch: 16, step: 97600, eta: 2.07 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.71624 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 16, step: 97700, eta: 2.07 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.78665 images/sec
Training: 2023-09-28 09:02:11,690 - loss nan, lr: 0.000125, epoch: 16, step: 97700, eta: 2.07 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.78665 images/sec
INFO:root:Save model to MS1M_v3_arcface_static_0.1/FresResNet50/16.
Training: 2023-09-28 09:02:13,932 - Save model to MS1M_v3_arcface_static_0.1/FresResNet50/16.
INFO:root:Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/15.
Training: 2023-09-28 09:02:13,932 - Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/15.
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 97800, eta: 2.06 hours, avg_reader_cost: 0.00181 sec, avg_batch_cost: 0.10101 sec, avg_samples: 26.88000, ips: 532.21150 images/sec
Training: 2023-09-28 09:02:24,068 - loss nan, lr: 0.000125, epoch: 17, step: 97800, eta: 2.06 hours, avg_reader_cost: 0.00181 sec, avg_batch_cost: 0.10101 sec, avg_samples: 26.88000, ips: 532.21150 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 97900, eta: 2.06 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.74132 images/sec
Training: 2023-09-28 09:02:35,887 - loss nan, lr: 0.000125, epoch: 17, step: 97900, eta: 2.06 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.74132 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 98000, eta: 2.05 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.79659 images/sec
Training: 2023-09-28 09:02:47,705 - loss nan, lr: 0.000125, epoch: 17, step: 98000, eta: 2.05 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.79659 images/sec
INFO:root:[lfw][98000]XNorm: 0.519999
Training: 2023-09-28 09:03:15,614 - [lfw][98000]XNorm: 0.519999
INFO:root:[lfw][98000]Accuracy-Flip: 0.89967+-0.01666
Training: 2023-09-28 09:03:15,615 - [lfw][98000]Accuracy-Flip: 0.89967+-0.01666
INFO:root:[lfw][98000]Accuracy-Highest: 0.90150
Training: 2023-09-28 09:03:15,615 - [lfw][98000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9094
Training: 2023-09-28 09:03:15,615 - test time: 27.9094
INFO:root:[cfp_fp][98000]XNorm: 0.486128
Training: 2023-09-28 09:03:47,810 - [cfp_fp][98000]XNorm: 0.486128
INFO:root:[cfp_fp][98000]Accuracy-Flip: 0.64757+-0.01743
Training: 2023-09-28 09:03:47,810 - [cfp_fp][98000]Accuracy-Flip: 0.64757+-0.01743
INFO:root:[cfp_fp][98000]Accuracy-Highest: 0.65071
Training: 2023-09-28 09:03:47,810 - [cfp_fp][98000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.1956
Training: 2023-09-28 09:03:47,810 - test time: 32.1956
INFO:root:[agedb_30][98000]XNorm: 0.494163
Training: 2023-09-28 09:04:15,606 - [agedb_30][98000]XNorm: 0.494163
INFO:root:[agedb_30][98000]Accuracy-Flip: 0.69067+-0.01988
Training: 2023-09-28 09:04:15,606 - [agedb_30][98000]Accuracy-Flip: 0.69067+-0.01988
INFO:root:[agedb_30][98000]Accuracy-Highest: 0.70083
Training: 2023-09-28 09:04:15,606 - [agedb_30][98000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7959
Training: 2023-09-28 09:04:15,606 - test time: 27.7959
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 98100, eta: 2.06 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11797 sec, avg_samples: 32.00000, ips: 542.53289 images/sec
Training: 2023-09-28 09:04:27,408 - loss nan, lr: 0.000125, epoch: 17, step: 98100, eta: 2.06 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11797 sec, avg_samples: 32.00000, ips: 542.53289 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 98200, eta: 2.05 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11842 sec, avg_samples: 32.00000, ips: 540.46511 images/sec
Training: 2023-09-28 09:04:39,255 - loss nan, lr: 0.000125, epoch: 17, step: 98200, eta: 2.05 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11842 sec, avg_samples: 32.00000, ips: 540.46511 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 98300, eta: 2.05 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11858 sec, avg_samples: 32.00000, ips: 539.72060 images/sec
Training: 2023-09-28 09:04:51,119 - loss nan, lr: 0.000125, epoch: 17, step: 98300, eta: 2.05 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11858 sec, avg_samples: 32.00000, ips: 539.72060 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 98400, eta: 2.04 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11854 sec, avg_samples: 32.00000, ips: 539.88016 images/sec
Training: 2023-09-28 09:05:02,979 - loss nan, lr: 0.000125, epoch: 17, step: 98400, eta: 2.04 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11854 sec, avg_samples: 32.00000, ips: 539.88016 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 98500, eta: 2.04 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11873 sec, avg_samples: 32.00000, ips: 539.03567 images/sec
Training: 2023-09-28 09:05:14,858 - loss nan, lr: 0.000125, epoch: 17, step: 98500, eta: 2.04 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11873 sec, avg_samples: 32.00000, ips: 539.03567 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 98600, eta: 2.03 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11875 sec, avg_samples: 32.00000, ips: 538.95160 images/sec
Training: 2023-09-28 09:05:26,739 - loss nan, lr: 0.000125, epoch: 17, step: 98600, eta: 2.03 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11875 sec, avg_samples: 32.00000, ips: 538.95160 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 98700, eta: 2.03 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11860 sec, avg_samples: 32.00000, ips: 539.62622 images/sec
Training: 2023-09-28 09:05:38,606 - loss nan, lr: 0.000125, epoch: 17, step: 98700, eta: 2.03 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11860 sec, avg_samples: 32.00000, ips: 539.62622 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 98800, eta: 2.02 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.79100 images/sec
Training: 2023-09-28 09:05:50,423 - loss nan, lr: 0.000125, epoch: 17, step: 98800, eta: 2.02 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.79100 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 98900, eta: 2.02 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11807 sec, avg_samples: 32.00000, ips: 542.03051 images/sec
Training: 2023-09-28 09:06:02,236 - loss nan, lr: 0.000125, epoch: 17, step: 98900, eta: 2.02 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11807 sec, avg_samples: 32.00000, ips: 542.03051 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 99000, eta: 2.01 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11809 sec, avg_samples: 32.00000, ips: 541.96946 images/sec
Training: 2023-09-28 09:06:14,049 - loss nan, lr: 0.000125, epoch: 17, step: 99000, eta: 2.01 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11809 sec, avg_samples: 32.00000, ips: 541.96946 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 99100, eta: 2.01 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.45691 images/sec
Training: 2023-09-28 09:06:25,875 - loss nan, lr: 0.000125, epoch: 17, step: 99100, eta: 2.01 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.45691 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 99200, eta: 2.00 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11849 sec, avg_samples: 32.00000, ips: 540.14374 images/sec
Training: 2023-09-28 09:06:37,729 - loss nan, lr: 0.000125, epoch: 17, step: 99200, eta: 2.00 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11849 sec, avg_samples: 32.00000, ips: 540.14374 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 99300, eta: 2.00 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11863 sec, avg_samples: 32.00000, ips: 539.49315 images/sec
Training: 2023-09-28 09:06:49,599 - loss nan, lr: 0.000125, epoch: 17, step: 99300, eta: 2.00 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11863 sec, avg_samples: 32.00000, ips: 539.49315 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 99400, eta: 1.99 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11876 sec, avg_samples: 32.00000, ips: 538.91980 images/sec
Training: 2023-09-28 09:07:01,481 - loss nan, lr: 0.000125, epoch: 17, step: 99400, eta: 1.99 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11876 sec, avg_samples: 32.00000, ips: 538.91980 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 99500, eta: 1.99 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11909 sec, avg_samples: 32.00000, ips: 537.40153 images/sec
Training: 2023-09-28 09:07:13,396 - loss nan, lr: 0.000125, epoch: 17, step: 99500, eta: 1.99 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11909 sec, avg_samples: 32.00000, ips: 537.40153 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 99600, eta: 1.98 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11906 sec, avg_samples: 32.00000, ips: 537.54263 images/sec
Training: 2023-09-28 09:07:25,309 - loss nan, lr: 0.000125, epoch: 17, step: 99600, eta: 1.98 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11906 sec, avg_samples: 32.00000, ips: 537.54263 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 99700, eta: 1.98 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11858 sec, avg_samples: 32.00000, ips: 539.72599 images/sec
Training: 2023-09-28 09:07:37,173 - loss nan, lr: 0.000125, epoch: 17, step: 99700, eta: 1.98 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11858 sec, avg_samples: 32.00000, ips: 539.72599 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 99800, eta: 1.97 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11865 sec, avg_samples: 32.00000, ips: 539.41527 images/sec
Training: 2023-09-28 09:07:49,042 - loss nan, lr: 0.000125, epoch: 17, step: 99800, eta: 1.97 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11865 sec, avg_samples: 32.00000, ips: 539.41527 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 99900, eta: 1.97 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11843 sec, avg_samples: 32.00000, ips: 540.42126 images/sec
Training: 2023-09-28 09:08:00,891 - loss nan, lr: 0.000125, epoch: 17, step: 99900, eta: 1.97 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11843 sec, avg_samples: 32.00000, ips: 540.42126 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 100000, eta: 1.96 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.66325 images/sec
Training: 2023-09-28 09:08:12,734 - loss nan, lr: 0.000125, epoch: 17, step: 100000, eta: 1.96 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.66325 images/sec
INFO:root:[lfw][100000]XNorm: 0.520320
Training: 2023-09-28 09:08:40,787 - [lfw][100000]XNorm: 0.520320
INFO:root:[lfw][100000]Accuracy-Flip: 0.89917+-0.02000
Training: 2023-09-28 09:08:40,787 - [lfw][100000]Accuracy-Flip: 0.89917+-0.02000
INFO:root:[lfw][100000]Accuracy-Highest: 0.90150
Training: 2023-09-28 09:08:40,787 - [lfw][100000]Accuracy-Highest: 0.90150
INFO:root:test time: 28.0531
Training: 2023-09-28 09:08:40,787 - test time: 28.0531
INFO:root:[cfp_fp][100000]XNorm: 0.487220
Training: 2023-09-28 09:09:13,126 - [cfp_fp][100000]XNorm: 0.487220
INFO:root:[cfp_fp][100000]Accuracy-Flip: 0.64043+-0.01867
Training: 2023-09-28 09:09:13,126 - [cfp_fp][100000]Accuracy-Flip: 0.64043+-0.01867
INFO:root:[cfp_fp][100000]Accuracy-Highest: 0.65071
Training: 2023-09-28 09:09:13,126 - [cfp_fp][100000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.3389
Training: 2023-09-28 09:09:13,126 - test time: 32.3389
INFO:root:[agedb_30][100000]XNorm: 0.493587
Training: 2023-09-28 09:09:40,993 - [agedb_30][100000]XNorm: 0.493587
INFO:root:[agedb_30][100000]Accuracy-Flip: 0.69117+-0.02199
Training: 2023-09-28 09:09:40,993 - [agedb_30][100000]Accuracy-Flip: 0.69117+-0.02199
INFO:root:[agedb_30][100000]Accuracy-Highest: 0.70083
Training: 2023-09-28 09:09:40,993 - [agedb_30][100000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.8670
Training: 2023-09-28 09:09:40,993 - test time: 27.8670
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 100100, eta: 1.97 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11848 sec, avg_samples: 32.00000, ips: 540.17767 images/sec
Training: 2023-09-28 09:09:52,848 - loss nan, lr: 0.000125, epoch: 17, step: 100100, eta: 1.97 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11848 sec, avg_samples: 32.00000, ips: 540.17767 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 100200, eta: 1.96 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.63014 images/sec
Training: 2023-09-28 09:10:04,669 - loss nan, lr: 0.000125, epoch: 17, step: 100200, eta: 1.96 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.63014 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 100300, eta: 1.96 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.18671 images/sec
Training: 2023-09-28 09:10:16,500 - loss nan, lr: 0.000125, epoch: 17, step: 100300, eta: 1.96 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.18671 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 100400, eta: 1.95 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.50554 images/sec
Training: 2023-09-28 09:10:28,324 - loss nan, lr: 0.000125, epoch: 17, step: 100400, eta: 1.95 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.50554 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 100500, eta: 1.95 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11873 sec, avg_samples: 32.00000, ips: 539.04453 images/sec
Training: 2023-09-28 09:10:40,204 - loss nan, lr: 0.000125, epoch: 17, step: 100500, eta: 1.95 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11873 sec, avg_samples: 32.00000, ips: 539.04453 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 100600, eta: 1.94 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11874 sec, avg_samples: 32.00000, ips: 538.99016 images/sec
Training: 2023-09-28 09:10:52,085 - loss nan, lr: 0.000125, epoch: 17, step: 100600, eta: 1.94 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11874 sec, avg_samples: 32.00000, ips: 538.99016 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 100700, eta: 1.94 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11864 sec, avg_samples: 32.00000, ips: 539.44429 images/sec
Training: 2023-09-28 09:11:03,954 - loss nan, lr: 0.000125, epoch: 17, step: 100700, eta: 1.94 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11864 sec, avg_samples: 32.00000, ips: 539.44429 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 100800, eta: 1.93 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11859 sec, avg_samples: 32.00000, ips: 539.65449 images/sec
Training: 2023-09-28 09:11:15,820 - loss nan, lr: 0.000125, epoch: 17, step: 100800, eta: 1.93 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11859 sec, avg_samples: 32.00000, ips: 539.65449 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 100900, eta: 1.93 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11876 sec, avg_samples: 32.00000, ips: 538.91655 images/sec
Training: 2023-09-28 09:11:27,703 - loss nan, lr: 0.000125, epoch: 17, step: 100900, eta: 1.93 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11876 sec, avg_samples: 32.00000, ips: 538.91655 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 101000, eta: 1.92 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11877 sec, avg_samples: 32.00000, ips: 538.86398 images/sec
Training: 2023-09-28 09:11:39,586 - loss nan, lr: 0.000125, epoch: 17, step: 101000, eta: 1.92 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11877 sec, avg_samples: 32.00000, ips: 538.86398 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 101100, eta: 1.92 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.75007 images/sec
Training: 2023-09-28 09:11:51,427 - loss nan, lr: 0.000125, epoch: 17, step: 101100, eta: 1.92 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.75007 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 101200, eta: 1.91 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11838 sec, avg_samples: 32.00000, ips: 540.63415 images/sec
Training: 2023-09-28 09:12:03,272 - loss nan, lr: 0.000125, epoch: 17, step: 101200, eta: 1.91 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11838 sec, avg_samples: 32.00000, ips: 540.63415 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 101300, eta: 1.91 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.49322 images/sec
Training: 2023-09-28 09:12:15,096 - loss nan, lr: 0.000125, epoch: 17, step: 101300, eta: 1.91 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.49322 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 101400, eta: 1.90 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.58586 images/sec
Training: 2023-09-28 09:12:26,918 - loss nan, lr: 0.000125, epoch: 17, step: 101400, eta: 1.90 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.58586 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 101500, eta: 1.90 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.45691 images/sec
Training: 2023-09-28 09:12:38,743 - loss nan, lr: 0.000125, epoch: 17, step: 101500, eta: 1.90 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.45691 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 101600, eta: 1.89 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.46886 images/sec
Training: 2023-09-28 09:12:50,568 - loss nan, lr: 0.000125, epoch: 17, step: 101600, eta: 1.89 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.46886 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 101700, eta: 1.89 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.46072 images/sec
Training: 2023-09-28 09:13:02,392 - loss nan, lr: 0.000125, epoch: 17, step: 101700, eta: 1.89 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.46072 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 101800, eta: 1.88 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.49530 images/sec
Training: 2023-09-28 09:13:14,216 - loss nan, lr: 0.000125, epoch: 17, step: 101800, eta: 1.88 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.49530 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 101900, eta: 1.88 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.60529 images/sec
Training: 2023-09-28 09:13:26,037 - loss nan, lr: 0.000125, epoch: 17, step: 101900, eta: 1.88 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.60529 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 102000, eta: 1.87 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.50071 images/sec
Training: 2023-09-28 09:13:37,861 - loss nan, lr: 0.000125, epoch: 17, step: 102000, eta: 1.87 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.50071 images/sec
INFO:root:[lfw][102000]XNorm: 0.520263
Training: 2023-09-28 09:14:05,793 - [lfw][102000]XNorm: 0.520263
INFO:root:[lfw][102000]Accuracy-Flip: 0.89567+-0.01753
Training: 2023-09-28 09:14:05,794 - [lfw][102000]Accuracy-Flip: 0.89567+-0.01753
INFO:root:[lfw][102000]Accuracy-Highest: 0.90150
Training: 2023-09-28 09:14:05,794 - [lfw][102000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9326
Training: 2023-09-28 09:14:05,794 - test time: 27.9326
INFO:root:[cfp_fp][102000]XNorm: 0.485774
Training: 2023-09-28 09:14:38,051 - [cfp_fp][102000]XNorm: 0.485774
INFO:root:[cfp_fp][102000]Accuracy-Flip: 0.64229+-0.01677
Training: 2023-09-28 09:14:38,051 - [cfp_fp][102000]Accuracy-Flip: 0.64229+-0.01677
INFO:root:[cfp_fp][102000]Accuracy-Highest: 0.65071
Training: 2023-09-28 09:14:38,051 - [cfp_fp][102000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2578
Training: 2023-09-28 09:14:38,051 - test time: 32.2578
INFO:root:[agedb_30][102000]XNorm: 0.493559
Training: 2023-09-28 09:15:05,758 - [agedb_30][102000]XNorm: 0.493559
INFO:root:[agedb_30][102000]Accuracy-Flip: 0.68167+-0.02083
Training: 2023-09-28 09:15:05,758 - [agedb_30][102000]Accuracy-Flip: 0.68167+-0.02083
INFO:root:[agedb_30][102000]Accuracy-Highest: 0.70083
Training: 2023-09-28 09:15:05,758 - [agedb_30][102000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7064
Training: 2023-09-28 09:15:05,758 - test time: 27.7064
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 102100, eta: 1.88 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11851 sec, avg_samples: 32.00000, ips: 540.05226 images/sec
Training: 2023-09-28 09:15:17,615 - loss nan, lr: 0.000125, epoch: 17, step: 102100, eta: 1.88 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11851 sec, avg_samples: 32.00000, ips: 540.05226 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 102200, eta: 1.87 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11824 sec, avg_samples: 32.00000, ips: 541.27305 images/sec
Training: 2023-09-28 09:15:29,445 - loss nan, lr: 0.000125, epoch: 17, step: 102200, eta: 1.87 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11824 sec, avg_samples: 32.00000, ips: 541.27305 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 102300, eta: 1.87 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11840 sec, avg_samples: 32.00000, ips: 540.53866 images/sec
Training: 2023-09-28 09:15:41,291 - loss nan, lr: 0.000125, epoch: 17, step: 102300, eta: 1.87 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11840 sec, avg_samples: 32.00000, ips: 540.53866 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 102400, eta: 1.86 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11834 sec, avg_samples: 32.00000, ips: 540.81919 images/sec
Training: 2023-09-28 09:15:53,130 - loss nan, lr: 0.000125, epoch: 17, step: 102400, eta: 1.86 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11834 sec, avg_samples: 32.00000, ips: 540.81919 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 102500, eta: 1.86 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11867 sec, avg_samples: 32.00000, ips: 539.30316 images/sec
Training: 2023-09-28 09:16:05,002 - loss nan, lr: 0.000125, epoch: 17, step: 102500, eta: 1.86 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11867 sec, avg_samples: 32.00000, ips: 539.30316 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 102600, eta: 1.85 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.36696 images/sec
Training: 2023-09-28 09:16:16,829 - loss nan, lr: 0.000125, epoch: 17, step: 102600, eta: 1.85 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.36696 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 102700, eta: 1.85 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11871 sec, avg_samples: 32.00000, ips: 539.13742 images/sec
Training: 2023-09-28 09:16:28,704 - loss nan, lr: 0.000125, epoch: 17, step: 102700, eta: 1.85 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11871 sec, avg_samples: 32.00000, ips: 539.13742 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 102800, eta: 1.84 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11886 sec, avg_samples: 32.00000, ips: 538.44946 images/sec
Training: 2023-09-28 09:16:40,595 - loss nan, lr: 0.000125, epoch: 17, step: 102800, eta: 1.84 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11886 sec, avg_samples: 32.00000, ips: 538.44946 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 102900, eta: 1.84 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11864 sec, avg_samples: 32.00000, ips: 539.45798 images/sec
Training: 2023-09-28 09:16:52,463 - loss nan, lr: 0.000125, epoch: 17, step: 102900, eta: 1.84 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11864 sec, avg_samples: 32.00000, ips: 539.45798 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 103000, eta: 1.83 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11862 sec, avg_samples: 32.00000, ips: 539.55928 images/sec
Training: 2023-09-28 09:17:04,329 - loss nan, lr: 0.000125, epoch: 17, step: 103000, eta: 1.83 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11862 sec, avg_samples: 32.00000, ips: 539.55928 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 103100, eta: 1.83 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11858 sec, avg_samples: 32.00000, ips: 539.71863 images/sec
Training: 2023-09-28 09:17:16,192 - loss nan, lr: 0.000125, epoch: 17, step: 103100, eta: 1.83 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11858 sec, avg_samples: 32.00000, ips: 539.71863 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 103200, eta: 1.82 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11857 sec, avg_samples: 32.00000, ips: 539.77666 images/sec
Training: 2023-09-28 09:17:28,053 - loss nan, lr: 0.000125, epoch: 17, step: 103200, eta: 1.82 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11857 sec, avg_samples: 32.00000, ips: 539.77666 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 103300, eta: 1.82 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11867 sec, avg_samples: 32.00000, ips: 539.32857 images/sec
Training: 2023-09-28 09:17:39,925 - loss nan, lr: 0.000125, epoch: 17, step: 103300, eta: 1.82 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11867 sec, avg_samples: 32.00000, ips: 539.32857 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 17, step: 103400, eta: 1.81 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11861 sec, avg_samples: 32.00000, ips: 539.60530 images/sec
Training: 2023-09-28 09:17:51,790 - loss nan, lr: 0.000125, epoch: 17, step: 103400, eta: 1.81 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11861 sec, avg_samples: 32.00000, ips: 539.60530 images/sec
INFO:root:Save model to MS1M_v3_arcface_static_0.1/FresResNet50/17.
Training: 2023-09-28 09:17:59,752 - Save model to MS1M_v3_arcface_static_0.1/FresResNet50/17.
INFO:root:Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/16.
Training: 2023-09-28 09:17:59,752 - Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/16.
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 103500, eta: 1.81 hours, avg_reader_cost: 0.00179 sec, avg_batch_cost: 0.04436 sec, avg_samples: 11.52000, ips: 519.44160 images/sec
Training: 2023-09-28 09:18:04,219 - loss nan, lr: 0.000125, epoch: 18, step: 103500, eta: 1.81 hours, avg_reader_cost: 0.00179 sec, avg_batch_cost: 0.04436 sec, avg_samples: 11.52000, ips: 519.44160 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 103600, eta: 1.80 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.32151 images/sec
Training: 2023-09-28 09:18:16,048 - loss nan, lr: 0.000125, epoch: 18, step: 103600, eta: 1.80 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.32151 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 103700, eta: 1.80 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.41253 images/sec
Training: 2023-09-28 09:18:27,874 - loss nan, lr: 0.000125, epoch: 18, step: 103700, eta: 1.80 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.41253 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 103800, eta: 1.79 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11848 sec, avg_samples: 32.00000, ips: 540.15376 images/sec
Training: 2023-09-28 09:18:39,728 - loss nan, lr: 0.000125, epoch: 18, step: 103800, eta: 1.79 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11848 sec, avg_samples: 32.00000, ips: 540.15376 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 103900, eta: 1.79 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.05852 images/sec
Training: 2023-09-28 09:18:51,561 - loss nan, lr: 0.000125, epoch: 18, step: 103900, eta: 1.79 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.05852 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 104000, eta: 1.78 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.58073 images/sec
Training: 2023-09-28 09:19:03,383 - loss nan, lr: 0.000125, epoch: 18, step: 104000, eta: 1.78 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.58073 images/sec
INFO:root:[lfw][104000]XNorm: 0.517650
Training: 2023-09-28 09:19:31,369 - [lfw][104000]XNorm: 0.517650
INFO:root:[lfw][104000]Accuracy-Flip: 0.89900+-0.01562
Training: 2023-09-28 09:19:31,369 - [lfw][104000]Accuracy-Flip: 0.89900+-0.01562
INFO:root:[lfw][104000]Accuracy-Highest: 0.90150
Training: 2023-09-28 09:19:31,369 - [lfw][104000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9861
Training: 2023-09-28 09:19:31,369 - test time: 27.9861
INFO:root:[cfp_fp][104000]XNorm: 0.483900
Training: 2023-09-28 09:20:03,602 - [cfp_fp][104000]XNorm: 0.483900
INFO:root:[cfp_fp][104000]Accuracy-Flip: 0.64471+-0.01795
Training: 2023-09-28 09:20:03,602 - [cfp_fp][104000]Accuracy-Flip: 0.64471+-0.01795
INFO:root:[cfp_fp][104000]Accuracy-Highest: 0.65071
Training: 2023-09-28 09:20:03,602 - [cfp_fp][104000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2333
Training: 2023-09-28 09:20:03,602 - test time: 32.2333
INFO:root:[agedb_30][104000]XNorm: 0.491951
Training: 2023-09-28 09:20:31,331 - [agedb_30][104000]XNorm: 0.491951
INFO:root:[agedb_30][104000]Accuracy-Flip: 0.68950+-0.01934
Training: 2023-09-28 09:20:31,331 - [agedb_30][104000]Accuracy-Flip: 0.68950+-0.01934
INFO:root:[agedb_30][104000]Accuracy-Highest: 0.70083
Training: 2023-09-28 09:20:31,331 - [agedb_30][104000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7284
Training: 2023-09-28 09:20:31,331 - test time: 27.7284
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 104100, eta: 1.79 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11803 sec, avg_samples: 32.00000, ips: 542.21797 images/sec
Training: 2023-09-28 09:20:43,139 - loss nan, lr: 0.000125, epoch: 18, step: 104100, eta: 1.79 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11803 sec, avg_samples: 32.00000, ips: 542.21797 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 104200, eta: 1.78 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11810 sec, avg_samples: 32.00000, ips: 541.89357 images/sec
Training: 2023-09-28 09:20:54,955 - loss nan, lr: 0.000125, epoch: 18, step: 104200, eta: 1.78 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11810 sec, avg_samples: 32.00000, ips: 541.89357 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 104300, eta: 1.78 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11889 sec, avg_samples: 32.00000, ips: 538.30869 images/sec
Training: 2023-09-28 09:21:06,850 - loss nan, lr: 0.000125, epoch: 18, step: 104300, eta: 1.78 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11889 sec, avg_samples: 32.00000, ips: 538.30869 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 104400, eta: 1.77 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11873 sec, avg_samples: 32.00000, ips: 539.04157 images/sec
Training: 2023-09-28 09:21:18,729 - loss nan, lr: 0.000125, epoch: 18, step: 104400, eta: 1.77 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11873 sec, avg_samples: 32.00000, ips: 539.04157 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 104500, eta: 1.77 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11834 sec, avg_samples: 32.00000, ips: 540.80498 images/sec
Training: 2023-09-28 09:21:30,568 - loss nan, lr: 0.000125, epoch: 18, step: 104500, eta: 1.77 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11834 sec, avg_samples: 32.00000, ips: 540.80498 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 104600, eta: 1.76 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.66350 images/sec
Training: 2023-09-28 09:21:42,388 - loss nan, lr: 0.000125, epoch: 18, step: 104600, eta: 1.76 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.66350 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 104700, eta: 1.76 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11872 sec, avg_samples: 32.00000, ips: 539.06976 images/sec
Training: 2023-09-28 09:21:54,267 - loss nan, lr: 0.000125, epoch: 18, step: 104700, eta: 1.76 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11872 sec, avg_samples: 32.00000, ips: 539.06976 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 104800, eta: 1.75 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11885 sec, avg_samples: 32.00000, ips: 538.47792 images/sec
Training: 2023-09-28 09:22:06,158 - loss nan, lr: 0.000125, epoch: 18, step: 104800, eta: 1.75 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11885 sec, avg_samples: 32.00000, ips: 538.47792 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 104900, eta: 1.75 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11889 sec, avg_samples: 32.00000, ips: 538.31599 images/sec
Training: 2023-09-28 09:22:18,054 - loss nan, lr: 0.000125, epoch: 18, step: 104900, eta: 1.75 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11889 sec, avg_samples: 32.00000, ips: 538.31599 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 105000, eta: 1.74 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11891 sec, avg_samples: 32.00000, ips: 538.22391 images/sec
Training: 2023-09-28 09:22:29,951 - loss nan, lr: 0.000125, epoch: 18, step: 105000, eta: 1.74 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11891 sec, avg_samples: 32.00000, ips: 538.22391 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 105100, eta: 1.74 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11888 sec, avg_samples: 32.00000, ips: 538.36110 images/sec
Training: 2023-09-28 09:22:41,845 - loss nan, lr: 0.000125, epoch: 18, step: 105100, eta: 1.74 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11888 sec, avg_samples: 32.00000, ips: 538.36110 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 105200, eta: 1.73 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11888 sec, avg_samples: 32.00000, ips: 538.37023 images/sec
Training: 2023-09-28 09:22:53,739 - loss nan, lr: 0.000125, epoch: 18, step: 105200, eta: 1.73 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11888 sec, avg_samples: 32.00000, ips: 538.37023 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 105300, eta: 1.73 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11887 sec, avg_samples: 32.00000, ips: 538.42499 images/sec
Training: 2023-09-28 09:23:05,632 - loss nan, lr: 0.000125, epoch: 18, step: 105300, eta: 1.73 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11887 sec, avg_samples: 32.00000, ips: 538.42499 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 105400, eta: 1.72 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11887 sec, avg_samples: 32.00000, ips: 538.41573 images/sec
Training: 2023-09-28 09:23:17,525 - loss nan, lr: 0.000125, epoch: 18, step: 105400, eta: 1.72 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11887 sec, avg_samples: 32.00000, ips: 538.41573 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 105500, eta: 1.72 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11886 sec, avg_samples: 32.00000, ips: 538.45101 images/sec
Training: 2023-09-28 09:23:29,417 - loss nan, lr: 0.000125, epoch: 18, step: 105500, eta: 1.72 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11886 sec, avg_samples: 32.00000, ips: 538.45101 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 105600, eta: 1.71 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11844 sec, avg_samples: 32.00000, ips: 540.34443 images/sec
Training: 2023-09-28 09:23:41,268 - loss nan, lr: 0.000125, epoch: 18, step: 105600, eta: 1.71 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11844 sec, avg_samples: 32.00000, ips: 540.34443 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 105700, eta: 1.71 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11863 sec, avg_samples: 32.00000, ips: 539.47340 images/sec
Training: 2023-09-28 09:23:53,138 - loss nan, lr: 0.000125, epoch: 18, step: 105700, eta: 1.71 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11863 sec, avg_samples: 32.00000, ips: 539.47340 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 105800, eta: 1.70 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11840 sec, avg_samples: 32.00000, ips: 540.54819 images/sec
Training: 2023-09-28 09:24:04,984 - loss nan, lr: 0.000125, epoch: 18, step: 105800, eta: 1.70 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11840 sec, avg_samples: 32.00000, ips: 540.54819 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 105900, eta: 1.70 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.41039 images/sec
Training: 2023-09-28 09:24:16,809 - loss nan, lr: 0.000125, epoch: 18, step: 105900, eta: 1.70 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.41039 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 106000, eta: 1.69 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.67735 images/sec
Training: 2023-09-28 09:24:28,629 - loss nan, lr: 0.000125, epoch: 18, step: 106000, eta: 1.69 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11815 sec, avg_samples: 32.00000, ips: 541.67735 images/sec
INFO:root:[lfw][106000]XNorm: 0.516442
Training: 2023-09-28 09:24:56,571 - [lfw][106000]XNorm: 0.516442
INFO:root:[lfw][106000]Accuracy-Flip: 0.90100+-0.01876
Training: 2023-09-28 09:24:56,571 - [lfw][106000]Accuracy-Flip: 0.90100+-0.01876
INFO:root:[lfw][106000]Accuracy-Highest: 0.90150
Training: 2023-09-28 09:24:56,571 - [lfw][106000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9415
Training: 2023-09-28 09:24:56,571 - test time: 27.9415
INFO:root:[cfp_fp][106000]XNorm: 0.482253
Training: 2023-09-28 09:25:28,817 - [cfp_fp][106000]XNorm: 0.482253
INFO:root:[cfp_fp][106000]Accuracy-Flip: 0.64329+-0.01851
Training: 2023-09-28 09:25:28,817 - [cfp_fp][106000]Accuracy-Flip: 0.64329+-0.01851
INFO:root:[cfp_fp][106000]Accuracy-Highest: 0.65071
Training: 2023-09-28 09:25:28,818 - [cfp_fp][106000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2465
Training: 2023-09-28 09:25:28,818 - test time: 32.2465
INFO:root:[agedb_30][106000]XNorm: 0.490307
Training: 2023-09-28 09:25:56,525 - [agedb_30][106000]XNorm: 0.490307
INFO:root:[agedb_30][106000]Accuracy-Flip: 0.69033+-0.02235
Training: 2023-09-28 09:25:56,525 - [agedb_30][106000]Accuracy-Flip: 0.69033+-0.02235
INFO:root:[agedb_30][106000]Accuracy-Highest: 0.70083
Training: 2023-09-28 09:25:56,525 - [agedb_30][106000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7079
Training: 2023-09-28 09:25:56,526 - test time: 27.7079
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 106100, eta: 1.70 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11798 sec, avg_samples: 32.00000, ips: 542.46911 images/sec
Training: 2023-09-28 09:26:08,329 - loss nan, lr: 0.000125, epoch: 18, step: 106100, eta: 1.70 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11798 sec, avg_samples: 32.00000, ips: 542.46911 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 106200, eta: 1.69 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11800 sec, avg_samples: 32.00000, ips: 542.35909 images/sec
Training: 2023-09-28 09:26:20,134 - loss nan, lr: 0.000125, epoch: 18, step: 106200, eta: 1.69 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11800 sec, avg_samples: 32.00000, ips: 542.35909 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 106300, eta: 1.69 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11852 sec, avg_samples: 32.00000, ips: 540.00266 images/sec
Training: 2023-09-28 09:26:31,993 - loss nan, lr: 0.000125, epoch: 18, step: 106300, eta: 1.69 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11852 sec, avg_samples: 32.00000, ips: 540.00266 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 106400, eta: 1.68 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11858 sec, avg_samples: 32.00000, ips: 539.73935 images/sec
Training: 2023-09-28 09:26:43,857 - loss nan, lr: 0.000125, epoch: 18, step: 106400, eta: 1.68 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11858 sec, avg_samples: 32.00000, ips: 539.73935 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 106500, eta: 1.68 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11836 sec, avg_samples: 32.00000, ips: 540.73341 images/sec
Training: 2023-09-28 09:26:55,698 - loss nan, lr: 0.000125, epoch: 18, step: 106500, eta: 1.68 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11836 sec, avg_samples: 32.00000, ips: 540.73341 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 106600, eta: 1.67 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11811 sec, avg_samples: 32.00000, ips: 541.85143 images/sec
Training: 2023-09-28 09:27:07,514 - loss nan, lr: 0.000125, epoch: 18, step: 106600, eta: 1.67 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11811 sec, avg_samples: 32.00000, ips: 541.85143 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 106700, eta: 1.67 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.54247 images/sec
Training: 2023-09-28 09:27:19,337 - loss nan, lr: 0.000125, epoch: 18, step: 106700, eta: 1.67 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.54247 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 106800, eta: 1.66 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11841 sec, avg_samples: 32.00000, ips: 540.50236 images/sec
Training: 2023-09-28 09:27:31,183 - loss nan, lr: 0.000125, epoch: 18, step: 106800, eta: 1.66 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11841 sec, avg_samples: 32.00000, ips: 540.50236 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 106900, eta: 1.66 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.73247 images/sec
Training: 2023-09-28 09:27:43,002 - loss nan, lr: 0.000125, epoch: 18, step: 106900, eta: 1.66 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11814 sec, avg_samples: 32.00000, ips: 541.73247 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 107000, eta: 1.65 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.63971 images/sec
Training: 2023-09-28 09:27:54,822 - loss nan, lr: 0.000125, epoch: 18, step: 107000, eta: 1.65 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.63971 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 107100, eta: 1.65 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11811 sec, avg_samples: 32.00000, ips: 541.87000 images/sec
Training: 2023-09-28 09:28:06,638 - loss nan, lr: 0.000125, epoch: 18, step: 107100, eta: 1.65 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11811 sec, avg_samples: 32.00000, ips: 541.87000 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 107200, eta: 1.64 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.79037 images/sec
Training: 2023-09-28 09:28:18,456 - loss nan, lr: 0.000125, epoch: 18, step: 107200, eta: 1.64 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.79037 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 107300, eta: 1.64 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11811 sec, avg_samples: 32.00000, ips: 541.84878 images/sec
Training: 2023-09-28 09:28:30,272 - loss nan, lr: 0.000125, epoch: 18, step: 107300, eta: 1.64 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11811 sec, avg_samples: 32.00000, ips: 541.84878 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 107400, eta: 1.63 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.84263 images/sec
Training: 2023-09-28 09:28:42,088 - loss nan, lr: 0.000125, epoch: 18, step: 107400, eta: 1.63 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.84263 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 107500, eta: 1.63 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.53692 images/sec
Training: 2023-09-28 09:28:53,911 - loss nan, lr: 0.000125, epoch: 18, step: 107500, eta: 1.63 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.53692 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 107600, eta: 1.62 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.65010 images/sec
Training: 2023-09-28 09:29:05,732 - loss nan, lr: 0.000125, epoch: 18, step: 107600, eta: 1.62 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.65010 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 107700, eta: 1.62 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.65126 images/sec
Training: 2023-09-28 09:29:17,552 - loss nan, lr: 0.000125, epoch: 18, step: 107700, eta: 1.62 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.65126 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 107800, eta: 1.61 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.62460 images/sec
Training: 2023-09-28 09:29:29,374 - loss nan, lr: 0.000125, epoch: 18, step: 107800, eta: 1.61 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.62460 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 107900, eta: 1.61 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.41374 images/sec
Training: 2023-09-28 09:29:41,199 - loss nan, lr: 0.000125, epoch: 18, step: 107900, eta: 1.61 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.41374 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 108000, eta: 1.60 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.51646 images/sec
Training: 2023-09-28 09:29:53,023 - loss nan, lr: 0.000125, epoch: 18, step: 108000, eta: 1.60 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.51646 images/sec
INFO:root:[lfw][108000]XNorm: 0.514178
Training: 2023-09-28 09:30:20,956 - [lfw][108000]XNorm: 0.514178
INFO:root:[lfw][108000]Accuracy-Flip: 0.89733+-0.01753
Training: 2023-09-28 09:30:20,957 - [lfw][108000]Accuracy-Flip: 0.89733+-0.01753
INFO:root:[lfw][108000]Accuracy-Highest: 0.90150
Training: 2023-09-28 09:30:20,957 - [lfw][108000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9335
Training: 2023-09-28 09:30:20,957 - test time: 27.9335
INFO:root:[cfp_fp][108000]XNorm: 0.481647
Training: 2023-09-28 09:30:53,211 - [cfp_fp][108000]XNorm: 0.481647
INFO:root:[cfp_fp][108000]Accuracy-Flip: 0.64700+-0.01757
Training: 2023-09-28 09:30:53,211 - [cfp_fp][108000]Accuracy-Flip: 0.64700+-0.01757
INFO:root:[cfp_fp][108000]Accuracy-Highest: 0.65071
Training: 2023-09-28 09:30:53,211 - [cfp_fp][108000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2545
Training: 2023-09-28 09:30:53,211 - test time: 32.2545
INFO:root:[agedb_30][108000]XNorm: 0.487820
Training: 2023-09-28 09:31:20,988 - [agedb_30][108000]XNorm: 0.487820
INFO:root:[agedb_30][108000]Accuracy-Flip: 0.67933+-0.02115
Training: 2023-09-28 09:31:20,989 - [agedb_30][108000]Accuracy-Flip: 0.67933+-0.02115
INFO:root:[agedb_30][108000]Accuracy-Highest: 0.70083
Training: 2023-09-28 09:31:20,989 - [agedb_30][108000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7774
Training: 2023-09-28 09:31:20,989 - test time: 27.7774
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 108100, eta: 1.61 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11786 sec, avg_samples: 32.00000, ips: 543.02758 images/sec
Training: 2023-09-28 09:31:32,780 - loss nan, lr: 0.000125, epoch: 18, step: 108100, eta: 1.61 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11786 sec, avg_samples: 32.00000, ips: 543.02758 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 108200, eta: 1.60 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11794 sec, avg_samples: 32.00000, ips: 542.63209 images/sec
Training: 2023-09-28 09:31:44,579 - loss nan, lr: 0.000125, epoch: 18, step: 108200, eta: 1.60 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11794 sec, avg_samples: 32.00000, ips: 542.63209 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 108300, eta: 1.60 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11810 sec, avg_samples: 32.00000, ips: 541.90481 images/sec
Training: 2023-09-28 09:31:56,395 - loss nan, lr: 0.000125, epoch: 18, step: 108300, eta: 1.60 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11810 sec, avg_samples: 32.00000, ips: 541.90481 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 108400, eta: 1.59 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.46610 images/sec
Training: 2023-09-28 09:32:08,220 - loss nan, lr: 0.000125, epoch: 18, step: 108400, eta: 1.59 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.46610 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 108500, eta: 1.59 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.39138 images/sec
Training: 2023-09-28 09:32:20,046 - loss nan, lr: 0.000125, epoch: 18, step: 108500, eta: 1.59 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.39138 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 108600, eta: 1.58 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.11649 images/sec
Training: 2023-09-28 09:32:31,879 - loss nan, lr: 0.000125, epoch: 18, step: 108600, eta: 1.58 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.11649 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 108700, eta: 1.58 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11852 sec, avg_samples: 32.00000, ips: 539.98173 images/sec
Training: 2023-09-28 09:32:43,738 - loss nan, lr: 0.000125, epoch: 18, step: 108700, eta: 1.58 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11852 sec, avg_samples: 32.00000, ips: 539.98173 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 108800, eta: 1.57 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11849 sec, avg_samples: 32.00000, ips: 540.13664 images/sec
Training: 2023-09-28 09:32:55,594 - loss nan, lr: 0.000125, epoch: 18, step: 108800, eta: 1.57 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11849 sec, avg_samples: 32.00000, ips: 540.13664 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 108900, eta: 1.57 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11850 sec, avg_samples: 32.00000, ips: 540.07457 images/sec
Training: 2023-09-28 09:33:07,451 - loss nan, lr: 0.000125, epoch: 18, step: 108900, eta: 1.57 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11850 sec, avg_samples: 32.00000, ips: 540.07457 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 109000, eta: 1.56 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11850 sec, avg_samples: 32.00000, ips: 540.08094 images/sec
Training: 2023-09-28 09:33:19,308 - loss nan, lr: 0.000125, epoch: 18, step: 109000, eta: 1.56 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11850 sec, avg_samples: 32.00000, ips: 540.08094 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 109100, eta: 1.56 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11856 sec, avg_samples: 32.00000, ips: 539.79660 images/sec
Training: 2023-09-28 09:33:31,171 - loss nan, lr: 0.000125, epoch: 18, step: 109100, eta: 1.56 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11856 sec, avg_samples: 32.00000, ips: 539.79660 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 18, step: 109200, eta: 1.55 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.66987 images/sec
Training: 2023-09-28 09:33:43,014 - loss nan, lr: 0.000125, epoch: 18, step: 109200, eta: 1.55 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.66987 images/sec
INFO:root:Save model to MS1M_v3_arcface_static_0.1/FresResNet50/18.
Training: 2023-09-28 09:33:44,785 - Save model to MS1M_v3_arcface_static_0.1/FresResNet50/18.
INFO:root:Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/17.
Training: 2023-09-28 09:33:44,785 - Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/17.
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 109300, eta: 1.55 hours, avg_reader_cost: 0.00180 sec, avg_batch_cost: 0.10587 sec, avg_samples: 28.16000, ips: 531.98660 images/sec
Training: 2023-09-28 09:33:55,407 - loss nan, lr: 0.000125, epoch: 19, step: 109300, eta: 1.55 hours, avg_reader_cost: 0.00180 sec, avg_batch_cost: 0.10587 sec, avg_samples: 28.16000, ips: 531.98660 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 109400, eta: 1.54 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.04429 images/sec
Training: 2023-09-28 09:34:07,241 - loss nan, lr: 0.000125, epoch: 19, step: 109400, eta: 1.54 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.04429 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 109500, eta: 1.54 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.64542 images/sec
Training: 2023-09-28 09:34:19,062 - loss nan, lr: 0.000125, epoch: 19, step: 109500, eta: 1.54 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.64542 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 109600, eta: 1.53 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.66375 images/sec
Training: 2023-09-28 09:34:30,905 - loss nan, lr: 0.000125, epoch: 19, step: 109600, eta: 1.53 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.66375 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 109700, eta: 1.53 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11825 sec, avg_samples: 32.00000, ips: 541.22686 images/sec
Training: 2023-09-28 09:34:42,735 - loss nan, lr: 0.000125, epoch: 19, step: 109700, eta: 1.53 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11825 sec, avg_samples: 32.00000, ips: 541.22686 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 109800, eta: 1.52 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.15474 images/sec
Training: 2023-09-28 09:34:54,568 - loss nan, lr: 0.000125, epoch: 19, step: 109800, eta: 1.52 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.15474 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 109900, eta: 1.52 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.50188 images/sec
Training: 2023-09-28 09:35:06,392 - loss nan, lr: 0.000125, epoch: 19, step: 109900, eta: 1.52 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11819 sec, avg_samples: 32.00000, ips: 541.50188 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 110000, eta: 1.51 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11854 sec, avg_samples: 32.00000, ips: 539.90554 images/sec
Training: 2023-09-28 09:35:18,252 - loss nan, lr: 0.000125, epoch: 19, step: 110000, eta: 1.51 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11854 sec, avg_samples: 32.00000, ips: 539.90554 images/sec
INFO:root:[lfw][110000]XNorm: 0.516208
Training: 2023-09-28 09:35:46,230 - [lfw][110000]XNorm: 0.516208
INFO:root:[lfw][110000]Accuracy-Flip: 0.89733+-0.01962
Training: 2023-09-28 09:35:46,230 - [lfw][110000]Accuracy-Flip: 0.89733+-0.01962
INFO:root:[lfw][110000]Accuracy-Highest: 0.90150
Training: 2023-09-28 09:35:46,230 - [lfw][110000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9783
Training: 2023-09-28 09:35:46,230 - test time: 27.9783
INFO:root:[cfp_fp][110000]XNorm: 0.481960
Training: 2023-09-28 09:36:18,517 - [cfp_fp][110000]XNorm: 0.481960
INFO:root:[cfp_fp][110000]Accuracy-Flip: 0.63586+-0.01357
Training: 2023-09-28 09:36:18,517 - [cfp_fp][110000]Accuracy-Flip: 0.63586+-0.01357
INFO:root:[cfp_fp][110000]Accuracy-Highest: 0.65071
Training: 2023-09-28 09:36:18,517 - [cfp_fp][110000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2866
Training: 2023-09-28 09:36:18,517 - test time: 32.2866
INFO:root:[agedb_30][110000]XNorm: 0.489163
Training: 2023-09-28 09:36:46,274 - [agedb_30][110000]XNorm: 0.489163
INFO:root:[agedb_30][110000]Accuracy-Flip: 0.68683+-0.01970
Training: 2023-09-28 09:36:46,274 - [agedb_30][110000]Accuracy-Flip: 0.68683+-0.01970
INFO:root:[agedb_30][110000]Accuracy-Highest: 0.70083
Training: 2023-09-28 09:36:46,274 - [agedb_30][110000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7570
Training: 2023-09-28 09:36:46,274 - test time: 27.7570
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 110100, eta: 1.52 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11807 sec, avg_samples: 32.00000, ips: 542.06225 images/sec
Training: 2023-09-28 09:36:58,087 - loss nan, lr: 0.000125, epoch: 19, step: 110100, eta: 1.52 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11807 sec, avg_samples: 32.00000, ips: 542.06225 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 110200, eta: 1.51 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11808 sec, avg_samples: 32.00000, ips: 542.02300 images/sec
Training: 2023-09-28 09:37:09,901 - loss nan, lr: 0.000125, epoch: 19, step: 110200, eta: 1.51 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11808 sec, avg_samples: 32.00000, ips: 542.02300 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 110300, eta: 1.51 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11825 sec, avg_samples: 32.00000, ips: 541.23947 images/sec
Training: 2023-09-28 09:37:21,732 - loss nan, lr: 0.000125, epoch: 19, step: 110300, eta: 1.51 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11825 sec, avg_samples: 32.00000, ips: 541.23947 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 110400, eta: 1.50 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.75028 images/sec
Training: 2023-09-28 09:37:33,574 - loss nan, lr: 0.000125, epoch: 19, step: 110400, eta: 1.50 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.75028 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 110500, eta: 1.50 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11836 sec, avg_samples: 32.00000, ips: 540.74065 images/sec
Training: 2023-09-28 09:37:45,415 - loss nan, lr: 0.000125, epoch: 19, step: 110500, eta: 1.50 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11836 sec, avg_samples: 32.00000, ips: 540.74065 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 110600, eta: 1.49 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.78541 images/sec
Training: 2023-09-28 09:37:57,256 - loss nan, lr: 0.000125, epoch: 19, step: 110600, eta: 1.49 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.78541 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 110700, eta: 1.49 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.69394 images/sec
Training: 2023-09-28 09:38:09,099 - loss nan, lr: 0.000125, epoch: 19, step: 110700, eta: 1.49 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.69394 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 110800, eta: 1.48 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.54211 images/sec
Training: 2023-09-28 09:38:20,922 - loss nan, lr: 0.000125, epoch: 19, step: 110800, eta: 1.48 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11818 sec, avg_samples: 32.00000, ips: 541.54211 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 110900, eta: 1.48 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.64910 images/sec
Training: 2023-09-28 09:38:32,743 - loss nan, lr: 0.000125, epoch: 19, step: 110900, eta: 1.48 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.64910 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 111000, eta: 1.47 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11839 sec, avg_samples: 32.00000, ips: 540.56741 images/sec
Training: 2023-09-28 09:38:44,588 - loss nan, lr: 0.000125, epoch: 19, step: 111000, eta: 1.47 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11839 sec, avg_samples: 32.00000, ips: 540.56741 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 111100, eta: 1.47 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11854 sec, avg_samples: 32.00000, ips: 539.90291 images/sec
Training: 2023-09-28 09:38:56,448 - loss nan, lr: 0.000125, epoch: 19, step: 111100, eta: 1.47 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11854 sec, avg_samples: 32.00000, ips: 539.90291 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 111200, eta: 1.46 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11857 sec, avg_samples: 32.00000, ips: 539.75734 images/sec
Training: 2023-09-28 09:39:08,312 - loss nan, lr: 0.000125, epoch: 19, step: 111200, eta: 1.46 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11857 sec, avg_samples: 32.00000, ips: 539.75734 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 111300, eta: 1.46 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11840 sec, avg_samples: 32.00000, ips: 540.55442 images/sec
Training: 2023-09-28 09:39:20,157 - loss nan, lr: 0.000125, epoch: 19, step: 111300, eta: 1.46 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11840 sec, avg_samples: 32.00000, ips: 540.55442 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 111400, eta: 1.45 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.15455 images/sec
Training: 2023-09-28 09:39:31,988 - loss nan, lr: 0.000125, epoch: 19, step: 111400, eta: 1.45 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.15455 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 111500, eta: 1.45 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11824 sec, avg_samples: 32.00000, ips: 541.26865 images/sec
Training: 2023-09-28 09:39:43,817 - loss nan, lr: 0.000125, epoch: 19, step: 111500, eta: 1.45 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11824 sec, avg_samples: 32.00000, ips: 541.26865 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 111600, eta: 1.44 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.17622 images/sec
Training: 2023-09-28 09:39:55,648 - loss nan, lr: 0.000125, epoch: 19, step: 111600, eta: 1.44 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.17622 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 111700, eta: 1.44 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11867 sec, avg_samples: 32.00000, ips: 539.31616 images/sec
Training: 2023-09-28 09:40:07,520 - loss nan, lr: 0.000125, epoch: 19, step: 111700, eta: 1.44 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11867 sec, avg_samples: 32.00000, ips: 539.31616 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 111800, eta: 1.43 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11882 sec, avg_samples: 32.00000, ips: 538.62908 images/sec
Training: 2023-09-28 09:40:19,407 - loss nan, lr: 0.000125, epoch: 19, step: 111800, eta: 1.43 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11882 sec, avg_samples: 32.00000, ips: 538.62908 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 111900, eta: 1.43 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11858 sec, avg_samples: 32.00000, ips: 539.73966 images/sec
Training: 2023-09-28 09:40:31,270 - loss nan, lr: 0.000125, epoch: 19, step: 111900, eta: 1.43 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11858 sec, avg_samples: 32.00000, ips: 539.73966 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 112000, eta: 1.42 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11838 sec, avg_samples: 32.00000, ips: 540.64587 images/sec
Training: 2023-09-28 09:40:43,113 - loss nan, lr: 0.000125, epoch: 19, step: 112000, eta: 1.42 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11838 sec, avg_samples: 32.00000, ips: 540.64587 images/sec
INFO:root:[lfw][112000]XNorm: 0.511340
Training: 2023-09-28 09:41:11,118 - [lfw][112000]XNorm: 0.511340
INFO:root:[lfw][112000]Accuracy-Flip: 0.90100+-0.01862
Training: 2023-09-28 09:41:11,118 - [lfw][112000]Accuracy-Flip: 0.90100+-0.01862
INFO:root:[lfw][112000]Accuracy-Highest: 0.90150
Training: 2023-09-28 09:41:11,118 - [lfw][112000]Accuracy-Highest: 0.90150
INFO:root:test time: 28.0041
Training: 2023-09-28 09:41:11,118 - test time: 28.0041
INFO:root:[cfp_fp][112000]XNorm: 0.478112
Training: 2023-09-28 09:41:43,421 - [cfp_fp][112000]XNorm: 0.478112
INFO:root:[cfp_fp][112000]Accuracy-Flip: 0.63914+-0.01675
Training: 2023-09-28 09:41:43,422 - [cfp_fp][112000]Accuracy-Flip: 0.63914+-0.01675
INFO:root:[cfp_fp][112000]Accuracy-Highest: 0.65071
Training: 2023-09-28 09:41:43,422 - [cfp_fp][112000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.3038
Training: 2023-09-28 09:41:43,422 - test time: 32.3038
INFO:root:[agedb_30][112000]XNorm: 0.485180
Training: 2023-09-28 09:42:11,224 - [agedb_30][112000]XNorm: 0.485180
INFO:root:[agedb_30][112000]Accuracy-Flip: 0.68817+-0.02020
Training: 2023-09-28 09:42:11,225 - [agedb_30][112000]Accuracy-Flip: 0.68817+-0.02020
INFO:root:[agedb_30][112000]Accuracy-Highest: 0.70083
Training: 2023-09-28 09:42:11,225 - [agedb_30][112000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.8030
Training: 2023-09-28 09:42:11,225 - test time: 27.8030
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 112100, eta: 1.43 hours, avg_reader_cost: 0.00008 sec, avg_batch_cost: 0.11842 sec, avg_samples: 32.00000, ips: 540.44246 images/sec
Training: 2023-09-28 09:42:23,074 - loss nan, lr: 0.000125, epoch: 19, step: 112100, eta: 1.43 hours, avg_reader_cost: 0.00008 sec, avg_batch_cost: 0.11842 sec, avg_samples: 32.00000, ips: 540.44246 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 112200, eta: 1.42 hours, avg_reader_cost: 0.00008 sec, avg_batch_cost: 0.11841 sec, avg_samples: 32.00000, ips: 540.48897 images/sec
Training: 2023-09-28 09:42:34,923 - loss nan, lr: 0.000125, epoch: 19, step: 112200, eta: 1.42 hours, avg_reader_cost: 0.00008 sec, avg_batch_cost: 0.11841 sec, avg_samples: 32.00000, ips: 540.48897 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 112300, eta: 1.42 hours, avg_reader_cost: 0.00008 sec, avg_batch_cost: 0.11852 sec, avg_samples: 32.00000, ips: 539.99078 images/sec
Training: 2023-09-28 09:42:46,782 - loss nan, lr: 0.000125, epoch: 19, step: 112300, eta: 1.42 hours, avg_reader_cost: 0.00008 sec, avg_batch_cost: 0.11852 sec, avg_samples: 32.00000, ips: 539.99078 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 112400, eta: 1.41 hours, avg_reader_cost: 0.00008 sec, avg_batch_cost: 0.11872 sec, avg_samples: 32.00000, ips: 539.06744 images/sec
Training: 2023-09-28 09:42:58,662 - loss nan, lr: 0.000125, epoch: 19, step: 112400, eta: 1.41 hours, avg_reader_cost: 0.00008 sec, avg_batch_cost: 0.11872 sec, avg_samples: 32.00000, ips: 539.06744 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 112500, eta: 1.41 hours, avg_reader_cost: 0.00008 sec, avg_batch_cost: 0.11872 sec, avg_samples: 32.00000, ips: 539.07746 images/sec
Training: 2023-09-28 09:43:10,541 - loss nan, lr: 0.000125, epoch: 19, step: 112500, eta: 1.41 hours, avg_reader_cost: 0.00008 sec, avg_batch_cost: 0.11872 sec, avg_samples: 32.00000, ips: 539.07746 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 112600, eta: 1.40 hours, avg_reader_cost: 0.00008 sec, avg_batch_cost: 0.11871 sec, avg_samples: 32.00000, ips: 539.10677 images/sec
Training: 2023-09-28 09:43:22,420 - loss nan, lr: 0.000125, epoch: 19, step: 112600, eta: 1.40 hours, avg_reader_cost: 0.00008 sec, avg_batch_cost: 0.11871 sec, avg_samples: 32.00000, ips: 539.10677 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 112700, eta: 1.40 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11871 sec, avg_samples: 32.00000, ips: 539.11923 images/sec
Training: 2023-09-28 09:43:34,299 - loss nan, lr: 0.000125, epoch: 19, step: 112700, eta: 1.40 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11871 sec, avg_samples: 32.00000, ips: 539.11923 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 112800, eta: 1.39 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11893 sec, avg_samples: 32.00000, ips: 538.13319 images/sec
Training: 2023-09-28 09:43:46,198 - loss nan, lr: 0.000125, epoch: 19, step: 112800, eta: 1.39 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11893 sec, avg_samples: 32.00000, ips: 538.13319 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 112900, eta: 1.39 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11862 sec, avg_samples: 32.00000, ips: 539.53772 images/sec
Training: 2023-09-28 09:43:58,065 - loss nan, lr: 0.000125, epoch: 19, step: 112900, eta: 1.39 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11862 sec, avg_samples: 32.00000, ips: 539.53772 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 113000, eta: 1.38 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11896 sec, avg_samples: 32.00000, ips: 537.99873 images/sec
Training: 2023-09-28 09:44:09,966 - loss nan, lr: 0.000125, epoch: 19, step: 113000, eta: 1.38 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11896 sec, avg_samples: 32.00000, ips: 537.99873 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 113100, eta: 1.38 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11841 sec, avg_samples: 32.00000, ips: 540.49279 images/sec
Training: 2023-09-28 09:44:21,813 - loss nan, lr: 0.000125, epoch: 19, step: 113100, eta: 1.38 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11841 sec, avg_samples: 32.00000, ips: 540.49279 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 113200, eta: 1.37 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.92310 images/sec
Training: 2023-09-28 09:44:33,650 - loss nan, lr: 0.000125, epoch: 19, step: 113200, eta: 1.37 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.92310 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 113300, eta: 1.37 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11874 sec, avg_samples: 32.00000, ips: 539.01321 images/sec
Training: 2023-09-28 09:44:45,529 - loss nan, lr: 0.000125, epoch: 19, step: 113300, eta: 1.37 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11874 sec, avg_samples: 32.00000, ips: 539.01321 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 113400, eta: 1.36 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11914 sec, avg_samples: 32.00000, ips: 537.18251 images/sec
Training: 2023-09-28 09:44:57,449 - loss nan, lr: 0.000125, epoch: 19, step: 113400, eta: 1.36 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11914 sec, avg_samples: 32.00000, ips: 537.18251 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 113500, eta: 1.36 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11869 sec, avg_samples: 32.00000, ips: 539.21150 images/sec
Training: 2023-09-28 09:45:09,324 - loss nan, lr: 0.000125, epoch: 19, step: 113500, eta: 1.36 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11869 sec, avg_samples: 32.00000, ips: 539.21150 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 113600, eta: 1.35 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11833 sec, avg_samples: 32.00000, ips: 540.85901 images/sec
Training: 2023-09-28 09:45:21,162 - loss nan, lr: 0.000125, epoch: 19, step: 113600, eta: 1.35 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11833 sec, avg_samples: 32.00000, ips: 540.85901 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 113700, eta: 1.35 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11834 sec, avg_samples: 32.00000, ips: 540.83650 images/sec
Training: 2023-09-28 09:45:33,000 - loss nan, lr: 0.000125, epoch: 19, step: 113700, eta: 1.35 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11834 sec, avg_samples: 32.00000, ips: 540.83650 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 113800, eta: 1.34 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11833 sec, avg_samples: 32.00000, ips: 540.85907 images/sec
Training: 2023-09-28 09:45:44,838 - loss nan, lr: 0.000125, epoch: 19, step: 113800, eta: 1.34 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11833 sec, avg_samples: 32.00000, ips: 540.85907 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 113900, eta: 1.34 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.76788 images/sec
Training: 2023-09-28 09:45:56,678 - loss nan, lr: 0.000125, epoch: 19, step: 113900, eta: 1.34 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.76788 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 114000, eta: 1.33 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11839 sec, avg_samples: 32.00000, ips: 540.60534 images/sec
Training: 2023-09-28 09:46:08,521 - loss nan, lr: 0.000125, epoch: 19, step: 114000, eta: 1.33 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11839 sec, avg_samples: 32.00000, ips: 540.60534 images/sec
INFO:root:[lfw][114000]XNorm: 0.511093
Training: 2023-09-28 09:46:36,529 - [lfw][114000]XNorm: 0.511093
INFO:root:[lfw][114000]Accuracy-Flip: 0.89750+-0.01758
Training: 2023-09-28 09:46:36,529 - [lfw][114000]Accuracy-Flip: 0.89750+-0.01758
INFO:root:[lfw][114000]Accuracy-Highest: 0.90150
Training: 2023-09-28 09:46:36,529 - [lfw][114000]Accuracy-Highest: 0.90150
INFO:root:test time: 28.0082
Training: 2023-09-28 09:46:36,530 - test time: 28.0082
INFO:root:[cfp_fp][114000]XNorm: 0.476264
Training: 2023-09-28 09:47:08,832 - [cfp_fp][114000]XNorm: 0.476264
INFO:root:[cfp_fp][114000]Accuracy-Flip: 0.64457+-0.02126
Training: 2023-09-28 09:47:08,832 - [cfp_fp][114000]Accuracy-Flip: 0.64457+-0.02126
INFO:root:[cfp_fp][114000]Accuracy-Highest: 0.65071
Training: 2023-09-28 09:47:08,832 - [cfp_fp][114000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.3027
Training: 2023-09-28 09:47:08,832 - test time: 32.3027
INFO:root:[agedb_30][114000]XNorm: 0.484486
Training: 2023-09-28 09:47:36,624 - [agedb_30][114000]XNorm: 0.484486
INFO:root:[agedb_30][114000]Accuracy-Flip: 0.68783+-0.01944
Training: 2023-09-28 09:47:36,624 - [agedb_30][114000]Accuracy-Flip: 0.68783+-0.01944
INFO:root:[agedb_30][114000]Accuracy-Highest: 0.70083
Training: 2023-09-28 09:47:36,624 - [agedb_30][114000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7916
Training: 2023-09-28 09:47:36,624 - test time: 27.7916
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 114100, eta: 1.34 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.60513 images/sec
Training: 2023-09-28 09:47:48,447 - loss nan, lr: 0.000125, epoch: 19, step: 114100, eta: 1.34 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11817 sec, avg_samples: 32.00000, ips: 541.60513 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 114200, eta: 1.33 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.33045 images/sec
Training: 2023-09-28 09:48:00,276 - loss nan, lr: 0.000125, epoch: 19, step: 114200, eta: 1.33 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11823 sec, avg_samples: 32.00000, ips: 541.33045 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 114300, eta: 1.33 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.04082 images/sec
Training: 2023-09-28 09:48:12,112 - loss nan, lr: 0.000125, epoch: 19, step: 114300, eta: 1.33 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.04082 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 114400, eta: 1.32 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11848 sec, avg_samples: 32.00000, ips: 540.16237 images/sec
Training: 2023-09-28 09:48:23,967 - loss nan, lr: 0.000125, epoch: 19, step: 114400, eta: 1.32 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11848 sec, avg_samples: 32.00000, ips: 540.16237 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 114500, eta: 1.32 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11887 sec, avg_samples: 32.00000, ips: 538.39946 images/sec
Training: 2023-09-28 09:48:35,860 - loss nan, lr: 0.000125, epoch: 19, step: 114500, eta: 1.32 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11887 sec, avg_samples: 32.00000, ips: 538.39946 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 114600, eta: 1.31 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11878 sec, avg_samples: 32.00000, ips: 538.79381 images/sec
Training: 2023-09-28 09:48:47,745 - loss nan, lr: 0.000125, epoch: 19, step: 114600, eta: 1.31 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11878 sec, avg_samples: 32.00000, ips: 538.79381 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 114700, eta: 1.31 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11866 sec, avg_samples: 32.00000, ips: 539.37110 images/sec
Training: 2023-09-28 09:48:59,617 - loss nan, lr: 0.000125, epoch: 19, step: 114700, eta: 1.31 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11866 sec, avg_samples: 32.00000, ips: 539.37110 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 114800, eta: 1.30 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11870 sec, avg_samples: 32.00000, ips: 539.16967 images/sec
Training: 2023-09-28 09:49:11,493 - loss nan, lr: 0.000125, epoch: 19, step: 114800, eta: 1.30 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11870 sec, avg_samples: 32.00000, ips: 539.16967 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 19, step: 114900, eta: 1.30 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11869 sec, avg_samples: 32.00000, ips: 539.22737 images/sec
Training: 2023-09-28 09:49:23,368 - loss nan, lr: 0.000125, epoch: 19, step: 114900, eta: 1.30 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11869 sec, avg_samples: 32.00000, ips: 539.22737 images/sec
INFO:root:Save model to MS1M_v3_arcface_static_0.1/FresResNet50/19.
Training: 2023-09-28 09:49:30,846 - Save model to MS1M_v3_arcface_static_0.1/FresResNet50/19.
INFO:root:Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/18.
Training: 2023-09-28 09:49:30,846 - Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/18.
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 115000, eta: 1.29 hours, avg_reader_cost: 0.00181 sec, avg_batch_cost: 0.04913 sec, avg_samples: 12.80000, ips: 521.07057 images/sec
Training: 2023-09-28 09:49:35,791 - loss nan, lr: 0.000125, epoch: 20, step: 115000, eta: 1.29 hours, avg_reader_cost: 0.00181 sec, avg_batch_cost: 0.04913 sec, avg_samples: 12.80000, ips: 521.07057 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 115100, eta: 1.29 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11839 sec, avg_samples: 32.00000, ips: 540.57342 images/sec
Training: 2023-09-28 09:49:47,636 - loss nan, lr: 0.000125, epoch: 20, step: 115100, eta: 1.29 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11839 sec, avg_samples: 32.00000, ips: 540.57342 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 115200, eta: 1.28 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11839 sec, avg_samples: 32.00000, ips: 540.57023 images/sec
Training: 2023-09-28 09:49:59,481 - loss nan, lr: 0.000125, epoch: 20, step: 115200, eta: 1.28 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11839 sec, avg_samples: 32.00000, ips: 540.57023 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 115300, eta: 1.28 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.78772 images/sec
Training: 2023-09-28 09:50:11,321 - loss nan, lr: 0.000125, epoch: 20, step: 115300, eta: 1.28 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.78772 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 115400, eta: 1.27 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11836 sec, avg_samples: 32.00000, ips: 540.73566 images/sec
Training: 2023-09-28 09:50:23,162 - loss nan, lr: 0.000125, epoch: 20, step: 115400, eta: 1.27 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11836 sec, avg_samples: 32.00000, ips: 540.73566 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 115500, eta: 1.27 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11839 sec, avg_samples: 32.00000, ips: 540.56517 images/sec
Training: 2023-09-28 09:50:35,006 - loss nan, lr: 0.000125, epoch: 20, step: 115500, eta: 1.27 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11839 sec, avg_samples: 32.00000, ips: 540.56517 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 115600, eta: 1.26 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11838 sec, avg_samples: 32.00000, ips: 540.65464 images/sec
Training: 2023-09-28 09:50:46,848 - loss nan, lr: 0.000125, epoch: 20, step: 115600, eta: 1.26 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11838 sec, avg_samples: 32.00000, ips: 540.65464 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 115700, eta: 1.26 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11843 sec, avg_samples: 32.00000, ips: 540.40995 images/sec
Training: 2023-09-28 09:50:58,696 - loss nan, lr: 0.000125, epoch: 20, step: 115700, eta: 1.26 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11843 sec, avg_samples: 32.00000, ips: 540.40995 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 115800, eta: 1.25 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11877 sec, avg_samples: 32.00000, ips: 538.87923 images/sec
Training: 2023-09-28 09:51:10,578 - loss nan, lr: 0.000125, epoch: 20, step: 115800, eta: 1.25 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11877 sec, avg_samples: 32.00000, ips: 538.87923 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 115900, eta: 1.25 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11879 sec, avg_samples: 32.00000, ips: 538.78479 images/sec
Training: 2023-09-28 09:51:22,462 - loss nan, lr: 0.000125, epoch: 20, step: 115900, eta: 1.25 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11879 sec, avg_samples: 32.00000, ips: 538.78479 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 116000, eta: 1.24 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11850 sec, avg_samples: 32.00000, ips: 540.09880 images/sec
Training: 2023-09-28 09:51:34,316 - loss nan, lr: 0.000125, epoch: 20, step: 116000, eta: 1.24 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11850 sec, avg_samples: 32.00000, ips: 540.09880 images/sec
INFO:root:[lfw][116000]XNorm: 0.511872
Training: 2023-09-28 09:52:02,334 - [lfw][116000]XNorm: 0.511872
INFO:root:[lfw][116000]Accuracy-Flip: 0.90033+-0.01545
Training: 2023-09-28 09:52:02,334 - [lfw][116000]Accuracy-Flip: 0.90033+-0.01545
INFO:root:[lfw][116000]Accuracy-Highest: 0.90150
Training: 2023-09-28 09:52:02,334 - [lfw][116000]Accuracy-Highest: 0.90150
INFO:root:test time: 28.0175
Training: 2023-09-28 09:52:02,334 - test time: 28.0175
INFO:root:[cfp_fp][116000]XNorm: 0.479640
Training: 2023-09-28 09:52:34,660 - [cfp_fp][116000]XNorm: 0.479640
INFO:root:[cfp_fp][116000]Accuracy-Flip: 0.63786+-0.01900
Training: 2023-09-28 09:52:34,660 - [cfp_fp][116000]Accuracy-Flip: 0.63786+-0.01900
INFO:root:[cfp_fp][116000]Accuracy-Highest: 0.65071
Training: 2023-09-28 09:52:34,660 - [cfp_fp][116000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.3258
Training: 2023-09-28 09:52:34,660 - test time: 32.3258
INFO:root:[agedb_30][116000]XNorm: 0.484903
Training: 2023-09-28 09:53:02,447 - [agedb_30][116000]XNorm: 0.484903
INFO:root:[agedb_30][116000]Accuracy-Flip: 0.68900+-0.02478
Training: 2023-09-28 09:53:02,447 - [agedb_30][116000]Accuracy-Flip: 0.68900+-0.02478
INFO:root:[agedb_30][116000]Accuracy-Highest: 0.70083
Training: 2023-09-28 09:53:02,448 - [agedb_30][116000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7875
Training: 2023-09-28 09:53:02,448 - test time: 27.7875
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 116100, eta: 1.24 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.26346 images/sec
Training: 2023-09-28 09:53:14,255 - loss nan, lr: 0.000125, epoch: 20, step: 116100, eta: 1.24 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.26346 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 116200, eta: 1.24 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.92032 images/sec
Training: 2023-09-28 09:53:26,092 - loss nan, lr: 0.000125, epoch: 20, step: 116200, eta: 1.24 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.92032 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 116300, eta: 1.24 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11804 sec, avg_samples: 32.00000, ips: 542.16744 images/sec
Training: 2023-09-28 09:53:37,901 - loss nan, lr: 0.000125, epoch: 20, step: 116300, eta: 1.24 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11804 sec, avg_samples: 32.00000, ips: 542.16744 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 116400, eta: 1.23 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.42334 images/sec
Training: 2023-09-28 09:53:49,727 - loss nan, lr: 0.000125, epoch: 20, step: 116400, eta: 1.23 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11821 sec, avg_samples: 32.00000, ips: 541.42334 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 116500, eta: 1.23 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11830 sec, avg_samples: 32.00000, ips: 541.00499 images/sec
Training: 2023-09-28 09:54:01,562 - loss nan, lr: 0.000125, epoch: 20, step: 116500, eta: 1.23 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11830 sec, avg_samples: 32.00000, ips: 541.00499 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 116600, eta: 1.22 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.91827 images/sec
Training: 2023-09-28 09:54:13,398 - loss nan, lr: 0.000125, epoch: 20, step: 116600, eta: 1.22 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.91827 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 116700, eta: 1.22 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.90901 images/sec
Training: 2023-09-28 09:54:25,235 - loss nan, lr: 0.000125, epoch: 20, step: 116700, eta: 1.22 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.90901 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 116800, eta: 1.21 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11857 sec, avg_samples: 32.00000, ips: 539.76317 images/sec
Training: 2023-09-28 09:54:37,099 - loss nan, lr: 0.000125, epoch: 20, step: 116800, eta: 1.21 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11857 sec, avg_samples: 32.00000, ips: 539.76317 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 116900, eta: 1.21 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11857 sec, avg_samples: 32.00000, ips: 539.78302 images/sec
Training: 2023-09-28 09:54:48,961 - loss nan, lr: 0.000125, epoch: 20, step: 116900, eta: 1.21 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11857 sec, avg_samples: 32.00000, ips: 539.78302 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 117000, eta: 1.20 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11859 sec, avg_samples: 32.00000, ips: 539.68844 images/sec
Training: 2023-09-28 09:55:00,826 - loss nan, lr: 0.000125, epoch: 20, step: 117000, eta: 1.20 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11859 sec, avg_samples: 32.00000, ips: 539.68844 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 117100, eta: 1.20 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11855 sec, avg_samples: 32.00000, ips: 539.86042 images/sec
Training: 2023-09-28 09:55:12,688 - loss nan, lr: 0.000125, epoch: 20, step: 117100, eta: 1.20 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11855 sec, avg_samples: 32.00000, ips: 539.86042 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 117200, eta: 1.19 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11857 sec, avg_samples: 32.00000, ips: 539.78087 images/sec
Training: 2023-09-28 09:55:24,550 - loss nan, lr: 0.000125, epoch: 20, step: 117200, eta: 1.19 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11857 sec, avg_samples: 32.00000, ips: 539.78087 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 117300, eta: 1.19 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11854 sec, avg_samples: 32.00000, ips: 539.88012 images/sec
Training: 2023-09-28 09:55:36,411 - loss nan, lr: 0.000125, epoch: 20, step: 117300, eta: 1.19 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11854 sec, avg_samples: 32.00000, ips: 539.88012 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 117400, eta: 1.18 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11854 sec, avg_samples: 32.00000, ips: 539.89376 images/sec
Training: 2023-09-28 09:55:48,272 - loss nan, lr: 0.000125, epoch: 20, step: 117400, eta: 1.18 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11854 sec, avg_samples: 32.00000, ips: 539.89376 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 117500, eta: 1.18 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11853 sec, avg_samples: 32.00000, ips: 539.92950 images/sec
Training: 2023-09-28 09:56:00,131 - loss nan, lr: 0.000125, epoch: 20, step: 117500, eta: 1.18 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11853 sec, avg_samples: 32.00000, ips: 539.92950 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 117600, eta: 1.17 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11906 sec, avg_samples: 32.00000, ips: 537.52343 images/sec
Training: 2023-09-28 09:56:12,044 - loss nan, lr: 0.000125, epoch: 20, step: 117600, eta: 1.17 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11906 sec, avg_samples: 32.00000, ips: 537.52343 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 117700, eta: 1.17 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11908 sec, avg_samples: 32.00000, ips: 537.46023 images/sec
Training: 2023-09-28 09:56:23,958 - loss nan, lr: 0.000125, epoch: 20, step: 117700, eta: 1.17 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11908 sec, avg_samples: 32.00000, ips: 537.46023 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 117800, eta: 1.16 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11909 sec, avg_samples: 32.00000, ips: 537.42160 images/sec
Training: 2023-09-28 09:56:35,873 - loss nan, lr: 0.000125, epoch: 20, step: 117800, eta: 1.16 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11909 sec, avg_samples: 32.00000, ips: 537.42160 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 117900, eta: 1.16 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11909 sec, avg_samples: 32.00000, ips: 537.41852 images/sec
Training: 2023-09-28 09:56:47,789 - loss nan, lr: 0.000125, epoch: 20, step: 117900, eta: 1.16 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11909 sec, avg_samples: 32.00000, ips: 537.41852 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 118000, eta: 1.15 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11910 sec, avg_samples: 32.00000, ips: 537.37905 images/sec
Training: 2023-09-28 09:56:59,705 - loss nan, lr: 0.000125, epoch: 20, step: 118000, eta: 1.15 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11910 sec, avg_samples: 32.00000, ips: 537.37905 images/sec
INFO:root:[lfw][118000]XNorm: 0.516875
Training: 2023-09-28 09:57:27,641 - [lfw][118000]XNorm: 0.516875
INFO:root:[lfw][118000]Accuracy-Flip: 0.90000+-0.01883
Training: 2023-09-28 09:57:27,641 - [lfw][118000]Accuracy-Flip: 0.90000+-0.01883
INFO:root:[lfw][118000]Accuracy-Highest: 0.90150
Training: 2023-09-28 09:57:27,641 - [lfw][118000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9362
Training: 2023-09-28 09:57:27,641 - test time: 27.9362
INFO:root:[cfp_fp][118000]XNorm: 0.483442
Training: 2023-09-28 09:57:59,911 - [cfp_fp][118000]XNorm: 0.483442
INFO:root:[cfp_fp][118000]Accuracy-Flip: 0.64600+-0.01767
Training: 2023-09-28 09:57:59,911 - [cfp_fp][118000]Accuracy-Flip: 0.64600+-0.01767
INFO:root:[cfp_fp][118000]Accuracy-Highest: 0.65071
Training: 2023-09-28 09:57:59,911 - [cfp_fp][118000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2698
Training: 2023-09-28 09:57:59,911 - test time: 32.2698
INFO:root:[agedb_30][118000]XNorm: 0.490089
Training: 2023-09-28 09:58:27,668 - [agedb_30][118000]XNorm: 0.490089
INFO:root:[agedb_30][118000]Accuracy-Flip: 0.68467+-0.02099
Training: 2023-09-28 09:58:27,668 - [agedb_30][118000]Accuracy-Flip: 0.68467+-0.02099
INFO:root:[agedb_30][118000]Accuracy-Highest: 0.70083
Training: 2023-09-28 09:58:27,668 - [agedb_30][118000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7567
Training: 2023-09-28 09:58:27,668 - test time: 27.7567
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 118100, eta: 1.15 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11851 sec, avg_samples: 32.00000, ips: 540.04067 images/sec
Training: 2023-09-28 09:58:39,525 - loss nan, lr: 0.000125, epoch: 20, step: 118100, eta: 1.15 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11851 sec, avg_samples: 32.00000, ips: 540.04067 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 118200, eta: 1.15 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11859 sec, avg_samples: 32.00000, ips: 539.67622 images/sec
Training: 2023-09-28 09:58:51,390 - loss nan, lr: 0.000125, epoch: 20, step: 118200, eta: 1.15 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11859 sec, avg_samples: 32.00000, ips: 539.67622 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 118300, eta: 1.15 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11852 sec, avg_samples: 32.00000, ips: 539.97852 images/sec
Training: 2023-09-28 09:59:03,249 - loss nan, lr: 0.000125, epoch: 20, step: 118300, eta: 1.15 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11852 sec, avg_samples: 32.00000, ips: 539.97852 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 118400, eta: 1.14 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11853 sec, avg_samples: 32.00000, ips: 539.92836 images/sec
Training: 2023-09-28 09:59:15,109 - loss nan, lr: 0.000125, epoch: 20, step: 118400, eta: 1.14 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11853 sec, avg_samples: 32.00000, ips: 539.92836 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 118500, eta: 1.14 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11879 sec, avg_samples: 32.00000, ips: 538.77626 images/sec
Training: 2023-09-28 09:59:26,994 - loss nan, lr: 0.000125, epoch: 20, step: 118500, eta: 1.14 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11879 sec, avg_samples: 32.00000, ips: 538.77626 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 118600, eta: 1.13 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11912 sec, avg_samples: 32.00000, ips: 537.25183 images/sec
Training: 2023-09-28 09:59:38,912 - loss nan, lr: 0.000125, epoch: 20, step: 118600, eta: 1.13 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11912 sec, avg_samples: 32.00000, ips: 537.25183 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 118700, eta: 1.13 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11912 sec, avg_samples: 32.00000, ips: 537.28357 images/sec
Training: 2023-09-28 09:59:50,830 - loss nan, lr: 0.000125, epoch: 20, step: 118700, eta: 1.13 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11912 sec, avg_samples: 32.00000, ips: 537.28357 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 118800, eta: 1.12 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11911 sec, avg_samples: 32.00000, ips: 537.30155 images/sec
Training: 2023-09-28 10:00:02,748 - loss nan, lr: 0.000125, epoch: 20, step: 118800, eta: 1.12 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11911 sec, avg_samples: 32.00000, ips: 537.30155 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 118900, eta: 1.12 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11908 sec, avg_samples: 32.00000, ips: 537.46599 images/sec
Training: 2023-09-28 10:00:14,662 - loss nan, lr: 0.000125, epoch: 20, step: 118900, eta: 1.12 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11908 sec, avg_samples: 32.00000, ips: 537.46599 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 119000, eta: 1.11 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11904 sec, avg_samples: 32.00000, ips: 537.64074 images/sec
Training: 2023-09-28 10:00:26,572 - loss nan, lr: 0.000125, epoch: 20, step: 119000, eta: 1.11 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11904 sec, avg_samples: 32.00000, ips: 537.64074 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 119100, eta: 1.11 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11905 sec, avg_samples: 32.00000, ips: 537.59577 images/sec
Training: 2023-09-28 10:00:38,483 - loss nan, lr: 0.000125, epoch: 20, step: 119100, eta: 1.11 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11905 sec, avg_samples: 32.00000, ips: 537.59577 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 119200, eta: 1.10 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11900 sec, avg_samples: 32.00000, ips: 537.83756 images/sec
Training: 2023-09-28 10:00:50,388 - loss nan, lr: 0.000125, epoch: 20, step: 119200, eta: 1.10 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11900 sec, avg_samples: 32.00000, ips: 537.83756 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 119300, eta: 1.10 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11914 sec, avg_samples: 32.00000, ips: 537.19660 images/sec
Training: 2023-09-28 10:01:02,308 - loss nan, lr: 0.000125, epoch: 20, step: 119300, eta: 1.10 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11914 sec, avg_samples: 32.00000, ips: 537.19660 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 119400, eta: 1.09 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11905 sec, avg_samples: 32.00000, ips: 537.57889 images/sec
Training: 2023-09-28 10:01:14,220 - loss nan, lr: 0.000125, epoch: 20, step: 119400, eta: 1.09 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11905 sec, avg_samples: 32.00000, ips: 537.57889 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 119500, eta: 1.09 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11865 sec, avg_samples: 32.00000, ips: 539.38378 images/sec
Training: 2023-09-28 10:01:26,091 - loss nan, lr: 0.000125, epoch: 20, step: 119500, eta: 1.09 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11865 sec, avg_samples: 32.00000, ips: 539.38378 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 119600, eta: 1.08 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11848 sec, avg_samples: 32.00000, ips: 540.17207 images/sec
Training: 2023-09-28 10:01:37,945 - loss nan, lr: 0.000125, epoch: 20, step: 119600, eta: 1.08 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11848 sec, avg_samples: 32.00000, ips: 540.17207 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 119700, eta: 1.08 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11848 sec, avg_samples: 32.00000, ips: 540.19307 images/sec
Training: 2023-09-28 10:01:49,799 - loss nan, lr: 0.000125, epoch: 20, step: 119700, eta: 1.08 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11848 sec, avg_samples: 32.00000, ips: 540.19307 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 119800, eta: 1.07 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11847 sec, avg_samples: 32.00000, ips: 540.20363 images/sec
Training: 2023-09-28 10:02:01,652 - loss nan, lr: 0.000125, epoch: 20, step: 119800, eta: 1.07 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11847 sec, avg_samples: 32.00000, ips: 540.20363 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 119900, eta: 1.07 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11848 sec, avg_samples: 32.00000, ips: 540.19374 images/sec
Training: 2023-09-28 10:02:13,506 - loss nan, lr: 0.000125, epoch: 20, step: 119900, eta: 1.07 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11848 sec, avg_samples: 32.00000, ips: 540.19374 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 120000, eta: 1.06 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11850 sec, avg_samples: 32.00000, ips: 540.09179 images/sec
Training: 2023-09-28 10:02:25,362 - loss nan, lr: 0.000125, epoch: 20, step: 120000, eta: 1.06 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11850 sec, avg_samples: 32.00000, ips: 540.09179 images/sec
INFO:root:[lfw][120000]XNorm: 0.516254
Training: 2023-09-28 10:02:53,348 - [lfw][120000]XNorm: 0.516254
INFO:root:[lfw][120000]Accuracy-Flip: 0.89800+-0.01364
Training: 2023-09-28 10:02:53,348 - [lfw][120000]Accuracy-Flip: 0.89800+-0.01364
INFO:root:[lfw][120000]Accuracy-Highest: 0.90150
Training: 2023-09-28 10:02:53,349 - [lfw][120000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9865
Training: 2023-09-28 10:02:53,349 - test time: 27.9865
INFO:root:[cfp_fp][120000]XNorm: 0.481563
Training: 2023-09-28 10:03:25,608 - [cfp_fp][120000]XNorm: 0.481563
INFO:root:[cfp_fp][120000]Accuracy-Flip: 0.63886+-0.01501
Training: 2023-09-28 10:03:25,608 - [cfp_fp][120000]Accuracy-Flip: 0.63886+-0.01501
INFO:root:[cfp_fp][120000]Accuracy-Highest: 0.65071
Training: 2023-09-28 10:03:25,608 - [cfp_fp][120000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2597
Training: 2023-09-28 10:03:25,608 - test time: 32.2597
INFO:root:[agedb_30][120000]XNorm: 0.489326
Training: 2023-09-28 10:03:53,354 - [agedb_30][120000]XNorm: 0.489326
INFO:root:[agedb_30][120000]Accuracy-Flip: 0.69033+-0.02240
Training: 2023-09-28 10:03:53,354 - [agedb_30][120000]Accuracy-Flip: 0.69033+-0.02240
INFO:root:[agedb_30][120000]Accuracy-Highest: 0.70083
Training: 2023-09-28 10:03:53,354 - [agedb_30][120000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7459
Training: 2023-09-28 10:03:53,354 - test time: 27.7459
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 120100, eta: 1.06 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11803 sec, avg_samples: 32.00000, ips: 542.23977 images/sec
Training: 2023-09-28 10:04:05,163 - loss nan, lr: 0.000125, epoch: 20, step: 120100, eta: 1.06 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11803 sec, avg_samples: 32.00000, ips: 542.23977 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 120200, eta: 1.06 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.07292 images/sec
Training: 2023-09-28 10:04:16,997 - loss nan, lr: 0.000125, epoch: 20, step: 120200, eta: 1.06 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11828 sec, avg_samples: 32.00000, ips: 541.07292 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 120300, eta: 1.06 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11866 sec, avg_samples: 32.00000, ips: 539.37016 images/sec
Training: 2023-09-28 10:04:28,869 - loss nan, lr: 0.000125, epoch: 20, step: 120300, eta: 1.06 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11866 sec, avg_samples: 32.00000, ips: 539.37016 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 120400, eta: 1.05 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11867 sec, avg_samples: 32.00000, ips: 539.32937 images/sec
Training: 2023-09-28 10:04:40,743 - loss nan, lr: 0.000125, epoch: 20, step: 120400, eta: 1.05 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11867 sec, avg_samples: 32.00000, ips: 539.32937 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 120500, eta: 1.05 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11866 sec, avg_samples: 32.00000, ips: 539.36755 images/sec
Training: 2023-09-28 10:04:52,615 - loss nan, lr: 0.000125, epoch: 20, step: 120500, eta: 1.05 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11866 sec, avg_samples: 32.00000, ips: 539.36755 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 120600, eta: 1.04 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11888 sec, avg_samples: 32.00000, ips: 538.35419 images/sec
Training: 2023-09-28 10:05:04,510 - loss nan, lr: 0.000125, epoch: 20, step: 120600, eta: 1.04 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11888 sec, avg_samples: 32.00000, ips: 538.35419 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 20, step: 120700, eta: 1.04 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11889 sec, avg_samples: 32.00000, ips: 538.32154 images/sec
Training: 2023-09-28 10:05:16,405 - loss nan, lr: 0.000125, epoch: 20, step: 120700, eta: 1.04 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11889 sec, avg_samples: 32.00000, ips: 538.32154 images/sec
INFO:root:Save model to MS1M_v3_arcface_static_0.1/FresResNet50/20.
Training: 2023-09-28 10:05:17,700 - Save model to MS1M_v3_arcface_static_0.1/FresResNet50/20.
INFO:root:Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/19.
Training: 2023-09-28 10:05:17,700 - Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/19.
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 120800, eta: 1.03 hours, avg_reader_cost: 0.00182 sec, avg_batch_cost: 0.11064 sec, avg_samples: 29.44000, ips: 532.18415 images/sec
Training: 2023-09-28 10:05:28,799 - loss nan, lr: 0.000125, epoch: 21, step: 120800, eta: 1.03 hours, avg_reader_cost: 0.00182 sec, avg_batch_cost: 0.11064 sec, avg_samples: 29.44000, ips: 532.18415 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 120900, eta: 1.03 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.91834 images/sec
Training: 2023-09-28 10:05:40,636 - loss nan, lr: 0.000125, epoch: 21, step: 120900, eta: 1.03 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.91834 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 121000, eta: 1.02 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.04235 images/sec
Training: 2023-09-28 10:05:52,470 - loss nan, lr: 0.000125, epoch: 21, step: 121000, eta: 1.02 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.04235 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 121100, eta: 1.02 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11824 sec, avg_samples: 32.00000, ips: 541.25754 images/sec
Training: 2023-09-28 10:06:04,300 - loss nan, lr: 0.000125, epoch: 21, step: 121100, eta: 1.02 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11824 sec, avg_samples: 32.00000, ips: 541.25754 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 121200, eta: 1.01 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11824 sec, avg_samples: 32.00000, ips: 541.27221 images/sec
Training: 2023-09-28 10:06:16,129 - loss nan, lr: 0.000125, epoch: 21, step: 121200, eta: 1.01 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11824 sec, avg_samples: 32.00000, ips: 541.27221 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 121300, eta: 1.01 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.15426 images/sec
Training: 2023-09-28 10:06:27,960 - loss nan, lr: 0.000125, epoch: 21, step: 121300, eta: 1.01 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11827 sec, avg_samples: 32.00000, ips: 541.15426 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 121400, eta: 1.00 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11825 sec, avg_samples: 32.00000, ips: 541.21462 images/sec
Training: 2023-09-28 10:06:39,791 - loss nan, lr: 0.000125, epoch: 21, step: 121400, eta: 1.00 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11825 sec, avg_samples: 32.00000, ips: 541.21462 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 121500, eta: 1.00 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.02415 images/sec
Training: 2023-09-28 10:06:51,625 - loss nan, lr: 0.000125, epoch: 21, step: 121500, eta: 1.00 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.02415 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 121600, eta: 0.99 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11862 sec, avg_samples: 32.00000, ips: 539.52939 images/sec
Training: 2023-09-28 10:07:03,493 - loss nan, lr: 0.000125, epoch: 21, step: 121600, eta: 0.99 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11862 sec, avg_samples: 32.00000, ips: 539.52939 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 121700, eta: 0.99 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11842 sec, avg_samples: 32.00000, ips: 540.45542 images/sec
Training: 2023-09-28 10:07:15,340 - loss nan, lr: 0.000125, epoch: 21, step: 121700, eta: 0.99 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11842 sec, avg_samples: 32.00000, ips: 540.45542 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 121800, eta: 0.98 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11849 sec, avg_samples: 32.00000, ips: 540.14740 images/sec
Training: 2023-09-28 10:07:27,195 - loss nan, lr: 0.000125, epoch: 21, step: 121800, eta: 0.98 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11849 sec, avg_samples: 32.00000, ips: 540.14740 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 121900, eta: 0.98 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11847 sec, avg_samples: 32.00000, ips: 540.20829 images/sec
Training: 2023-09-28 10:07:39,048 - loss nan, lr: 0.000125, epoch: 21, step: 121900, eta: 0.98 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11847 sec, avg_samples: 32.00000, ips: 540.20829 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 122000, eta: 0.97 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11853 sec, avg_samples: 32.00000, ips: 539.92577 images/sec
Training: 2023-09-28 10:07:50,908 - loss nan, lr: 0.000125, epoch: 21, step: 122000, eta: 0.97 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11853 sec, avg_samples: 32.00000, ips: 539.92577 images/sec
INFO:root:[lfw][122000]XNorm: 0.513066
Training: 2023-09-28 10:08:18,885 - [lfw][122000]XNorm: 0.513066
INFO:root:[lfw][122000]Accuracy-Flip: 0.89683+-0.01582
Training: 2023-09-28 10:08:18,885 - [lfw][122000]Accuracy-Flip: 0.89683+-0.01582
INFO:root:[lfw][122000]Accuracy-Highest: 0.90150
Training: 2023-09-28 10:08:18,885 - [lfw][122000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9767
Training: 2023-09-28 10:08:18,885 - test time: 27.9767
INFO:root:[cfp_fp][122000]XNorm: 0.480272
Training: 2023-09-28 10:08:51,176 - [cfp_fp][122000]XNorm: 0.480272
INFO:root:[cfp_fp][122000]Accuracy-Flip: 0.63814+-0.01628
Training: 2023-09-28 10:08:51,176 - [cfp_fp][122000]Accuracy-Flip: 0.63814+-0.01628
INFO:root:[cfp_fp][122000]Accuracy-Highest: 0.65071
Training: 2023-09-28 10:08:51,176 - [cfp_fp][122000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2913
Training: 2023-09-28 10:08:51,176 - test time: 32.2913
INFO:root:[agedb_30][122000]XNorm: 0.486994
Training: 2023-09-28 10:09:18,954 - [agedb_30][122000]XNorm: 0.486994
INFO:root:[agedb_30][122000]Accuracy-Flip: 0.69133+-0.01963
Training: 2023-09-28 10:09:18,954 - [agedb_30][122000]Accuracy-Flip: 0.69133+-0.01963
INFO:root:[agedb_30][122000]Accuracy-Highest: 0.70083
Training: 2023-09-28 10:09:18,954 - [agedb_30][122000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7776
Training: 2023-09-28 10:09:18,954 - test time: 27.7776
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 122100, eta: 0.97 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11842 sec, avg_samples: 32.00000, ips: 540.45184 images/sec
Training: 2023-09-28 10:09:30,802 - loss nan, lr: 0.000125, epoch: 21, step: 122100, eta: 0.97 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11842 sec, avg_samples: 32.00000, ips: 540.45184 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 122200, eta: 0.97 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11862 sec, avg_samples: 32.00000, ips: 539.53355 images/sec
Training: 2023-09-28 10:09:42,671 - loss nan, lr: 0.000125, epoch: 21, step: 122200, eta: 0.97 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11862 sec, avg_samples: 32.00000, ips: 539.53355 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 122300, eta: 0.96 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11860 sec, avg_samples: 32.00000, ips: 539.62697 images/sec
Training: 2023-09-28 10:09:54,537 - loss nan, lr: 0.000125, epoch: 21, step: 122300, eta: 0.96 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11860 sec, avg_samples: 32.00000, ips: 539.62697 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 122400, eta: 0.96 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.75174 images/sec
Training: 2023-09-28 10:10:06,378 - loss nan, lr: 0.000125, epoch: 21, step: 122400, eta: 0.96 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.75174 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 122500, eta: 0.96 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11846 sec, avg_samples: 32.00000, ips: 540.27467 images/sec
Training: 2023-09-28 10:10:18,228 - loss nan, lr: 0.000125, epoch: 21, step: 122500, eta: 0.96 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11846 sec, avg_samples: 32.00000, ips: 540.27467 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 122600, eta: 0.95 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11875 sec, avg_samples: 32.00000, ips: 538.96605 images/sec
Training: 2023-09-28 10:10:30,108 - loss nan, lr: 0.000125, epoch: 21, step: 122600, eta: 0.95 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11875 sec, avg_samples: 32.00000, ips: 538.96605 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 122700, eta: 0.95 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11865 sec, avg_samples: 32.00000, ips: 539.40882 images/sec
Training: 2023-09-28 10:10:41,979 - loss nan, lr: 0.000125, epoch: 21, step: 122700, eta: 0.95 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11865 sec, avg_samples: 32.00000, ips: 539.40882 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 122800, eta: 0.94 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11900 sec, avg_samples: 32.00000, ips: 537.82657 images/sec
Training: 2023-09-28 10:10:53,885 - loss nan, lr: 0.000125, epoch: 21, step: 122800, eta: 0.94 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11900 sec, avg_samples: 32.00000, ips: 537.82657 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 122900, eta: 0.94 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11902 sec, avg_samples: 32.00000, ips: 537.73562 images/sec
Training: 2023-09-28 10:11:05,793 - loss nan, lr: 0.000125, epoch: 21, step: 122900, eta: 0.94 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11902 sec, avg_samples: 32.00000, ips: 537.73562 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 123000, eta: 0.93 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11898 sec, avg_samples: 32.00000, ips: 537.90113 images/sec
Training: 2023-09-28 10:11:17,698 - loss nan, lr: 0.000125, epoch: 21, step: 123000, eta: 0.93 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11898 sec, avg_samples: 32.00000, ips: 537.90113 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 123100, eta: 0.93 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11892 sec, avg_samples: 32.00000, ips: 538.18668 images/sec
Training: 2023-09-28 10:11:29,596 - loss nan, lr: 0.000125, epoch: 21, step: 123100, eta: 0.93 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11892 sec, avg_samples: 32.00000, ips: 538.18668 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 123200, eta: 0.92 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11892 sec, avg_samples: 32.00000, ips: 538.17517 images/sec
Training: 2023-09-28 10:11:41,495 - loss nan, lr: 0.000125, epoch: 21, step: 123200, eta: 0.92 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11892 sec, avg_samples: 32.00000, ips: 538.17517 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 123300, eta: 0.92 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11890 sec, avg_samples: 32.00000, ips: 538.24952 images/sec
Training: 2023-09-28 10:11:53,392 - loss nan, lr: 0.000125, epoch: 21, step: 123300, eta: 0.92 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11890 sec, avg_samples: 32.00000, ips: 538.24952 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 123400, eta: 0.91 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11875 sec, avg_samples: 32.00000, ips: 538.96220 images/sec
Training: 2023-09-28 10:12:05,272 - loss nan, lr: 0.000125, epoch: 21, step: 123400, eta: 0.91 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11875 sec, avg_samples: 32.00000, ips: 538.96220 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 123500, eta: 0.91 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11871 sec, avg_samples: 32.00000, ips: 539.14143 images/sec
Training: 2023-09-28 10:12:17,148 - loss nan, lr: 0.000125, epoch: 21, step: 123500, eta: 0.91 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11871 sec, avg_samples: 32.00000, ips: 539.14143 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 123600, eta: 0.90 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11849 sec, avg_samples: 32.00000, ips: 540.13135 images/sec
Training: 2023-09-28 10:12:29,001 - loss nan, lr: 0.000125, epoch: 21, step: 123600, eta: 0.90 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11849 sec, avg_samples: 32.00000, ips: 540.13135 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 123700, eta: 0.90 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11873 sec, avg_samples: 32.00000, ips: 539.05032 images/sec
Training: 2023-09-28 10:12:40,879 - loss nan, lr: 0.000125, epoch: 21, step: 123700, eta: 0.90 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11873 sec, avg_samples: 32.00000, ips: 539.05032 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 123800, eta: 0.89 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11843 sec, avg_samples: 32.00000, ips: 540.41605 images/sec
Training: 2023-09-28 10:12:52,726 - loss nan, lr: 0.000125, epoch: 21, step: 123800, eta: 0.89 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11843 sec, avg_samples: 32.00000, ips: 540.41605 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 123900, eta: 0.89 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11839 sec, avg_samples: 32.00000, ips: 540.57259 images/sec
Training: 2023-09-28 10:13:04,571 - loss nan, lr: 0.000125, epoch: 21, step: 123900, eta: 0.89 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11839 sec, avg_samples: 32.00000, ips: 540.57259 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 124000, eta: 0.89 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11842 sec, avg_samples: 32.00000, ips: 540.42889 images/sec
Training: 2023-09-28 10:13:16,418 - loss nan, lr: 0.000125, epoch: 21, step: 124000, eta: 0.89 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11842 sec, avg_samples: 32.00000, ips: 540.42889 images/sec
INFO:root:[lfw][124000]XNorm: 0.513704
Training: 2023-09-28 10:13:44,369 - [lfw][124000]XNorm: 0.513704
INFO:root:[lfw][124000]Accuracy-Flip: 0.89583+-0.01680
Training: 2023-09-28 10:13:44,370 - [lfw][124000]Accuracy-Flip: 0.89583+-0.01680
INFO:root:[lfw][124000]Accuracy-Highest: 0.90150
Training: 2023-09-28 10:13:44,370 - [lfw][124000]Accuracy-Highest: 0.90150
INFO:root:test time: 27.9513
Training: 2023-09-28 10:13:44,370 - test time: 27.9513
INFO:root:[cfp_fp][124000]XNorm: 0.480541
Training: 2023-09-28 10:14:16,631 - [cfp_fp][124000]XNorm: 0.480541
INFO:root:[cfp_fp][124000]Accuracy-Flip: 0.64586+-0.01691
Training: 2023-09-28 10:14:16,631 - [cfp_fp][124000]Accuracy-Flip: 0.64586+-0.01691
INFO:root:[cfp_fp][124000]Accuracy-Highest: 0.65071
Training: 2023-09-28 10:14:16,631 - [cfp_fp][124000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2618
Training: 2023-09-28 10:14:16,632 - test time: 32.2618
INFO:root:[agedb_30][124000]XNorm: 0.487418
Training: 2023-09-28 10:14:44,388 - [agedb_30][124000]XNorm: 0.487418
INFO:root:[agedb_30][124000]Accuracy-Flip: 0.68983+-0.02345
Training: 2023-09-28 10:14:44,388 - [agedb_30][124000]Accuracy-Flip: 0.68983+-0.02345
INFO:root:[agedb_30][124000]Accuracy-Highest: 0.70083
Training: 2023-09-28 10:14:44,388 - [agedb_30][124000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7566
Training: 2023-09-28 10:14:44,388 - test time: 27.7566
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 124100, eta: 0.88 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.80462 images/sec
Training: 2023-09-28 10:14:56,206 - loss nan, lr: 0.000125, epoch: 21, step: 124100, eta: 0.88 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.80462 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 124200, eta: 0.88 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11854 sec, avg_samples: 32.00000, ips: 539.88771 images/sec
Training: 2023-09-28 10:15:08,065 - loss nan, lr: 0.000125, epoch: 21, step: 124200, eta: 0.88 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11854 sec, avg_samples: 32.00000, ips: 539.88771 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 124300, eta: 0.87 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11850 sec, avg_samples: 32.00000, ips: 540.08992 images/sec
Training: 2023-09-28 10:15:19,920 - loss nan, lr: 0.000125, epoch: 21, step: 124300, eta: 0.87 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11850 sec, avg_samples: 32.00000, ips: 540.08992 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 124400, eta: 0.87 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11846 sec, avg_samples: 32.00000, ips: 540.25510 images/sec
Training: 2023-09-28 10:15:31,771 - loss nan, lr: 0.000125, epoch: 21, step: 124400, eta: 0.87 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11846 sec, avg_samples: 32.00000, ips: 540.25510 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 124500, eta: 0.87 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11848 sec, avg_samples: 32.00000, ips: 540.16975 images/sec
Training: 2023-09-28 10:15:43,625 - loss nan, lr: 0.000125, epoch: 21, step: 124500, eta: 0.87 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11848 sec, avg_samples: 32.00000, ips: 540.16975 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 124600, eta: 0.86 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11875 sec, avg_samples: 32.00000, ips: 538.95836 images/sec
Training: 2023-09-28 10:15:55,505 - loss nan, lr: 0.000125, epoch: 21, step: 124600, eta: 0.86 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11875 sec, avg_samples: 32.00000, ips: 538.95836 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 124700, eta: 0.86 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11872 sec, avg_samples: 32.00000, ips: 539.08330 images/sec
Training: 2023-09-28 10:16:07,382 - loss nan, lr: 0.000125, epoch: 21, step: 124700, eta: 0.86 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11872 sec, avg_samples: 32.00000, ips: 539.08330 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 124800, eta: 0.85 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11871 sec, avg_samples: 32.00000, ips: 539.14458 images/sec
Training: 2023-09-28 10:16:19,258 - loss nan, lr: 0.000125, epoch: 21, step: 124800, eta: 0.85 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11871 sec, avg_samples: 32.00000, ips: 539.14458 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 124900, eta: 0.85 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11874 sec, avg_samples: 32.00000, ips: 539.00700 images/sec
Training: 2023-09-28 10:16:31,136 - loss nan, lr: 0.000125, epoch: 21, step: 124900, eta: 0.85 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11874 sec, avg_samples: 32.00000, ips: 539.00700 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 125000, eta: 0.84 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11871 sec, avg_samples: 32.00000, ips: 539.11119 images/sec
Training: 2023-09-28 10:16:43,013 - loss nan, lr: 0.000125, epoch: 21, step: 125000, eta: 0.84 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11871 sec, avg_samples: 32.00000, ips: 539.11119 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 125100, eta: 0.84 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11875 sec, avg_samples: 32.00000, ips: 538.94610 images/sec
Training: 2023-09-28 10:16:54,893 - loss nan, lr: 0.000125, epoch: 21, step: 125100, eta: 0.84 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11875 sec, avg_samples: 32.00000, ips: 538.94610 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 125200, eta: 0.83 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11872 sec, avg_samples: 32.00000, ips: 539.06402 images/sec
Training: 2023-09-28 10:17:06,771 - loss nan, lr: 0.000125, epoch: 21, step: 125200, eta: 0.83 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11872 sec, avg_samples: 32.00000, ips: 539.06402 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 125300, eta: 0.83 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11871 sec, avg_samples: 32.00000, ips: 539.12745 images/sec
Training: 2023-09-28 10:17:18,647 - loss nan, lr: 0.000125, epoch: 21, step: 125300, eta: 0.83 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11871 sec, avg_samples: 32.00000, ips: 539.12745 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 125400, eta: 0.82 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11874 sec, avg_samples: 32.00000, ips: 539.00481 images/sec
Training: 2023-09-28 10:17:30,526 - loss nan, lr: 0.000125, epoch: 21, step: 125400, eta: 0.82 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11874 sec, avg_samples: 32.00000, ips: 539.00481 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 125500, eta: 0.82 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11857 sec, avg_samples: 32.00000, ips: 539.77979 images/sec
Training: 2023-09-28 10:17:42,388 - loss nan, lr: 0.000125, epoch: 21, step: 125500, eta: 0.82 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11857 sec, avg_samples: 32.00000, ips: 539.77979 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 125600, eta: 0.81 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11838 sec, avg_samples: 32.00000, ips: 540.64035 images/sec
Training: 2023-09-28 10:17:54,231 - loss nan, lr: 0.000125, epoch: 21, step: 125600, eta: 0.81 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11838 sec, avg_samples: 32.00000, ips: 540.64035 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 125700, eta: 0.81 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11836 sec, avg_samples: 32.00000, ips: 540.72568 images/sec
Training: 2023-09-28 10:18:06,072 - loss nan, lr: 0.000125, epoch: 21, step: 125700, eta: 0.81 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11836 sec, avg_samples: 32.00000, ips: 540.72568 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 125800, eta: 0.80 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.68905 images/sec
Training: 2023-09-28 10:18:17,913 - loss nan, lr: 0.000125, epoch: 21, step: 125800, eta: 0.80 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.68905 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 125900, eta: 0.80 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11838 sec, avg_samples: 32.00000, ips: 540.63920 images/sec
Training: 2023-09-28 10:18:29,756 - loss nan, lr: 0.000125, epoch: 21, step: 125900, eta: 0.80 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11838 sec, avg_samples: 32.00000, ips: 540.63920 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 126000, eta: 0.80 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11838 sec, avg_samples: 32.00000, ips: 540.63554 images/sec
Training: 2023-09-28 10:18:41,599 - loss nan, lr: 0.000125, epoch: 21, step: 126000, eta: 0.80 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11838 sec, avg_samples: 32.00000, ips: 540.63554 images/sec
INFO:root:[lfw][126000]XNorm: 0.506317
Training: 2023-09-28 10:19:09,612 - [lfw][126000]XNorm: 0.506317
INFO:root:[lfw][126000]Accuracy-Flip: 0.89517+-0.01791
Training: 2023-09-28 10:19:09,612 - [lfw][126000]Accuracy-Flip: 0.89517+-0.01791
INFO:root:[lfw][126000]Accuracy-Highest: 0.90150
Training: 2023-09-28 10:19:09,612 - [lfw][126000]Accuracy-Highest: 0.90150
INFO:root:test time: 28.0131
Training: 2023-09-28 10:19:09,612 - test time: 28.0131
INFO:root:[cfp_fp][126000]XNorm: 0.474156
Training: 2023-09-28 10:19:41,946 - [cfp_fp][126000]XNorm: 0.474156
INFO:root:[cfp_fp][126000]Accuracy-Flip: 0.64471+-0.01893
Training: 2023-09-28 10:19:41,946 - [cfp_fp][126000]Accuracy-Flip: 0.64471+-0.01893
INFO:root:[cfp_fp][126000]Accuracy-Highest: 0.65071
Training: 2023-09-28 10:19:41,946 - [cfp_fp][126000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.3342
Training: 2023-09-28 10:19:41,946 - test time: 32.3342
INFO:root:[agedb_30][126000]XNorm: 0.480469
Training: 2023-09-28 10:20:09,740 - [agedb_30][126000]XNorm: 0.480469
INFO:root:[agedb_30][126000]Accuracy-Flip: 0.68667+-0.02111
Training: 2023-09-28 10:20:09,740 - [agedb_30][126000]Accuracy-Flip: 0.68667+-0.02111
INFO:root:[agedb_30][126000]Accuracy-Highest: 0.70083
Training: 2023-09-28 10:20:09,740 - [agedb_30][126000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7939
Training: 2023-09-28 10:20:09,740 - test time: 27.7939
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 126100, eta: 0.79 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11858 sec, avg_samples: 32.00000, ips: 539.72780 images/sec
Training: 2023-09-28 10:20:21,603 - loss nan, lr: 0.000125, epoch: 21, step: 126100, eta: 0.79 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11858 sec, avg_samples: 32.00000, ips: 539.72780 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 126200, eta: 0.79 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11861 sec, avg_samples: 32.00000, ips: 539.56235 images/sec
Training: 2023-09-28 10:20:33,469 - loss nan, lr: 0.000125, epoch: 21, step: 126200, eta: 0.79 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11861 sec, avg_samples: 32.00000, ips: 539.56235 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 126300, eta: 0.78 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11811 sec, avg_samples: 32.00000, ips: 541.87934 images/sec
Training: 2023-09-28 10:20:45,285 - loss nan, lr: 0.000125, epoch: 21, step: 126300, eta: 0.78 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11811 sec, avg_samples: 32.00000, ips: 541.87934 images/sec
INFO:root:loss nan, lr: 0.000125, epoch: 21, step: 126400, eta: 0.78 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.75483 images/sec
Training: 2023-09-28 10:20:57,104 - loss nan, lr: 0.000125, epoch: 21, step: 126400, eta: 0.78 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.75483 images/sec
INFO:root:Save model to MS1M_v3_arcface_static_0.1/FresResNet50/21.
Training: 2023-09-28 10:21:04,069 - Save model to MS1M_v3_arcface_static_0.1/FresResNet50/21.
INFO:root:Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/20.
Training: 2023-09-28 10:21:04,070 - Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/20.
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 126500, eta: 0.78 hours, avg_reader_cost: 0.00178 sec, avg_batch_cost: 0.05379 sec, avg_samples: 14.08000, ips: 523.48128 images/sec
Training: 2023-09-28 10:21:09,482 - loss nan, lr: 0.000013, epoch: 22, step: 126500, eta: 0.78 hours, avg_reader_cost: 0.00178 sec, avg_batch_cost: 0.05379 sec, avg_samples: 14.08000, ips: 523.48128 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 126600, eta: 0.77 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.04829 images/sec
Training: 2023-09-28 10:21:21,317 - loss nan, lr: 0.000013, epoch: 22, step: 126600, eta: 0.77 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.04829 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 126700, eta: 0.77 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.20291 images/sec
Training: 2023-09-28 10:21:33,147 - loss nan, lr: 0.000013, epoch: 22, step: 126700, eta: 0.77 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11826 sec, avg_samples: 32.00000, ips: 541.20291 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 126800, eta: 0.76 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11833 sec, avg_samples: 32.00000, ips: 540.86311 images/sec
Training: 2023-09-28 10:21:44,986 - loss nan, lr: 0.000013, epoch: 22, step: 126800, eta: 0.76 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11833 sec, avg_samples: 32.00000, ips: 540.86311 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 126900, eta: 0.76 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11836 sec, avg_samples: 32.00000, ips: 540.70586 images/sec
Training: 2023-09-28 10:21:56,827 - loss nan, lr: 0.000013, epoch: 22, step: 126900, eta: 0.76 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11836 sec, avg_samples: 32.00000, ips: 540.70586 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 127000, eta: 0.75 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11863 sec, avg_samples: 32.00000, ips: 539.47626 images/sec
Training: 2023-09-28 10:22:08,697 - loss nan, lr: 0.000013, epoch: 22, step: 127000, eta: 0.75 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11863 sec, avg_samples: 32.00000, ips: 539.47626 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 127100, eta: 0.75 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.75830 images/sec
Training: 2023-09-28 10:22:20,537 - loss nan, lr: 0.000013, epoch: 22, step: 127100, eta: 0.75 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.75830 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 127200, eta: 0.74 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11840 sec, avg_samples: 32.00000, ips: 540.53589 images/sec
Training: 2023-09-28 10:22:32,381 - loss nan, lr: 0.000013, epoch: 22, step: 127200, eta: 0.74 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11840 sec, avg_samples: 32.00000, ips: 540.53589 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 127300, eta: 0.74 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11853 sec, avg_samples: 32.00000, ips: 539.97043 images/sec
Training: 2023-09-28 10:22:44,239 - loss nan, lr: 0.000013, epoch: 22, step: 127300, eta: 0.74 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11853 sec, avg_samples: 32.00000, ips: 539.97043 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 127400, eta: 0.73 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11830 sec, avg_samples: 32.00000, ips: 540.98780 images/sec
Training: 2023-09-28 10:22:56,074 - loss nan, lr: 0.000013, epoch: 22, step: 127400, eta: 0.73 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11830 sec, avg_samples: 32.00000, ips: 540.98780 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 127500, eta: 0.73 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.03746 images/sec
Training: 2023-09-28 10:23:07,909 - loss nan, lr: 0.000013, epoch: 22, step: 127500, eta: 0.73 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.03746 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 127600, eta: 0.72 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11858 sec, avg_samples: 32.00000, ips: 539.72004 images/sec
Training: 2023-09-28 10:23:19,772 - loss nan, lr: 0.000013, epoch: 22, step: 127600, eta: 0.72 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11858 sec, avg_samples: 32.00000, ips: 539.72004 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 127700, eta: 0.72 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11893 sec, avg_samples: 32.00000, ips: 538.13337 images/sec
Training: 2023-09-28 10:23:31,670 - loss nan, lr: 0.000013, epoch: 22, step: 127700, eta: 0.72 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11893 sec, avg_samples: 32.00000, ips: 538.13337 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 127800, eta: 0.71 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11856 sec, avg_samples: 32.00000, ips: 539.79543 images/sec
Training: 2023-09-28 10:23:43,531 - loss nan, lr: 0.000013, epoch: 22, step: 127800, eta: 0.71 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11856 sec, avg_samples: 32.00000, ips: 539.79543 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 127900, eta: 0.71 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11836 sec, avg_samples: 32.00000, ips: 540.73617 images/sec
Training: 2023-09-28 10:23:55,371 - loss nan, lr: 0.000013, epoch: 22, step: 127900, eta: 0.71 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11836 sec, avg_samples: 32.00000, ips: 540.73617 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 128000, eta: 0.71 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11852 sec, avg_samples: 32.00000, ips: 539.98247 images/sec
Training: 2023-09-28 10:24:07,228 - loss nan, lr: 0.000013, epoch: 22, step: 128000, eta: 0.71 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11852 sec, avg_samples: 32.00000, ips: 539.98247 images/sec
INFO:root:[lfw][128000]XNorm: 0.508447
Training: 2023-09-28 10:24:35,257 - [lfw][128000]XNorm: 0.508447
INFO:root:[lfw][128000]Accuracy-Flip: 0.90200+-0.01770
Training: 2023-09-28 10:24:35,257 - [lfw][128000]Accuracy-Flip: 0.90200+-0.01770
INFO:root:[lfw][128000]Accuracy-Highest: 0.90200
Training: 2023-09-28 10:24:35,257 - [lfw][128000]Accuracy-Highest: 0.90200
INFO:root:test time: 28.0287
Training: 2023-09-28 10:24:35,257 - test time: 28.0287
INFO:root:[cfp_fp][128000]XNorm: 0.475012
Training: 2023-09-28 10:25:07,587 - [cfp_fp][128000]XNorm: 0.475012
INFO:root:[cfp_fp][128000]Accuracy-Flip: 0.64000+-0.01818
Training: 2023-09-28 10:25:07,587 - [cfp_fp][128000]Accuracy-Flip: 0.64000+-0.01818
INFO:root:[cfp_fp][128000]Accuracy-Highest: 0.65071
Training: 2023-09-28 10:25:07,587 - [cfp_fp][128000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.3299
Training: 2023-09-28 10:25:07,587 - test time: 32.3299
INFO:root:[agedb_30][128000]XNorm: 0.482530
Training: 2023-09-28 10:25:35,403 - [agedb_30][128000]XNorm: 0.482530
INFO:root:[agedb_30][128000]Accuracy-Flip: 0.69017+-0.02383
Training: 2023-09-28 10:25:35,404 - [agedb_30][128000]Accuracy-Flip: 0.69017+-0.02383
INFO:root:[agedb_30][128000]Accuracy-Highest: 0.70083
Training: 2023-09-28 10:25:35,404 - [agedb_30][128000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.8165
Training: 2023-09-28 10:25:35,404 - test time: 27.8165
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 128100, eta: 0.70 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.75562 images/sec
Training: 2023-09-28 10:25:47,223 - loss nan, lr: 0.000013, epoch: 22, step: 128100, eta: 0.70 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11813 sec, avg_samples: 32.00000, ips: 541.75562 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 128200, eta: 0.70 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.36552 images/sec
Training: 2023-09-28 10:25:59,051 - loss nan, lr: 0.000013, epoch: 22, step: 128200, eta: 0.70 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.36552 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 128300, eta: 0.69 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11806 sec, avg_samples: 32.00000, ips: 542.09476 images/sec
Training: 2023-09-28 10:26:10,862 - loss nan, lr: 0.000013, epoch: 22, step: 128300, eta: 0.69 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11806 sec, avg_samples: 32.00000, ips: 542.09476 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 128400, eta: 0.69 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11804 sec, avg_samples: 32.00000, ips: 542.19203 images/sec
Training: 2023-09-28 10:26:22,671 - loss nan, lr: 0.000013, epoch: 22, step: 128400, eta: 0.69 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11804 sec, avg_samples: 32.00000, ips: 542.19203 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 128500, eta: 0.69 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.03320 images/sec
Training: 2023-09-28 10:26:34,506 - loss nan, lr: 0.000013, epoch: 22, step: 128500, eta: 0.69 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.03320 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 128600, eta: 0.68 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11831 sec, avg_samples: 32.00000, ips: 540.94938 images/sec
Training: 2023-09-28 10:26:46,342 - loss nan, lr: 0.000013, epoch: 22, step: 128600, eta: 0.68 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11831 sec, avg_samples: 32.00000, ips: 540.94938 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 128700, eta: 0.68 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.88726 images/sec
Training: 2023-09-28 10:26:58,180 - loss nan, lr: 0.000013, epoch: 22, step: 128700, eta: 0.68 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.88726 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 128800, eta: 0.67 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11848 sec, avg_samples: 32.00000, ips: 540.15970 images/sec
Training: 2023-09-28 10:27:10,034 - loss nan, lr: 0.000013, epoch: 22, step: 128800, eta: 0.67 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11848 sec, avg_samples: 32.00000, ips: 540.15970 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 128900, eta: 0.67 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11861 sec, avg_samples: 32.00000, ips: 539.58650 images/sec
Training: 2023-09-28 10:27:21,901 - loss nan, lr: 0.000013, epoch: 22, step: 128900, eta: 0.67 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11861 sec, avg_samples: 32.00000, ips: 539.58650 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 129000, eta: 0.66 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11847 sec, avg_samples: 32.00000, ips: 540.20077 images/sec
Training: 2023-09-28 10:27:33,754 - loss nan, lr: 0.000013, epoch: 22, step: 129000, eta: 0.66 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11847 sec, avg_samples: 32.00000, ips: 540.20077 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 129100, eta: 0.66 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11883 sec, avg_samples: 32.00000, ips: 538.56559 images/sec
Training: 2023-09-28 10:27:45,643 - loss nan, lr: 0.000013, epoch: 22, step: 129100, eta: 0.66 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11883 sec, avg_samples: 32.00000, ips: 538.56559 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 129200, eta: 0.65 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.35027 images/sec
Training: 2023-09-28 10:27:57,471 - loss nan, lr: 0.000013, epoch: 22, step: 129200, eta: 0.65 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11822 sec, avg_samples: 32.00000, ips: 541.35027 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 129300, eta: 0.65 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.75200 images/sec
Training: 2023-09-28 10:28:09,312 - loss nan, lr: 0.000013, epoch: 22, step: 129300, eta: 0.65 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.75200 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 129400, eta: 0.64 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11825 sec, avg_samples: 32.00000, ips: 541.24872 images/sec
Training: 2023-09-28 10:28:21,141 - loss nan, lr: 0.000013, epoch: 22, step: 129400, eta: 0.64 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11825 sec, avg_samples: 32.00000, ips: 541.24872 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 129500, eta: 0.64 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11831 sec, avg_samples: 32.00000, ips: 540.93728 images/sec
Training: 2023-09-28 10:28:32,978 - loss nan, lr: 0.000013, epoch: 22, step: 129500, eta: 0.64 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11831 sec, avg_samples: 32.00000, ips: 540.93728 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 129600, eta: 0.63 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11841 sec, avg_samples: 32.00000, ips: 540.50334 images/sec
Training: 2023-09-28 10:28:44,825 - loss nan, lr: 0.000013, epoch: 22, step: 129600, eta: 0.63 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11841 sec, avg_samples: 32.00000, ips: 540.50334 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 129700, eta: 0.63 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.03509 images/sec
Training: 2023-09-28 10:28:56,659 - loss nan, lr: 0.000013, epoch: 22, step: 129700, eta: 0.63 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.03509 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 129800, eta: 0.62 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11870 sec, avg_samples: 32.00000, ips: 539.18269 images/sec
Training: 2023-09-28 10:29:08,534 - loss nan, lr: 0.000013, epoch: 22, step: 129800, eta: 0.62 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11870 sec, avg_samples: 32.00000, ips: 539.18269 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 129900, eta: 0.62 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11841 sec, avg_samples: 32.00000, ips: 540.47825 images/sec
Training: 2023-09-28 10:29:20,381 - loss nan, lr: 0.000013, epoch: 22, step: 129900, eta: 0.62 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11841 sec, avg_samples: 32.00000, ips: 540.47825 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 130000, eta: 0.62 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11845 sec, avg_samples: 32.00000, ips: 540.31906 images/sec
Training: 2023-09-28 10:29:32,232 - loss nan, lr: 0.000013, epoch: 22, step: 130000, eta: 0.62 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11845 sec, avg_samples: 32.00000, ips: 540.31906 images/sec
INFO:root:[lfw][130000]XNorm: 0.511729
Training: 2023-09-28 10:30:00,342 - [lfw][130000]XNorm: 0.511729
INFO:root:[lfw][130000]Accuracy-Flip: 0.90033+-0.01577
Training: 2023-09-28 10:30:00,343 - [lfw][130000]Accuracy-Flip: 0.90033+-0.01577
INFO:root:[lfw][130000]Accuracy-Highest: 0.90200
Training: 2023-09-28 10:30:00,343 - [lfw][130000]Accuracy-Highest: 0.90200
INFO:root:test time: 28.1107
Training: 2023-09-28 10:30:00,343 - test time: 28.1107
INFO:root:[cfp_fp][130000]XNorm: 0.479541
Training: 2023-09-28 10:30:32,685 - [cfp_fp][130000]XNorm: 0.479541
INFO:root:[cfp_fp][130000]Accuracy-Flip: 0.64300+-0.01624
Training: 2023-09-28 10:30:32,686 - [cfp_fp][130000]Accuracy-Flip: 0.64300+-0.01624
INFO:root:[cfp_fp][130000]Accuracy-Highest: 0.65071
Training: 2023-09-28 10:30:32,686 - [cfp_fp][130000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.3430
Training: 2023-09-28 10:30:32,686 - test time: 32.3430
INFO:root:[agedb_30][130000]XNorm: 0.485963
Training: 2023-09-28 10:31:00,483 - [agedb_30][130000]XNorm: 0.485963
INFO:root:[agedb_30][130000]Accuracy-Flip: 0.68867+-0.02107
Training: 2023-09-28 10:31:00,483 - [agedb_30][130000]Accuracy-Flip: 0.68867+-0.02107
INFO:root:[agedb_30][130000]Accuracy-Highest: 0.70083
Training: 2023-09-28 10:31:00,483 - [agedb_30][130000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7974
Training: 2023-09-28 10:31:00,483 - test time: 27.7974
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 130100, eta: 0.61 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11797 sec, avg_samples: 32.00000, ips: 542.52642 images/sec
Training: 2023-09-28 10:31:12,285 - loss nan, lr: 0.000013, epoch: 22, step: 130100, eta: 0.61 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11797 sec, avg_samples: 32.00000, ips: 542.52642 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 130200, eta: 0.61 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11800 sec, avg_samples: 32.00000, ips: 542.39508 images/sec
Training: 2023-09-28 10:31:24,089 - loss nan, lr: 0.000013, epoch: 22, step: 130200, eta: 0.61 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11800 sec, avg_samples: 32.00000, ips: 542.39508 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 130300, eta: 0.60 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11800 sec, avg_samples: 32.00000, ips: 542.36429 images/sec
Training: 2023-09-28 10:31:35,894 - loss nan, lr: 0.000013, epoch: 22, step: 130300, eta: 0.60 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11800 sec, avg_samples: 32.00000, ips: 542.36429 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 130400, eta: 0.60 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.47756 images/sec
Training: 2023-09-28 10:31:47,719 - loss nan, lr: 0.000013, epoch: 22, step: 130400, eta: 0.60 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11820 sec, avg_samples: 32.00000, ips: 541.47756 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 130500, eta: 0.60 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11840 sec, avg_samples: 32.00000, ips: 540.54539 images/sec
Training: 2023-09-28 10:31:59,565 - loss nan, lr: 0.000013, epoch: 22, step: 130500, eta: 0.60 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11840 sec, avg_samples: 32.00000, ips: 540.54539 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 130600, eta: 0.59 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11848 sec, avg_samples: 32.00000, ips: 540.19545 images/sec
Training: 2023-09-28 10:32:11,419 - loss nan, lr: 0.000013, epoch: 22, step: 130600, eta: 0.59 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11848 sec, avg_samples: 32.00000, ips: 540.19545 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 130700, eta: 0.59 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11847 sec, avg_samples: 32.00000, ips: 540.22474 images/sec
Training: 2023-09-28 10:32:23,272 - loss nan, lr: 0.000013, epoch: 22, step: 130700, eta: 0.59 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11847 sec, avg_samples: 32.00000, ips: 540.22474 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 130800, eta: 0.58 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11833 sec, avg_samples: 32.00000, ips: 540.88196 images/sec
Training: 2023-09-28 10:32:35,110 - loss nan, lr: 0.000013, epoch: 22, step: 130800, eta: 0.58 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11833 sec, avg_samples: 32.00000, ips: 540.88196 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 130900, eta: 0.58 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11851 sec, avg_samples: 32.00000, ips: 540.04821 images/sec
Training: 2023-09-28 10:32:46,967 - loss nan, lr: 0.000013, epoch: 22, step: 130900, eta: 0.58 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11851 sec, avg_samples: 32.00000, ips: 540.04821 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 131000, eta: 0.57 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11831 sec, avg_samples: 32.00000, ips: 540.93449 images/sec
Training: 2023-09-28 10:32:58,804 - loss nan, lr: 0.000013, epoch: 22, step: 131000, eta: 0.57 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11831 sec, avg_samples: 32.00000, ips: 540.93449 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 131100, eta: 0.57 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.03729 images/sec
Training: 2023-09-28 10:33:10,638 - loss nan, lr: 0.000013, epoch: 22, step: 131100, eta: 0.57 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.03729 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 131200, eta: 0.56 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.75042 images/sec
Training: 2023-09-28 10:33:22,479 - loss nan, lr: 0.000013, epoch: 22, step: 131200, eta: 0.56 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.75042 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 131300, eta: 0.56 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11857 sec, avg_samples: 32.00000, ips: 539.77129 images/sec
Training: 2023-09-28 10:33:34,342 - loss nan, lr: 0.000013, epoch: 22, step: 131300, eta: 0.56 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11857 sec, avg_samples: 32.00000, ips: 539.77129 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 131400, eta: 0.55 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11856 sec, avg_samples: 32.00000, ips: 539.79819 images/sec
Training: 2023-09-28 10:33:46,205 - loss nan, lr: 0.000013, epoch: 22, step: 131400, eta: 0.55 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11856 sec, avg_samples: 32.00000, ips: 539.79819 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 131500, eta: 0.55 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11852 sec, avg_samples: 32.00000, ips: 539.97822 images/sec
Training: 2023-09-28 10:33:58,063 - loss nan, lr: 0.000013, epoch: 22, step: 131500, eta: 0.55 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11852 sec, avg_samples: 32.00000, ips: 539.97822 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 131600, eta: 0.54 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11853 sec, avg_samples: 32.00000, ips: 539.95297 images/sec
Training: 2023-09-28 10:34:09,923 - loss nan, lr: 0.000013, epoch: 22, step: 131600, eta: 0.54 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11853 sec, avg_samples: 32.00000, ips: 539.95297 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 131700, eta: 0.54 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11854 sec, avg_samples: 32.00000, ips: 539.89598 images/sec
Training: 2023-09-28 10:34:21,783 - loss nan, lr: 0.000013, epoch: 22, step: 131700, eta: 0.54 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11854 sec, avg_samples: 32.00000, ips: 539.89598 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 131800, eta: 0.53 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11854 sec, avg_samples: 32.00000, ips: 539.89816 images/sec
Training: 2023-09-28 10:34:33,644 - loss nan, lr: 0.000013, epoch: 22, step: 131800, eta: 0.53 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11854 sec, avg_samples: 32.00000, ips: 539.89816 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 131900, eta: 0.53 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11854 sec, avg_samples: 32.00000, ips: 539.89685 images/sec
Training: 2023-09-28 10:34:45,504 - loss nan, lr: 0.000013, epoch: 22, step: 131900, eta: 0.53 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11854 sec, avg_samples: 32.00000, ips: 539.89685 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 132000, eta: 0.53 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11853 sec, avg_samples: 32.00000, ips: 539.96740 images/sec
Training: 2023-09-28 10:34:57,363 - loss nan, lr: 0.000013, epoch: 22, step: 132000, eta: 0.53 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11853 sec, avg_samples: 32.00000, ips: 539.96740 images/sec
INFO:root:[lfw][132000]XNorm: 0.512132
Training: 2023-09-28 10:35:25,362 - [lfw][132000]XNorm: 0.512132
INFO:root:[lfw][132000]Accuracy-Flip: 0.89733+-0.01841
Training: 2023-09-28 10:35:25,363 - [lfw][132000]Accuracy-Flip: 0.89733+-0.01841
INFO:root:[lfw][132000]Accuracy-Highest: 0.90200
Training: 2023-09-28 10:35:25,363 - [lfw][132000]Accuracy-Highest: 0.90200
INFO:root:test time: 27.9994
Training: 2023-09-28 10:35:25,363 - test time: 27.9994
INFO:root:[cfp_fp][132000]XNorm: 0.478057
Training: 2023-09-28 10:35:57,667 - [cfp_fp][132000]XNorm: 0.478057
INFO:root:[cfp_fp][132000]Accuracy-Flip: 0.64000+-0.01885
Training: 2023-09-28 10:35:57,667 - [cfp_fp][132000]Accuracy-Flip: 0.64000+-0.01885
INFO:root:[cfp_fp][132000]Accuracy-Highest: 0.65071
Training: 2023-09-28 10:35:57,667 - [cfp_fp][132000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.3042
Training: 2023-09-28 10:35:57,667 - test time: 32.3042
INFO:root:[agedb_30][132000]XNorm: 0.485578
Training: 2023-09-28 10:36:25,451 - [agedb_30][132000]XNorm: 0.485578
INFO:root:[agedb_30][132000]Accuracy-Flip: 0.68633+-0.02100
Training: 2023-09-28 10:36:25,452 - [agedb_30][132000]Accuracy-Flip: 0.68633+-0.02100
INFO:root:[agedb_30][132000]Accuracy-Highest: 0.70083
Training: 2023-09-28 10:36:25,452 - [agedb_30][132000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7847
Training: 2023-09-28 10:36:25,452 - test time: 27.7847
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 132100, eta: 0.52 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.65010 images/sec
Training: 2023-09-28 10:36:37,273 - loss nan, lr: 0.000013, epoch: 22, step: 132100, eta: 0.52 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.65010 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 22, step: 132200, eta: 0.52 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.77222 images/sec
Training: 2023-09-28 10:36:49,115 - loss nan, lr: 0.000013, epoch: 22, step: 132200, eta: 0.52 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.77222 images/sec
INFO:root:Save model to MS1M_v3_arcface_static_0.1/FresResNet50/22.
Training: 2023-09-28 10:36:49,934 - Save model to MS1M_v3_arcface_static_0.1/FresResNet50/22.
INFO:root:Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/21.
Training: 2023-09-28 10:36:49,934 - Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/21.
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 132300, eta: 0.51 hours, avg_reader_cost: 0.00181 sec, avg_batch_cost: 0.11521 sec, avg_samples: 30.72000, ips: 533.28510 images/sec
Training: 2023-09-28 10:37:01,491 - loss nan, lr: 0.000013, epoch: 23, step: 132300, eta: 0.51 hours, avg_reader_cost: 0.00181 sec, avg_batch_cost: 0.11521 sec, avg_samples: 30.72000, ips: 533.28510 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 132400, eta: 0.51 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.82925 images/sec
Training: 2023-09-28 10:37:13,308 - loss nan, lr: 0.000013, epoch: 23, step: 132400, eta: 0.51 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11812 sec, avg_samples: 32.00000, ips: 541.82925 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 132500, eta: 0.50 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11851 sec, avg_samples: 32.00000, ips: 540.01933 images/sec
Training: 2023-09-28 10:37:25,165 - loss nan, lr: 0.000013, epoch: 23, step: 132500, eta: 0.50 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11851 sec, avg_samples: 32.00000, ips: 540.01933 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 132600, eta: 0.50 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11879 sec, avg_samples: 32.00000, ips: 538.75788 images/sec
Training: 2023-09-28 10:37:37,049 - loss nan, lr: 0.000013, epoch: 23, step: 132600, eta: 0.50 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11879 sec, avg_samples: 32.00000, ips: 538.75788 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 132700, eta: 0.50 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11881 sec, avg_samples: 32.00000, ips: 538.68852 images/sec
Training: 2023-09-28 10:37:48,935 - loss nan, lr: 0.000013, epoch: 23, step: 132700, eta: 0.50 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11881 sec, avg_samples: 32.00000, ips: 538.68852 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 132800, eta: 0.49 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11899 sec, avg_samples: 32.00000, ips: 537.88108 images/sec
Training: 2023-09-28 10:38:00,840 - loss nan, lr: 0.000013, epoch: 23, step: 132800, eta: 0.49 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11899 sec, avg_samples: 32.00000, ips: 537.88108 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 132900, eta: 0.49 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11881 sec, avg_samples: 32.00000, ips: 538.69304 images/sec
Training: 2023-09-28 10:38:12,727 - loss nan, lr: 0.000013, epoch: 23, step: 132900, eta: 0.49 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11881 sec, avg_samples: 32.00000, ips: 538.69304 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 133000, eta: 0.48 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11852 sec, avg_samples: 32.00000, ips: 539.99415 images/sec
Training: 2023-09-28 10:38:24,586 - loss nan, lr: 0.000013, epoch: 23, step: 133000, eta: 0.48 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11852 sec, avg_samples: 32.00000, ips: 539.99415 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 133100, eta: 0.48 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.91248 images/sec
Training: 2023-09-28 10:38:36,423 - loss nan, lr: 0.000013, epoch: 23, step: 133100, eta: 0.48 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.91248 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 133200, eta: 0.47 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11879 sec, avg_samples: 32.00000, ips: 538.77446 images/sec
Training: 2023-09-28 10:38:48,307 - loss nan, lr: 0.000013, epoch: 23, step: 133200, eta: 0.47 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11879 sec, avg_samples: 32.00000, ips: 538.77446 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 133300, eta: 0.47 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11882 sec, avg_samples: 32.00000, ips: 538.63604 images/sec
Training: 2023-09-28 10:39:00,194 - loss nan, lr: 0.000013, epoch: 23, step: 133300, eta: 0.47 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11882 sec, avg_samples: 32.00000, ips: 538.63604 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 133400, eta: 0.46 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11880 sec, avg_samples: 32.00000, ips: 538.71494 images/sec
Training: 2023-09-28 10:39:12,079 - loss nan, lr: 0.000013, epoch: 23, step: 133400, eta: 0.46 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11880 sec, avg_samples: 32.00000, ips: 538.71494 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 133500, eta: 0.46 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11886 sec, avg_samples: 32.00000, ips: 538.44589 images/sec
Training: 2023-09-28 10:39:23,970 - loss nan, lr: 0.000013, epoch: 23, step: 133500, eta: 0.46 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11886 sec, avg_samples: 32.00000, ips: 538.44589 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 133600, eta: 0.45 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11886 sec, avg_samples: 32.00000, ips: 538.43944 images/sec
Training: 2023-09-28 10:39:35,861 - loss nan, lr: 0.000013, epoch: 23, step: 133600, eta: 0.45 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11886 sec, avg_samples: 32.00000, ips: 538.43944 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 133700, eta: 0.45 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11883 sec, avg_samples: 32.00000, ips: 538.59601 images/sec
Training: 2023-09-28 10:39:47,749 - loss nan, lr: 0.000013, epoch: 23, step: 133700, eta: 0.45 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11883 sec, avg_samples: 32.00000, ips: 538.59601 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 133800, eta: 0.45 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11884 sec, avg_samples: 32.00000, ips: 538.53878 images/sec
Training: 2023-09-28 10:39:59,638 - loss nan, lr: 0.000013, epoch: 23, step: 133800, eta: 0.45 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11884 sec, avg_samples: 32.00000, ips: 538.53878 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 133900, eta: 0.44 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11892 sec, avg_samples: 32.00000, ips: 538.17126 images/sec
Training: 2023-09-28 10:40:11,535 - loss nan, lr: 0.000013, epoch: 23, step: 133900, eta: 0.44 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11892 sec, avg_samples: 32.00000, ips: 538.17126 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 134000, eta: 0.44 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11911 sec, avg_samples: 32.00000, ips: 537.31260 images/sec
Training: 2023-09-28 10:40:23,451 - loss nan, lr: 0.000013, epoch: 23, step: 134000, eta: 0.44 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11911 sec, avg_samples: 32.00000, ips: 537.31260 images/sec
INFO:root:[lfw][134000]XNorm: 0.509854
Training: 2023-09-28 10:40:51,522 - [lfw][134000]XNorm: 0.509854
INFO:root:[lfw][134000]Accuracy-Flip: 0.89783+-0.01892
Training: 2023-09-28 10:40:51,522 - [lfw][134000]Accuracy-Flip: 0.89783+-0.01892
INFO:root:[lfw][134000]Accuracy-Highest: 0.90200
Training: 2023-09-28 10:40:51,522 - [lfw][134000]Accuracy-Highest: 0.90200
INFO:root:test time: 28.0714
Training: 2023-09-28 10:40:51,522 - test time: 28.0714
INFO:root:[cfp_fp][134000]XNorm: 0.476263
Training: 2023-09-28 10:41:23,826 - [cfp_fp][134000]XNorm: 0.476263
INFO:root:[cfp_fp][134000]Accuracy-Flip: 0.64000+-0.01768
Training: 2023-09-28 10:41:23,826 - [cfp_fp][134000]Accuracy-Flip: 0.64000+-0.01768
INFO:root:[cfp_fp][134000]Accuracy-Highest: 0.65071
Training: 2023-09-28 10:41:23,826 - [cfp_fp][134000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.3042
Training: 2023-09-28 10:41:23,826 - test time: 32.3042
INFO:root:[agedb_30][134000]XNorm: 0.483115
Training: 2023-09-28 10:41:51,624 - [agedb_30][134000]XNorm: 0.483115
INFO:root:[agedb_30][134000]Accuracy-Flip: 0.68967+-0.02290
Training: 2023-09-28 10:41:51,624 - [agedb_30][134000]Accuracy-Flip: 0.68967+-0.02290
INFO:root:[agedb_30][134000]Accuracy-Highest: 0.70083
Training: 2023-09-28 10:41:51,624 - [agedb_30][134000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7978
Training: 2023-09-28 10:41:51,624 - test time: 27.7978
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 134100, eta: 0.43 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11849 sec, avg_samples: 32.00000, ips: 540.13055 images/sec
Training: 2023-09-28 10:42:03,478 - loss nan, lr: 0.000013, epoch: 23, step: 134100, eta: 0.43 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11849 sec, avg_samples: 32.00000, ips: 540.13055 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 134200, eta: 0.43 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11858 sec, avg_samples: 32.00000, ips: 539.73739 images/sec
Training: 2023-09-28 10:42:15,341 - loss nan, lr: 0.000013, epoch: 23, step: 134200, eta: 0.43 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11858 sec, avg_samples: 32.00000, ips: 539.73739 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 134300, eta: 0.42 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.05983 images/sec
Training: 2023-09-28 10:42:27,175 - loss nan, lr: 0.000013, epoch: 23, step: 134300, eta: 0.42 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11829 sec, avg_samples: 32.00000, ips: 541.05983 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 134400, eta: 0.42 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11842 sec, avg_samples: 32.00000, ips: 540.45527 images/sec
Training: 2023-09-28 10:42:39,022 - loss nan, lr: 0.000013, epoch: 23, step: 134400, eta: 0.42 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11842 sec, avg_samples: 32.00000, ips: 540.45527 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 134500, eta: 0.41 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.65467 images/sec
Training: 2023-09-28 10:42:50,843 - loss nan, lr: 0.000013, epoch: 23, step: 134500, eta: 0.41 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11816 sec, avg_samples: 32.00000, ips: 541.65467 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 134600, eta: 0.41 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.67940 images/sec
Training: 2023-09-28 10:43:02,685 - loss nan, lr: 0.000013, epoch: 23, step: 134600, eta: 0.41 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.67940 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 134700, eta: 0.41 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11841 sec, avg_samples: 32.00000, ips: 540.51460 images/sec
Training: 2023-09-28 10:43:14,531 - loss nan, lr: 0.000013, epoch: 23, step: 134700, eta: 0.41 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11841 sec, avg_samples: 32.00000, ips: 540.51460 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 134800, eta: 0.40 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11839 sec, avg_samples: 32.00000, ips: 540.56443 images/sec
Training: 2023-09-28 10:43:26,376 - loss nan, lr: 0.000013, epoch: 23, step: 134800, eta: 0.40 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11839 sec, avg_samples: 32.00000, ips: 540.56443 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 134900, eta: 0.40 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11840 sec, avg_samples: 32.00000, ips: 540.56076 images/sec
Training: 2023-09-28 10:43:38,221 - loss nan, lr: 0.000013, epoch: 23, step: 134900, eta: 0.40 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11840 sec, avg_samples: 32.00000, ips: 540.56076 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 135000, eta: 0.39 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11844 sec, avg_samples: 32.00000, ips: 540.36856 images/sec
Training: 2023-09-28 10:43:50,070 - loss nan, lr: 0.000013, epoch: 23, step: 135000, eta: 0.39 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11844 sec, avg_samples: 32.00000, ips: 540.36856 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 135100, eta: 0.39 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11852 sec, avg_samples: 32.00000, ips: 540.00518 images/sec
Training: 2023-09-28 10:44:01,928 - loss nan, lr: 0.000013, epoch: 23, step: 135100, eta: 0.39 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11852 sec, avg_samples: 32.00000, ips: 540.00518 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 135200, eta: 0.38 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11849 sec, avg_samples: 32.00000, ips: 540.13565 images/sec
Training: 2023-09-28 10:44:13,783 - loss nan, lr: 0.000013, epoch: 23, step: 135200, eta: 0.38 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11849 sec, avg_samples: 32.00000, ips: 540.13565 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 135300, eta: 0.38 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11846 sec, avg_samples: 32.00000, ips: 540.25592 images/sec
Training: 2023-09-28 10:44:25,635 - loss nan, lr: 0.000013, epoch: 23, step: 135300, eta: 0.38 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11846 sec, avg_samples: 32.00000, ips: 540.25592 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 135400, eta: 0.37 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11883 sec, avg_samples: 32.00000, ips: 538.58877 images/sec
Training: 2023-09-28 10:44:37,522 - loss nan, lr: 0.000013, epoch: 23, step: 135400, eta: 0.37 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11883 sec, avg_samples: 32.00000, ips: 538.58877 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 135500, eta: 0.37 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11847 sec, avg_samples: 32.00000, ips: 540.24359 images/sec
Training: 2023-09-28 10:44:49,373 - loss nan, lr: 0.000013, epoch: 23, step: 135500, eta: 0.37 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11847 sec, avg_samples: 32.00000, ips: 540.24359 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 135600, eta: 0.36 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11845 sec, avg_samples: 32.00000, ips: 540.32735 images/sec
Training: 2023-09-28 10:45:01,223 - loss nan, lr: 0.000013, epoch: 23, step: 135600, eta: 0.36 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11845 sec, avg_samples: 32.00000, ips: 540.32735 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 135700, eta: 0.36 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11862 sec, avg_samples: 32.00000, ips: 539.55525 images/sec
Training: 2023-09-28 10:45:13,089 - loss nan, lr: 0.000013, epoch: 23, step: 135700, eta: 0.36 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11862 sec, avg_samples: 32.00000, ips: 539.55525 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 135800, eta: 0.36 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11917 sec, avg_samples: 32.00000, ips: 537.03127 images/sec
Training: 2023-09-28 10:45:25,013 - loss nan, lr: 0.000013, epoch: 23, step: 135800, eta: 0.36 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11917 sec, avg_samples: 32.00000, ips: 537.03127 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 135900, eta: 0.35 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11921 sec, avg_samples: 32.00000, ips: 536.86957 images/sec
Training: 2023-09-28 10:45:36,940 - loss nan, lr: 0.000013, epoch: 23, step: 135900, eta: 0.35 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11921 sec, avg_samples: 32.00000, ips: 536.86957 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 136000, eta: 0.35 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11922 sec, avg_samples: 32.00000, ips: 536.83263 images/sec
Training: 2023-09-28 10:45:48,868 - loss nan, lr: 0.000013, epoch: 23, step: 136000, eta: 0.35 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11922 sec, avg_samples: 32.00000, ips: 536.83263 images/sec
INFO:root:[lfw][136000]XNorm: 0.511527
Training: 2023-09-28 10:46:16,918 - [lfw][136000]XNorm: 0.511527
INFO:root:[lfw][136000]Accuracy-Flip: 0.89883+-0.01635
Training: 2023-09-28 10:46:16,918 - [lfw][136000]Accuracy-Flip: 0.89883+-0.01635
INFO:root:[lfw][136000]Accuracy-Highest: 0.90200
Training: 2023-09-28 10:46:16,918 - [lfw][136000]Accuracy-Highest: 0.90200
INFO:root:test time: 28.0501
Training: 2023-09-28 10:46:16,918 - test time: 28.0501
INFO:root:[cfp_fp][136000]XNorm: 0.478363
Training: 2023-09-28 10:46:49,229 - [cfp_fp][136000]XNorm: 0.478363
INFO:root:[cfp_fp][136000]Accuracy-Flip: 0.64000+-0.01855
Training: 2023-09-28 10:46:49,229 - [cfp_fp][136000]Accuracy-Flip: 0.64000+-0.01855
INFO:root:[cfp_fp][136000]Accuracy-Highest: 0.65071
Training: 2023-09-28 10:46:49,229 - [cfp_fp][136000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.3110
Training: 2023-09-28 10:46:49,229 - test time: 32.3110
INFO:root:[agedb_30][136000]XNorm: 0.485648
Training: 2023-09-28 10:47:17,012 - [agedb_30][136000]XNorm: 0.485648
INFO:root:[agedb_30][136000]Accuracy-Flip: 0.69267+-0.02394
Training: 2023-09-28 10:47:17,012 - [agedb_30][136000]Accuracy-Flip: 0.69267+-0.02394
INFO:root:[agedb_30][136000]Accuracy-Highest: 0.70083
Training: 2023-09-28 10:47:17,012 - [agedb_30][136000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7830
Training: 2023-09-28 10:47:17,012 - test time: 27.7830
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 136100, eta: 0.34 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11834 sec, avg_samples: 32.00000, ips: 540.83485 images/sec
Training: 2023-09-28 10:47:28,852 - loss nan, lr: 0.000013, epoch: 23, step: 136100, eta: 0.34 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11834 sec, avg_samples: 32.00000, ips: 540.83485 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 136200, eta: 0.34 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.67128 images/sec
Training: 2023-09-28 10:47:40,695 - loss nan, lr: 0.000013, epoch: 23, step: 136200, eta: 0.34 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11837 sec, avg_samples: 32.00000, ips: 540.67128 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 136300, eta: 0.33 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.90626 images/sec
Training: 2023-09-28 10:47:52,534 - loss nan, lr: 0.000013, epoch: 23, step: 136300, eta: 0.33 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.90626 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 136400, eta: 0.33 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11831 sec, avg_samples: 32.00000, ips: 540.96527 images/sec
Training: 2023-09-28 10:48:04,371 - loss nan, lr: 0.000013, epoch: 23, step: 136400, eta: 0.33 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11831 sec, avg_samples: 32.00000, ips: 540.96527 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 136500, eta: 0.32 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.88761 images/sec
Training: 2023-09-28 10:48:16,210 - loss nan, lr: 0.000013, epoch: 23, step: 136500, eta: 0.32 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11832 sec, avg_samples: 32.00000, ips: 540.88761 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 136600, eta: 0.32 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11838 sec, avg_samples: 32.00000, ips: 540.64862 images/sec
Training: 2023-09-28 10:48:28,054 - loss nan, lr: 0.000013, epoch: 23, step: 136600, eta: 0.32 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11838 sec, avg_samples: 32.00000, ips: 540.64862 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 136700, eta: 0.32 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11884 sec, avg_samples: 32.00000, ips: 538.56173 images/sec
Training: 2023-09-28 10:48:39,943 - loss nan, lr: 0.000013, epoch: 23, step: 136700, eta: 0.32 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11884 sec, avg_samples: 32.00000, ips: 538.56173 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 136800, eta: 0.31 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11888 sec, avg_samples: 32.00000, ips: 538.36026 images/sec
Training: 2023-09-28 10:48:51,838 - loss nan, lr: 0.000013, epoch: 23, step: 136800, eta: 0.31 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11888 sec, avg_samples: 32.00000, ips: 538.36026 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 136900, eta: 0.31 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11878 sec, avg_samples: 32.00000, ips: 538.79557 images/sec
Training: 2023-09-28 10:49:03,723 - loss nan, lr: 0.000013, epoch: 23, step: 136900, eta: 0.31 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11878 sec, avg_samples: 32.00000, ips: 538.79557 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 137000, eta: 0.30 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11889 sec, avg_samples: 32.00000, ips: 538.29413 images/sec
Training: 2023-09-28 10:49:15,618 - loss nan, lr: 0.000013, epoch: 23, step: 137000, eta: 0.30 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11889 sec, avg_samples: 32.00000, ips: 538.29413 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 137100, eta: 0.30 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11905 sec, avg_samples: 32.00000, ips: 537.56948 images/sec
Training: 2023-09-28 10:49:27,530 - loss nan, lr: 0.000013, epoch: 23, step: 137100, eta: 0.30 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11905 sec, avg_samples: 32.00000, ips: 537.56948 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 137200, eta: 0.29 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11888 sec, avg_samples: 32.00000, ips: 538.33875 images/sec
Training: 2023-09-28 10:49:39,426 - loss nan, lr: 0.000013, epoch: 23, step: 137200, eta: 0.29 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11888 sec, avg_samples: 32.00000, ips: 538.33875 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 137300, eta: 0.29 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11839 sec, avg_samples: 32.00000, ips: 540.57499 images/sec
Training: 2023-09-28 10:49:51,271 - loss nan, lr: 0.000013, epoch: 23, step: 137300, eta: 0.29 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11839 sec, avg_samples: 32.00000, ips: 540.57499 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 137400, eta: 0.28 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11848 sec, avg_samples: 32.00000, ips: 540.17505 images/sec
Training: 2023-09-28 10:50:03,124 - loss nan, lr: 0.000013, epoch: 23, step: 137400, eta: 0.28 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11848 sec, avg_samples: 32.00000, ips: 540.17505 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 137500, eta: 0.28 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11882 sec, avg_samples: 32.00000, ips: 538.63841 images/sec
Training: 2023-09-28 10:50:15,013 - loss nan, lr: 0.000013, epoch: 23, step: 137500, eta: 0.28 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11882 sec, avg_samples: 32.00000, ips: 538.63841 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 137600, eta: 0.27 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11882 sec, avg_samples: 32.00000, ips: 538.64153 images/sec
Training: 2023-09-28 10:50:26,902 - loss nan, lr: 0.000013, epoch: 23, step: 137600, eta: 0.27 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11882 sec, avg_samples: 32.00000, ips: 538.64153 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 137700, eta: 0.27 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11881 sec, avg_samples: 32.00000, ips: 538.65888 images/sec
Training: 2023-09-28 10:50:38,790 - loss nan, lr: 0.000013, epoch: 23, step: 137700, eta: 0.27 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11881 sec, avg_samples: 32.00000, ips: 538.65888 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 137800, eta: 0.27 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11879 sec, avg_samples: 32.00000, ips: 538.76436 images/sec
Training: 2023-09-28 10:50:50,676 - loss nan, lr: 0.000013, epoch: 23, step: 137800, eta: 0.27 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11879 sec, avg_samples: 32.00000, ips: 538.76436 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 23, step: 137900, eta: 0.26 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11866 sec, avg_samples: 32.00000, ips: 539.34650 images/sec
Training: 2023-09-28 10:51:02,547 - loss nan, lr: 0.000013, epoch: 23, step: 137900, eta: 0.26 hours, avg_reader_cost: 0.00005 sec, avg_batch_cost: 0.11866 sec, avg_samples: 32.00000, ips: 539.34650 images/sec
INFO:root:Save model to MS1M_v3_arcface_static_0.1/FresResNet50/23.
Training: 2023-09-28 10:51:09,067 - Save model to MS1M_v3_arcface_static_0.1/FresResNet50/23.
INFO:root:Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/22.
Training: 2023-09-28 10:51:09,067 - Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/22.
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 138000, eta: 0.26 hours, avg_reader_cost: 0.00176 sec, avg_batch_cost: 0.05855 sec, avg_samples: 15.36000, ips: 524.72033 images/sec
Training: 2023-09-28 10:51:14,954 - loss nan, lr: 0.000013, epoch: 24, step: 138000, eta: 0.26 hours, avg_reader_cost: 0.00176 sec, avg_batch_cost: 0.05855 sec, avg_samples: 15.36000, ips: 524.72033 images/sec
INFO:root:[lfw][138000]XNorm: 0.511242
Training: 2023-09-28 10:51:42,954 - [lfw][138000]XNorm: 0.511242
INFO:root:[lfw][138000]Accuracy-Flip: 0.89867+-0.01579
Training: 2023-09-28 10:51:42,954 - [lfw][138000]Accuracy-Flip: 0.89867+-0.01579
INFO:root:[lfw][138000]Accuracy-Highest: 0.90200
Training: 2023-09-28 10:51:42,954 - [lfw][138000]Accuracy-Highest: 0.90200
INFO:root:test time: 28.0000
Training: 2023-09-28 10:51:42,954 - test time: 28.0000
INFO:root:[cfp_fp][138000]XNorm: 0.477478
Training: 2023-09-28 10:52:15,244 - [cfp_fp][138000]XNorm: 0.477478
INFO:root:[cfp_fp][138000]Accuracy-Flip: 0.64386+-0.01847
Training: 2023-09-28 10:52:15,244 - [cfp_fp][138000]Accuracy-Flip: 0.64386+-0.01847
INFO:root:[cfp_fp][138000]Accuracy-Highest: 0.65071
Training: 2023-09-28 10:52:15,244 - [cfp_fp][138000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.2900
Training: 2023-09-28 10:52:15,244 - test time: 32.2900
INFO:root:[agedb_30][138000]XNorm: 0.485993
Training: 2023-09-28 10:52:43,010 - [agedb_30][138000]XNorm: 0.485993
INFO:root:[agedb_30][138000]Accuracy-Flip: 0.68633+-0.02188
Training: 2023-09-28 10:52:43,010 - [agedb_30][138000]Accuracy-Flip: 0.68633+-0.02188
INFO:root:[agedb_30][138000]Accuracy-Highest: 0.70083
Training: 2023-09-28 10:52:43,010 - [agedb_30][138000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.7657
Training: 2023-09-28 10:52:43,010 - test time: 27.7657
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 138100, eta: 0.25 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11796 sec, avg_samples: 32.00000, ips: 542.53891 images/sec
Training: 2023-09-28 10:52:54,812 - loss nan, lr: 0.000013, epoch: 24, step: 138100, eta: 0.25 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11796 sec, avg_samples: 32.00000, ips: 542.53891 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 138200, eta: 0.25 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11848 sec, avg_samples: 32.00000, ips: 540.16384 images/sec
Training: 2023-09-28 10:53:06,666 - loss nan, lr: 0.000013, epoch: 24, step: 138200, eta: 0.25 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11848 sec, avg_samples: 32.00000, ips: 540.16384 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 138300, eta: 0.24 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11848 sec, avg_samples: 32.00000, ips: 540.17177 images/sec
Training: 2023-09-28 10:53:18,521 - loss nan, lr: 0.000013, epoch: 24, step: 138300, eta: 0.24 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11848 sec, avg_samples: 32.00000, ips: 540.17177 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 138400, eta: 0.24 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11867 sec, avg_samples: 32.00000, ips: 539.28960 images/sec
Training: 2023-09-28 10:53:30,395 - loss nan, lr: 0.000013, epoch: 24, step: 138400, eta: 0.24 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11867 sec, avg_samples: 32.00000, ips: 539.28960 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 138500, eta: 0.23 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11885 sec, avg_samples: 32.00000, ips: 538.48822 images/sec
Training: 2023-09-28 10:53:42,286 - loss nan, lr: 0.000013, epoch: 24, step: 138500, eta: 0.23 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11885 sec, avg_samples: 32.00000, ips: 538.48822 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 138600, eta: 0.23 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11884 sec, avg_samples: 32.00000, ips: 538.52763 images/sec
Training: 2023-09-28 10:53:54,177 - loss nan, lr: 0.000013, epoch: 24, step: 138600, eta: 0.23 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11884 sec, avg_samples: 32.00000, ips: 538.52763 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 138700, eta: 0.23 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11852 sec, avg_samples: 32.00000, ips: 539.97707 images/sec
Training: 2023-09-28 10:54:06,035 - loss nan, lr: 0.000013, epoch: 24, step: 138700, eta: 0.23 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11852 sec, avg_samples: 32.00000, ips: 539.97707 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 138800, eta: 0.22 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11866 sec, avg_samples: 32.00000, ips: 539.37133 images/sec
Training: 2023-09-28 10:54:17,907 - loss nan, lr: 0.000013, epoch: 24, step: 138800, eta: 0.22 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11866 sec, avg_samples: 32.00000, ips: 539.37133 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 138900, eta: 0.22 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11871 sec, avg_samples: 32.00000, ips: 539.14126 images/sec
Training: 2023-09-28 10:54:29,785 - loss nan, lr: 0.000013, epoch: 24, step: 138900, eta: 0.22 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11871 sec, avg_samples: 32.00000, ips: 539.14126 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 139000, eta: 0.21 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.76269 images/sec
Training: 2023-09-28 10:54:41,625 - loss nan, lr: 0.000013, epoch: 24, step: 139000, eta: 0.21 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.76269 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 139100, eta: 0.21 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11824 sec, avg_samples: 32.00000, ips: 541.28206 images/sec
Training: 2023-09-28 10:54:53,454 - loss nan, lr: 0.000013, epoch: 24, step: 139100, eta: 0.21 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11824 sec, avg_samples: 32.00000, ips: 541.28206 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 139200, eta: 0.20 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.75650 images/sec
Training: 2023-09-28 10:55:05,294 - loss nan, lr: 0.000013, epoch: 24, step: 139200, eta: 0.20 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11835 sec, avg_samples: 32.00000, ips: 540.75650 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 139300, eta: 0.20 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11872 sec, avg_samples: 32.00000, ips: 539.09398 images/sec
Training: 2023-09-28 10:55:17,172 - loss nan, lr: 0.000013, epoch: 24, step: 139300, eta: 0.20 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11872 sec, avg_samples: 32.00000, ips: 539.09398 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 139400, eta: 0.19 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11860 sec, avg_samples: 32.00000, ips: 539.61729 images/sec
Training: 2023-09-28 10:55:29,039 - loss nan, lr: 0.000013, epoch: 24, step: 139400, eta: 0.19 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11860 sec, avg_samples: 32.00000, ips: 539.61729 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 139500, eta: 0.19 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11845 sec, avg_samples: 32.00000, ips: 540.29480 images/sec
Training: 2023-09-28 10:55:40,890 - loss nan, lr: 0.000013, epoch: 24, step: 139500, eta: 0.19 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11845 sec, avg_samples: 32.00000, ips: 540.29480 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 139600, eta: 0.18 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11841 sec, avg_samples: 32.00000, ips: 540.50392 images/sec
Training: 2023-09-28 10:55:52,737 - loss nan, lr: 0.000013, epoch: 24, step: 139600, eta: 0.18 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11841 sec, avg_samples: 32.00000, ips: 540.50392 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 139700, eta: 0.18 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11838 sec, avg_samples: 32.00000, ips: 540.61947 images/sec
Training: 2023-09-28 10:56:04,580 - loss nan, lr: 0.000013, epoch: 24, step: 139700, eta: 0.18 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11838 sec, avg_samples: 32.00000, ips: 540.61947 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 139800, eta: 0.18 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11858 sec, avg_samples: 32.00000, ips: 539.73937 images/sec
Training: 2023-09-28 10:56:16,444 - loss nan, lr: 0.000013, epoch: 24, step: 139800, eta: 0.18 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11858 sec, avg_samples: 32.00000, ips: 539.73937 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 139900, eta: 0.17 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11870 sec, avg_samples: 32.00000, ips: 539.17507 images/sec
Training: 2023-09-28 10:56:28,320 - loss nan, lr: 0.000013, epoch: 24, step: 139900, eta: 0.17 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11870 sec, avg_samples: 32.00000, ips: 539.17507 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 140000, eta: 0.17 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11847 sec, avg_samples: 32.00000, ips: 540.21162 images/sec
Training: 2023-09-28 10:56:40,173 - loss nan, lr: 0.000013, epoch: 24, step: 140000, eta: 0.17 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11847 sec, avg_samples: 32.00000, ips: 540.21162 images/sec
INFO:root:[lfw][140000]XNorm: 0.507364
Training: 2023-09-28 10:57:08,210 - [lfw][140000]XNorm: 0.507364
INFO:root:[lfw][140000]Accuracy-Flip: 0.89417+-0.01664
Training: 2023-09-28 10:57:08,211 - [lfw][140000]Accuracy-Flip: 0.89417+-0.01664
INFO:root:[lfw][140000]Accuracy-Highest: 0.90200
Training: 2023-09-28 10:57:08,211 - [lfw][140000]Accuracy-Highest: 0.90200
INFO:root:test time: 28.0377
Training: 2023-09-28 10:57:08,211 - test time: 28.0377
INFO:root:[cfp_fp][140000]XNorm: 0.475320
Training: 2023-09-28 10:57:40,580 - [cfp_fp][140000]XNorm: 0.475320
INFO:root:[cfp_fp][140000]Accuracy-Flip: 0.64000+-0.01725
Training: 2023-09-28 10:57:40,581 - [cfp_fp][140000]Accuracy-Flip: 0.64000+-0.01725
INFO:root:[cfp_fp][140000]Accuracy-Highest: 0.65071
Training: 2023-09-28 10:57:40,581 - [cfp_fp][140000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.3700
Training: 2023-09-28 10:57:40,581 - test time: 32.3700
INFO:root:[agedb_30][140000]XNorm: 0.482694
Training: 2023-09-28 10:58:08,431 - [agedb_30][140000]XNorm: 0.482694
INFO:root:[agedb_30][140000]Accuracy-Flip: 0.68450+-0.02156
Training: 2023-09-28 10:58:08,432 - [agedb_30][140000]Accuracy-Flip: 0.68450+-0.02156
INFO:root:[agedb_30][140000]Accuracy-Highest: 0.70083
Training: 2023-09-28 10:58:08,432 - [agedb_30][140000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.8510
Training: 2023-09-28 10:58:08,432 - test time: 27.8510
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 140100, eta: 0.16 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11794 sec, avg_samples: 32.00000, ips: 542.64830 images/sec
Training: 2023-09-28 10:58:20,231 - loss nan, lr: 0.000013, epoch: 24, step: 140100, eta: 0.16 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11794 sec, avg_samples: 32.00000, ips: 542.64830 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 140200, eta: 0.16 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.28475 images/sec
Training: 2023-09-28 10:58:32,038 - loss nan, lr: 0.000013, epoch: 24, step: 140200, eta: 0.16 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11802 sec, avg_samples: 32.00000, ips: 542.28475 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 140300, eta: 0.15 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11801 sec, avg_samples: 32.00000, ips: 542.31081 images/sec
Training: 2023-09-28 10:58:43,844 - loss nan, lr: 0.000013, epoch: 24, step: 140300, eta: 0.15 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11801 sec, avg_samples: 32.00000, ips: 542.31081 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 140400, eta: 0.15 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11842 sec, avg_samples: 32.00000, ips: 540.42764 images/sec
Training: 2023-09-28 10:58:55,692 - loss nan, lr: 0.000013, epoch: 24, step: 140400, eta: 0.15 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11842 sec, avg_samples: 32.00000, ips: 540.42764 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 140500, eta: 0.14 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11882 sec, avg_samples: 32.00000, ips: 538.63785 images/sec
Training: 2023-09-28 10:59:07,579 - loss nan, lr: 0.000013, epoch: 24, step: 140500, eta: 0.14 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11882 sec, avg_samples: 32.00000, ips: 538.63785 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 140600, eta: 0.14 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11882 sec, avg_samples: 32.00000, ips: 538.62648 images/sec
Training: 2023-09-28 10:59:19,468 - loss nan, lr: 0.000013, epoch: 24, step: 140600, eta: 0.14 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11882 sec, avg_samples: 32.00000, ips: 538.62648 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 140700, eta: 0.14 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11892 sec, avg_samples: 32.00000, ips: 538.17556 images/sec
Training: 2023-09-28 10:59:31,366 - loss nan, lr: 0.000013, epoch: 24, step: 140700, eta: 0.14 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11892 sec, avg_samples: 32.00000, ips: 538.17556 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 140800, eta: 0.13 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11897 sec, avg_samples: 32.00000, ips: 537.92962 images/sec
Training: 2023-09-28 10:59:43,271 - loss nan, lr: 0.000013, epoch: 24, step: 140800, eta: 0.13 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11897 sec, avg_samples: 32.00000, ips: 537.92962 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 140900, eta: 0.13 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11895 sec, avg_samples: 32.00000, ips: 538.03040 images/sec
Training: 2023-09-28 10:59:55,174 - loss nan, lr: 0.000013, epoch: 24, step: 140900, eta: 0.13 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11895 sec, avg_samples: 32.00000, ips: 538.03040 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 141000, eta: 0.12 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11896 sec, avg_samples: 32.00000, ips: 538.01441 images/sec
Training: 2023-09-28 11:00:07,077 - loss nan, lr: 0.000013, epoch: 24, step: 141000, eta: 0.12 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11896 sec, avg_samples: 32.00000, ips: 538.01441 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 141100, eta: 0.12 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11893 sec, avg_samples: 32.00000, ips: 538.11727 images/sec
Training: 2023-09-28 11:00:18,977 - loss nan, lr: 0.000013, epoch: 24, step: 141100, eta: 0.12 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11893 sec, avg_samples: 32.00000, ips: 538.11727 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 141200, eta: 0.11 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11899 sec, avg_samples: 32.00000, ips: 537.87876 images/sec
Training: 2023-09-28 11:00:30,883 - loss nan, lr: 0.000013, epoch: 24, step: 141200, eta: 0.11 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11899 sec, avg_samples: 32.00000, ips: 537.87876 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 141300, eta: 0.11 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11897 sec, avg_samples: 32.00000, ips: 537.94599 images/sec
Training: 2023-09-28 11:00:42,788 - loss nan, lr: 0.000013, epoch: 24, step: 141300, eta: 0.11 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11897 sec, avg_samples: 32.00000, ips: 537.94599 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 141400, eta: 0.10 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11894 sec, avg_samples: 32.00000, ips: 538.08014 images/sec
Training: 2023-09-28 11:00:54,689 - loss nan, lr: 0.000013, epoch: 24, step: 141400, eta: 0.10 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11894 sec, avg_samples: 32.00000, ips: 538.08014 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 141500, eta: 0.10 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11887 sec, avg_samples: 32.00000, ips: 538.41711 images/sec
Training: 2023-09-28 11:01:06,582 - loss nan, lr: 0.000013, epoch: 24, step: 141500, eta: 0.10 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11887 sec, avg_samples: 32.00000, ips: 538.41711 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 141600, eta: 0.09 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11874 sec, avg_samples: 32.00000, ips: 538.98906 images/sec
Training: 2023-09-28 11:01:18,462 - loss nan, lr: 0.000013, epoch: 24, step: 141600, eta: 0.09 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11874 sec, avg_samples: 32.00000, ips: 538.98906 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 141700, eta: 0.09 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11880 sec, avg_samples: 32.00000, ips: 538.69927 images/sec
Training: 2023-09-28 11:01:30,348 - loss nan, lr: 0.000013, epoch: 24, step: 141700, eta: 0.09 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11880 sec, avg_samples: 32.00000, ips: 538.69927 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 141800, eta: 0.09 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11875 sec, avg_samples: 32.00000, ips: 538.92610 images/sec
Training: 2023-09-28 11:01:42,230 - loss nan, lr: 0.000013, epoch: 24, step: 141800, eta: 0.09 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11875 sec, avg_samples: 32.00000, ips: 538.92610 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 141900, eta: 0.08 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11876 sec, avg_samples: 32.00000, ips: 538.89127 images/sec
Training: 2023-09-28 11:01:54,111 - loss nan, lr: 0.000013, epoch: 24, step: 141900, eta: 0.08 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11876 sec, avg_samples: 32.00000, ips: 538.89127 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 142000, eta: 0.08 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11876 sec, avg_samples: 32.00000, ips: 538.89688 images/sec
Training: 2023-09-28 11:02:05,993 - loss nan, lr: 0.000013, epoch: 24, step: 142000, eta: 0.08 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11876 sec, avg_samples: 32.00000, ips: 538.89688 images/sec
INFO:root:[lfw][142000]XNorm: 0.509473
Training: 2023-09-28 11:02:34,012 - [lfw][142000]XNorm: 0.509473
INFO:root:[lfw][142000]Accuracy-Flip: 0.89833+-0.01800
Training: 2023-09-28 11:02:34,012 - [lfw][142000]Accuracy-Flip: 0.89833+-0.01800
INFO:root:[lfw][142000]Accuracy-Highest: 0.90200
Training: 2023-09-28 11:02:34,012 - [lfw][142000]Accuracy-Highest: 0.90200
INFO:root:test time: 28.0195
Training: 2023-09-28 11:02:34,012 - test time: 28.0195
INFO:root:[cfp_fp][142000]XNorm: 0.475712
Training: 2023-09-28 11:03:06,372 - [cfp_fp][142000]XNorm: 0.475712
INFO:root:[cfp_fp][142000]Accuracy-Flip: 0.64257+-0.01761
Training: 2023-09-28 11:03:06,372 - [cfp_fp][142000]Accuracy-Flip: 0.64257+-0.01761
INFO:root:[cfp_fp][142000]Accuracy-Highest: 0.65071
Training: 2023-09-28 11:03:06,372 - [cfp_fp][142000]Accuracy-Highest: 0.65071
INFO:root:test time: 32.3597
Training: 2023-09-28 11:03:06,372 - test time: 32.3597
INFO:root:[agedb_30][142000]XNorm: 0.483028
Training: 2023-09-28 11:03:34,172 - [agedb_30][142000]XNorm: 0.483028
INFO:root:[agedb_30][142000]Accuracy-Flip: 0.68217+-0.02240
Training: 2023-09-28 11:03:34,173 - [agedb_30][142000]Accuracy-Flip: 0.68217+-0.02240
INFO:root:[agedb_30][142000]Accuracy-Highest: 0.70083
Training: 2023-09-28 11:03:34,173 - [agedb_30][142000]Accuracy-Highest: 0.70083
INFO:root:test time: 27.8006
Training: 2023-09-28 11:03:34,173 - test time: 27.8006
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 142100, eta: 0.07 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11863 sec, avg_samples: 32.00000, ips: 539.48310 images/sec
Training: 2023-09-28 11:03:46,042 - loss nan, lr: 0.000013, epoch: 24, step: 142100, eta: 0.07 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11863 sec, avg_samples: 32.00000, ips: 539.48310 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 142200, eta: 0.07 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11874 sec, avg_samples: 32.00000, ips: 538.99575 images/sec
Training: 2023-09-28 11:03:57,923 - loss nan, lr: 0.000013, epoch: 24, step: 142200, eta: 0.07 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11874 sec, avg_samples: 32.00000, ips: 538.99575 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 142300, eta: 0.06 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11875 sec, avg_samples: 32.00000, ips: 538.93855 images/sec
Training: 2023-09-28 11:04:09,805 - loss nan, lr: 0.000013, epoch: 24, step: 142300, eta: 0.06 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11875 sec, avg_samples: 32.00000, ips: 538.93855 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 142400, eta: 0.06 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11876 sec, avg_samples: 32.00000, ips: 538.89780 images/sec
Training: 2023-09-28 11:04:21,687 - loss nan, lr: 0.000013, epoch: 24, step: 142400, eta: 0.06 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11876 sec, avg_samples: 32.00000, ips: 538.89780 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 142500, eta: 0.05 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11920 sec, avg_samples: 32.00000, ips: 536.91268 images/sec
Training: 2023-09-28 11:04:33,614 - loss nan, lr: 0.000013, epoch: 24, step: 142500, eta: 0.05 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11920 sec, avg_samples: 32.00000, ips: 536.91268 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 142600, eta: 0.05 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11918 sec, avg_samples: 32.00000, ips: 537.01237 images/sec
Training: 2023-09-28 11:04:45,538 - loss nan, lr: 0.000013, epoch: 24, step: 142600, eta: 0.05 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11918 sec, avg_samples: 32.00000, ips: 537.01237 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 142700, eta: 0.05 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11917 sec, avg_samples: 32.00000, ips: 537.05981 images/sec
Training: 2023-09-28 11:04:57,462 - loss nan, lr: 0.000013, epoch: 24, step: 142700, eta: 0.05 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.11917 sec, avg_samples: 32.00000, ips: 537.05981 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 142800, eta: 0.04 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11920 sec, avg_samples: 32.00000, ips: 536.91597 images/sec
Training: 2023-09-28 11:05:09,387 - loss nan, lr: 0.000013, epoch: 24, step: 142800, eta: 0.04 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11920 sec, avg_samples: 32.00000, ips: 536.91597 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 142900, eta: 0.04 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11919 sec, avg_samples: 32.00000, ips: 536.94877 images/sec
Training: 2023-09-28 11:05:21,311 - loss nan, lr: 0.000013, epoch: 24, step: 142900, eta: 0.04 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11919 sec, avg_samples: 32.00000, ips: 536.94877 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 143000, eta: 0.03 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11922 sec, avg_samples: 32.00000, ips: 536.84496 images/sec
Training: 2023-09-28 11:05:33,238 - loss nan, lr: 0.000013, epoch: 24, step: 143000, eta: 0.03 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11922 sec, avg_samples: 32.00000, ips: 536.84496 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 143100, eta: 0.03 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11920 sec, avg_samples: 32.00000, ips: 536.89025 images/sec
Training: 2023-09-28 11:05:45,163 - loss nan, lr: 0.000013, epoch: 24, step: 143100, eta: 0.03 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11920 sec, avg_samples: 32.00000, ips: 536.89025 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 143200, eta: 0.02 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11920 sec, avg_samples: 32.00000, ips: 536.93056 images/sec
Training: 2023-09-28 11:05:57,088 - loss nan, lr: 0.000013, epoch: 24, step: 143200, eta: 0.02 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11920 sec, avg_samples: 32.00000, ips: 536.93056 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 143300, eta: 0.02 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11920 sec, avg_samples: 32.00000, ips: 536.93354 images/sec
Training: 2023-09-28 11:06:09,013 - loss nan, lr: 0.000013, epoch: 24, step: 143300, eta: 0.02 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11920 sec, avg_samples: 32.00000, ips: 536.93354 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 143400, eta: 0.01 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11920 sec, avg_samples: 32.00000, ips: 536.89719 images/sec
Training: 2023-09-28 11:06:20,938 - loss nan, lr: 0.000013, epoch: 24, step: 143400, eta: 0.01 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11920 sec, avg_samples: 32.00000, ips: 536.89719 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 143500, eta: 0.01 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11923 sec, avg_samples: 32.00000, ips: 536.77434 images/sec
Training: 2023-09-28 11:06:32,867 - loss nan, lr: 0.000013, epoch: 24, step: 143500, eta: 0.01 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11923 sec, avg_samples: 32.00000, ips: 536.77434 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 143600, eta: 0.00 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11923 sec, avg_samples: 32.00000, ips: 536.79302 images/sec
Training: 2023-09-28 11:06:44,795 - loss nan, lr: 0.000013, epoch: 24, step: 143600, eta: 0.00 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.11923 sec, avg_samples: 32.00000, ips: 536.79302 images/sec
INFO:root:loss nan, lr: 0.000013, epoch: 24, step: 143700, eta: -0.00 hours, avg_reader_cost: 0.00011 sec, avg_batch_cost: 0.11925 sec, avg_samples: 32.00000, ips: 536.70095 images/sec
Training: 2023-09-28 11:06:56,724 - loss nan, lr: 0.000013, epoch: 24, step: 143700, eta: -0.00 hours, avg_reader_cost: 0.00011 sec, avg_batch_cost: 0.11925 sec, avg_samples: 32.00000, ips: 536.70095 images/sec
INFO:root:Save model to MS1M_v3_arcface_static_0.1/FresResNet50/24.
Training: 2023-09-28 11:06:56,966 - Save model to MS1M_v3_arcface_static_0.1/FresResNet50/24.
INFO:root:Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/23.
Training: 2023-09-28 11:06:56,966 - Remove checkpoint MS1M_v3_arcface_static_0.1/FresResNet50/23.
grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
Traceback (most recent call last):
  File "tools/train.py", line 24, in <module>
    args = parser.parse_args()
  File "/paddle/train/arcface_paddle/configs/argparser.py", line 72, in parse_args
    cfg = get_config(user_namespace.config_file)
  File "/paddle/train/arcface_paddle/configs/argparser.py", line 51, in get_config
    config = importlib.import_module("configs.config")
  File "/usr/lib/python3.7/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1006, in _gcd_import
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 967, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/paddle/train/arcface_paddle/configs/config.py", line 15, in <module>
    from easydict import EasyDict as edict
ModuleNotFoundError: No module named 'easydict'
grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
usage: train.py [-h] --config_file CONFIG_FILE [--is_static IS_STATIC]
                [--backbone BACKBONE] [--classifier CLASSIFIER]
                [--embedding_size EMBEDDING_SIZE]
                [--model_parallel MODEL_PARALLEL]
                [--sample_ratio SAMPLE_RATIO] [--loss LOSS]
                [--dropout DROPOUT] [--fp16 FP16]
                [--init_loss_scaling INIT_LOSS_SCALING]
                [--max_loss_scaling MAX_LOSS_SCALING]
                [--incr_every_n_steps INCR_EVERY_N_STEPS]
                [--decr_every_n_nan_or_inf DECR_EVERY_N_NAN_OR_INF]
                [--incr_ratio INCR_RATIO] [--decr_ratio DECR_RATIO]
                [--use_dynamic_loss_scaling USE_DYNAMIC_LOSS_SCALING]
                [--custom_white_list CUSTOM_WHITE_LIST]
                [--custom_black_list CUSTOM_BLACK_LIST] [--lr LR]
                [--lr_decay LR_DECAY] [--weight_decay WEIGHT_DECAY]
                [--momentum MOMENTUM] [--train_unit TRAIN_UNIT]
                [--warmup_num WARMUP_NUM] [--train_num TRAIN_NUM]
                [--decay_boundaries DECAY_BOUNDARIES]
                [--use_synthetic_dataset USE_SYNTHETIC_DATASET]
                [--dataset DATASET] [--data_dir DATA_DIR]
                [--label_file LABEL_FILE] [--is_bin IS_BIN]
                [--num_classes NUM_CLASSES] [--batch_size BATCH_SIZE]
                [--num_workers NUM_WORKERS]
                [--do_validation_while_train DO_VALIDATION_WHILE_TRAIN]
                [--validation_interval_step VALIDATION_INTERVAL_STEP]
                [--val_targets VAL_TARGETS] [--logdir LOGDIR]
                [--log_interval_step LOG_INTERVAL_STEP] [--output OUTPUT]
                [--resume RESUME] [--checkpoint_dir CHECKPOINT_DIR]
                [--max_num_last_checkpoint MAX_NUM_LAST_CHECKPOINT]
train.py: error: argument --num_classes: expected one argument
grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:34: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  method='lar', copy_X=True, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:164: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  method='lar', copy_X=True, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:281: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:865: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1121: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1149: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, positive=False):
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1379: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1621: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1755: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, positive=False):
/usr/local/lib/python3.7/dist-packages/sklearn/decomposition/_lda.py:28: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  EPS = np.finfo(np.float).eps
INFO:root:rank: 0
Training: 2023-10-17 10:50:31,992 - rank: 0
INFO:root:--------args----------
Training: 2023-10-17 10:50:31,992 - --------args----------
INFO:root:config_file: configs/ms1mv3_r50.py
Training: 2023-10-17 10:50:31,992 - config_file: configs/ms1mv3_r50.py
INFO:root:is_static: True
Training: 2023-10-17 10:50:31,992 - is_static: True
INFO:root:backbone: FresResNet101
Training: 2023-10-17 10:50:31,992 - backbone: FresResNet101
INFO:root:classifier: LargeScaleClassifier
Training: 2023-10-17 10:50:31,992 - classifier: LargeScaleClassifier
INFO:root:embedding_size: 512
Training: 2023-10-17 10:50:31,992 - embedding_size: 512
INFO:root:model_parallel: True
Training: 2023-10-17 10:50:31,992 - model_parallel: True
INFO:root:sample_ratio: 0.1
Training: 2023-10-17 10:50:31,992 - sample_ratio: 0.1
INFO:root:loss: ArcFace
Training: 2023-10-17 10:50:31,992 - loss: ArcFace
INFO:root:dropout: 0.0
Training: 2023-10-17 10:50:31,992 - dropout: 0.0
INFO:root:fp16: True
Training: 2023-10-17 10:50:31,992 - fp16: True
INFO:root:init_loss_scaling: 27648.0
Training: 2023-10-17 10:50:31,992 - init_loss_scaling: 27648.0
INFO:root:max_loss_scaling: 128.0
Training: 2023-10-17 10:50:31,992 - max_loss_scaling: 128.0
INFO:root:incr_every_n_steps: 2000
Training: 2023-10-17 10:50:31,992 - incr_every_n_steps: 2000
INFO:root:decr_every_n_nan_or_inf: 1
Training: 2023-10-17 10:50:31,992 - decr_every_n_nan_or_inf: 1
INFO:root:incr_ratio: 2.0
Training: 2023-10-17 10:50:31,992 - incr_ratio: 2.0
INFO:root:decr_ratio: 0.5
Training: 2023-10-17 10:50:31,992 - decr_ratio: 0.5
INFO:root:use_dynamic_loss_scaling: True
Training: 2023-10-17 10:50:31,993 - use_dynamic_loss_scaling: True
INFO:root:custom_white_list: []
Training: 2023-10-17 10:50:31,993 - custom_white_list: []
INFO:root:custom_black_list: []
Training: 2023-10-17 10:50:31,993 - custom_black_list: []
INFO:root:lr: 0.1
Training: 2023-10-17 10:50:31,993 - lr: 0.1
INFO:root:lr_decay: 0.1
Training: 2023-10-17 10:50:31,993 - lr_decay: 0.1
INFO:root:weight_decay: 0.0005
Training: 2023-10-17 10:50:31,993 - weight_decay: 0.0005
INFO:root:momentum: 0.9
Training: 2023-10-17 10:50:31,993 - momentum: 0.9
INFO:root:train_unit: epoch
Training: 2023-10-17 10:50:31,993 - train_unit: epoch
INFO:root:warmup_num: 0
Training: 2023-10-17 10:50:31,993 - warmup_num: 0
INFO:root:train_num: 25
Training: 2023-10-17 10:50:31,993 - train_num: 25
INFO:root:decay_boundaries: [10, 16, 22]
Training: 2023-10-17 10:50:31,993 - decay_boundaries: [10, 16, 22]
INFO:root:use_synthetic_dataset: False
Training: 2023-10-17 10:50:31,993 - use_synthetic_dataset: False
INFO:root:dataset: MS1M_v3
Training: 2023-10-17 10:50:31,993 - dataset: MS1M_v3
INFO:root:data_dir: /paddle/dataset/
Training: 2023-10-17 10:50:31,993 - data_dir: /paddle/dataset/
INFO:root:label_file: /paddle/dataset/label.txt
Training: 2023-10-17 10:50:31,993 - label_file: /paddle/dataset/label.txt
INFO:root:is_bin: False
Training: 2023-10-17 10:50:31,993 - is_bin: False
INFO:root:num_classes: 657077
Training: 2023-10-17 10:50:31,993 - num_classes: 657077
INFO:root:batch_size: 32
Training: 2023-10-17 10:50:31,993 - batch_size: 32
INFO:root:num_workers: 2
Training: 2023-10-17 10:50:31,993 - num_workers: 2
INFO:root:do_validation_while_train: True
Training: 2023-10-17 10:50:31,993 - do_validation_while_train: True
INFO:root:validation_interval_step: 2000
Training: 2023-10-17 10:50:31,993 - validation_interval_step: 2000
INFO:root:val_targets: ['lfw', 'cfp_fp', 'agedb_30']
Training: 2023-10-17 10:50:31,993 - val_targets: ['lfw', 'cfp_fp', 'agedb_30']
INFO:root:logdir: ./log
Training: 2023-10-17 10:50:31,993 - logdir: ./log
INFO:root:log_interval_step: 100
Training: 2023-10-17 10:50:31,993 - log_interval_step: 100
INFO:root:output: model
Training: 2023-10-17 10:50:31,993 - output: model
INFO:root:resume: False
Training: 2023-10-17 10:50:31,993 - resume: False
INFO:root:checkpoint_dir: None
Training: 2023-10-17 10:50:31,993 - checkpoint_dir: None
INFO:root:max_num_last_checkpoint: 1
Training: 2023-10-17 10:50:31,993 - max_num_last_checkpoint: 1
INFO:root:------------------------

Training: 2023-10-17 10:50:31,993 - ------------------------

Traceback (most recent call last):
  File "tools/train.py", line 35, in <module>
    train(args)
  File "/paddle/train/arcface_paddle/static/train.py", line 67, in train
    is_bin=args.is_bin)
  File "/paddle/train/arcface_paddle/datasets/common_dataset.py", line 48, in __init__
    with open(label_file, "r") as fin:
FileNotFoundError: [Errno 2] No such file or directory: '/paddle/dataset/label.txt'
grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:34: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  method='lar', copy_X=True, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:164: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  method='lar', copy_X=True, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:281: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:865: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1121: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1149: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, positive=False):
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1379: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1621: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1755: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, positive=False):
/usr/local/lib/python3.7/dist-packages/sklearn/decomposition/_lda.py:28: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  EPS = np.finfo(np.float).eps
INFO:root:rank: 0
Training: 2023-10-17 10:52:20,081 - rank: 0
INFO:root:--------args----------
Training: 2023-10-17 10:52:20,081 - --------args----------
INFO:root:config_file: configs/ms1mv3_r100.py
Training: 2023-10-17 10:52:20,081 - config_file: configs/ms1mv3_r100.py
INFO:root:is_static: True
Training: 2023-10-17 10:52:20,081 - is_static: True
INFO:root:backbone: FresResNet101
Training: 2023-10-17 10:52:20,081 - backbone: FresResNet101
INFO:root:classifier: LargeScaleClassifier
Training: 2023-10-17 10:52:20,081 - classifier: LargeScaleClassifier
INFO:root:embedding_size: 512
Training: 2023-10-17 10:52:20,081 - embedding_size: 512
INFO:root:model_parallel: True
Training: 2023-10-17 10:52:20,081 - model_parallel: True
INFO:root:sample_ratio: 0.1
Training: 2023-10-17 10:52:20,081 - sample_ratio: 0.1
INFO:root:loss: ArcFace
Training: 2023-10-17 10:52:20,081 - loss: ArcFace
INFO:root:dropout: 0.0
Training: 2023-10-17 10:52:20,081 - dropout: 0.0
INFO:root:fp16: True
Training: 2023-10-17 10:52:20,081 - fp16: True
INFO:root:init_loss_scaling: 27648.0
Training: 2023-10-17 10:52:20,082 - init_loss_scaling: 27648.0
INFO:root:max_loss_scaling: 128.0
Training: 2023-10-17 10:52:20,082 - max_loss_scaling: 128.0
INFO:root:incr_every_n_steps: 2000
Training: 2023-10-17 10:52:20,082 - incr_every_n_steps: 2000
INFO:root:decr_every_n_nan_or_inf: 1
Training: 2023-10-17 10:52:20,082 - decr_every_n_nan_or_inf: 1
INFO:root:incr_ratio: 2.0
Training: 2023-10-17 10:52:20,082 - incr_ratio: 2.0
INFO:root:decr_ratio: 0.5
Training: 2023-10-17 10:52:20,082 - decr_ratio: 0.5
INFO:root:use_dynamic_loss_scaling: True
Training: 2023-10-17 10:52:20,082 - use_dynamic_loss_scaling: True
INFO:root:custom_white_list: []
Training: 2023-10-17 10:52:20,082 - custom_white_list: []
INFO:root:custom_black_list: []
Training: 2023-10-17 10:52:20,082 - custom_black_list: []
INFO:root:lr: 0.1
Training: 2023-10-17 10:52:20,082 - lr: 0.1
INFO:root:lr_decay: 0.1
Training: 2023-10-17 10:52:20,082 - lr_decay: 0.1
INFO:root:weight_decay: 0.0005
Training: 2023-10-17 10:52:20,082 - weight_decay: 0.0005
INFO:root:momentum: 0.9
Training: 2023-10-17 10:52:20,082 - momentum: 0.9
INFO:root:train_unit: epoch
Training: 2023-10-17 10:52:20,082 - train_unit: epoch
INFO:root:warmup_num: 0
Training: 2023-10-17 10:52:20,082 - warmup_num: 0
INFO:root:train_num: 25
Training: 2023-10-17 10:52:20,082 - train_num: 25
INFO:root:decay_boundaries: [10, 16, 22]
Training: 2023-10-17 10:52:20,082 - decay_boundaries: [10, 16, 22]
INFO:root:use_synthetic_dataset: False
Training: 2023-10-17 10:52:20,082 - use_synthetic_dataset: False
INFO:root:dataset: MS1M_v3
Training: 2023-10-17 10:52:20,082 - dataset: MS1M_v3
INFO:root:data_dir: ../dataset/
Training: 2023-10-17 10:52:20,082 - data_dir: ../dataset/
INFO:root:label_file: ../dataset/label.txt
Training: 2023-10-17 10:52:20,082 - label_file: ../dataset/label.txt
INFO:root:is_bin: False
Training: 2023-10-17 10:52:20,082 - is_bin: False
INFO:root:num_classes: 657077
Training: 2023-10-17 10:52:20,082 - num_classes: 657077
INFO:root:batch_size: 32
Training: 2023-10-17 10:52:20,082 - batch_size: 32
INFO:root:num_workers: 2
Training: 2023-10-17 10:52:20,082 - num_workers: 2
INFO:root:do_validation_while_train: True
Training: 2023-10-17 10:52:20,082 - do_validation_while_train: True
INFO:root:validation_interval_step: 2000
Training: 2023-10-17 10:52:20,082 - validation_interval_step: 2000
INFO:root:val_targets: ['lfw', 'cfp_fp', 'agedb_30']
Training: 2023-10-17 10:52:20,082 - val_targets: ['lfw', 'cfp_fp', 'agedb_30']
INFO:root:logdir: ./log
Training: 2023-10-17 10:52:20,082 - logdir: ./log
INFO:root:log_interval_step: 100
Training: 2023-10-17 10:52:20,082 - log_interval_step: 100
INFO:root:output: model
Training: 2023-10-17 10:52:20,083 - output: model
INFO:root:resume: False
Training: 2023-10-17 10:52:20,083 - resume: False
INFO:root:checkpoint_dir: None
Training: 2023-10-17 10:52:20,083 - checkpoint_dir: None
INFO:root:max_num_last_checkpoint: 1
Training: 2023-10-17 10:52:20,083 - max_num_last_checkpoint: 1
INFO:root:------------------------

Training: 2023-10-17 10:52:20,083 - ------------------------

INFO:root:read label file finished, total num: 4574213
Training: 2023-10-17 10:52:20,335 - read label file finished, total num: 4574213
INFO:root:world_size: 2
Training: 2023-10-17 10:52:20,335 - world_size: 2
INFO:root:total_batch_size: 64
Training: 2023-10-17 10:52:20,335 - total_batch_size: 64
INFO:root:warmup_steps: 0
Training: 2023-10-17 10:52:20,335 - warmup_steps: 0
INFO:root:steps_per_epoch: 71472
Training: 2023-10-17 10:52:20,335 - steps_per_epoch: 71472
INFO:root:total_steps: 1786800
Training: 2023-10-17 10:52:20,335 - total_steps: 1786800
INFO:root:total_epoch: 25
Training: 2023-10-17 10:52:20,335 - total_epoch: 25
INFO:root:decay_steps: [714720, 1143552, 1572384]
Training: 2023-10-17 10:52:20,335 - decay_steps: [714720, 1143552, 1572384]
Traceback (most recent call last):
  File "tools/train.py", line 35, in <module>
    train(args)
  File "/paddle/train/arcface_paddle/static/train.py", line 132, in train
    margin_loss_params=margin_loss_params, )
  File "/paddle/train/arcface_paddle/static/static_model.py", line 79, in __init__
    self.backbone_class_name))(
  File "<string>", line 1, in <module>
AttributeError: module 'static.backbones' has no attribute 'FresResNet101'
grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:34: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  method='lar', copy_X=True, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:164: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  method='lar', copy_X=True, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:281: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:865: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1121: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1149: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, positive=False):
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1379: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1621: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1755: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, positive=False):
/usr/local/lib/python3.7/dist-packages/sklearn/decomposition/_lda.py:28: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  EPS = np.finfo(np.float).eps
INFO:root:rank: 0
Training: 2023-10-17 10:53:56,189 - rank: 0
INFO:root:--------args----------
Training: 2023-10-17 10:53:56,189 - --------args----------
INFO:root:config_file: configs/ms1mv3_r50.py
Training: 2023-10-17 10:53:56,189 - config_file: configs/ms1mv3_r50.py
INFO:root:is_static: True
Training: 2023-10-17 10:53:56,189 - is_static: True
INFO:root:backbone: FresResNet101
Training: 2023-10-17 10:53:56,189 - backbone: FresResNet101
INFO:root:classifier: LargeScaleClassifier
Training: 2023-10-17 10:53:56,189 - classifier: LargeScaleClassifier
INFO:root:embedding_size: 512
Training: 2023-10-17 10:53:56,189 - embedding_size: 512
INFO:root:model_parallel: True
Training: 2023-10-17 10:53:56,189 - model_parallel: True
INFO:root:sample_ratio: 0.1
Training: 2023-10-17 10:53:56,189 - sample_ratio: 0.1
INFO:root:loss: ArcFace
Training: 2023-10-17 10:53:56,189 - loss: ArcFace
INFO:root:dropout: 0.0
Training: 2023-10-17 10:53:56,189 - dropout: 0.0
INFO:root:fp16: True
Training: 2023-10-17 10:53:56,189 - fp16: True
INFO:root:init_loss_scaling: 27648.0
Training: 2023-10-17 10:53:56,189 - init_loss_scaling: 27648.0
INFO:root:max_loss_scaling: 128.0
Training: 2023-10-17 10:53:56,189 - max_loss_scaling: 128.0
INFO:root:incr_every_n_steps: 2000
Training: 2023-10-17 10:53:56,189 - incr_every_n_steps: 2000
INFO:root:decr_every_n_nan_or_inf: 1
Training: 2023-10-17 10:53:56,189 - decr_every_n_nan_or_inf: 1
INFO:root:incr_ratio: 2.0
Training: 2023-10-17 10:53:56,189 - incr_ratio: 2.0
INFO:root:decr_ratio: 0.5
Training: 2023-10-17 10:53:56,189 - decr_ratio: 0.5
INFO:root:use_dynamic_loss_scaling: True
Training: 2023-10-17 10:53:56,189 - use_dynamic_loss_scaling: True
INFO:root:custom_white_list: []
Training: 2023-10-17 10:53:56,189 - custom_white_list: []
INFO:root:custom_black_list: []
Training: 2023-10-17 10:53:56,189 - custom_black_list: []
INFO:root:lr: 0.1
Training: 2023-10-17 10:53:56,189 - lr: 0.1
INFO:root:lr_decay: 0.1
Training: 2023-10-17 10:53:56,190 - lr_decay: 0.1
INFO:root:weight_decay: 0.0005
Training: 2023-10-17 10:53:56,190 - weight_decay: 0.0005
INFO:root:momentum: 0.9
Training: 2023-10-17 10:53:56,190 - momentum: 0.9
INFO:root:train_unit: epoch
Training: 2023-10-17 10:53:56,190 - train_unit: epoch
INFO:root:warmup_num: 0
Training: 2023-10-17 10:53:56,190 - warmup_num: 0
INFO:root:train_num: 25
Training: 2023-10-17 10:53:56,190 - train_num: 25
INFO:root:decay_boundaries: [10, 16, 22]
Training: 2023-10-17 10:53:56,190 - decay_boundaries: [10, 16, 22]
INFO:root:use_synthetic_dataset: False
Training: 2023-10-17 10:53:56,190 - use_synthetic_dataset: False
INFO:root:dataset: MS1M_v3
Training: 2023-10-17 10:53:56,190 - dataset: MS1M_v3
INFO:root:data_dir: ../dataset/
Training: 2023-10-17 10:53:56,190 - data_dir: ../dataset/
INFO:root:label_file: ../dataset/label.txt
Training: 2023-10-17 10:53:56,190 - label_file: ../dataset/label.txt
INFO:root:is_bin: False
Training: 2023-10-17 10:53:56,190 - is_bin: False
INFO:root:num_classes: 657077
Training: 2023-10-17 10:53:56,190 - num_classes: 657077
INFO:root:batch_size: 32
Training: 2023-10-17 10:53:56,190 - batch_size: 32
INFO:root:num_workers: 2
Training: 2023-10-17 10:53:56,190 - num_workers: 2
INFO:root:do_validation_while_train: True
Training: 2023-10-17 10:53:56,190 - do_validation_while_train: True
INFO:root:validation_interval_step: 2000
Training: 2023-10-17 10:53:56,190 - validation_interval_step: 2000
INFO:root:val_targets: ['lfw', 'cfp_fp', 'agedb_30']
Training: 2023-10-17 10:53:56,190 - val_targets: ['lfw', 'cfp_fp', 'agedb_30']
INFO:root:logdir: ./log
Training: 2023-10-17 10:53:56,190 - logdir: ./log
INFO:root:log_interval_step: 100
Training: 2023-10-17 10:53:56,190 - log_interval_step: 100
INFO:root:output: model
Training: 2023-10-17 10:53:56,190 - output: model
INFO:root:resume: False
Training: 2023-10-17 10:53:56,190 - resume: False
INFO:root:checkpoint_dir: None
Training: 2023-10-17 10:53:56,190 - checkpoint_dir: None
INFO:root:max_num_last_checkpoint: 1
Training: 2023-10-17 10:53:56,190 - max_num_last_checkpoint: 1
INFO:root:------------------------

Training: 2023-10-17 10:53:56,190 - ------------------------

INFO:root:read label file finished, total num: 4574213
Training: 2023-10-17 10:53:56,457 - read label file finished, total num: 4574213
INFO:root:world_size: 2
Training: 2023-10-17 10:53:56,457 - world_size: 2
INFO:root:total_batch_size: 64
Training: 2023-10-17 10:53:56,457 - total_batch_size: 64
INFO:root:warmup_steps: 0
Training: 2023-10-17 10:53:56,457 - warmup_steps: 0
INFO:root:steps_per_epoch: 71472
Training: 2023-10-17 10:53:56,457 - steps_per_epoch: 71472
INFO:root:total_steps: 1786800
Training: 2023-10-17 10:53:56,457 - total_steps: 1786800
INFO:root:total_epoch: 25
Training: 2023-10-17 10:53:56,457 - total_epoch: 25
INFO:root:decay_steps: [714720, 1143552, 1572384]
Training: 2023-10-17 10:53:56,457 - decay_steps: [714720, 1143552, 1572384]
Traceback (most recent call last):
  File "tools/train.py", line 35, in <module>
    train(args)
  File "/paddle/train/arcface_paddle/static/train.py", line 132, in train
    margin_loss_params=margin_loss_params, )
  File "/paddle/train/arcface_paddle/static/static_model.py", line 79, in __init__
    self.backbone_class_name))(
  File "<string>", line 1, in <module>
AttributeError: module 'static.backbones' has no attribute 'FresResNet101'
grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:34: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  method='lar', copy_X=True, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:164: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  method='lar', copy_X=True, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:281: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:865: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1121: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1149: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, positive=False):
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1379: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1621: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1755: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, positive=False):
/usr/local/lib/python3.7/dist-packages/sklearn/decomposition/_lda.py:28: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  EPS = np.finfo(np.float).eps
INFO:root:rank: 0
Training: 2023-10-17 10:56:19,911 - rank: 0
INFO:root:--------args----------
Training: 2023-10-17 10:56:19,911 - --------args----------
INFO:root:config_file: configs/ms1mv3_r50.py
Training: 2023-10-17 10:56:19,911 - config_file: configs/ms1mv3_r50.py
INFO:root:is_static: True
Training: 2023-10-17 10:56:19,911 - is_static: True
INFO:root:backbone: FresResNet50
Training: 2023-10-17 10:56:19,911 - backbone: FresResNet50
INFO:root:classifier: LargeScaleClassifier
Training: 2023-10-17 10:56:19,911 - classifier: LargeScaleClassifier
INFO:root:embedding_size: 512
Training: 2023-10-17 10:56:19,911 - embedding_size: 512
INFO:root:model_parallel: True
Training: 2023-10-17 10:56:19,911 - model_parallel: True
INFO:root:sample_ratio: 0.1
Training: 2023-10-17 10:56:19,911 - sample_ratio: 0.1
INFO:root:loss: ArcFace
Training: 2023-10-17 10:56:19,911 - loss: ArcFace
INFO:root:dropout: 0.0
Training: 2023-10-17 10:56:19,911 - dropout: 0.0
INFO:root:fp16: True
Training: 2023-10-17 10:56:19,911 - fp16: True
INFO:root:init_loss_scaling: 27648.0
Training: 2023-10-17 10:56:19,911 - init_loss_scaling: 27648.0
INFO:root:max_loss_scaling: 128.0
Training: 2023-10-17 10:56:19,911 - max_loss_scaling: 128.0
INFO:root:incr_every_n_steps: 2000
Training: 2023-10-17 10:56:19,911 - incr_every_n_steps: 2000
INFO:root:decr_every_n_nan_or_inf: 1
Training: 2023-10-17 10:56:19,911 - decr_every_n_nan_or_inf: 1
INFO:root:incr_ratio: 2.0
Training: 2023-10-17 10:56:19,911 - incr_ratio: 2.0
INFO:root:decr_ratio: 0.5
Training: 2023-10-17 10:56:19,911 - decr_ratio: 0.5
INFO:root:use_dynamic_loss_scaling: True
Training: 2023-10-17 10:56:19,911 - use_dynamic_loss_scaling: True
INFO:root:custom_white_list: []
Training: 2023-10-17 10:56:19,911 - custom_white_list: []
INFO:root:custom_black_list: []
Training: 2023-10-17 10:56:19,911 - custom_black_list: []
INFO:root:lr: 0.1
Training: 2023-10-17 10:56:19,911 - lr: 0.1
INFO:root:lr_decay: 0.1
Training: 2023-10-17 10:56:19,911 - lr_decay: 0.1
INFO:root:weight_decay: 0.0005
Training: 2023-10-17 10:56:19,912 - weight_decay: 0.0005
INFO:root:momentum: 0.9
Training: 2023-10-17 10:56:19,912 - momentum: 0.9
INFO:root:train_unit: epoch
Training: 2023-10-17 10:56:19,912 - train_unit: epoch
INFO:root:warmup_num: 0
Training: 2023-10-17 10:56:19,912 - warmup_num: 0
INFO:root:train_num: 25
Training: 2023-10-17 10:56:19,912 - train_num: 25
INFO:root:decay_boundaries: [10, 16, 22]
Training: 2023-10-17 10:56:19,912 - decay_boundaries: [10, 16, 22]
INFO:root:use_synthetic_dataset: False
Training: 2023-10-17 10:56:19,912 - use_synthetic_dataset: False
INFO:root:dataset: MS1M_v3
Training: 2023-10-17 10:56:19,912 - dataset: MS1M_v3
INFO:root:data_dir: ../dataset/
Training: 2023-10-17 10:56:19,912 - data_dir: ../dataset/
INFO:root:label_file: ../dataset/label.txt
Training: 2023-10-17 10:56:19,912 - label_file: ../dataset/label.txt
INFO:root:is_bin: False
Training: 2023-10-17 10:56:19,912 - is_bin: False
INFO:root:num_classes: 657077
Training: 2023-10-17 10:56:19,912 - num_classes: 657077
INFO:root:batch_size: 32
Training: 2023-10-17 10:56:19,912 - batch_size: 32
INFO:root:num_workers: 2
Training: 2023-10-17 10:56:19,912 - num_workers: 2
INFO:root:do_validation_while_train: True
Training: 2023-10-17 10:56:19,912 - do_validation_while_train: True
INFO:root:validation_interval_step: 2000
Training: 2023-10-17 10:56:19,912 - validation_interval_step: 2000
INFO:root:val_targets: ['lfw', 'cfp_fp', 'agedb_30']
Training: 2023-10-17 10:56:19,912 - val_targets: ['lfw', 'cfp_fp', 'agedb_30']
INFO:root:logdir: ./log
Training: 2023-10-17 10:56:19,912 - logdir: ./log
INFO:root:log_interval_step: 100
Training: 2023-10-17 10:56:19,912 - log_interval_step: 100
INFO:root:output: model
Training: 2023-10-17 10:56:19,912 - output: model
INFO:root:resume: False
Training: 2023-10-17 10:56:19,912 - resume: False
INFO:root:checkpoint_dir: None
Training: 2023-10-17 10:56:19,912 - checkpoint_dir: None
INFO:root:max_num_last_checkpoint: 1
Training: 2023-10-17 10:56:19,912 - max_num_last_checkpoint: 1
INFO:root:------------------------

Training: 2023-10-17 10:56:19,912 - ------------------------

INFO:root:read label file finished, total num: 4574213
Training: 2023-10-17 10:56:20,182 - read label file finished, total num: 4574213
INFO:root:world_size: 2
Training: 2023-10-17 10:56:20,182 - world_size: 2
INFO:root:total_batch_size: 64
Training: 2023-10-17 10:56:20,182 - total_batch_size: 64
INFO:root:warmup_steps: 0
Training: 2023-10-17 10:56:20,182 - warmup_steps: 0
INFO:root:steps_per_epoch: 71472
Training: 2023-10-17 10:56:20,182 - steps_per_epoch: 71472
INFO:root:total_steps: 1786800
Training: 2023-10-17 10:56:20,182 - total_steps: 1786800
INFO:root:total_epoch: 25
Training: 2023-10-17 10:56:20,182 - total_epoch: 25
INFO:root:decay_steps: [714720, 1143552, 1572384]
Training: 2023-10-17 10:56:20,182 - decay_steps: [714720, 1143552, 1572384]
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in conv2d only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'Out' in guassian_random only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in batch_norm only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in prelu only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in elementwise_add only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'y' in elementwise_add only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in fc only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'Out' in xavier_init only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'dtype' in create_parameter only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'tensor' in all_gather only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in split only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input[0]' in concat only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input[1]' in concat only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in gather only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'X' in norm only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in matmul only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'y' in matmul only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'logits' in margin_cross_entropy only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/framework.py:744: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  elif dtype == np.bool:
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in check_finite_and_unscale only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in update_loss_scaling only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/optimizer/momentum.py:262: UserWarning: Accumulating with FP16 in optimizer can lead to poor accuracy or slow convergence.Consider using multi_precision=True option of the Momentum optimizer.
  "Accumulating with FP16 in optimizer can lead to poor accuracy or slow convergence."
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:45783']
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
loading bin 0
loading bin 1000
loading bin 2000


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   ImagingJpegDecode

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1697540191 (unix time) try "date -d @1697540191" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x71f) received by PID 1905 (TID 0x7fe401a15740) from PID 1823 ***]

grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:34: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  method='lar', copy_X=True, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:164: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  method='lar', copy_X=True, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:281: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:865: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1121: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1149: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, positive=False):
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1379: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1621: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1755: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, positive=False):
/usr/local/lib/python3.7/dist-packages/sklearn/decomposition/_lda.py:28: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  EPS = np.finfo(np.float).eps
INFO:root:rank: 0
Training: 2023-10-17 11:00:57,518 - rank: 0
INFO:root:--------args----------
Training: 2023-10-17 11:00:57,518 - --------args----------
INFO:root:config_file: configs/ms1mv3_r50.py
Training: 2023-10-17 11:00:57,518 - config_file: configs/ms1mv3_r50.py
INFO:root:is_static: True
Training: 2023-10-17 11:00:57,518 - is_static: True
INFO:root:backbone: FresResNet101
Training: 2023-10-17 11:00:57,518 - backbone: FresResNet101
INFO:root:classifier: LargeScaleClassifier
Training: 2023-10-17 11:00:57,519 - classifier: LargeScaleClassifier
INFO:root:embedding_size: 512
Training: 2023-10-17 11:00:57,519 - embedding_size: 512
INFO:root:model_parallel: True
Training: 2023-10-17 11:00:57,519 - model_parallel: True
INFO:root:sample_ratio: 0.1
Training: 2023-10-17 11:00:57,519 - sample_ratio: 0.1
INFO:root:loss: ArcFace
Training: 2023-10-17 11:00:57,519 - loss: ArcFace
INFO:root:dropout: 0.0
Training: 2023-10-17 11:00:57,519 - dropout: 0.0
INFO:root:fp16: True
Training: 2023-10-17 11:00:57,519 - fp16: True
INFO:root:init_loss_scaling: 27648.0
Training: 2023-10-17 11:00:57,519 - init_loss_scaling: 27648.0
INFO:root:max_loss_scaling: 128.0
Training: 2023-10-17 11:00:57,519 - max_loss_scaling: 128.0
INFO:root:incr_every_n_steps: 2000
Training: 2023-10-17 11:00:57,519 - incr_every_n_steps: 2000
INFO:root:decr_every_n_nan_or_inf: 1
Training: 2023-10-17 11:00:57,519 - decr_every_n_nan_or_inf: 1
INFO:root:incr_ratio: 2.0
Training: 2023-10-17 11:00:57,519 - incr_ratio: 2.0
INFO:root:decr_ratio: 0.5
Training: 2023-10-17 11:00:57,519 - decr_ratio: 0.5
INFO:root:use_dynamic_loss_scaling: True
Training: 2023-10-17 11:00:57,519 - use_dynamic_loss_scaling: True
INFO:root:custom_white_list: []
Training: 2023-10-17 11:00:57,519 - custom_white_list: []
INFO:root:custom_black_list: []
Training: 2023-10-17 11:00:57,519 - custom_black_list: []
INFO:root:lr: 0.1
Training: 2023-10-17 11:00:57,519 - lr: 0.1
INFO:root:lr_decay: 0.1
Training: 2023-10-17 11:00:57,519 - lr_decay: 0.1
INFO:root:weight_decay: 0.0005
Training: 2023-10-17 11:00:57,519 - weight_decay: 0.0005
INFO:root:momentum: 0.9
Training: 2023-10-17 11:00:57,519 - momentum: 0.9
INFO:root:train_unit: epoch
Training: 2023-10-17 11:00:57,519 - train_unit: epoch
INFO:root:warmup_num: 0
Training: 2023-10-17 11:00:57,519 - warmup_num: 0
INFO:root:train_num: 25
Training: 2023-10-17 11:00:57,519 - train_num: 25
INFO:root:decay_boundaries: [10, 16, 22]
Training: 2023-10-17 11:00:57,519 - decay_boundaries: [10, 16, 22]
INFO:root:use_synthetic_dataset: False
Training: 2023-10-17 11:00:57,519 - use_synthetic_dataset: False
INFO:root:dataset: MS1M_v3
Training: 2023-10-17 11:00:57,519 - dataset: MS1M_v3
INFO:root:data_dir: ../dataset/
Training: 2023-10-17 11:00:57,519 - data_dir: ../dataset/
INFO:root:label_file: ../dataset/label.txt
Training: 2023-10-17 11:00:57,519 - label_file: ../dataset/label.txt
INFO:root:is_bin: False
Training: 2023-10-17 11:00:57,519 - is_bin: False
INFO:root:num_classes: 657077
Training: 2023-10-17 11:00:57,519 - num_classes: 657077
INFO:root:batch_size: 32
Training: 2023-10-17 11:00:57,519 - batch_size: 32
INFO:root:num_workers: 2
Training: 2023-10-17 11:00:57,520 - num_workers: 2
INFO:root:do_validation_while_train: True
Training: 2023-10-17 11:00:57,520 - do_validation_while_train: True
INFO:root:validation_interval_step: 2000
Training: 2023-10-17 11:00:57,520 - validation_interval_step: 2000
INFO:root:val_targets: ['lfw', 'cfp_fp', 'agedb_30']
Training: 2023-10-17 11:00:57,520 - val_targets: ['lfw', 'cfp_fp', 'agedb_30']
INFO:root:logdir: ./log
Training: 2023-10-17 11:00:57,520 - logdir: ./log
INFO:root:log_interval_step: 100
Training: 2023-10-17 11:00:57,520 - log_interval_step: 100
INFO:root:output: model
Training: 2023-10-17 11:00:57,520 - output: model
INFO:root:resume: False
Training: 2023-10-17 11:00:57,520 - resume: False
INFO:root:checkpoint_dir: None
Training: 2023-10-17 11:00:57,520 - checkpoint_dir: None
INFO:root:max_num_last_checkpoint: 1
Training: 2023-10-17 11:00:57,520 - max_num_last_checkpoint: 1
INFO:root:------------------------

Training: 2023-10-17 11:00:57,520 - ------------------------

INFO:root:read label file finished, total num: 4574213
Training: 2023-10-17 11:00:57,793 - read label file finished, total num: 4574213
INFO:root:world_size: 2
Training: 2023-10-17 11:00:57,794 - world_size: 2
INFO:root:total_batch_size: 64
Training: 2023-10-17 11:00:57,794 - total_batch_size: 64
INFO:root:warmup_steps: 0
Training: 2023-10-17 11:00:57,794 - warmup_steps: 0
INFO:root:steps_per_epoch: 71472
Training: 2023-10-17 11:00:57,794 - steps_per_epoch: 71472
INFO:root:total_steps: 1786800
Training: 2023-10-17 11:00:57,794 - total_steps: 1786800
INFO:root:total_epoch: 25
Training: 2023-10-17 11:00:57,794 - total_epoch: 25
INFO:root:decay_steps: [714720, 1143552, 1572384]
Training: 2023-10-17 11:00:57,794 - decay_steps: [714720, 1143552, 1572384]
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in conv2d only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'Out' in guassian_random only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in batch_norm only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in prelu only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in elementwise_add only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'y' in elementwise_add only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in fc only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'Out' in xavier_init only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'dtype' in create_parameter only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'tensor' in all_gather only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in split only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input[0]' in concat only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input[1]' in concat only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in gather only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'X' in norm only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in matmul only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'y' in matmul only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'logits' in margin_cross_entropy only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/framework.py:744: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  elif dtype == np.bool:
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in check_finite_and_unscale only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in update_loss_scaling only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/optimizer/momentum.py:262: UserWarning: Accumulating with FP16 in optimizer can lead to poor accuracy or slow convergence.Consider using multi_precision=True option of the Momentum optimizer.
  "Accumulating with FP16 in optimizer can lead to poor accuracy or slow convergence."
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:45879']
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
(14000, 3, 112, 112)
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
W1017 11:01:19.173463  2041 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 12.0, Runtime API Version: 11.2
W1017 11:01:19.175122  2041 device_context.cc:465] device: 0, cuDNN Version: 8.1.
I1017 11:01:20.774173  2041 gen_comm_id_helper.cc:190] Server listening on: 127.0.0.1:35575 successful.
INFO:root:loss 48.1796, lr: 0.012500, epoch: 0, step: 100, eta: 135.67 hours, avg_reader_cost: 0.01164 sec, avg_batch_cost: 0.25829 sec, avg_samples: 32.00000, ips: 247.78089 images/sec
Training: 2023-10-17 11:01:46,723 - loss 48.1796, lr: 0.012500, epoch: 0, step: 100, eta: 135.67 hours, avg_reader_cost: 0.01164 sec, avg_batch_cost: 0.25829 sec, avg_samples: 32.00000, ips: 247.78089 images/sec
INFO:root:loss 46.9215, lr: 0.012500, epoch: 0, step: 200, eta: 124.60 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.22850 sec, avg_samples: 32.00000, ips: 280.08359 images/sec
Training: 2023-10-17 11:02:09,579 - loss 46.9215, lr: 0.012500, epoch: 0, step: 200, eta: 124.60 hours, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.22850 sec, avg_samples: 32.00000, ips: 280.08359 images/sec
INFO:root:loss 46.3607, lr: 0.012500, epoch: 0, step: 300, eta: 120.88 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.22850 sec, avg_samples: 32.00000, ips: 280.08407 images/sec
Training: 2023-10-17 11:02:32,436 - loss 46.3607, lr: 0.012500, epoch: 0, step: 300, eta: 120.88 hours, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.22850 sec, avg_samples: 32.00000, ips: 280.08407 images/sec


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::framework::Executor::RunPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, bool, bool, bool)
1   paddle::framework::Executor::RunPartialPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, long, long, bool, bool, bool)
2   paddle::framework::OperatorBase::Run(paddle::framework::Scope const&, paddle::platform::Place const&)
3   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&) const
4   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&, paddle::framework::RuntimeContext*) const
5   std::_Function_handler<void (paddle::framework::ExecutionContext const&), paddle::framework::OpKernelRegistrarFunctor<paddle::platform::CUDAPlace, false, 1ul, paddle::operators::ClassCenterSampleCUDAKernel<paddle::platform::CUDADeviceContext, long>, paddle::operators::ClassCenterSampleCUDAKernel<paddle::platform::CUDADeviceContext, int> >::operator()(char const*, char const*, int) const::{lambda(paddle::framework::ExecutionContext const&)#1}>::_M_invoke(std::_Any_data const&, paddle::framework::ExecutionContext const&)
6   paddle::framework::TensorCopySync(paddle::framework::Tensor const&, paddle::platform::Place const&, paddle::framework::Tensor*)
7   paddle::framework::TensorCopySync(paddle::framework::Tensor const&, paddle::platform::Place const&, paddle::framework::Tensor*)
8   void paddle::memory::Copy<paddle::platform::CPUPlace, paddle::platform::CUDAPlace>(paddle::platform::CPUPlace, void*, paddle::platform::CUDAPlace, void const*, unsigned long, CUstream_st*)
9   paddle::platform::GpuMemcpySync(void*, void const*, unsigned long, cudaMemcpyKind)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1697540558 (unix time) try "date -d @1697540558" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x7a7) received by PID 2041 (TID 0x7f3600df5740) from PID 1959 ***]

grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:34: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  method='lar', copy_X=True, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:164: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  method='lar', copy_X=True, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:281: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:865: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1121: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1149: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, positive=False):
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1379: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1621: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1755: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, positive=False):
/usr/local/lib/python3.7/dist-packages/sklearn/decomposition/_lda.py:28: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  EPS = np.finfo(np.float).eps
INFO:root:rank: 0
Training: 2023-10-17 11:26:23,840 - rank: 0
INFO:root:--------args----------
Training: 2023-10-17 11:26:23,840 - --------args----------
INFO:root:config_file: configs/ms1mv3_r50.py
Training: 2023-10-17 11:26:23,840 - config_file: configs/ms1mv3_r50.py
INFO:root:is_static: True
Training: 2023-10-17 11:26:23,840 - is_static: True
INFO:root:backbone: FresResNet101
Training: 2023-10-17 11:26:23,840 - backbone: FresResNet101
INFO:root:classifier: LargeScaleClassifier
Training: 2023-10-17 11:26:23,840 - classifier: LargeScaleClassifier
INFO:root:embedding_size: 512
Training: 2023-10-17 11:26:23,840 - embedding_size: 512
INFO:root:model_parallel: True
Training: 2023-10-17 11:26:23,840 - model_parallel: True
INFO:root:sample_ratio: 0.1
Training: 2023-10-17 11:26:23,840 - sample_ratio: 0.1
INFO:root:loss: ArcFace
Training: 2023-10-17 11:26:23,840 - loss: ArcFace
INFO:root:dropout: 0.0
Training: 2023-10-17 11:26:23,840 - dropout: 0.0
INFO:root:fp16: True
Training: 2023-10-17 11:26:23,840 - fp16: True
INFO:root:init_loss_scaling: 27648.0
Training: 2023-10-17 11:26:23,840 - init_loss_scaling: 27648.0
INFO:root:max_loss_scaling: 128.0
Training: 2023-10-17 11:26:23,840 - max_loss_scaling: 128.0
INFO:root:incr_every_n_steps: 2000
Training: 2023-10-17 11:26:23,840 - incr_every_n_steps: 2000
INFO:root:decr_every_n_nan_or_inf: 1
Training: 2023-10-17 11:26:23,840 - decr_every_n_nan_or_inf: 1
INFO:root:incr_ratio: 2.0
Training: 2023-10-17 11:26:23,840 - incr_ratio: 2.0
INFO:root:decr_ratio: 0.5
Training: 2023-10-17 11:26:23,840 - decr_ratio: 0.5
INFO:root:use_dynamic_loss_scaling: True
Training: 2023-10-17 11:26:23,840 - use_dynamic_loss_scaling: True
INFO:root:custom_white_list: []
Training: 2023-10-17 11:26:23,841 - custom_white_list: []
INFO:root:custom_black_list: []
Training: 2023-10-17 11:26:23,841 - custom_black_list: []
INFO:root:lr: 0.1
Training: 2023-10-17 11:26:23,841 - lr: 0.1
INFO:root:lr_decay: 0.1
Training: 2023-10-17 11:26:23,841 - lr_decay: 0.1
INFO:root:weight_decay: 0.0005
Training: 2023-10-17 11:26:23,841 - weight_decay: 0.0005
INFO:root:momentum: 0.9
Training: 2023-10-17 11:26:23,841 - momentum: 0.9
INFO:root:train_unit: epoch
Training: 2023-10-17 11:26:23,841 - train_unit: epoch
INFO:root:warmup_num: 0
Training: 2023-10-17 11:26:23,841 - warmup_num: 0
INFO:root:train_num: 25
Training: 2023-10-17 11:26:23,841 - train_num: 25
INFO:root:decay_boundaries: [10, 16, 22]
Training: 2023-10-17 11:26:23,841 - decay_boundaries: [10, 16, 22]
INFO:root:use_synthetic_dataset: False
Training: 2023-10-17 11:26:23,841 - use_synthetic_dataset: False
INFO:root:dataset: MS1M_v3
Training: 2023-10-17 11:26:23,841 - dataset: MS1M_v3
INFO:root:data_dir: ../dataset/
Training: 2023-10-17 11:26:23,841 - data_dir: ../dataset/
INFO:root:label_file: ../dataset/label.txt
Training: 2023-10-17 11:26:23,841 - label_file: ../dataset/label.txt
INFO:root:is_bin: False
Training: 2023-10-17 11:26:23,841 - is_bin: False
INFO:root:num_classes: 657077
Training: 2023-10-17 11:26:23,841 - num_classes: 657077
INFO:root:batch_size: 32
Training: 2023-10-17 11:26:23,841 - batch_size: 32
INFO:root:num_workers: 2
Training: 2023-10-17 11:26:23,841 - num_workers: 2
INFO:root:do_validation_while_train: True
Training: 2023-10-17 11:26:23,841 - do_validation_while_train: True
INFO:root:validation_interval_step: 2000
Training: 2023-10-17 11:26:23,841 - validation_interval_step: 2000
INFO:root:val_targets: ['lfw', 'cfp_fp', 'agedb_30']
Training: 2023-10-17 11:26:23,841 - val_targets: ['lfw', 'cfp_fp', 'agedb_30']
INFO:root:logdir: ./log
Training: 2023-10-17 11:26:23,841 - logdir: ./log
INFO:root:log_interval_step: 100
Training: 2023-10-17 11:26:23,841 - log_interval_step: 100
INFO:root:output: model
Training: 2023-10-17 11:26:23,841 - output: model
INFO:root:resume: False
Training: 2023-10-17 11:26:23,841 - resume: False
INFO:root:checkpoint_dir: None
Training: 2023-10-17 11:26:23,841 - checkpoint_dir: None
INFO:root:max_num_last_checkpoint: 1
Training: 2023-10-17 11:26:23,841 - max_num_last_checkpoint: 1
INFO:root:------------------------

Training: 2023-10-17 11:26:23,841 - ------------------------

INFO:root:read label file finished, total num: 4574213
Training: 2023-10-17 11:26:24,115 - read label file finished, total num: 4574213
INFO:root:world_size: 2
Training: 2023-10-17 11:26:24,115 - world_size: 2
INFO:root:total_batch_size: 64
Training: 2023-10-17 11:26:24,115 - total_batch_size: 64
INFO:root:warmup_steps: 0
Training: 2023-10-17 11:26:24,115 - warmup_steps: 0
INFO:root:steps_per_epoch: 71472
Training: 2023-10-17 11:26:24,115 - steps_per_epoch: 71472
INFO:root:total_steps: 1786800
Training: 2023-10-17 11:26:24,115 - total_steps: 1786800
INFO:root:total_epoch: 25
Training: 2023-10-17 11:26:24,115 - total_epoch: 25
INFO:root:decay_steps: [714720, 1143552, 1572384]
Training: 2023-10-17 11:26:24,115 - decay_steps: [714720, 1143552, 1572384]
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in conv2d only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'Out' in guassian_random only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in batch_norm only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in prelu only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in elementwise_add only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'y' in elementwise_add only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in fc only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'Out' in xavier_init only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'dtype' in create_parameter only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'tensor' in all_gather only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in split only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input[0]' in concat only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input[1]' in concat only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in gather only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'X' in norm only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in matmul only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'y' in matmul only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'logits' in margin_cross_entropy only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/framework.py:744: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  elif dtype == np.bool:
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in check_finite_and_unscale only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in update_loss_scaling only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/optimizer/momentum.py:262: UserWarning: Accumulating with FP16 in optimizer can lead to poor accuracy or slow convergence.Consider using multi_precision=True option of the Momentum optimizer.
  "Accumulating with FP16 in optimizer can lead to poor accuracy or slow convergence."
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:53573']
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
(14000, 3, 112, 112)
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
W1017 11:26:45.453773  2831 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 12.0, Runtime API Version: 11.2
W1017 11:26:45.455456  2831 device_context.cc:465] device: 0, cuDNN Version: 8.1.
I1017 11:26:47.064931  2831 gen_comm_id_helper.cc:190] Server listening on: 127.0.0.1:45611 successful.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::framework::Executor::RunPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, bool, bool, bool)
1   paddle::framework::Executor::RunPartialPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, long, long, bool, bool, bool)
2   paddle::framework::OperatorBase::Run(paddle::framework::Scope const&, paddle::platform::Place const&)
3   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&) const
4   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&, paddle::framework::RuntimeContext*) const
5   std::_Function_handler<void (paddle::framework::ExecutionContext const&), paddle::framework::OpKernelRegistrarFunctor<paddle::platform::CUDAPlace, false, 1ul, paddle::operators::ClassCenterSampleCUDAKernel<paddle::platform::CUDADeviceContext, long>, paddle::operators::ClassCenterSampleCUDAKernel<paddle::platform::CUDADeviceContext, int> >::operator()(char const*, char const*, int) const::{lambda(paddle::framework::ExecutionContext const&)#1}>::_M_invoke(std::_Any_data const&, paddle::framework::ExecutionContext const&)
6   paddle::framework::TensorCopySync(paddle::framework::Tensor const&, paddle::platform::Place const&, paddle::framework::Tensor*)
7   paddle::framework::TensorCopySync(paddle::framework::Tensor const&, paddle::platform::Place const&, paddle::framework::Tensor*)
8   void paddle::memory::Copy<paddle::platform::CPUPlace, paddle::platform::CUDAPlace>(paddle::platform::CPUPlace, void*, paddle::platform::CUDAPlace, void const*, unsigned long, CUstream_st*)
9   paddle::platform::GpuMemcpySync(void*, void const*, unsigned long, cudaMemcpyKind)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1697542015 (unix time) try "date -d @1697542015" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0xabb) received by PID 2831 (TID 0x7f7171e08740) from PID 2747 ***]

grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:34: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  method='lar', copy_X=True, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:164: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  method='lar', copy_X=True, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:281: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:865: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1121: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1149: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, positive=False):
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1379: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1621: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1755: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, positive=False):
/usr/local/lib/python3.7/dist-packages/sklearn/decomposition/_lda.py:28: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  EPS = np.finfo(np.float).eps
INFO:root:rank: 0
Training: 2023-10-17 11:27:28,468 - rank: 0
INFO:root:--------args----------
Training: 2023-10-17 11:27:28,468 - --------args----------
INFO:root:config_file: configs/ms1mv3_r50.py
Training: 2023-10-17 11:27:28,468 - config_file: configs/ms1mv3_r50.py
INFO:root:is_static: True
Training: 2023-10-17 11:27:28,468 - is_static: True
INFO:root:backbone: FresResNet101
Training: 2023-10-17 11:27:28,468 - backbone: FresResNet101
INFO:root:classifier: LargeScaleClassifier
Training: 2023-10-17 11:27:28,468 - classifier: LargeScaleClassifier
INFO:root:embedding_size: 512
Training: 2023-10-17 11:27:28,468 - embedding_size: 512
INFO:root:model_parallel: True
Training: 2023-10-17 11:27:28,468 - model_parallel: True
INFO:root:sample_ratio: 0.1
Training: 2023-10-17 11:27:28,468 - sample_ratio: 0.1
INFO:root:loss: ArcFace
Training: 2023-10-17 11:27:28,468 - loss: ArcFace
INFO:root:dropout: 0.0
Training: 2023-10-17 11:27:28,468 - dropout: 0.0
INFO:root:fp16: True
Training: 2023-10-17 11:27:28,468 - fp16: True
INFO:root:init_loss_scaling: 27648.0
Training: 2023-10-17 11:27:28,468 - init_loss_scaling: 27648.0
INFO:root:max_loss_scaling: 128.0
Training: 2023-10-17 11:27:28,468 - max_loss_scaling: 128.0
INFO:root:incr_every_n_steps: 2000
Training: 2023-10-17 11:27:28,468 - incr_every_n_steps: 2000
INFO:root:decr_every_n_nan_or_inf: 1
Training: 2023-10-17 11:27:28,468 - decr_every_n_nan_or_inf: 1
INFO:root:incr_ratio: 2.0
Training: 2023-10-17 11:27:28,468 - incr_ratio: 2.0
INFO:root:decr_ratio: 0.5
Training: 2023-10-17 11:27:28,468 - decr_ratio: 0.5
INFO:root:use_dynamic_loss_scaling: True
Training: 2023-10-17 11:27:28,468 - use_dynamic_loss_scaling: True
INFO:root:custom_white_list: []
Training: 2023-10-17 11:27:28,468 - custom_white_list: []
INFO:root:custom_black_list: []
Training: 2023-10-17 11:27:28,469 - custom_black_list: []
INFO:root:lr: 0.1
Training: 2023-10-17 11:27:28,469 - lr: 0.1
INFO:root:lr_decay: 0.1
Training: 2023-10-17 11:27:28,469 - lr_decay: 0.1
INFO:root:weight_decay: 0.0005
Training: 2023-10-17 11:27:28,469 - weight_decay: 0.0005
INFO:root:momentum: 0.9
Training: 2023-10-17 11:27:28,469 - momentum: 0.9
INFO:root:train_unit: epoch
Training: 2023-10-17 11:27:28,469 - train_unit: epoch
INFO:root:warmup_num: 0
Training: 2023-10-17 11:27:28,469 - warmup_num: 0
INFO:root:train_num: 25
Training: 2023-10-17 11:27:28,469 - train_num: 25
INFO:root:decay_boundaries: [10, 16, 22]
Training: 2023-10-17 11:27:28,469 - decay_boundaries: [10, 16, 22]
INFO:root:use_synthetic_dataset: False
Training: 2023-10-17 11:27:28,469 - use_synthetic_dataset: False
INFO:root:dataset: MS1M_v3
Training: 2023-10-17 11:27:28,469 - dataset: MS1M_v3
INFO:root:data_dir: ../dataset/
Training: 2023-10-17 11:27:28,469 - data_dir: ../dataset/
INFO:root:label_file: ../dataset/label.txt
Training: 2023-10-17 11:27:28,469 - label_file: ../dataset/label.txt
INFO:root:is_bin: False
Training: 2023-10-17 11:27:28,469 - is_bin: False
INFO:root:num_classes: 657077
Training: 2023-10-17 11:27:28,469 - num_classes: 657077
INFO:root:batch_size: 32
Training: 2023-10-17 11:27:28,469 - batch_size: 32
INFO:root:num_workers: 2
Training: 2023-10-17 11:27:28,469 - num_workers: 2
INFO:root:do_validation_while_train: True
Training: 2023-10-17 11:27:28,469 - do_validation_while_train: True
INFO:root:validation_interval_step: 2000
Training: 2023-10-17 11:27:28,469 - validation_interval_step: 2000
INFO:root:val_targets: ['lfw', 'cfp_fp', 'agedb_30']
Training: 2023-10-17 11:27:28,469 - val_targets: ['lfw', 'cfp_fp', 'agedb_30']
INFO:root:logdir: ./log
Training: 2023-10-17 11:27:28,469 - logdir: ./log
INFO:root:log_interval_step: 100
Training: 2023-10-17 11:27:28,469 - log_interval_step: 100
INFO:root:output: model
Training: 2023-10-17 11:27:28,469 - output: model
INFO:root:resume: False
Training: 2023-10-17 11:27:28,469 - resume: False
INFO:root:checkpoint_dir: None
Training: 2023-10-17 11:27:28,469 - checkpoint_dir: None
INFO:root:max_num_last_checkpoint: 1
Training: 2023-10-17 11:27:28,469 - max_num_last_checkpoint: 1
INFO:root:------------------------

Training: 2023-10-17 11:27:28,469 - ------------------------

INFO:root:read label file finished, total num: 4574213
Training: 2023-10-17 11:27:28,739 - read label file finished, total num: 4574213
INFO:root:world_size: 2
Training: 2023-10-17 11:27:28,739 - world_size: 2
INFO:root:total_batch_size: 64
Training: 2023-10-17 11:27:28,739 - total_batch_size: 64
INFO:root:warmup_steps: 0
Training: 2023-10-17 11:27:28,739 - warmup_steps: 0
INFO:root:steps_per_epoch: 71472
Training: 2023-10-17 11:27:28,739 - steps_per_epoch: 71472
INFO:root:total_steps: 1786800
Training: 2023-10-17 11:27:28,739 - total_steps: 1786800
INFO:root:total_epoch: 25
Training: 2023-10-17 11:27:28,739 - total_epoch: 25
INFO:root:decay_steps: [714720, 1143552, 1572384]
Training: 2023-10-17 11:27:28,739 - decay_steps: [714720, 1143552, 1572384]
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in conv2d only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'Out' in guassian_random only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in batch_norm only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in prelu only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in elementwise_add only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'y' in elementwise_add only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in fc only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'Out' in xavier_init only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'dtype' in create_parameter only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'tensor' in all_gather only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in split only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input[0]' in concat only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input[1]' in concat only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in gather only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'X' in norm only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in matmul only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'y' in matmul only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'logits' in margin_cross_entropy only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/framework.py:744: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  elif dtype == np.bool:
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in check_finite_and_unscale only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in update_loss_scaling only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/optimizer/momentum.py:262: UserWarning: Accumulating with FP16 in optimizer can lead to poor accuracy or slow convergence.Consider using multi_precision=True option of the Momentum optimizer.
  "Accumulating with FP16 in optimizer can lead to poor accuracy or slow convergence."
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:41215']
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
(14000, 3, 112, 112)
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
W1017 11:27:50.205190  3004 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 12.0, Runtime API Version: 11.2
W1017 11:27:50.206842  3004 device_context.cc:465] device: 0, cuDNN Version: 8.1.
I1017 11:27:51.814949  3004 gen_comm_id_helper.cc:190] Server listening on: 127.0.0.1:52069 successful.
ERROR: Unexpected BUS error encountered in DataLoader worker. This might be caused by insufficient shared memory (shm), please check whether use_shared_memory is set and storage space in /dev/shm is enough
 ERROR: Unexpected BUS error encountered in DataLoader worker. This might be caused by insufficient shared memory (shm), please check whether use_shared_memory is set and storage space in /dev/shm is enough
 Traceback (most recent call last):
  File "tools/train.py", line 35, in <module>
    train(args)
  File "/paddle/train/arcface_paddle/static/train.py", line 199, in train
    for step, data in enumerate(train_loader):
  File "/usr/local/lib/python3.7/dist-packages/paddle/fluid/dataloader/dataloader_iter.py", line 715, in __next__
Exception in thread Thread-2:
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/paddle/fluid/dataloader/dataloader_iter.py", line 583, in _get_data
    data = self._data_queue.get(timeout=self._timeout)
  File "/usr/lib/python3.7/multiprocessing/queues.py", line 105, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3.7/threading.py", line 926, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.7/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/lib/python3.7/dist-packages/paddle/fluid/dataloader/dataloader_iter.py", line 505, in _thread_loop
    batch = self._get_data()
  File "/usr/local/lib/python3.7/dist-packages/paddle/fluid/dataloader/dataloader_iter.py", line 599, in _get_data
    "pids: {}".format(len(failed_workers), pids))
RuntimeError: DataLoader 2 workers exit unexpectedly, pids: 3057, 3059
    
data = self._reader.read_next()
SystemError: (Fatal) Blocking queue is killed because the data reader raises an exception.
  [Hint: Expected killed_ != true, but received killed_:1 == true:1.] (at /paddle/paddle/fluid/operators/reader/blocking_queue.h:166)

grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:34: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  method='lar', copy_X=True, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:164: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  method='lar', copy_X=True, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:281: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:865: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1121: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1149: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, positive=False):
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1379: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1621: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1755: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, positive=False):
/usr/local/lib/python3.7/dist-packages/sklearn/decomposition/_lda.py:28: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  EPS = np.finfo(np.float).eps
INFO:root:rank: 0
Training: 2023-10-17 11:28:38,200 - rank: 0
INFO:root:--------args----------
Training: 2023-10-17 11:28:38,200 - --------args----------
INFO:root:config_file: configs/ms1mv3_r50.py
Training: 2023-10-17 11:28:38,200 - config_file: configs/ms1mv3_r50.py
INFO:root:is_static: True
Training: 2023-10-17 11:28:38,200 - is_static: True
INFO:root:backbone: FresResNet101
Training: 2023-10-17 11:28:38,200 - backbone: FresResNet101
INFO:root:classifier: LargeScaleClassifier
Training: 2023-10-17 11:28:38,200 - classifier: LargeScaleClassifier
INFO:root:embedding_size: 512
Training: 2023-10-17 11:28:38,200 - embedding_size: 512
INFO:root:model_parallel: True
Training: 2023-10-17 11:28:38,201 - model_parallel: True
INFO:root:sample_ratio: 0.1
Training: 2023-10-17 11:28:38,201 - sample_ratio: 0.1
INFO:root:loss: ArcFace
Training: 2023-10-17 11:28:38,201 - loss: ArcFace
INFO:root:dropout: 0.0
Training: 2023-10-17 11:28:38,201 - dropout: 0.0
INFO:root:fp16: True
Training: 2023-10-17 11:28:38,201 - fp16: True
INFO:root:init_loss_scaling: 27648.0
Training: 2023-10-17 11:28:38,201 - init_loss_scaling: 27648.0
INFO:root:max_loss_scaling: 128.0
Training: 2023-10-17 11:28:38,201 - max_loss_scaling: 128.0
INFO:root:incr_every_n_steps: 2000
Training: 2023-10-17 11:28:38,201 - incr_every_n_steps: 2000
INFO:root:decr_every_n_nan_or_inf: 1
Training: 2023-10-17 11:28:38,201 - decr_every_n_nan_or_inf: 1
INFO:root:incr_ratio: 2.0
Training: 2023-10-17 11:28:38,201 - incr_ratio: 2.0
INFO:root:decr_ratio: 0.5
Training: 2023-10-17 11:28:38,201 - decr_ratio: 0.5
INFO:root:use_dynamic_loss_scaling: True
Training: 2023-10-17 11:28:38,201 - use_dynamic_loss_scaling: True
INFO:root:custom_white_list: []
Training: 2023-10-17 11:28:38,201 - custom_white_list: []
INFO:root:custom_black_list: []
Training: 2023-10-17 11:28:38,201 - custom_black_list: []
INFO:root:lr: 0.1
Training: 2023-10-17 11:28:38,201 - lr: 0.1
INFO:root:lr_decay: 0.1
Training: 2023-10-17 11:28:38,201 - lr_decay: 0.1
INFO:root:weight_decay: 0.0005
Training: 2023-10-17 11:28:38,201 - weight_decay: 0.0005
INFO:root:momentum: 0.9
Training: 2023-10-17 11:28:38,201 - momentum: 0.9
INFO:root:train_unit: epoch
Training: 2023-10-17 11:28:38,201 - train_unit: epoch
INFO:root:warmup_num: 0
Training: 2023-10-17 11:28:38,201 - warmup_num: 0
INFO:root:train_num: 25
Training: 2023-10-17 11:28:38,201 - train_num: 25
INFO:root:decay_boundaries: [10, 16, 22]
Training: 2023-10-17 11:28:38,201 - decay_boundaries: [10, 16, 22]
INFO:root:use_synthetic_dataset: False
Training: 2023-10-17 11:28:38,201 - use_synthetic_dataset: False
INFO:root:dataset: MS1M_v3
Training: 2023-10-17 11:28:38,201 - dataset: MS1M_v3
INFO:root:data_dir: ../dataset/
Training: 2023-10-17 11:28:38,201 - data_dir: ../dataset/
INFO:root:label_file: ../dataset/label.txt
Training: 2023-10-17 11:28:38,201 - label_file: ../dataset/label.txt
INFO:root:is_bin: False
Training: 2023-10-17 11:28:38,201 - is_bin: False
INFO:root:num_classes: 657077
Training: 2023-10-17 11:28:38,201 - num_classes: 657077
INFO:root:batch_size: 16
Training: 2023-10-17 11:28:38,201 - batch_size: 16
INFO:root:num_workers: 2
Training: 2023-10-17 11:28:38,201 - num_workers: 2
INFO:root:do_validation_while_train: True
Training: 2023-10-17 11:28:38,201 - do_validation_while_train: True
INFO:root:validation_interval_step: 2000
Training: 2023-10-17 11:28:38,201 - validation_interval_step: 2000
INFO:root:val_targets: ['lfw', 'cfp_fp', 'agedb_30']
Training: 2023-10-17 11:28:38,202 - val_targets: ['lfw', 'cfp_fp', 'agedb_30']
INFO:root:logdir: ./log
Training: 2023-10-17 11:28:38,202 - logdir: ./log
INFO:root:log_interval_step: 100
Training: 2023-10-17 11:28:38,202 - log_interval_step: 100
INFO:root:output: model
Training: 2023-10-17 11:28:38,202 - output: model
INFO:root:resume: False
Training: 2023-10-17 11:28:38,202 - resume: False
INFO:root:checkpoint_dir: None
Training: 2023-10-17 11:28:38,202 - checkpoint_dir: None
INFO:root:max_num_last_checkpoint: 1
Training: 2023-10-17 11:28:38,202 - max_num_last_checkpoint: 1
INFO:root:------------------------

Training: 2023-10-17 11:28:38,202 - ------------------------

INFO:root:read label file finished, total num: 4574213
Training: 2023-10-17 11:28:38,471 - read label file finished, total num: 4574213
INFO:root:world_size: 2
Training: 2023-10-17 11:28:38,472 - world_size: 2
INFO:root:total_batch_size: 32
Training: 2023-10-17 11:28:38,472 - total_batch_size: 32
INFO:root:warmup_steps: 0
Training: 2023-10-17 11:28:38,472 - warmup_steps: 0
INFO:root:steps_per_epoch: 142944
Training: 2023-10-17 11:28:38,472 - steps_per_epoch: 142944
INFO:root:total_steps: 3573600
Training: 2023-10-17 11:28:38,472 - total_steps: 3573600
INFO:root:total_epoch: 25
Training: 2023-10-17 11:28:38,472 - total_epoch: 25
INFO:root:decay_steps: [1429440, 2287104, 3144768]
Training: 2023-10-17 11:28:38,472 - decay_steps: [1429440, 2287104, 3144768]
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in conv2d only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'Out' in guassian_random only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in batch_norm only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in prelu only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in elementwise_add only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'y' in elementwise_add only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in fc only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'Out' in xavier_init only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'dtype' in create_parameter only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'tensor' in all_gather only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in split only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input[0]' in concat only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input[1]' in concat only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in gather only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'X' in norm only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in matmul only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'y' in matmul only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'logits' in margin_cross_entropy only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/framework.py:744: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  elif dtype == np.bool:
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in check_finite_and_unscale only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in update_loss_scaling only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/optimizer/momentum.py:262: UserWarning: Accumulating with FP16 in optimizer can lead to poor accuracy or slow convergence.Consider using multi_precision=True option of the Momentum optimizer.
  "Accumulating with FP16 in optimizer can lead to poor accuracy or slow convergence."
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:34519']
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
(14000, 3, 112, 112)
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
W1017 11:28:59.841547  3157 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 12.0, Runtime API Version: 11.2
W1017 11:28:59.843228  3157 device_context.cc:465] device: 0, cuDNN Version: 8.1.
I1017 11:29:01.444902  3157 gen_comm_id_helper.cc:190] Server listening on: 127.0.0.1:53713 successful.
ERROR: Unexpected BUS error encountered in DataLoader worker. This might be caused by insufficient shared memory (shm), please check whether use_shared_memory is set and storage space in /dev/shm is enough
 ERROR: Unexpected BUS error encountered in DataLoader worker. This might be caused by insufficient shared memory (shm), please check whether use_shared_memory is set and storage space in /dev/shm is enough
 Traceback (most recent call last):
  File "tools/train.py", line 35, in <module>
    train(args)
  File "/paddle/train/arcface_paddle/static/train.py", line 199, in train
    for step, data in enumerate(train_loader):
  File "/usr/local/lib/python3.7/dist-packages/paddle/fluid/dataloader/dataloader_iter.py", line 715, in __next__
    data = self._reader.read_next()
  File "/usr/local/lib/python3.7/dist-packages/paddle/fluid/multiprocess_utils.py", line 134, in __handler__
    core._throw_error_if_process_failed()
SystemError: (Fatal) DataLoader process (pid   1. If run DataLoader by DataLoader.from_generator(...), queue capacity is set by from_generator(..., capacity=xx, ...).
  2. If run DataLoader by DataLoader(dataset, ...), queue capacity is set as 2 times of the max value of num_workers and len(places).
  3. If run by DataLoader(dataset, ..., use_shared_memory=True), set use_shared_memory=False for not using shared memory.) exited is killed by signal: 3209.
  It may be caused by insufficient shared storage space. This problem usually occurs when using docker as a development environment.
  Please use command `df -h` to check the storage space of `/dev/shm`. Shared storage space needs to be greater than (DataLoader Num * DataLoader queue capacity * 1 batch data size).
  You can solve this problem by increasing the shared storage space or reducing the queue capacity appropriately.
Bus error (at /paddle/paddle/fluid/imperative/data_loader.cc:177)

grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:34: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  method='lar', copy_X=True, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:164: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  method='lar', copy_X=True, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:281: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:865: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1121: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1149: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, positive=False):
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1379: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1621: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1755: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, positive=False):
/usr/local/lib/python3.7/dist-packages/sklearn/decomposition/_lda.py:28: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  EPS = np.finfo(np.float).eps
INFO:root:rank: 0
Training: 2023-10-17 11:37:01,644 - rank: 0
INFO:root:--------args----------
Training: 2023-10-17 11:37:01,644 - --------args----------
INFO:root:config_file: configs/ms1mv3_r50.py
Training: 2023-10-17 11:37:01,644 - config_file: configs/ms1mv3_r50.py
INFO:root:is_static: True
Training: 2023-10-17 11:37:01,644 - is_static: True
INFO:root:backbone: FresResNet101
Training: 2023-10-17 11:37:01,644 - backbone: FresResNet101
INFO:root:classifier: LargeScaleClassifier
Training: 2023-10-17 11:37:01,644 - classifier: LargeScaleClassifier
INFO:root:embedding_size: 512
Training: 2023-10-17 11:37:01,644 - embedding_size: 512
INFO:root:model_parallel: True
Training: 2023-10-17 11:37:01,644 - model_parallel: True
INFO:root:sample_ratio: 0.1
Training: 2023-10-17 11:37:01,644 - sample_ratio: 0.1
INFO:root:loss: ArcFace
Training: 2023-10-17 11:37:01,644 - loss: ArcFace
INFO:root:dropout: 0.0
Training: 2023-10-17 11:37:01,644 - dropout: 0.0
INFO:root:fp16: True
Training: 2023-10-17 11:37:01,644 - fp16: True
INFO:root:init_loss_scaling: 27648.0
Training: 2023-10-17 11:37:01,644 - init_loss_scaling: 27648.0
INFO:root:max_loss_scaling: 128.0
Training: 2023-10-17 11:37:01,644 - max_loss_scaling: 128.0
INFO:root:incr_every_n_steps: 2000
Training: 2023-10-17 11:37:01,644 - incr_every_n_steps: 2000
INFO:root:decr_every_n_nan_or_inf: 1
Training: 2023-10-17 11:37:01,644 - decr_every_n_nan_or_inf: 1
INFO:root:incr_ratio: 2.0
Training: 2023-10-17 11:37:01,644 - incr_ratio: 2.0
INFO:root:decr_ratio: 0.5
Training: 2023-10-17 11:37:01,644 - decr_ratio: 0.5
INFO:root:use_dynamic_loss_scaling: True
Training: 2023-10-17 11:37:01,644 - use_dynamic_loss_scaling: True
INFO:root:custom_white_list: []
Training: 2023-10-17 11:37:01,644 - custom_white_list: []
INFO:root:custom_black_list: []
Training: 2023-10-17 11:37:01,644 - custom_black_list: []
INFO:root:lr: 0.1
Training: 2023-10-17 11:37:01,644 - lr: 0.1
INFO:root:lr_decay: 0.1
Training: 2023-10-17 11:37:01,645 - lr_decay: 0.1
INFO:root:weight_decay: 0.0005
Training: 2023-10-17 11:37:01,645 - weight_decay: 0.0005
INFO:root:momentum: 0.9
Training: 2023-10-17 11:37:01,645 - momentum: 0.9
INFO:root:train_unit: epoch
Training: 2023-10-17 11:37:01,645 - train_unit: epoch
INFO:root:warmup_num: 0
Training: 2023-10-17 11:37:01,645 - warmup_num: 0
INFO:root:train_num: 25
Training: 2023-10-17 11:37:01,645 - train_num: 25
INFO:root:decay_boundaries: [10, 16, 22]
Training: 2023-10-17 11:37:01,645 - decay_boundaries: [10, 16, 22]
INFO:root:use_synthetic_dataset: False
Training: 2023-10-17 11:37:01,645 - use_synthetic_dataset: False
INFO:root:dataset: MS1M_v3
Training: 2023-10-17 11:37:01,645 - dataset: MS1M_v3
INFO:root:data_dir: ../dataset/
Training: 2023-10-17 11:37:01,645 - data_dir: ../dataset/
INFO:root:label_file: ../dataset/label.txt
Training: 2023-10-17 11:37:01,645 - label_file: ../dataset/label.txt
INFO:root:is_bin: False
Training: 2023-10-17 11:37:01,645 - is_bin: False
INFO:root:num_classes: 657077
Training: 2023-10-17 11:37:01,645 - num_classes: 657077
INFO:root:batch_size: 16
Training: 2023-10-17 11:37:01,645 - batch_size: 16
INFO:root:num_workers: 2
Training: 2023-10-17 11:37:01,645 - num_workers: 2
INFO:root:do_validation_while_train: True
Training: 2023-10-17 11:37:01,645 - do_validation_while_train: True
INFO:root:validation_interval_step: 2000
Training: 2023-10-17 11:37:01,645 - validation_interval_step: 2000
INFO:root:val_targets: ['lfw', 'cfp_fp', 'agedb_30']
Training: 2023-10-17 11:37:01,645 - val_targets: ['lfw', 'cfp_fp', 'agedb_30']
INFO:root:logdir: ./log
Training: 2023-10-17 11:37:01,645 - logdir: ./log
INFO:root:log_interval_step: 100
Training: 2023-10-17 11:37:01,645 - log_interval_step: 100
INFO:root:output: model
Training: 2023-10-17 11:37:01,645 - output: model
INFO:root:resume: False
Training: 2023-10-17 11:37:01,645 - resume: False
INFO:root:checkpoint_dir: None
Training: 2023-10-17 11:37:01,645 - checkpoint_dir: None
INFO:root:max_num_last_checkpoint: 1
Training: 2023-10-17 11:37:01,645 - max_num_last_checkpoint: 1
INFO:root:------------------------

Training: 2023-10-17 11:37:01,645 - ------------------------

INFO:root:read label file finished, total num: 4574213
Training: 2023-10-17 11:37:01,941 - read label file finished, total num: 4574213
INFO:root:world_size: 2
Training: 2023-10-17 11:37:01,941 - world_size: 2
INFO:root:total_batch_size: 32
Training: 2023-10-17 11:37:01,941 - total_batch_size: 32
INFO:root:warmup_steps: 0
Training: 2023-10-17 11:37:01,941 - warmup_steps: 0
INFO:root:steps_per_epoch: 142944
Training: 2023-10-17 11:37:01,941 - steps_per_epoch: 142944
INFO:root:total_steps: 3573600
Training: 2023-10-17 11:37:01,941 - total_steps: 3573600
INFO:root:total_epoch: 25
Training: 2023-10-17 11:37:01,941 - total_epoch: 25
INFO:root:decay_steps: [1429440, 2287104, 3144768]
Training: 2023-10-17 11:37:01,941 - decay_steps: [1429440, 2287104, 3144768]
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in conv2d only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'Out' in guassian_random only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in batch_norm only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in prelu only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in elementwise_add only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'y' in elementwise_add only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in fc only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'Out' in xavier_init only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'dtype' in create_parameter only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'tensor' in all_gather only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in split only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input[0]' in concat only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input[1]' in concat only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in gather only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'X' in norm only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in matmul only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'y' in matmul only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'logits' in margin_cross_entropy only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/framework.py:744: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  elif dtype == np.bool:
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in check_finite_and_unscale only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in update_loss_scaling only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/optimizer/momentum.py:262: UserWarning: Accumulating with FP16 in optimizer can lead to poor accuracy or slow convergence.Consider using multi_precision=True option of the Momentum optimizer.
  "Accumulating with FP16 in optimizer can lead to poor accuracy or slow convergence."
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:46099']
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:46099']
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
(14000, 3, 112, 112)
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
W1017 11:37:26.553273  3324 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 12.0, Runtime API Version: 11.2
W1017 11:37:26.555027  3324 device_context.cc:465] device: 0, cuDNN Version: 8.1.
I1017 11:37:28.208784  3324 gen_comm_id_helper.cc:190] Server listening on: 127.0.0.1:51507 successful.
ERROR: Unexpected BUS error encountered in DataLoader worker. This might be caused by insufficient shared memory (shm), please check whether use_shared_memory is set and storage space in /dev/shm is enough
 ERROR: Unexpected BUS error encountered in DataLoader worker. This might be caused by insufficient shared memory (shm), please check whether use_shared_memory is set and storage space in /dev/shm is enough
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::MultiDeviceFeedReader<paddle::operators::reader::LoDTensorBlockingQueue>::ReadNext[abi:cxx11]()
1   paddle::pybind::MultiDeviceFeedReader<paddle::operators::reader::LoDTensorBlockingQueue>::CheckNextStatus()
2   paddle::pybind::MultiDeviceFeedReader<paddle::operators::reader::LoDTensorBlockingQueue>::WaitFutures(std::__exception_ptr::exception_ptr*)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1697542653 (unix time) try "date -d @1697542653" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0xcaa) received by PID 3324 (TID 0x7f1cd51ad740) from PID 3242 ***]

grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:34: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  method='lar', copy_X=True, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:164: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  method='lar', copy_X=True, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:281: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:865: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1121: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1149: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, positive=False):
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1379: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1621: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1755: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, positive=False):
/usr/local/lib/python3.7/dist-packages/sklearn/decomposition/_lda.py:28: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  EPS = np.finfo(np.float).eps
INFO:root:rank: 0
Training: 2023-10-17 11:39:04,045 - rank: 0
INFO:root:--------args----------
Training: 2023-10-17 11:39:04,045 - --------args----------
INFO:root:config_file: configs/ms1mv3_r50.py
Training: 2023-10-17 11:39:04,045 - config_file: configs/ms1mv3_r50.py
INFO:root:is_static: True
Training: 2023-10-17 11:39:04,045 - is_static: True
INFO:root:backbone: FresResNet101
Training: 2023-10-17 11:39:04,045 - backbone: FresResNet101
INFO:root:classifier: LargeScaleClassifier
Training: 2023-10-17 11:39:04,045 - classifier: LargeScaleClassifier
INFO:root:embedding_size: 512
Training: 2023-10-17 11:39:04,045 - embedding_size: 512
INFO:root:model_parallel: True
Training: 2023-10-17 11:39:04,045 - model_parallel: True
INFO:root:sample_ratio: 0.1
Training: 2023-10-17 11:39:04,046 - sample_ratio: 0.1
INFO:root:loss: ArcFace
Training: 2023-10-17 11:39:04,046 - loss: ArcFace
INFO:root:dropout: 0.0
Training: 2023-10-17 11:39:04,046 - dropout: 0.0
INFO:root:fp16: True
Training: 2023-10-17 11:39:04,046 - fp16: True
INFO:root:init_loss_scaling: 27648.0
Training: 2023-10-17 11:39:04,046 - init_loss_scaling: 27648.0
INFO:root:max_loss_scaling: 128.0
Training: 2023-10-17 11:39:04,046 - max_loss_scaling: 128.0
INFO:root:incr_every_n_steps: 2000
Training: 2023-10-17 11:39:04,046 - incr_every_n_steps: 2000
INFO:root:decr_every_n_nan_or_inf: 1
Training: 2023-10-17 11:39:04,046 - decr_every_n_nan_or_inf: 1
INFO:root:incr_ratio: 2.0
Training: 2023-10-17 11:39:04,046 - incr_ratio: 2.0
INFO:root:decr_ratio: 0.5
Training: 2023-10-17 11:39:04,046 - decr_ratio: 0.5
INFO:root:use_dynamic_loss_scaling: True
Training: 2023-10-17 11:39:04,046 - use_dynamic_loss_scaling: True
INFO:root:custom_white_list: []
Training: 2023-10-17 11:39:04,046 - custom_white_list: []
INFO:root:custom_black_list: []
Training: 2023-10-17 11:39:04,046 - custom_black_list: []
INFO:root:lr: 0.1
Training: 2023-10-17 11:39:04,046 - lr: 0.1
INFO:root:lr_decay: 0.1
Training: 2023-10-17 11:39:04,046 - lr_decay: 0.1
INFO:root:weight_decay: 0.0005
Training: 2023-10-17 11:39:04,046 - weight_decay: 0.0005
INFO:root:momentum: 0.9
Training: 2023-10-17 11:39:04,046 - momentum: 0.9
INFO:root:train_unit: epoch
Training: 2023-10-17 11:39:04,046 - train_unit: epoch
INFO:root:warmup_num: 0
Training: 2023-10-17 11:39:04,046 - warmup_num: 0
INFO:root:train_num: 25
Training: 2023-10-17 11:39:04,046 - train_num: 25
INFO:root:decay_boundaries: [10, 16, 22]
Training: 2023-10-17 11:39:04,046 - decay_boundaries: [10, 16, 22]
INFO:root:use_synthetic_dataset: False
Training: 2023-10-17 11:39:04,046 - use_synthetic_dataset: False
INFO:root:dataset: MS1M_v3
Training: 2023-10-17 11:39:04,046 - dataset: MS1M_v3
INFO:root:data_dir: ../dataset/
Training: 2023-10-17 11:39:04,046 - data_dir: ../dataset/
INFO:root:label_file: ../dataset/label.txt
Training: 2023-10-17 11:39:04,046 - label_file: ../dataset/label.txt
INFO:root:is_bin: False
Training: 2023-10-17 11:39:04,046 - is_bin: False
INFO:root:num_classes: 657077
Training: 2023-10-17 11:39:04,046 - num_classes: 657077
INFO:root:batch_size: 16
Training: 2023-10-17 11:39:04,046 - batch_size: 16
INFO:root:num_workers: 2
Training: 2023-10-17 11:39:04,046 - num_workers: 2
INFO:root:do_validation_while_train: True
Training: 2023-10-17 11:39:04,046 - do_validation_while_train: True
INFO:root:validation_interval_step: 2000
Training: 2023-10-17 11:39:04,047 - validation_interval_step: 2000
INFO:root:val_targets: ['lfw', 'cfp_fp', 'agedb_30']
Training: 2023-10-17 11:39:04,047 - val_targets: ['lfw', 'cfp_fp', 'agedb_30']
INFO:root:logdir: ./log
Training: 2023-10-17 11:39:04,047 - logdir: ./log
INFO:root:log_interval_step: 100
Training: 2023-10-17 11:39:04,047 - log_interval_step: 100
INFO:root:output: model
Training: 2023-10-17 11:39:04,047 - output: model
INFO:root:resume: False
Training: 2023-10-17 11:39:04,047 - resume: False
INFO:root:checkpoint_dir: None
Training: 2023-10-17 11:39:04,047 - checkpoint_dir: None
INFO:root:max_num_last_checkpoint: 1
Training: 2023-10-17 11:39:04,047 - max_num_last_checkpoint: 1
INFO:root:------------------------

Training: 2023-10-17 11:39:04,047 - ------------------------

INFO:root:read label file finished, total num: 4574213
Training: 2023-10-17 11:39:04,329 - read label file finished, total num: 4574213
INFO:root:world_size: 2
Training: 2023-10-17 11:39:04,330 - world_size: 2
INFO:root:total_batch_size: 32
Training: 2023-10-17 11:39:04,330 - total_batch_size: 32
INFO:root:warmup_steps: 0
Training: 2023-10-17 11:39:04,330 - warmup_steps: 0
INFO:root:steps_per_epoch: 142944
Training: 2023-10-17 11:39:04,330 - steps_per_epoch: 142944
INFO:root:total_steps: 3573600
Training: 2023-10-17 11:39:04,330 - total_steps: 3573600
INFO:root:total_epoch: 25
Training: 2023-10-17 11:39:04,330 - total_epoch: 25
INFO:root:decay_steps: [1429440, 2287104, 3144768]
Training: 2023-10-17 11:39:04,330 - decay_steps: [1429440, 2287104, 3144768]
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in conv2d only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'Out' in guassian_random only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in batch_norm only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in prelu only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in elementwise_add only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'y' in elementwise_add only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in fc only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'Out' in xavier_init only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'dtype' in create_parameter only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'tensor' in all_gather only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in split only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input[0]' in concat only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input[1]' in concat only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in gather only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'X' in norm only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in matmul only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'y' in matmul only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'logits' in margin_cross_entropy only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/framework.py:744: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  elif dtype == np.bool:
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in check_finite_and_unscale only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in update_loss_scaling only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/optimizer/momentum.py:262: UserWarning: Accumulating with FP16 in optimizer can lead to poor accuracy or slow convergence.Consider using multi_precision=True option of the Momentum optimizer.
  "Accumulating with FP16 in optimizer can lead to poor accuracy or slow convergence."
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:52207']
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
(14000, 3, 112, 112)
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
W1017 11:39:25.791182  3506 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 12.0, Runtime API Version: 11.2
W1017 11:39:25.792959  3506 device_context.cc:465] device: 0, cuDNN Version: 8.1.
I1017 11:39:27.450927  3506 gen_comm_id_helper.cc:190] Server listening on: 127.0.0.1:41697 successful.
ERROR: Unexpected BUS error encountered in DataLoader worker. This might be caused by insufficient shared memory (shm), please check whether use_shared_memory is set and storage space in /dev/shm is enough
 ERROR: Unexpected BUS error encountered in DataLoader worker. This might be caused by insufficient shared memory (shm), please check whether use_shared_memory is set and storage space in /dev/shm is enough
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::MultiDeviceFeedReader<paddle::operators::reader::LoDTensorBlockingQueue>::ReadNext[abi:cxx11]()
1   paddle::pybind::MultiDeviceFeedReader<paddle::operators::reader::LoDTensorBlockingQueue>::CheckNextStatus()
2   paddle::pybind::MultiDeviceFeedReader<paddle::operators::reader::LoDTensorBlockingQueue>::WaitFutures(std::__exception_ptr::exception_ptr*)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1697542770 (unix time) try "date -d @1697542770" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0xd60) received by PID 3506 (TID 0x7f9ca5de2740) from PID 3424 ***]

grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:34: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  method='lar', copy_X=True, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:164: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  method='lar', copy_X=True, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:281: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:865: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1121: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1149: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, positive=False):
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1379: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1621: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1755: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, positive=False):
/usr/local/lib/python3.7/dist-packages/sklearn/decomposition/_lda.py:28: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  EPS = np.finfo(np.float).eps
INFO:root:rank: 0
Training: 2023-10-17 11:40:00,604 - rank: 0
INFO:root:--------args----------
Training: 2023-10-17 11:40:00,604 - --------args----------
INFO:root:config_file: configs/ms1mv3_r50.py
Training: 2023-10-17 11:40:00,604 - config_file: configs/ms1mv3_r50.py
INFO:root:is_static: True
Training: 2023-10-17 11:40:00,604 - is_static: True
INFO:root:backbone: FresResNet101
Training: 2023-10-17 11:40:00,604 - backbone: FresResNet101
INFO:root:classifier: LargeScaleClassifier
Training: 2023-10-17 11:40:00,604 - classifier: LargeScaleClassifier
INFO:root:embedding_size: 512
Training: 2023-10-17 11:40:00,604 - embedding_size: 512
INFO:root:model_parallel: True
Training: 2023-10-17 11:40:00,604 - model_parallel: True
INFO:root:sample_ratio: 0.1
Training: 2023-10-17 11:40:00,604 - sample_ratio: 0.1
INFO:root:loss: ArcFace
Training: 2023-10-17 11:40:00,604 - loss: ArcFace
INFO:root:dropout: 0.0
Training: 2023-10-17 11:40:00,604 - dropout: 0.0
INFO:root:fp16: True
Training: 2023-10-17 11:40:00,604 - fp16: True
INFO:root:init_loss_scaling: 27648.0
Training: 2023-10-17 11:40:00,604 - init_loss_scaling: 27648.0
INFO:root:max_loss_scaling: 128.0
Training: 2023-10-17 11:40:00,604 - max_loss_scaling: 128.0
INFO:root:incr_every_n_steps: 2000
Training: 2023-10-17 11:40:00,604 - incr_every_n_steps: 2000
INFO:root:decr_every_n_nan_or_inf: 1
Training: 2023-10-17 11:40:00,604 - decr_every_n_nan_or_inf: 1
INFO:root:incr_ratio: 2.0
Training: 2023-10-17 11:40:00,604 - incr_ratio: 2.0
INFO:root:decr_ratio: 0.5
Training: 2023-10-17 11:40:00,604 - decr_ratio: 0.5
INFO:root:use_dynamic_loss_scaling: True
Training: 2023-10-17 11:40:00,604 - use_dynamic_loss_scaling: True
INFO:root:custom_white_list: []
Training: 2023-10-17 11:40:00,604 - custom_white_list: []
INFO:root:custom_black_list: []
Training: 2023-10-17 11:40:00,604 - custom_black_list: []
INFO:root:lr: 0.1
Training: 2023-10-17 11:40:00,604 - lr: 0.1
INFO:root:lr_decay: 0.1
Training: 2023-10-17 11:40:00,604 - lr_decay: 0.1
INFO:root:weight_decay: 0.0005
Training: 2023-10-17 11:40:00,604 - weight_decay: 0.0005
INFO:root:momentum: 0.9
Training: 2023-10-17 11:40:00,604 - momentum: 0.9
INFO:root:train_unit: epoch
Training: 2023-10-17 11:40:00,605 - train_unit: epoch
INFO:root:warmup_num: 0
Training: 2023-10-17 11:40:00,605 - warmup_num: 0
INFO:root:train_num: 25
Training: 2023-10-17 11:40:00,605 - train_num: 25
INFO:root:decay_boundaries: [10, 16, 22]
Training: 2023-10-17 11:40:00,605 - decay_boundaries: [10, 16, 22]
INFO:root:use_synthetic_dataset: False
Training: 2023-10-17 11:40:00,605 - use_synthetic_dataset: False
INFO:root:dataset: MS1M_v3
Training: 2023-10-17 11:40:00,605 - dataset: MS1M_v3
INFO:root:data_dir: ../dataset/
Training: 2023-10-17 11:40:00,605 - data_dir: ../dataset/
INFO:root:label_file: ../dataset/label.txt
Training: 2023-10-17 11:40:00,605 - label_file: ../dataset/label.txt
INFO:root:is_bin: False
Training: 2023-10-17 11:40:00,605 - is_bin: False
INFO:root:num_classes: 657077
Training: 2023-10-17 11:40:00,605 - num_classes: 657077
INFO:root:batch_size: 16
Training: 2023-10-17 11:40:00,605 - batch_size: 16
INFO:root:num_workers: 0
Training: 2023-10-17 11:40:00,605 - num_workers: 0
INFO:root:do_validation_while_train: True
Training: 2023-10-17 11:40:00,605 - do_validation_while_train: True
INFO:root:validation_interval_step: 2000
Training: 2023-10-17 11:40:00,605 - validation_interval_step: 2000
INFO:root:val_targets: ['lfw', 'cfp_fp', 'agedb_30']
Training: 2023-10-17 11:40:00,605 - val_targets: ['lfw', 'cfp_fp', 'agedb_30']
INFO:root:logdir: ./log
Training: 2023-10-17 11:40:00,605 - logdir: ./log
INFO:root:log_interval_step: 100
Training: 2023-10-17 11:40:00,605 - log_interval_step: 100
INFO:root:output: model
Training: 2023-10-17 11:40:00,605 - output: model
INFO:root:resume: False
Training: 2023-10-17 11:40:00,605 - resume: False
INFO:root:checkpoint_dir: None
Training: 2023-10-17 11:40:00,605 - checkpoint_dir: None
INFO:root:max_num_last_checkpoint: 1
Training: 2023-10-17 11:40:00,605 - max_num_last_checkpoint: 1
INFO:root:------------------------

Training: 2023-10-17 11:40:00,605 - ------------------------

INFO:root:read label file finished, total num: 4574213
Training: 2023-10-17 11:40:00,868 - read label file finished, total num: 4574213
INFO:root:world_size: 2
Training: 2023-10-17 11:40:00,868 - world_size: 2
INFO:root:total_batch_size: 32
Training: 2023-10-17 11:40:00,868 - total_batch_size: 32
INFO:root:warmup_steps: 0
Training: 2023-10-17 11:40:00,868 - warmup_steps: 0
INFO:root:steps_per_epoch: 142944
Training: 2023-10-17 11:40:00,868 - steps_per_epoch: 142944
INFO:root:total_steps: 3573600
Training: 2023-10-17 11:40:00,868 - total_steps: 3573600
INFO:root:total_epoch: 25
Training: 2023-10-17 11:40:00,868 - total_epoch: 25
INFO:root:decay_steps: [1429440, 2287104, 3144768]
Training: 2023-10-17 11:40:00,868 - decay_steps: [1429440, 2287104, 3144768]
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in conv2d only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'Out' in guassian_random only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in batch_norm only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in prelu only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in elementwise_add only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'y' in elementwise_add only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in fc only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'Out' in xavier_init only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'dtype' in create_parameter only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'tensor' in all_gather only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in split only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input[0]' in concat only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input[1]' in concat only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in gather only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'X' in norm only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in matmul only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'y' in matmul only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'logits' in margin_cross_entropy only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/framework.py:744: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  elif dtype == np.bool:
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in check_finite_and_unscale only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in update_loss_scaling only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/optimizer/momentum.py:262: UserWarning: Accumulating with FP16 in optimizer can lead to poor accuracy or slow convergence.Consider using multi_precision=True option of the Momentum optimizer.
  "Accumulating with FP16 in optimizer can lead to poor accuracy or slow convergence."
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:36749']
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
(14000, 3, 112, 112)
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
W1017 11:40:22.411968  3659 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 12.0, Runtime API Version: 11.2
W1017 11:40:22.413758  3659 device_context.cc:465] device: 0, cuDNN Version: 8.1.
I1017 11:40:24.067739  3659 gen_comm_id_helper.cc:190] Server listening on: 127.0.0.1:36093 successful.
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 100, eta: 216.03 hours, avg_reader_cost: 0.01059 sec, avg_batch_cost: 0.20142 sec, avg_samples: 16.00000, ips: 158.87023 images/sec
Training: 2023-10-17 11:40:44,333 - loss nan, lr: 0.006250, epoch: 0, step: 100, eta: 216.03 hours, avg_reader_cost: 0.01059 sec, avg_batch_cost: 0.20142 sec, avg_samples: 16.00000, ips: 158.87023 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 200, eta: 194.49 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.17397 sec, avg_samples: 16.00000, ips: 183.94106 images/sec
Training: 2023-10-17 11:41:01,736 - loss nan, lr: 0.006250, epoch: 0, step: 200, eta: 194.49 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.17397 sec, avg_samples: 16.00000, ips: 183.94106 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 300, eta: 187.24 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.17393 sec, avg_samples: 16.00000, ips: 183.98545 images/sec
Training: 2023-10-17 11:41:19,134 - loss nan, lr: 0.006250, epoch: 0, step: 300, eta: 187.24 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.17393 sec, avg_samples: 16.00000, ips: 183.98545 images/sec
INFO:root:loss nan, lr: 0.006250, epoch: 0, step: 400, eta: 183.50 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.17348 sec, avg_samples: 16.00000, ips: 184.46234 images/sec
Training: 2023-10-17 11:41:36,487 - loss nan, lr: 0.006250, epoch: 0, step: 400, eta: 183.50 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.17348 sec, avg_samples: 16.00000, ips: 184.46234 images/sec


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::framework::Executor::RunPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, bool, bool, bool)
1   paddle::framework::Executor::RunPartialPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, long, long, bool, bool, bool)
2   paddle::framework::OperatorBase::Run(paddle::framework::Scope const&, paddle::platform::Place const&)
3   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&) const
4   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&, paddle::framework::RuntimeContext*) const
5   std::_Function_handler<void (paddle::framework::ExecutionContext const&), paddle::framework::OpKernelRegistrarFunctor<paddle::platform::CUDAPlace, false, 0ul, paddle::operators::CSyncCalcStreamKernel<float> >::operator()(char const*, char const*, int) const::{lambda(paddle::framework::ExecutionContext const&)#1}>::_M_invoke(std::_Any_data const&, paddle::framework::ExecutionContext const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1697542905 (unix time) try "date -d @1697542905" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0xdf9) received by PID 3659 (TID 0x7f21f56b0740) from PID 3577 ***]

grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:34: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  method='lar', copy_X=True, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:164: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  method='lar', copy_X=True, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:281: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:865: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1121: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1149: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, positive=False):
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1379: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1621: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1755: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, positive=False):
/usr/local/lib/python3.7/dist-packages/sklearn/decomposition/_lda.py:28: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  EPS = np.finfo(np.float).eps
INFO:root:rank: 0
Training: 2023-10-17 11:42:02,578 - rank: 0
INFO:root:--------args----------
Training: 2023-10-17 11:42:02,578 - --------args----------
INFO:root:config_file: configs/ms1mv3_r50.py
Training: 2023-10-17 11:42:02,579 - config_file: configs/ms1mv3_r50.py
INFO:root:is_static: True
Training: 2023-10-17 11:42:02,579 - is_static: True
INFO:root:backbone: FresResNet101
Training: 2023-10-17 11:42:02,579 - backbone: FresResNet101
INFO:root:classifier: LargeScaleClassifier
Training: 2023-10-17 11:42:02,579 - classifier: LargeScaleClassifier
INFO:root:embedding_size: 512
Training: 2023-10-17 11:42:02,579 - embedding_size: 512
INFO:root:model_parallel: True
Training: 2023-10-17 11:42:02,579 - model_parallel: True
INFO:root:sample_ratio: 0.1
Training: 2023-10-17 11:42:02,579 - sample_ratio: 0.1
INFO:root:loss: ArcFace
Training: 2023-10-17 11:42:02,579 - loss: ArcFace
INFO:root:dropout: 0.0
Training: 2023-10-17 11:42:02,579 - dropout: 0.0
INFO:root:fp16: True
Training: 2023-10-17 11:42:02,579 - fp16: True
INFO:root:init_loss_scaling: 27648.0
Training: 2023-10-17 11:42:02,579 - init_loss_scaling: 27648.0
INFO:root:max_loss_scaling: 128.0
Training: 2023-10-17 11:42:02,579 - max_loss_scaling: 128.0
INFO:root:incr_every_n_steps: 2000
Training: 2023-10-17 11:42:02,579 - incr_every_n_steps: 2000
INFO:root:decr_every_n_nan_or_inf: 1
Training: 2023-10-17 11:42:02,579 - decr_every_n_nan_or_inf: 1
INFO:root:incr_ratio: 2.0
Training: 2023-10-17 11:42:02,579 - incr_ratio: 2.0
INFO:root:decr_ratio: 0.5
Training: 2023-10-17 11:42:02,579 - decr_ratio: 0.5
INFO:root:use_dynamic_loss_scaling: True
Training: 2023-10-17 11:42:02,579 - use_dynamic_loss_scaling: True
INFO:root:custom_white_list: []
Training: 2023-10-17 11:42:02,579 - custom_white_list: []
INFO:root:custom_black_list: []
Training: 2023-10-17 11:42:02,579 - custom_black_list: []
INFO:root:lr: 0.1
Training: 2023-10-17 11:42:02,579 - lr: 0.1
INFO:root:lr_decay: 0.1
Training: 2023-10-17 11:42:02,579 - lr_decay: 0.1
INFO:root:weight_decay: 0.0005
Training: 2023-10-17 11:42:02,579 - weight_decay: 0.0005
INFO:root:momentum: 0.9
Training: 2023-10-17 11:42:02,579 - momentum: 0.9
INFO:root:train_unit: epoch
Training: 2023-10-17 11:42:02,579 - train_unit: epoch
INFO:root:warmup_num: 0
Training: 2023-10-17 11:42:02,579 - warmup_num: 0
INFO:root:train_num: 25
Training: 2023-10-17 11:42:02,579 - train_num: 25
INFO:root:decay_boundaries: [10, 16, 22]
Training: 2023-10-17 11:42:02,579 - decay_boundaries: [10, 16, 22]
INFO:root:use_synthetic_dataset: False
Training: 2023-10-17 11:42:02,579 - use_synthetic_dataset: False
INFO:root:dataset: MS1M_v3
Training: 2023-10-17 11:42:02,579 - dataset: MS1M_v3
INFO:root:data_dir: ../dataset/
Training: 2023-10-17 11:42:02,579 - data_dir: ../dataset/
INFO:root:label_file: ../dataset/label.txt
Training: 2023-10-17 11:42:02,580 - label_file: ../dataset/label.txt
INFO:root:is_bin: False
Training: 2023-10-17 11:42:02,580 - is_bin: False
INFO:root:num_classes: 657077
Training: 2023-10-17 11:42:02,580 - num_classes: 657077
INFO:root:batch_size: 64
Training: 2023-10-17 11:42:02,580 - batch_size: 64
INFO:root:num_workers: 0
Training: 2023-10-17 11:42:02,580 - num_workers: 0
INFO:root:do_validation_while_train: True
Training: 2023-10-17 11:42:02,580 - do_validation_while_train: True
INFO:root:validation_interval_step: 2000
Training: 2023-10-17 11:42:02,580 - validation_interval_step: 2000
INFO:root:val_targets: ['lfw', 'cfp_fp', 'agedb_30']
Training: 2023-10-17 11:42:02,580 - val_targets: ['lfw', 'cfp_fp', 'agedb_30']
INFO:root:logdir: ./log
Training: 2023-10-17 11:42:02,580 - logdir: ./log
INFO:root:log_interval_step: 100
Training: 2023-10-17 11:42:02,580 - log_interval_step: 100
INFO:root:output: model
Training: 2023-10-17 11:42:02,580 - output: model
INFO:root:resume: False
Training: 2023-10-17 11:42:02,580 - resume: False
INFO:root:checkpoint_dir: None
Training: 2023-10-17 11:42:02,580 - checkpoint_dir: None
INFO:root:max_num_last_checkpoint: 1
Training: 2023-10-17 11:42:02,580 - max_num_last_checkpoint: 1
INFO:root:------------------------

Training: 2023-10-17 11:42:02,580 - ------------------------

INFO:root:read label file finished, total num: 4574213
Training: 2023-10-17 11:42:02,863 - read label file finished, total num: 4574213
INFO:root:world_size: 2
Training: 2023-10-17 11:42:02,864 - world_size: 2
INFO:root:total_batch_size: 128
Training: 2023-10-17 11:42:02,864 - total_batch_size: 128
INFO:root:warmup_steps: 0
Training: 2023-10-17 11:42:02,864 - warmup_steps: 0
INFO:root:steps_per_epoch: 35736
Training: 2023-10-17 11:42:02,864 - steps_per_epoch: 35736
INFO:root:total_steps: 893400
Training: 2023-10-17 11:42:02,864 - total_steps: 893400
INFO:root:total_epoch: 25
Training: 2023-10-17 11:42:02,864 - total_epoch: 25
INFO:root:decay_steps: [357360, 571776, 786192]
Training: 2023-10-17 11:42:02,864 - decay_steps: [357360, 571776, 786192]
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in conv2d only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'Out' in guassian_random only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in batch_norm only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in prelu only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in elementwise_add only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'y' in elementwise_add only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in fc only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'Out' in xavier_init only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'dtype' in create_parameter only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'tensor' in all_gather only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in split only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input[0]' in concat only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input[1]' in concat only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in gather only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'X' in norm only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in matmul only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'y' in matmul only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'logits' in margin_cross_entropy only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/framework.py:744: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  elif dtype == np.bool:
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in check_finite_and_unscale only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in update_loss_scaling only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/optimizer/momentum.py:262: UserWarning: Accumulating with FP16 in optimizer can lead to poor accuracy or slow convergence.Consider using multi_precision=True option of the Momentum optimizer.
  "Accumulating with FP16 in optimizer can lead to poor accuracy or slow convergence."
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:34109']
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
(14000, 3, 112, 112)
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
W1017 11:42:24.577876  3805 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 12.0, Runtime API Version: 11.2
W1017 11:42:24.579671  3805 device_context.cc:465] device: 0, cuDNN Version: 8.1.
I1017 11:42:26.239616  3805 gen_comm_id_helper.cc:190] Server listening on: 127.0.0.1:41497 successful.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::framework::Executor::RunPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, bool, bool, bool)
1   paddle::framework::Executor::RunPartialPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, long, long, bool, bool, bool)
2   paddle::framework::OperatorBase::Run(paddle::framework::Scope const&, paddle::platform::Place const&)
3   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&) const
4   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&, paddle::framework::RuntimeContext*) const
5   std::_Function_handler<void (paddle::framework::ExecutionContext const&), paddle::framework::OpKernelRegistrarFunctor<paddle::platform::CUDAPlace, false, 2ul, paddle::operators::CUDNNConvGradOpKernel<float>, paddle::operators::CUDNNConvGradOpKernel<double>, paddle::operators::CUDNNConvGradOpKernel<paddle::platform::float16>, paddle::operators::CUDNNConvGradOpKernel<paddle::platform::bfloat16> >::operator()(char const*, char const*, int) const::{lambda(paddle::framework::ExecutionContext const&)#1}>::_M_invoke(std::_Any_data const&, paddle::framework::ExecutionContext const&)
6   paddle::operators::CUDNNConvGradOpKernel<paddle::platform::float16>::Compute(paddle::framework::ExecutionContext const&) const
7   cudnnConvolutionBwdFilterAlgo_t paddle::operators::SearchAlgorithm<cudnnConvolutionBwdFilterAlgoPerf_t>::Find<paddle::platform::float16>(paddle::operators::ConvArgs const&, bool, bool, paddle::framework::ExecutionContext const&)
8   paddle::framework::AlgorithmsCache<cudnnConvolutionBwdFilterAlgo_t>::GetAlgorithm(std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, int, long, std::function<cudnnConvolutionBwdFilterAlgo_t ()>)
9   std::_Function_handler<cudnnConvolutionBwdFilterAlgo_t (), paddle::operators::SearchAlgorithm<cudnnConvolutionBwdFilterAlgoPerf_t>::Find<paddle::platform::float16>(paddle::operators::ConvArgs const&, bool, bool, paddle::framework::ExecutionContext const&)::{lambda()#2}>::_M_invoke(std::_Any_data const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1697542950 (unix time) try "date -d @1697542950" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0xe8b) received by PID 3805 (TID 0x7f3ec682f740) from PID 3723 ***]

grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:34: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  method='lar', copy_X=True, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:164: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  method='lar', copy_X=True, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:281: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:865: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1121: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1149: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, positive=False):
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1379: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1621: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1755: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, positive=False):
/usr/local/lib/python3.7/dist-packages/sklearn/decomposition/_lda.py:28: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  EPS = np.finfo(np.float).eps
INFO:root:rank: 0
Training: 2023-10-17 11:42:38,795 - rank: 0
INFO:root:--------args----------
Training: 2023-10-17 11:42:38,795 - --------args----------
INFO:root:config_file: configs/ms1mv3_r50.py
Training: 2023-10-17 11:42:38,795 - config_file: configs/ms1mv3_r50.py
INFO:root:is_static: True
Training: 2023-10-17 11:42:38,795 - is_static: True
INFO:root:backbone: FresResNet101
Training: 2023-10-17 11:42:38,795 - backbone: FresResNet101
INFO:root:classifier: LargeScaleClassifier
Training: 2023-10-17 11:42:38,795 - classifier: LargeScaleClassifier
INFO:root:embedding_size: 512
Training: 2023-10-17 11:42:38,795 - embedding_size: 512
INFO:root:model_parallel: True
Training: 2023-10-17 11:42:38,795 - model_parallel: True
INFO:root:sample_ratio: 0.1
Training: 2023-10-17 11:42:38,795 - sample_ratio: 0.1
INFO:root:loss: ArcFace
Training: 2023-10-17 11:42:38,795 - loss: ArcFace
INFO:root:dropout: 0.0
Training: 2023-10-17 11:42:38,795 - dropout: 0.0
INFO:root:fp16: True
Training: 2023-10-17 11:42:38,795 - fp16: True
INFO:root:init_loss_scaling: 27648.0
Training: 2023-10-17 11:42:38,795 - init_loss_scaling: 27648.0
INFO:root:max_loss_scaling: 128.0
Training: 2023-10-17 11:42:38,795 - max_loss_scaling: 128.0
INFO:root:incr_every_n_steps: 2000
Training: 2023-10-17 11:42:38,796 - incr_every_n_steps: 2000
INFO:root:decr_every_n_nan_or_inf: 1
Training: 2023-10-17 11:42:38,796 - decr_every_n_nan_or_inf: 1
INFO:root:incr_ratio: 2.0
Training: 2023-10-17 11:42:38,796 - incr_ratio: 2.0
INFO:root:decr_ratio: 0.5
Training: 2023-10-17 11:42:38,796 - decr_ratio: 0.5
INFO:root:use_dynamic_loss_scaling: True
Training: 2023-10-17 11:42:38,796 - use_dynamic_loss_scaling: True
INFO:root:custom_white_list: []
Training: 2023-10-17 11:42:38,796 - custom_white_list: []
INFO:root:custom_black_list: []
Training: 2023-10-17 11:42:38,796 - custom_black_list: []
INFO:root:lr: 0.1
Training: 2023-10-17 11:42:38,796 - lr: 0.1
INFO:root:lr_decay: 0.1
Training: 2023-10-17 11:42:38,796 - lr_decay: 0.1
INFO:root:weight_decay: 0.0005
Training: 2023-10-17 11:42:38,796 - weight_decay: 0.0005
INFO:root:momentum: 0.9
Training: 2023-10-17 11:42:38,796 - momentum: 0.9
INFO:root:train_unit: epoch
Training: 2023-10-17 11:42:38,796 - train_unit: epoch
INFO:root:warmup_num: 0
Training: 2023-10-17 11:42:38,796 - warmup_num: 0
INFO:root:train_num: 25
Training: 2023-10-17 11:42:38,796 - train_num: 25
INFO:root:decay_boundaries: [10, 16, 22]
Training: 2023-10-17 11:42:38,796 - decay_boundaries: [10, 16, 22]
INFO:root:use_synthetic_dataset: False
Training: 2023-10-17 11:42:38,796 - use_synthetic_dataset: False
INFO:root:dataset: MS1M_v3
Training: 2023-10-17 11:42:38,796 - dataset: MS1M_v3
INFO:root:data_dir: ../dataset/
Training: 2023-10-17 11:42:38,796 - data_dir: ../dataset/
INFO:root:label_file: ../dataset/label.txt
Training: 2023-10-17 11:42:38,796 - label_file: ../dataset/label.txt
INFO:root:is_bin: False
Training: 2023-10-17 11:42:38,796 - is_bin: False
INFO:root:num_classes: 657077
Training: 2023-10-17 11:42:38,796 - num_classes: 657077
INFO:root:batch_size: 64
Training: 2023-10-17 11:42:38,796 - batch_size: 64
INFO:root:num_workers: 0
Training: 2023-10-17 11:42:38,796 - num_workers: 0
INFO:root:do_validation_while_train: True
Training: 2023-10-17 11:42:38,796 - do_validation_while_train: True
INFO:root:validation_interval_step: 2000
Training: 2023-10-17 11:42:38,796 - validation_interval_step: 2000
INFO:root:val_targets: ['lfw', 'cfp_fp', 'agedb_30']
Training: 2023-10-17 11:42:38,796 - val_targets: ['lfw', 'cfp_fp', 'agedb_30']
INFO:root:logdir: ./log
Training: 2023-10-17 11:42:38,796 - logdir: ./log
INFO:root:log_interval_step: 100
Training: 2023-10-17 11:42:38,796 - log_interval_step: 100
INFO:root:output: model
Training: 2023-10-17 11:42:38,796 - output: model
INFO:root:resume: False
Training: 2023-10-17 11:42:38,796 - resume: False
INFO:root:checkpoint_dir: None
Training: 2023-10-17 11:42:38,796 - checkpoint_dir: None
INFO:root:max_num_last_checkpoint: 1
Training: 2023-10-17 11:42:38,796 - max_num_last_checkpoint: 1
INFO:root:------------------------

Training: 2023-10-17 11:42:38,797 - ------------------------

INFO:root:read label file finished, total num: 4574213
Training: 2023-10-17 11:42:39,080 - read label file finished, total num: 4574213
INFO:root:world_size: 2
Training: 2023-10-17 11:42:39,080 - world_size: 2
INFO:root:total_batch_size: 128
Training: 2023-10-17 11:42:39,080 - total_batch_size: 128
INFO:root:warmup_steps: 0
Training: 2023-10-17 11:42:39,080 - warmup_steps: 0
INFO:root:steps_per_epoch: 35736
Training: 2023-10-17 11:42:39,080 - steps_per_epoch: 35736
INFO:root:total_steps: 893400
Training: 2023-10-17 11:42:39,080 - total_steps: 893400
INFO:root:total_epoch: 25
Training: 2023-10-17 11:42:39,080 - total_epoch: 25
INFO:root:decay_steps: [357360, 571776, 786192]
Training: 2023-10-17 11:42:39,080 - decay_steps: [357360, 571776, 786192]
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in conv2d only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'Out' in guassian_random only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in batch_norm only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in prelu only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in elementwise_add only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'y' in elementwise_add only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in fc only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'Out' in xavier_init only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'dtype' in create_parameter only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'tensor' in all_gather only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input' in split only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input[0]' in concat only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'input[1]' in concat only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in gather only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'X' in norm only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in matmul only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'y' in matmul only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'logits' in margin_cross_entropy only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/framework.py:744: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  elif dtype == np.bool:
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in check_finite_and_unscale only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/data_feeder.py:128: UserWarning: The data type of 'x' in update_loss_scaling only support float16 in GPU now. 
  (input_name, op_name, extra_message))
/usr/local/lib/python3.7/dist-packages/paddle/optimizer/momentum.py:262: UserWarning: Accumulating with FP16 in optimizer can lead to poor accuracy or slow convergence.Consider using multi_precision=True option of the Momentum optimizer.
  "Accumulating with FP16 in optimizer can lead to poor accuracy or slow convergence."
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:48933']
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
(14000, 3, 112, 112)
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
W1017 11:43:00.576776  3953 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 12.0, Runtime API Version: 11.2
W1017 11:43:00.578559  3953 device_context.cc:465] device: 0, cuDNN Version: 8.1.
I1017 11:43:02.226886  3953 gen_comm_id_helper.cc:190] Server listening on: 127.0.0.1:48917 successful.
INFO:root:loss 48.1591, lr: 0.025000, epoch: 0, step: 100, eta: 96.19 hours, avg_reader_cost: 0.01089 sec, avg_batch_cost: 0.37316 sec, avg_samples: 64.00000, ips: 343.01823 images/sec
Training: 2023-10-17 11:43:39,668 - loss 48.1591, lr: 0.025000, epoch: 0, step: 100, eta: 96.19 hours, avg_reader_cost: 0.01089 sec, avg_batch_cost: 0.37316 sec, avg_samples: 64.00000, ips: 343.01823 images/sec
INFO:root:loss 46.3567, lr: 0.025000, epoch: 0, step: 200, eta: 90.17 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33889 sec, avg_samples: 64.00000, ips: 377.69893 images/sec
Training: 2023-10-17 11:44:13,564 - loss 46.3567, lr: 0.025000, epoch: 0, step: 200, eta: 90.17 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33889 sec, avg_samples: 64.00000, ips: 377.69893 images/sec
INFO:root:loss 46.0437, lr: 0.025000, epoch: 0, step: 300, eta: 88.15 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33904 sec, avg_samples: 64.00000, ips: 377.53363 images/sec
Training: 2023-10-17 11:44:47,474 - loss 46.0437, lr: 0.025000, epoch: 0, step: 300, eta: 88.15 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33904 sec, avg_samples: 64.00000, ips: 377.53363 images/sec
INFO:root:loss 45.9425, lr: 0.025000, epoch: 0, step: 400, eta: 87.14 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33912 sec, avg_samples: 64.00000, ips: 377.44574 images/sec
Training: 2023-10-17 11:45:21,391 - loss 45.9425, lr: 0.025000, epoch: 0, step: 400, eta: 87.14 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33912 sec, avg_samples: 64.00000, ips: 377.44574 images/sec
INFO:root:loss 45.8287, lr: 0.025000, epoch: 0, step: 500, eta: 86.53 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33914 sec, avg_samples: 64.00000, ips: 377.42776 images/sec
Training: 2023-10-17 11:45:55,310 - loss 45.8287, lr: 0.025000, epoch: 0, step: 500, eta: 86.53 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33914 sec, avg_samples: 64.00000, ips: 377.42776 images/sec
INFO:root:loss 45.8613, lr: 0.025000, epoch: 0, step: 600, eta: 86.12 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33906 sec, avg_samples: 64.00000, ips: 377.51492 images/sec
Training: 2023-10-17 11:46:29,221 - loss 45.8613, lr: 0.025000, epoch: 0, step: 600, eta: 86.12 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33906 sec, avg_samples: 64.00000, ips: 377.51492 images/sec
INFO:root:loss 45.7601, lr: 0.025000, epoch: 0, step: 700, eta: 85.83 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33916 sec, avg_samples: 64.00000, ips: 377.40827 images/sec
Training: 2023-10-17 11:47:03,142 - loss 45.7601, lr: 0.025000, epoch: 0, step: 700, eta: 85.83 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33916 sec, avg_samples: 64.00000, ips: 377.40827 images/sec
INFO:root:loss 45.7643, lr: 0.025000, epoch: 0, step: 800, eta: 85.60 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33908 sec, avg_samples: 64.00000, ips: 377.49397 images/sec
Training: 2023-10-17 11:47:37,055 - loss 45.7643, lr: 0.025000, epoch: 0, step: 800, eta: 85.60 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33908 sec, avg_samples: 64.00000, ips: 377.49397 images/sec
INFO:root:loss 45.6996, lr: 0.025000, epoch: 0, step: 900, eta: 85.42 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33907 sec, avg_samples: 64.00000, ips: 377.49915 images/sec
Training: 2023-10-17 11:48:10,968 - loss 45.6996, lr: 0.025000, epoch: 0, step: 900, eta: 85.42 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33907 sec, avg_samples: 64.00000, ips: 377.49915 images/sec
INFO:root:loss 45.7345, lr: 0.025000, epoch: 0, step: 1000, eta: 85.28 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33914 sec, avg_samples: 64.00000, ips: 377.42487 images/sec
Training: 2023-10-17 11:48:44,888 - loss 45.7345, lr: 0.025000, epoch: 0, step: 1000, eta: 85.28 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33914 sec, avg_samples: 64.00000, ips: 377.42487 images/sec
INFO:root:loss 45.6792, lr: 0.025000, epoch: 0, step: 1100, eta: 85.16 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33905 sec, avg_samples: 64.00000, ips: 377.52547 images/sec
Training: 2023-10-17 11:49:18,799 - loss 45.6792, lr: 0.025000, epoch: 0, step: 1100, eta: 85.16 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33905 sec, avg_samples: 64.00000, ips: 377.52547 images/sec
INFO:root:loss 45.7764, lr: 0.025000, epoch: 0, step: 1200, eta: 85.06 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33895 sec, avg_samples: 64.00000, ips: 377.63836 images/sec
Training: 2023-10-17 11:49:52,701 - loss 45.7764, lr: 0.025000, epoch: 0, step: 1200, eta: 85.06 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33895 sec, avg_samples: 64.00000, ips: 377.63836 images/sec
INFO:root:loss 45.7741, lr: 0.025000, epoch: 0, step: 1300, eta: 84.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33869 sec, avg_samples: 64.00000, ips: 377.92777 images/sec
Training: 2023-10-17 11:50:26,577 - loss 45.7741, lr: 0.025000, epoch: 0, step: 1300, eta: 84.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33869 sec, avg_samples: 64.00000, ips: 377.92777 images/sec
INFO:root:loss 45.7261, lr: 0.025000, epoch: 0, step: 1400, eta: 84.88 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33861 sec, avg_samples: 64.00000, ips: 378.01488 images/sec
Training: 2023-10-17 11:51:00,445 - loss 45.7261, lr: 0.025000, epoch: 0, step: 1400, eta: 84.88 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33861 sec, avg_samples: 64.00000, ips: 378.01488 images/sec
INFO:root:loss 45.7372, lr: 0.025000, epoch: 0, step: 1500, eta: 84.81 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33880 sec, avg_samples: 64.00000, ips: 377.80478 images/sec
Training: 2023-10-17 11:51:34,332 - loss 45.7372, lr: 0.025000, epoch: 0, step: 1500, eta: 84.81 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33880 sec, avg_samples: 64.00000, ips: 377.80478 images/sec
INFO:root:loss 45.6459, lr: 0.025000, epoch: 0, step: 1600, eta: 84.74 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33858 sec, avg_samples: 64.00000, ips: 378.05359 images/sec
Training: 2023-10-17 11:52:08,197 - loss 45.6459, lr: 0.025000, epoch: 0, step: 1600, eta: 84.74 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33858 sec, avg_samples: 64.00000, ips: 378.05359 images/sec
INFO:root:loss 45.6684, lr: 0.025000, epoch: 0, step: 1700, eta: 84.69 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33882 sec, avg_samples: 64.00000, ips: 377.77906 images/sec
Training: 2023-10-17 11:52:42,086 - loss 45.6684, lr: 0.025000, epoch: 0, step: 1700, eta: 84.69 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33882 sec, avg_samples: 64.00000, ips: 377.77906 images/sec
INFO:root:loss 45.6229, lr: 0.025000, epoch: 0, step: 1800, eta: 84.64 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33894 sec, avg_samples: 64.00000, ips: 377.64573 images/sec
Training: 2023-10-17 11:53:15,987 - loss 45.6229, lr: 0.025000, epoch: 0, step: 1800, eta: 84.64 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33894 sec, avg_samples: 64.00000, ips: 377.64573 images/sec
INFO:root:loss 45.6909, lr: 0.025000, epoch: 0, step: 1900, eta: 84.59 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33880 sec, avg_samples: 64.00000, ips: 377.80898 images/sec
Training: 2023-10-17 11:53:49,874 - loss 45.6909, lr: 0.025000, epoch: 0, step: 1900, eta: 84.59 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33880 sec, avg_samples: 64.00000, ips: 377.80898 images/sec
INFO:root:loss 45.6622, lr: 0.025000, epoch: 0, step: 2000, eta: 84.55 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33880 sec, avg_samples: 64.00000, ips: 377.80144 images/sec
Training: 2023-10-17 11:54:23,761 - loss 45.6622, lr: 0.025000, epoch: 0, step: 2000, eta: 84.55 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33880 sec, avg_samples: 64.00000, ips: 377.80144 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][2000]XNorm: 17.594634
Training: 2023-10-17 11:54:55,792 - [lfw][2000]XNorm: 17.594634
INFO:root:[lfw][2000]Accuracy-Flip: 0.59933+-0.01877
Training: 2023-10-17 11:54:55,792 - [lfw][2000]Accuracy-Flip: 0.59933+-0.01877
INFO:root:[lfw][2000]Accuracy-Highest: 0.59933
Training: 2023-10-17 11:54:55,792 - [lfw][2000]Accuracy-Highest: 0.59933
INFO:root:test time: 32.0307
Training: 2023-10-17 11:54:55,792 - test time: 32.0307
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][2000]XNorm: 35.625959
Training: 2023-10-17 11:55:32,923 - [cfp_fp][2000]XNorm: 35.625959
INFO:root:[cfp_fp][2000]Accuracy-Flip: 0.58143+-0.01562
Training: 2023-10-17 11:55:32,923 - [cfp_fp][2000]Accuracy-Flip: 0.58143+-0.01562
INFO:root:[cfp_fp][2000]Accuracy-Highest: 0.58143
Training: 2023-10-17 11:55:32,923 - [cfp_fp][2000]Accuracy-Highest: 0.58143
INFO:root:test time: 37.1307
Training: 2023-10-17 11:55:32,923 - test time: 37.1307
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][2000]XNorm: 17.567836
Training: 2023-10-17 11:56:04,925 - [agedb_30][2000]XNorm: 17.567836
INFO:root:[agedb_30][2000]Accuracy-Flip: 0.52783+-0.00730
Training: 2023-10-17 11:56:04,925 - [agedb_30][2000]Accuracy-Flip: 0.52783+-0.00730
INFO:root:[agedb_30][2000]Accuracy-Highest: 0.52783
Training: 2023-10-17 11:56:04,925 - [agedb_30][2000]Accuracy-Highest: 0.52783
INFO:root:test time: 32.0024
Training: 2023-10-17 11:56:04,925 - test time: 32.0024
INFO:root:loss 45.6804, lr: 0.025000, epoch: 0, step: 2100, eta: 96.42 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33771 sec, avg_samples: 64.00000, ips: 379.01923 images/sec
Training: 2023-10-17 11:56:38,704 - loss 45.6804, lr: 0.025000, epoch: 0, step: 2100, eta: 96.42 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33771 sec, avg_samples: 64.00000, ips: 379.01923 images/sec
INFO:root:loss 45.7296, lr: 0.025000, epoch: 0, step: 2200, eta: 95.84 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33882 sec, avg_samples: 64.00000, ips: 377.78711 images/sec
Training: 2023-10-17 11:57:12,592 - loss 45.7296, lr: 0.025000, epoch: 0, step: 2200, eta: 95.84 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33882 sec, avg_samples: 64.00000, ips: 377.78711 images/sec
INFO:root:loss 45.6614, lr: 0.025000, epoch: 0, step: 2300, eta: 95.31 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33885 sec, avg_samples: 64.00000, ips: 377.74574 images/sec
Training: 2023-10-17 11:57:46,483 - loss 45.6614, lr: 0.025000, epoch: 0, step: 2300, eta: 95.31 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33885 sec, avg_samples: 64.00000, ips: 377.74574 images/sec
INFO:root:loss 45.6580, lr: 0.025000, epoch: 0, step: 2400, eta: 94.82 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33921 sec, avg_samples: 64.00000, ips: 377.34389 images/sec
Training: 2023-10-17 11:58:20,410 - loss 45.6580, lr: 0.025000, epoch: 0, step: 2400, eta: 94.82 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33921 sec, avg_samples: 64.00000, ips: 377.34389 images/sec
INFO:root:loss 45.7053, lr: 0.025000, epoch: 0, step: 2500, eta: 94.38 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33925 sec, avg_samples: 64.00000, ips: 377.30055 images/sec
Training: 2023-10-17 11:58:54,341 - loss 45.7053, lr: 0.025000, epoch: 0, step: 2500, eta: 94.38 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33925 sec, avg_samples: 64.00000, ips: 377.30055 images/sec
INFO:root:loss 45.6693, lr: 0.025000, epoch: 0, step: 2600, eta: 93.97 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33924 sec, avg_samples: 64.00000, ips: 377.31842 images/sec
Training: 2023-10-17 11:59:28,270 - loss 45.6693, lr: 0.025000, epoch: 0, step: 2600, eta: 93.97 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33924 sec, avg_samples: 64.00000, ips: 377.31842 images/sec
INFO:root:loss 45.6481, lr: 0.025000, epoch: 0, step: 2700, eta: 93.59 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33916 sec, avg_samples: 64.00000, ips: 377.40141 images/sec
Training: 2023-10-17 12:00:02,192 - loss 45.6481, lr: 0.025000, epoch: 0, step: 2700, eta: 93.59 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33916 sec, avg_samples: 64.00000, ips: 377.40141 images/sec
INFO:root:loss 45.6223, lr: 0.025000, epoch: 0, step: 2800, eta: 93.23 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33877 sec, avg_samples: 64.00000, ips: 377.83336 images/sec
Training: 2023-10-17 12:00:36,076 - loss 45.6223, lr: 0.025000, epoch: 0, step: 2800, eta: 93.23 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33877 sec, avg_samples: 64.00000, ips: 377.83336 images/sec
INFO:root:loss 45.7277, lr: 0.025000, epoch: 0, step: 2900, eta: 92.89 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33887 sec, avg_samples: 64.00000, ips: 377.72249 images/sec
Training: 2023-10-17 12:01:09,968 - loss 45.7277, lr: 0.025000, epoch: 0, step: 2900, eta: 92.89 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33887 sec, avg_samples: 64.00000, ips: 377.72249 images/sec
INFO:root:loss 45.6783, lr: 0.025000, epoch: 0, step: 3000, eta: 92.58 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33907 sec, avg_samples: 64.00000, ips: 377.50741 images/sec
Training: 2023-10-17 12:01:43,880 - loss 45.6783, lr: 0.025000, epoch: 0, step: 3000, eta: 92.58 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33907 sec, avg_samples: 64.00000, ips: 377.50741 images/sec
INFO:root:loss 45.6954, lr: 0.025000, epoch: 0, step: 3100, eta: 92.30 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33943 sec, avg_samples: 64.00000, ips: 377.10293 images/sec
Training: 2023-10-17 12:02:17,828 - loss 45.6954, lr: 0.025000, epoch: 0, step: 3100, eta: 92.30 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33943 sec, avg_samples: 64.00000, ips: 377.10293 images/sec
INFO:root:loss 45.6705, lr: 0.025000, epoch: 0, step: 3200, eta: 92.02 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33934 sec, avg_samples: 64.00000, ips: 377.19915 images/sec
Training: 2023-10-17 12:02:51,768 - loss 45.6705, lr: 0.025000, epoch: 0, step: 3200, eta: 92.02 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33934 sec, avg_samples: 64.00000, ips: 377.19915 images/sec
INFO:root:loss 45.6691, lr: 0.025000, epoch: 0, step: 3300, eta: 91.77 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33918 sec, avg_samples: 64.00000, ips: 377.38271 images/sec
Training: 2023-10-17 12:03:25,691 - loss 45.6691, lr: 0.025000, epoch: 0, step: 3300, eta: 91.77 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33918 sec, avg_samples: 64.00000, ips: 377.38271 images/sec
INFO:root:loss 45.6322, lr: 0.025000, epoch: 0, step: 3400, eta: 91.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33899 sec, avg_samples: 64.00000, ips: 377.58672 images/sec
Training: 2023-10-17 12:03:59,596 - loss 45.6322, lr: 0.025000, epoch: 0, step: 3400, eta: 91.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33899 sec, avg_samples: 64.00000, ips: 377.58672 images/sec
INFO:root:loss 45.7065, lr: 0.025000, epoch: 0, step: 3500, eta: 91.29 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33915 sec, avg_samples: 64.00000, ips: 377.41145 images/sec
Training: 2023-10-17 12:04:33,517 - loss 45.7065, lr: 0.025000, epoch: 0, step: 3500, eta: 91.29 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33915 sec, avg_samples: 64.00000, ips: 377.41145 images/sec
INFO:root:loss 45.7709, lr: 0.025000, epoch: 0, step: 3600, eta: 91.08 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33894 sec, avg_samples: 64.00000, ips: 377.64629 images/sec
Training: 2023-10-17 12:05:07,416 - loss 45.7709, lr: 0.025000, epoch: 0, step: 3600, eta: 91.08 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33894 sec, avg_samples: 64.00000, ips: 377.64629 images/sec
INFO:root:loss 45.7744, lr: 0.025000, epoch: 0, step: 3700, eta: 90.87 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33913 sec, avg_samples: 64.00000, ips: 377.43843 images/sec
Training: 2023-10-17 12:05:41,335 - loss 45.7744, lr: 0.025000, epoch: 0, step: 3700, eta: 90.87 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33913 sec, avg_samples: 64.00000, ips: 377.43843 images/sec
INFO:root:loss 45.6686, lr: 0.025000, epoch: 0, step: 3800, eta: 90.67 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33913 sec, avg_samples: 64.00000, ips: 377.43467 images/sec
Training: 2023-10-17 12:06:15,254 - loss 45.6686, lr: 0.025000, epoch: 0, step: 3800, eta: 90.67 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33913 sec, avg_samples: 64.00000, ips: 377.43467 images/sec
INFO:root:loss 45.7003, lr: 0.025000, epoch: 0, step: 3900, eta: 90.49 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33912 sec, avg_samples: 64.00000, ips: 377.45042 images/sec
Training: 2023-10-17 12:06:49,171 - loss 45.7003, lr: 0.025000, epoch: 0, step: 3900, eta: 90.49 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33912 sec, avg_samples: 64.00000, ips: 377.45042 images/sec
INFO:root:loss 45.7227, lr: 0.025000, epoch: 0, step: 4000, eta: 90.31 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33905 sec, avg_samples: 64.00000, ips: 377.52142 images/sec
Training: 2023-10-17 12:07:23,082 - loss 45.7227, lr: 0.025000, epoch: 0, step: 4000, eta: 90.31 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33905 sec, avg_samples: 64.00000, ips: 377.52142 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][4000]XNorm: 14.574146
Training: 2023-10-17 12:07:55,152 - [lfw][4000]XNorm: 14.574146
INFO:root:[lfw][4000]Accuracy-Flip: 0.61583+-0.02351
Training: 2023-10-17 12:07:55,153 - [lfw][4000]Accuracy-Flip: 0.61583+-0.02351
INFO:root:[lfw][4000]Accuracy-Highest: 0.61583
Training: 2023-10-17 12:07:55,153 - [lfw][4000]Accuracy-Highest: 0.61583
INFO:root:test time: 32.0707
Training: 2023-10-17 12:07:55,153 - test time: 32.0707
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][4000]XNorm: 14.003850
Training: 2023-10-17 12:08:32,374 - [cfp_fp][4000]XNorm: 14.003850
INFO:root:[cfp_fp][4000]Accuracy-Flip: 0.56429+-0.01375
Training: 2023-10-17 12:08:32,374 - [cfp_fp][4000]Accuracy-Flip: 0.56429+-0.01375
INFO:root:[cfp_fp][4000]Accuracy-Highest: 0.58143
Training: 2023-10-17 12:08:32,374 - [cfp_fp][4000]Accuracy-Highest: 0.58143
INFO:root:test time: 37.2214
Training: 2023-10-17 12:08:32,374 - test time: 37.2214
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][4000]XNorm: 14.297668
Training: 2023-10-17 12:09:04,466 - [agedb_30][4000]XNorm: 14.297668
INFO:root:[agedb_30][4000]Accuracy-Flip: 0.49833+-0.00573
Training: 2023-10-17 12:09:04,466 - [agedb_30][4000]Accuracy-Flip: 0.49833+-0.00573
INFO:root:[agedb_30][4000]Accuracy-Highest: 0.52783
Training: 2023-10-17 12:09:04,466 - [agedb_30][4000]Accuracy-Highest: 0.52783
INFO:root:test time: 32.0917
Training: 2023-10-17 12:09:04,466 - test time: 32.0917
INFO:root:loss 45.6484, lr: 0.025000, epoch: 0, step: 4100, eta: 96.24 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.80572 images/sec
Training: 2023-10-17 12:09:38,262 - loss 45.6484, lr: 0.025000, epoch: 0, step: 4100, eta: 96.24 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.80572 images/sec
INFO:root:loss 45.6758, lr: 0.025000, epoch: 0, step: 4200, eta: 95.93 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33919 sec, avg_samples: 64.00000, ips: 377.36801 images/sec
Training: 2023-10-17 12:10:12,187 - loss 45.6758, lr: 0.025000, epoch: 0, step: 4200, eta: 95.93 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33919 sec, avg_samples: 64.00000, ips: 377.36801 images/sec
INFO:root:loss 45.6979, lr: 0.025000, epoch: 0, step: 4300, eta: 95.64 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33961 sec, avg_samples: 64.00000, ips: 376.90328 images/sec
Training: 2023-10-17 12:10:46,154 - loss 45.6979, lr: 0.025000, epoch: 0, step: 4300, eta: 95.64 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33961 sec, avg_samples: 64.00000, ips: 376.90328 images/sec
INFO:root:loss 45.7169, lr: 0.025000, epoch: 0, step: 4400, eta: 95.37 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33968 sec, avg_samples: 64.00000, ips: 376.82896 images/sec
Training: 2023-10-17 12:11:20,127 - loss 45.7169, lr: 0.025000, epoch: 0, step: 4400, eta: 95.37 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33968 sec, avg_samples: 64.00000, ips: 376.82896 images/sec
INFO:root:loss 45.6618, lr: 0.025000, epoch: 0, step: 4500, eta: 95.10 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33962 sec, avg_samples: 64.00000, ips: 376.89676 images/sec
Training: 2023-10-17 12:11:54,095 - loss 45.6618, lr: 0.025000, epoch: 0, step: 4500, eta: 95.10 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33962 sec, avg_samples: 64.00000, ips: 376.89676 images/sec
INFO:root:loss 45.6437, lr: 0.025000, epoch: 0, step: 4600, eta: 94.85 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33950 sec, avg_samples: 64.00000, ips: 377.02308 images/sec
Training: 2023-10-17 12:12:28,051 - loss 45.6437, lr: 0.025000, epoch: 0, step: 4600, eta: 94.85 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33950 sec, avg_samples: 64.00000, ips: 377.02308 images/sec
INFO:root:loss 45.6762, lr: 0.025000, epoch: 0, step: 4700, eta: 94.60 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33924 sec, avg_samples: 64.00000, ips: 377.31221 images/sec
Training: 2023-10-17 12:13:01,981 - loss 45.6762, lr: 0.025000, epoch: 0, step: 4700, eta: 94.60 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33924 sec, avg_samples: 64.00000, ips: 377.31221 images/sec
INFO:root:loss 45.6284, lr: 0.025000, epoch: 0, step: 4800, eta: 94.36 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33901 sec, avg_samples: 64.00000, ips: 377.56984 images/sec
Training: 2023-10-17 12:13:35,887 - loss 45.6284, lr: 0.025000, epoch: 0, step: 4800, eta: 94.36 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33901 sec, avg_samples: 64.00000, ips: 377.56984 images/sec
INFO:root:loss 45.6484, lr: 0.025000, epoch: 0, step: 4900, eta: 94.13 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33908 sec, avg_samples: 64.00000, ips: 377.49387 images/sec
Training: 2023-10-17 12:14:09,801 - loss 45.6484, lr: 0.025000, epoch: 0, step: 4900, eta: 94.13 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33908 sec, avg_samples: 64.00000, ips: 377.49387 images/sec
INFO:root:loss 45.6767, lr: 0.025000, epoch: 0, step: 5000, eta: 93.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33898 sec, avg_samples: 64.00000, ips: 377.60343 images/sec
Training: 2023-10-17 12:14:43,705 - loss 45.6767, lr: 0.025000, epoch: 0, step: 5000, eta: 93.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33898 sec, avg_samples: 64.00000, ips: 377.60343 images/sec
INFO:root:loss 45.6923, lr: 0.025000, epoch: 0, step: 5100, eta: 93.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33894 sec, avg_samples: 64.00000, ips: 377.64760 images/sec
Training: 2023-10-17 12:15:17,604 - loss 45.6923, lr: 0.025000, epoch: 0, step: 5100, eta: 93.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33894 sec, avg_samples: 64.00000, ips: 377.64760 images/sec
INFO:root:loss 45.6569, lr: 0.025000, epoch: 0, step: 5200, eta: 93.50 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33901 sec, avg_samples: 64.00000, ips: 377.56910 images/sec
Training: 2023-10-17 12:15:51,511 - loss 45.6569, lr: 0.025000, epoch: 0, step: 5200, eta: 93.50 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33901 sec, avg_samples: 64.00000, ips: 377.56910 images/sec
INFO:root:loss 45.6175, lr: 0.025000, epoch: 0, step: 5300, eta: 93.30 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33908 sec, avg_samples: 64.00000, ips: 377.48887 images/sec
Training: 2023-10-17 12:16:25,425 - loss 45.6175, lr: 0.025000, epoch: 0, step: 5300, eta: 93.30 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33908 sec, avg_samples: 64.00000, ips: 377.48887 images/sec
INFO:root:loss 45.6616, lr: 0.025000, epoch: 0, step: 5400, eta: 93.11 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33880 sec, avg_samples: 64.00000, ips: 377.80923 images/sec
Training: 2023-10-17 12:16:59,311 - loss 45.6616, lr: 0.025000, epoch: 0, step: 5400, eta: 93.11 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33880 sec, avg_samples: 64.00000, ips: 377.80923 images/sec
INFO:root:loss 45.6648, lr: 0.025000, epoch: 0, step: 5500, eta: 92.93 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33864 sec, avg_samples: 64.00000, ips: 377.98801 images/sec
Training: 2023-10-17 12:17:33,182 - loss 45.6648, lr: 0.025000, epoch: 0, step: 5500, eta: 92.93 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33864 sec, avg_samples: 64.00000, ips: 377.98801 images/sec
INFO:root:loss 45.7255, lr: 0.025000, epoch: 0, step: 5600, eta: 92.75 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33911 sec, avg_samples: 64.00000, ips: 377.45525 images/sec
Training: 2023-10-17 12:18:07,099 - loss 45.7255, lr: 0.025000, epoch: 0, step: 5600, eta: 92.75 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33911 sec, avg_samples: 64.00000, ips: 377.45525 images/sec
INFO:root:loss 45.6877, lr: 0.025000, epoch: 0, step: 5700, eta: 92.58 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33899 sec, avg_samples: 64.00000, ips: 377.59593 images/sec
Training: 2023-10-17 12:18:41,003 - loss 45.6877, lr: 0.025000, epoch: 0, step: 5700, eta: 92.58 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33899 sec, avg_samples: 64.00000, ips: 377.59593 images/sec
INFO:root:loss 45.6666, lr: 0.025000, epoch: 0, step: 5800, eta: 92.42 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33901 sec, avg_samples: 64.00000, ips: 377.56603 images/sec
Training: 2023-10-17 12:19:14,911 - loss 45.6666, lr: 0.025000, epoch: 0, step: 5800, eta: 92.42 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33901 sec, avg_samples: 64.00000, ips: 377.56603 images/sec
INFO:root:loss 45.6321, lr: 0.025000, epoch: 0, step: 5900, eta: 92.26 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33904 sec, avg_samples: 64.00000, ips: 377.53249 images/sec
Training: 2023-10-17 12:19:48,821 - loss 45.6321, lr: 0.025000, epoch: 0, step: 5900, eta: 92.26 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33904 sec, avg_samples: 64.00000, ips: 377.53249 images/sec
INFO:root:loss 45.7044, lr: 0.025000, epoch: 0, step: 6000, eta: 92.10 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33887 sec, avg_samples: 64.00000, ips: 377.72226 images/sec
Training: 2023-10-17 12:20:22,714 - loss 45.7044, lr: 0.025000, epoch: 0, step: 6000, eta: 92.10 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33887 sec, avg_samples: 64.00000, ips: 377.72226 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][6000]XNorm: 12.039701
Training: 2023-10-17 12:20:54,747 - [lfw][6000]XNorm: 12.039701
INFO:root:[lfw][6000]Accuracy-Flip: 0.58917+-0.02328
Training: 2023-10-17 12:20:54,747 - [lfw][6000]Accuracy-Flip: 0.58917+-0.02328
INFO:root:[lfw][6000]Accuracy-Highest: 0.61583
Training: 2023-10-17 12:20:54,747 - [lfw][6000]Accuracy-Highest: 0.61583
INFO:root:test time: 32.0329
Training: 2023-10-17 12:20:54,747 - test time: 32.0329
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][6000]XNorm: 12.224510
Training: 2023-10-17 12:21:31,995 - [cfp_fp][6000]XNorm: 12.224510
INFO:root:[cfp_fp][6000]Accuracy-Flip: 0.57814+-0.01739
Training: 2023-10-17 12:21:31,995 - [cfp_fp][6000]Accuracy-Flip: 0.57814+-0.01739
INFO:root:[cfp_fp][6000]Accuracy-Highest: 0.58143
Training: 2023-10-17 12:21:31,995 - [cfp_fp][6000]Accuracy-Highest: 0.58143
INFO:root:test time: 37.2478
Training: 2023-10-17 12:21:31,995 - test time: 37.2478
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][6000]XNorm: 11.926153
Training: 2023-10-17 12:22:04,004 - [agedb_30][6000]XNorm: 11.926153
INFO:root:[agedb_30][6000]Accuracy-Flip: 0.52317+-0.01752
Training: 2023-10-17 12:22:04,004 - [agedb_30][6000]Accuracy-Flip: 0.52317+-0.01752
INFO:root:[agedb_30][6000]Accuracy-Highest: 0.52783
Training: 2023-10-17 12:22:04,004 - [agedb_30][6000]Accuracy-Highest: 0.52783
INFO:root:test time: 32.0089
Training: 2023-10-17 12:22:04,004 - test time: 32.0089
INFO:root:loss 45.6513, lr: 0.025000, epoch: 0, step: 6100, eta: 96.04 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33776 sec, avg_samples: 64.00000, ips: 378.96898 images/sec
Training: 2023-10-17 12:22:37,786 - loss 45.6513, lr: 0.025000, epoch: 0, step: 6100, eta: 96.04 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33776 sec, avg_samples: 64.00000, ips: 378.96898 images/sec
INFO:root:loss 45.6283, lr: 0.025000, epoch: 0, step: 6200, eta: 95.83 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33894 sec, avg_samples: 64.00000, ips: 377.64649 images/sec
Training: 2023-10-17 12:23:11,686 - loss 45.6283, lr: 0.025000, epoch: 0, step: 6200, eta: 95.83 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33894 sec, avg_samples: 64.00000, ips: 377.64649 images/sec
INFO:root:loss 45.7189, lr: 0.025000, epoch: 0, step: 6300, eta: 95.62 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33900 sec, avg_samples: 64.00000, ips: 377.58571 images/sec
Training: 2023-10-17 12:23:45,591 - loss 45.7189, lr: 0.025000, epoch: 0, step: 6300, eta: 95.62 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33900 sec, avg_samples: 64.00000, ips: 377.58571 images/sec
INFO:root:loss 45.6597, lr: 0.025000, epoch: 0, step: 6400, eta: 95.42 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33903 sec, avg_samples: 64.00000, ips: 377.55076 images/sec
Training: 2023-10-17 12:24:19,500 - loss 45.6597, lr: 0.025000, epoch: 0, step: 6400, eta: 95.42 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33903 sec, avg_samples: 64.00000, ips: 377.55076 images/sec
INFO:root:loss 45.6467, lr: 0.025000, epoch: 0, step: 6500, eta: 95.23 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33904 sec, avg_samples: 64.00000, ips: 377.53392 images/sec
Training: 2023-10-17 12:24:53,410 - loss 45.6467, lr: 0.025000, epoch: 0, step: 6500, eta: 95.23 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33904 sec, avg_samples: 64.00000, ips: 377.53392 images/sec
INFO:root:loss 45.5826, lr: 0.025000, epoch: 0, step: 6600, eta: 95.04 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33900 sec, avg_samples: 64.00000, ips: 377.57960 images/sec
Training: 2023-10-17 12:25:27,316 - loss 45.5826, lr: 0.025000, epoch: 0, step: 6600, eta: 95.04 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33900 sec, avg_samples: 64.00000, ips: 377.57960 images/sec
INFO:root:loss 45.5517, lr: 0.025000, epoch: 0, step: 6700, eta: 94.86 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33891 sec, avg_samples: 64.00000, ips: 377.68088 images/sec
Training: 2023-10-17 12:26:01,213 - loss 45.5517, lr: 0.025000, epoch: 0, step: 6700, eta: 94.86 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33891 sec, avg_samples: 64.00000, ips: 377.68088 images/sec
INFO:root:loss 45.5734, lr: 0.025000, epoch: 0, step: 6800, eta: 94.68 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33893 sec, avg_samples: 64.00000, ips: 377.65473 images/sec
Training: 2023-10-17 12:26:35,112 - loss 45.5734, lr: 0.025000, epoch: 0, step: 6800, eta: 94.68 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33893 sec, avg_samples: 64.00000, ips: 377.65473 images/sec
INFO:root:loss 45.5504, lr: 0.025000, epoch: 0, step: 6900, eta: 94.51 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33893 sec, avg_samples: 64.00000, ips: 377.65768 images/sec
Training: 2023-10-17 12:27:09,011 - loss 45.5504, lr: 0.025000, epoch: 0, step: 6900, eta: 94.51 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33893 sec, avg_samples: 64.00000, ips: 377.65768 images/sec
INFO:root:loss 45.5674, lr: 0.025000, epoch: 0, step: 7000, eta: 94.34 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33892 sec, avg_samples: 64.00000, ips: 377.66871 images/sec
Training: 2023-10-17 12:27:42,908 - loss 45.5674, lr: 0.025000, epoch: 0, step: 7000, eta: 94.34 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33892 sec, avg_samples: 64.00000, ips: 377.66871 images/sec
INFO:root:loss 45.5432, lr: 0.025000, epoch: 0, step: 7100, eta: 94.17 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33893 sec, avg_samples: 64.00000, ips: 377.66356 images/sec
Training: 2023-10-17 12:28:16,806 - loss 45.5432, lr: 0.025000, epoch: 0, step: 7100, eta: 94.17 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33893 sec, avg_samples: 64.00000, ips: 377.66356 images/sec
INFO:root:loss 45.5089, lr: 0.025000, epoch: 0, step: 7200, eta: 94.02 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33888 sec, avg_samples: 64.00000, ips: 377.71456 images/sec
Training: 2023-10-17 12:28:50,700 - loss 45.5089, lr: 0.025000, epoch: 0, step: 7200, eta: 94.02 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33888 sec, avg_samples: 64.00000, ips: 377.71456 images/sec
INFO:root:loss 45.5603, lr: 0.025000, epoch: 0, step: 7300, eta: 93.86 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33890 sec, avg_samples: 64.00000, ips: 377.68766 images/sec
Training: 2023-10-17 12:29:24,596 - loss 45.5603, lr: 0.025000, epoch: 0, step: 7300, eta: 93.86 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33890 sec, avg_samples: 64.00000, ips: 377.68766 images/sec
INFO:root:loss 45.6033, lr: 0.025000, epoch: 0, step: 7400, eta: 93.71 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33899 sec, avg_samples: 64.00000, ips: 377.59470 images/sec
Training: 2023-10-17 12:29:58,500 - loss 45.6033, lr: 0.025000, epoch: 0, step: 7400, eta: 93.71 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33899 sec, avg_samples: 64.00000, ips: 377.59470 images/sec
INFO:root:loss 45.5240, lr: 0.025000, epoch: 0, step: 7500, eta: 93.56 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33914 sec, avg_samples: 64.00000, ips: 377.43044 images/sec
Training: 2023-10-17 12:30:32,419 - loss 45.5240, lr: 0.025000, epoch: 0, step: 7500, eta: 93.56 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33914 sec, avg_samples: 64.00000, ips: 377.43044 images/sec
INFO:root:loss 45.5423, lr: 0.025000, epoch: 0, step: 7600, eta: 93.42 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33919 sec, avg_samples: 64.00000, ips: 377.36748 images/sec
Training: 2023-10-17 12:31:06,343 - loss 45.5423, lr: 0.025000, epoch: 0, step: 7600, eta: 93.42 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33919 sec, avg_samples: 64.00000, ips: 377.36748 images/sec
INFO:root:loss 45.4966, lr: 0.025000, epoch: 0, step: 7700, eta: 93.28 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33902 sec, avg_samples: 64.00000, ips: 377.56219 images/sec
Training: 2023-10-17 12:31:40,252 - loss 45.4966, lr: 0.025000, epoch: 0, step: 7700, eta: 93.28 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33902 sec, avg_samples: 64.00000, ips: 377.56219 images/sec
INFO:root:loss 45.4800, lr: 0.025000, epoch: 0, step: 7800, eta: 93.14 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33887 sec, avg_samples: 64.00000, ips: 377.72170 images/sec
Training: 2023-10-17 12:32:14,145 - loss 45.4800, lr: 0.025000, epoch: 0, step: 7800, eta: 93.14 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33887 sec, avg_samples: 64.00000, ips: 377.72170 images/sec
INFO:root:loss 45.4992, lr: 0.025000, epoch: 0, step: 7900, eta: 93.01 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33933 sec, avg_samples: 64.00000, ips: 377.21891 images/sec
Training: 2023-10-17 12:32:48,084 - loss 45.4992, lr: 0.025000, epoch: 0, step: 7900, eta: 93.01 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33933 sec, avg_samples: 64.00000, ips: 377.21891 images/sec
INFO:root:loss 45.4986, lr: 0.025000, epoch: 0, step: 8000, eta: 92.88 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33920 sec, avg_samples: 64.00000, ips: 377.36387 images/sec
Training: 2023-10-17 12:33:22,008 - loss 45.4986, lr: 0.025000, epoch: 0, step: 8000, eta: 92.88 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33920 sec, avg_samples: 64.00000, ips: 377.36387 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][8000]XNorm: 10.227294
Training: 2023-10-17 12:33:54,053 - [lfw][8000]XNorm: 10.227294
INFO:root:[lfw][8000]Accuracy-Flip: 0.63467+-0.01741
Training: 2023-10-17 12:33:54,053 - [lfw][8000]Accuracy-Flip: 0.63467+-0.01741
INFO:root:[lfw][8000]Accuracy-Highest: 0.63467
Training: 2023-10-17 12:33:54,053 - [lfw][8000]Accuracy-Highest: 0.63467
INFO:root:test time: 32.0445
Training: 2023-10-17 12:33:54,053 - test time: 32.0445
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][8000]XNorm: 10.003856
Training: 2023-10-17 12:34:31,223 - [cfp_fp][8000]XNorm: 10.003856
INFO:root:[cfp_fp][8000]Accuracy-Flip: 0.60057+-0.02069
Training: 2023-10-17 12:34:31,223 - [cfp_fp][8000]Accuracy-Flip: 0.60057+-0.02069
INFO:root:[cfp_fp][8000]Accuracy-Highest: 0.60057
Training: 2023-10-17 12:34:31,223 - [cfp_fp][8000]Accuracy-Highest: 0.60057
INFO:root:test time: 37.1695
Training: 2023-10-17 12:34:31,223 - test time: 37.1695
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][8000]XNorm: 10.143129
Training: 2023-10-17 12:35:03,246 - [agedb_30][8000]XNorm: 10.143129
INFO:root:[agedb_30][8000]Accuracy-Flip: 0.50983+-0.01641
Training: 2023-10-17 12:35:03,246 - [agedb_30][8000]Accuracy-Flip: 0.50983+-0.01641
INFO:root:[agedb_30][8000]Accuracy-Highest: 0.52783
Training: 2023-10-17 12:35:03,246 - [agedb_30][8000]Accuracy-Highest: 0.52783
INFO:root:test time: 32.0230
Training: 2023-10-17 12:35:03,246 - test time: 32.0230
INFO:root:loss 45.3993, lr: 0.025000, epoch: 0, step: 8100, eta: 95.82 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33781 sec, avg_samples: 64.00000, ips: 378.91473 images/sec
Training: 2023-10-17 12:35:37,032 - loss 45.3993, lr: 0.025000, epoch: 0, step: 8100, eta: 95.82 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33781 sec, avg_samples: 64.00000, ips: 378.91473 images/sec
INFO:root:loss 45.4084, lr: 0.025000, epoch: 0, step: 8200, eta: 95.66 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33907 sec, avg_samples: 64.00000, ips: 377.49990 images/sec
Training: 2023-10-17 12:36:10,946 - loss 45.4084, lr: 0.025000, epoch: 0, step: 8200, eta: 95.66 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33907 sec, avg_samples: 64.00000, ips: 377.49990 images/sec
INFO:root:loss 45.4057, lr: 0.025000, epoch: 0, step: 8300, eta: 95.50 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33915 sec, avg_samples: 64.00000, ips: 377.41217 images/sec
Training: 2023-10-17 12:36:44,867 - loss 45.4057, lr: 0.025000, epoch: 0, step: 8300, eta: 95.50 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33915 sec, avg_samples: 64.00000, ips: 377.41217 images/sec
INFO:root:loss 45.4135, lr: 0.025000, epoch: 0, step: 8400, eta: 95.34 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33915 sec, avg_samples: 64.00000, ips: 377.41832 images/sec
Training: 2023-10-17 12:37:18,787 - loss 45.4135, lr: 0.025000, epoch: 0, step: 8400, eta: 95.34 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33915 sec, avg_samples: 64.00000, ips: 377.41832 images/sec
INFO:root:loss 45.3394, lr: 0.025000, epoch: 0, step: 8500, eta: 95.19 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33922 sec, avg_samples: 64.00000, ips: 377.33751 images/sec
Training: 2023-10-17 12:37:52,716 - loss 45.3394, lr: 0.025000, epoch: 0, step: 8500, eta: 95.19 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33922 sec, avg_samples: 64.00000, ips: 377.33751 images/sec
INFO:root:loss 45.3514, lr: 0.025000, epoch: 0, step: 8600, eta: 95.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33910 sec, avg_samples: 64.00000, ips: 377.46867 images/sec
Training: 2023-10-17 12:38:26,632 - loss 45.3514, lr: 0.025000, epoch: 0, step: 8600, eta: 95.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33910 sec, avg_samples: 64.00000, ips: 377.46867 images/sec
INFO:root:loss 45.3726, lr: 0.025000, epoch: 0, step: 8700, eta: 94.90 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33912 sec, avg_samples: 64.00000, ips: 377.45047 images/sec
Training: 2023-10-17 12:39:00,550 - loss 45.3726, lr: 0.025000, epoch: 0, step: 8700, eta: 94.90 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33912 sec, avg_samples: 64.00000, ips: 377.45047 images/sec
INFO:root:loss 45.4111, lr: 0.025000, epoch: 0, step: 8800, eta: 94.76 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33918 sec, avg_samples: 64.00000, ips: 377.38616 images/sec
Training: 2023-10-17 12:39:34,474 - loss 45.4111, lr: 0.025000, epoch: 0, step: 8800, eta: 94.76 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33918 sec, avg_samples: 64.00000, ips: 377.38616 images/sec
INFO:root:loss 45.3832, lr: 0.025000, epoch: 0, step: 8900, eta: 94.62 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33920 sec, avg_samples: 64.00000, ips: 377.35711 images/sec
Training: 2023-10-17 12:40:08,400 - loss 45.3832, lr: 0.025000, epoch: 0, step: 8900, eta: 94.62 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33920 sec, avg_samples: 64.00000, ips: 377.35711 images/sec
INFO:root:loss 45.4499, lr: 0.025000, epoch: 0, step: 9000, eta: 94.48 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33912 sec, avg_samples: 64.00000, ips: 377.44454 images/sec
Training: 2023-10-17 12:40:42,319 - loss 45.4499, lr: 0.025000, epoch: 0, step: 9000, eta: 94.48 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33912 sec, avg_samples: 64.00000, ips: 377.44454 images/sec
INFO:root:loss 45.3163, lr: 0.025000, epoch: 0, step: 9100, eta: 94.35 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33936 sec, avg_samples: 64.00000, ips: 377.18038 images/sec
Training: 2023-10-17 12:41:16,261 - loss 45.3163, lr: 0.025000, epoch: 0, step: 9100, eta: 94.35 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33936 sec, avg_samples: 64.00000, ips: 377.18038 images/sec
INFO:root:loss 45.3301, lr: 0.025000, epoch: 0, step: 9200, eta: 94.22 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33916 sec, avg_samples: 64.00000, ips: 377.39978 images/sec
Training: 2023-10-17 12:41:50,183 - loss 45.3301, lr: 0.025000, epoch: 0, step: 9200, eta: 94.22 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33916 sec, avg_samples: 64.00000, ips: 377.39978 images/sec
INFO:root:loss 45.2846, lr: 0.025000, epoch: 0, step: 9300, eta: 94.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33919 sec, avg_samples: 64.00000, ips: 377.36532 images/sec
Training: 2023-10-17 12:42:24,109 - loss 45.2846, lr: 0.025000, epoch: 0, step: 9300, eta: 94.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33919 sec, avg_samples: 64.00000, ips: 377.36532 images/sec
INFO:root:loss 45.3057, lr: 0.025000, epoch: 0, step: 9400, eta: 93.97 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33913 sec, avg_samples: 64.00000, ips: 377.43904 images/sec
Training: 2023-10-17 12:42:58,028 - loss 45.3057, lr: 0.025000, epoch: 0, step: 9400, eta: 93.97 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33913 sec, avg_samples: 64.00000, ips: 377.43904 images/sec
INFO:root:loss 45.2856, lr: 0.025000, epoch: 0, step: 9500, eta: 93.84 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33933 sec, avg_samples: 64.00000, ips: 377.21256 images/sec
Training: 2023-10-17 12:43:31,967 - loss 45.2856, lr: 0.025000, epoch: 0, step: 9500, eta: 93.84 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33933 sec, avg_samples: 64.00000, ips: 377.21256 images/sec
INFO:root:loss 45.3198, lr: 0.025000, epoch: 0, step: 9600, eta: 93.72 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33909 sec, avg_samples: 64.00000, ips: 377.48200 images/sec
Training: 2023-10-17 12:44:05,882 - loss 45.3198, lr: 0.025000, epoch: 0, step: 9600, eta: 93.72 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33909 sec, avg_samples: 64.00000, ips: 377.48200 images/sec
INFO:root:loss 45.2454, lr: 0.025000, epoch: 0, step: 9700, eta: 93.61 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33917 sec, avg_samples: 64.00000, ips: 377.39525 images/sec
Training: 2023-10-17 12:44:39,804 - loss 45.2454, lr: 0.025000, epoch: 0, step: 9700, eta: 93.61 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33917 sec, avg_samples: 64.00000, ips: 377.39525 images/sec
INFO:root:loss 45.2407, lr: 0.025000, epoch: 0, step: 9800, eta: 93.49 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33898 sec, avg_samples: 64.00000, ips: 377.59996 images/sec
Training: 2023-10-17 12:45:13,709 - loss 45.2407, lr: 0.025000, epoch: 0, step: 9800, eta: 93.49 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33898 sec, avg_samples: 64.00000, ips: 377.59996 images/sec
INFO:root:loss 45.2562, lr: 0.025000, epoch: 0, step: 9900, eta: 93.38 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33940 sec, avg_samples: 64.00000, ips: 377.13976 images/sec
Training: 2023-10-17 12:45:47,654 - loss 45.2562, lr: 0.025000, epoch: 0, step: 9900, eta: 93.38 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33940 sec, avg_samples: 64.00000, ips: 377.13976 images/sec
INFO:root:loss 45.2534, lr: 0.025000, epoch: 0, step: 10000, eta: 93.26 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33923 sec, avg_samples: 64.00000, ips: 377.32137 images/sec
Training: 2023-10-17 12:46:21,583 - loss 45.2534, lr: 0.025000, epoch: 0, step: 10000, eta: 93.26 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33923 sec, avg_samples: 64.00000, ips: 377.32137 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][10000]XNorm: 8.739516
Training: 2023-10-17 12:46:53,645 - [lfw][10000]XNorm: 8.739516
INFO:root:[lfw][10000]Accuracy-Flip: 0.68950+-0.02047
Training: 2023-10-17 12:46:53,645 - [lfw][10000]Accuracy-Flip: 0.68950+-0.02047
INFO:root:[lfw][10000]Accuracy-Highest: 0.68950
Training: 2023-10-17 12:46:53,645 - [lfw][10000]Accuracy-Highest: 0.68950
INFO:root:test time: 32.0616
Training: 2023-10-17 12:46:53,645 - test time: 32.0616
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][10000]XNorm: 9.000480
Training: 2023-10-17 12:47:30,858 - [cfp_fp][10000]XNorm: 9.000480
INFO:root:[cfp_fp][10000]Accuracy-Flip: 0.59343+-0.01264
Training: 2023-10-17 12:47:30,858 - [cfp_fp][10000]Accuracy-Flip: 0.59343+-0.01264
INFO:root:[cfp_fp][10000]Accuracy-Highest: 0.60057
Training: 2023-10-17 12:47:30,858 - [cfp_fp][10000]Accuracy-Highest: 0.60057
INFO:root:test time: 37.2128
Training: 2023-10-17 12:47:30,858 - test time: 37.2128
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][10000]XNorm: 8.813144
Training: 2023-10-17 12:48:02,914 - [agedb_30][10000]XNorm: 8.813144
INFO:root:[agedb_30][10000]Accuracy-Flip: 0.53133+-0.01443
Training: 2023-10-17 12:48:02,915 - [agedb_30][10000]Accuracy-Flip: 0.53133+-0.01443
INFO:root:[agedb_30][10000]Accuracy-Highest: 0.53133
Training: 2023-10-17 12:48:02,915 - [agedb_30][10000]Accuracy-Highest: 0.53133
INFO:root:test time: 32.0566
Training: 2023-10-17 12:48:02,915 - test time: 32.0566
INFO:root:loss 45.2245, lr: 0.025000, epoch: 0, step: 10100, eta: 95.61 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33827 sec, avg_samples: 64.00000, ips: 378.39495 images/sec
Training: 2023-10-17 12:48:36,748 - loss 45.2245, lr: 0.025000, epoch: 0, step: 10100, eta: 95.61 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33827 sec, avg_samples: 64.00000, ips: 378.39495 images/sec
INFO:root:loss 45.1845, lr: 0.025000, epoch: 0, step: 10200, eta: 95.48 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33866 sec, avg_samples: 64.00000, ips: 377.96228 images/sec
Training: 2023-10-17 12:49:10,621 - loss 45.1845, lr: 0.025000, epoch: 0, step: 10200, eta: 95.48 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33866 sec, avg_samples: 64.00000, ips: 377.96228 images/sec
INFO:root:loss 45.1946, lr: 0.025000, epoch: 0, step: 10300, eta: 95.35 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33913 sec, avg_samples: 64.00000, ips: 377.43426 images/sec
Training: 2023-10-17 12:49:44,540 - loss 45.1946, lr: 0.025000, epoch: 0, step: 10300, eta: 95.35 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33913 sec, avg_samples: 64.00000, ips: 377.43426 images/sec
INFO:root:loss 45.1900, lr: 0.025000, epoch: 0, step: 10400, eta: 95.22 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33891 sec, avg_samples: 64.00000, ips: 377.67711 images/sec
Training: 2023-10-17 12:50:18,439 - loss 45.1900, lr: 0.025000, epoch: 0, step: 10400, eta: 95.22 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33891 sec, avg_samples: 64.00000, ips: 377.67711 images/sec
INFO:root:loss 45.2128, lr: 0.025000, epoch: 0, step: 10500, eta: 95.10 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33889 sec, avg_samples: 64.00000, ips: 377.70099 images/sec
Training: 2023-10-17 12:50:52,334 - loss 45.2128, lr: 0.025000, epoch: 0, step: 10500, eta: 95.10 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33889 sec, avg_samples: 64.00000, ips: 377.70099 images/sec
INFO:root:loss 45.1915, lr: 0.025000, epoch: 0, step: 10600, eta: 94.97 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33905 sec, avg_samples: 64.00000, ips: 377.52311 images/sec
Training: 2023-10-17 12:51:26,244 - loss 45.1915, lr: 0.025000, epoch: 0, step: 10600, eta: 94.97 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33905 sec, avg_samples: 64.00000, ips: 377.52311 images/sec
INFO:root:loss 45.1111, lr: 0.025000, epoch: 0, step: 10700, eta: 94.85 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33918 sec, avg_samples: 64.00000, ips: 377.37678 images/sec
Training: 2023-10-17 12:52:00,168 - loss 45.1111, lr: 0.025000, epoch: 0, step: 10700, eta: 94.85 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33918 sec, avg_samples: 64.00000, ips: 377.37678 images/sec
INFO:root:loss 45.1876, lr: 0.025000, epoch: 0, step: 10800, eta: 94.73 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33929 sec, avg_samples: 64.00000, ips: 377.25743 images/sec
Training: 2023-10-17 12:52:34,103 - loss 45.1876, lr: 0.025000, epoch: 0, step: 10800, eta: 94.73 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33929 sec, avg_samples: 64.00000, ips: 377.25743 images/sec
INFO:root:loss 45.1062, lr: 0.025000, epoch: 0, step: 10900, eta: 94.62 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33908 sec, avg_samples: 64.00000, ips: 377.48835 images/sec
Training: 2023-10-17 12:53:08,017 - loss 45.1062, lr: 0.025000, epoch: 0, step: 10900, eta: 94.62 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33908 sec, avg_samples: 64.00000, ips: 377.48835 images/sec
INFO:root:loss 45.1278, lr: 0.025000, epoch: 0, step: 11000, eta: 94.50 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33909 sec, avg_samples: 64.00000, ips: 377.48506 images/sec
Training: 2023-10-17 12:53:41,931 - loss 45.1278, lr: 0.025000, epoch: 0, step: 11000, eta: 94.50 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33909 sec, avg_samples: 64.00000, ips: 377.48506 images/sec
INFO:root:loss 45.1410, lr: 0.025000, epoch: 0, step: 11100, eta: 94.39 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33905 sec, avg_samples: 64.00000, ips: 377.52354 images/sec
Training: 2023-10-17 12:54:15,842 - loss 45.1410, lr: 0.025000, epoch: 0, step: 11100, eta: 94.39 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33905 sec, avg_samples: 64.00000, ips: 377.52354 images/sec
INFO:root:loss 45.0811, lr: 0.025000, epoch: 0, step: 11200, eta: 94.28 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33943 sec, avg_samples: 64.00000, ips: 377.10724 images/sec
Training: 2023-10-17 12:54:49,790 - loss 45.0811, lr: 0.025000, epoch: 0, step: 11200, eta: 94.28 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33943 sec, avg_samples: 64.00000, ips: 377.10724 images/sec
INFO:root:loss 45.0680, lr: 0.025000, epoch: 0, step: 11300, eta: 94.17 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33910 sec, avg_samples: 64.00000, ips: 377.46432 images/sec
Training: 2023-10-17 12:55:23,706 - loss 45.0680, lr: 0.025000, epoch: 0, step: 11300, eta: 94.17 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33910 sec, avg_samples: 64.00000, ips: 377.46432 images/sec
INFO:root:loss 45.0066, lr: 0.025000, epoch: 0, step: 11400, eta: 94.06 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33897 sec, avg_samples: 64.00000, ips: 377.61697 images/sec
Training: 2023-10-17 12:55:57,608 - loss 45.0066, lr: 0.025000, epoch: 0, step: 11400, eta: 94.06 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33897 sec, avg_samples: 64.00000, ips: 377.61697 images/sec
INFO:root:loss 45.0626, lr: 0.025000, epoch: 0, step: 11500, eta: 93.95 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33904 sec, avg_samples: 64.00000, ips: 377.53992 images/sec
Training: 2023-10-17 12:56:31,518 - loss 45.0626, lr: 0.025000, epoch: 0, step: 11500, eta: 93.95 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33904 sec, avg_samples: 64.00000, ips: 377.53992 images/sec
INFO:root:loss 44.9612, lr: 0.025000, epoch: 0, step: 11600, eta: 93.85 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33892 sec, avg_samples: 64.00000, ips: 377.67404 images/sec
Training: 2023-10-17 12:57:05,415 - loss 44.9612, lr: 0.025000, epoch: 0, step: 11600, eta: 93.85 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33892 sec, avg_samples: 64.00000, ips: 377.67404 images/sec
INFO:root:loss 45.0090, lr: 0.025000, epoch: 0, step: 11700, eta: 93.75 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33903 sec, avg_samples: 64.00000, ips: 377.54786 images/sec
Training: 2023-10-17 12:57:39,323 - loss 45.0090, lr: 0.025000, epoch: 0, step: 11700, eta: 93.75 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33903 sec, avg_samples: 64.00000, ips: 377.54786 images/sec
INFO:root:loss 44.9928, lr: 0.025000, epoch: 0, step: 11800, eta: 93.65 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33887 sec, avg_samples: 64.00000, ips: 377.72398 images/sec
Training: 2023-10-17 12:58:13,216 - loss 44.9928, lr: 0.025000, epoch: 0, step: 11800, eta: 93.65 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33887 sec, avg_samples: 64.00000, ips: 377.72398 images/sec
INFO:root:loss 44.9139, lr: 0.025000, epoch: 0, step: 11900, eta: 93.55 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33894 sec, avg_samples: 64.00000, ips: 377.64278 images/sec
Training: 2023-10-17 12:58:47,115 - loss 44.9139, lr: 0.025000, epoch: 0, step: 11900, eta: 93.55 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33894 sec, avg_samples: 64.00000, ips: 377.64278 images/sec
INFO:root:loss 44.9423, lr: 0.025000, epoch: 0, step: 12000, eta: 93.45 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33891 sec, avg_samples: 64.00000, ips: 377.67900 images/sec
Training: 2023-10-17 12:59:21,012 - loss 44.9423, lr: 0.025000, epoch: 0, step: 12000, eta: 93.45 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33891 sec, avg_samples: 64.00000, ips: 377.67900 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][12000]XNorm: 8.174289
Training: 2023-10-17 12:59:53,093 - [lfw][12000]XNorm: 8.174289
INFO:root:[lfw][12000]Accuracy-Flip: 0.73217+-0.01464
Training: 2023-10-17 12:59:53,093 - [lfw][12000]Accuracy-Flip: 0.73217+-0.01464
INFO:root:[lfw][12000]Accuracy-Highest: 0.73217
Training: 2023-10-17 12:59:53,093 - [lfw][12000]Accuracy-Highest: 0.73217
INFO:root:test time: 32.0810
Training: 2023-10-17 12:59:53,093 - test time: 32.0810
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][12000]XNorm: 8.338179
Training: 2023-10-17 13:00:30,336 - [cfp_fp][12000]XNorm: 8.338179
INFO:root:[cfp_fp][12000]Accuracy-Flip: 0.58814+-0.01615
Training: 2023-10-17 13:00:30,336 - [cfp_fp][12000]Accuracy-Flip: 0.58814+-0.01615
INFO:root:[cfp_fp][12000]Accuracy-Highest: 0.60057
Training: 2023-10-17 13:00:30,336 - [cfp_fp][12000]Accuracy-Highest: 0.60057
INFO:root:test time: 37.2429
Training: 2023-10-17 13:00:30,336 - test time: 37.2429
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][12000]XNorm: 8.223183
Training: 2023-10-17 13:01:02,459 - [agedb_30][12000]XNorm: 8.223183
INFO:root:[agedb_30][12000]Accuracy-Flip: 0.56350+-0.01750
Training: 2023-10-17 13:01:02,459 - [agedb_30][12000]Accuracy-Flip: 0.56350+-0.01750
INFO:root:[agedb_30][12000]Accuracy-Highest: 0.56350
Training: 2023-10-17 13:01:02,459 - [agedb_30][12000]Accuracy-Highest: 0.56350
INFO:root:test time: 32.1232
Training: 2023-10-17 13:01:02,460 - test time: 32.1232
INFO:root:loss 44.8610, lr: 0.025000, epoch: 0, step: 12100, eta: 95.40 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76995 images/sec
Training: 2023-10-17 13:01:36,260 - loss 44.8610, lr: 0.025000, epoch: 0, step: 12100, eta: 95.40 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76995 images/sec
INFO:root:loss 44.9385, lr: 0.025000, epoch: 0, step: 12200, eta: 95.29 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33901 sec, avg_samples: 64.00000, ips: 377.57044 images/sec
Training: 2023-10-17 13:02:10,167 - loss 44.9385, lr: 0.025000, epoch: 0, step: 12200, eta: 95.29 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33901 sec, avg_samples: 64.00000, ips: 377.57044 images/sec
INFO:root:loss 44.8890, lr: 0.025000, epoch: 0, step: 12300, eta: 95.18 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33880 sec, avg_samples: 64.00000, ips: 377.80028 images/sec
Training: 2023-10-17 13:02:44,052 - loss 44.8890, lr: 0.025000, epoch: 0, step: 12300, eta: 95.18 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33880 sec, avg_samples: 64.00000, ips: 377.80028 images/sec
INFO:root:loss 44.8366, lr: 0.025000, epoch: 0, step: 12400, eta: 95.07 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33868 sec, avg_samples: 64.00000, ips: 377.93419 images/sec
Training: 2023-10-17 13:03:17,926 - loss 44.8366, lr: 0.025000, epoch: 0, step: 12400, eta: 95.07 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33868 sec, avg_samples: 64.00000, ips: 377.93419 images/sec
INFO:root:loss 44.8702, lr: 0.025000, epoch: 0, step: 12500, eta: 94.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33890 sec, avg_samples: 64.00000, ips: 377.69686 images/sec
Training: 2023-10-17 13:03:51,821 - loss 44.8702, lr: 0.025000, epoch: 0, step: 12500, eta: 94.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33890 sec, avg_samples: 64.00000, ips: 377.69686 images/sec
INFO:root:loss 44.7822, lr: 0.025000, epoch: 0, step: 12600, eta: 94.85 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33896 sec, avg_samples: 64.00000, ips: 377.62797 images/sec
Training: 2023-10-17 13:04:25,723 - loss 44.7822, lr: 0.025000, epoch: 0, step: 12600, eta: 94.85 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33896 sec, avg_samples: 64.00000, ips: 377.62797 images/sec
INFO:root:loss 44.7593, lr: 0.025000, epoch: 0, step: 12700, eta: 94.75 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33903 sec, avg_samples: 64.00000, ips: 377.54891 images/sec
Training: 2023-10-17 13:04:59,631 - loss 44.7593, lr: 0.025000, epoch: 0, step: 12700, eta: 94.75 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33903 sec, avg_samples: 64.00000, ips: 377.54891 images/sec
INFO:root:loss 44.7535, lr: 0.025000, epoch: 0, step: 12800, eta: 94.65 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33893 sec, avg_samples: 64.00000, ips: 377.66290 images/sec
Training: 2023-10-17 13:05:33,529 - loss 44.7535, lr: 0.025000, epoch: 0, step: 12800, eta: 94.65 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33893 sec, avg_samples: 64.00000, ips: 377.66290 images/sec
INFO:root:loss 44.7766, lr: 0.025000, epoch: 0, step: 12900, eta: 94.54 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33895 sec, avg_samples: 64.00000, ips: 377.63155 images/sec
Training: 2023-10-17 13:06:07,430 - loss 44.7766, lr: 0.025000, epoch: 0, step: 12900, eta: 94.54 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33895 sec, avg_samples: 64.00000, ips: 377.63155 images/sec
INFO:root:loss 44.7208, lr: 0.025000, epoch: 0, step: 13000, eta: 94.44 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33881 sec, avg_samples: 64.00000, ips: 377.79457 images/sec
Training: 2023-10-17 13:06:41,316 - loss 44.7208, lr: 0.025000, epoch: 0, step: 13000, eta: 94.44 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33881 sec, avg_samples: 64.00000, ips: 377.79457 images/sec
INFO:root:loss 44.7030, lr: 0.025000, epoch: 0, step: 13100, eta: 94.34 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33873 sec, avg_samples: 64.00000, ips: 377.87682 images/sec
Training: 2023-10-17 13:07:15,195 - loss 44.7030, lr: 0.025000, epoch: 0, step: 13100, eta: 94.34 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33873 sec, avg_samples: 64.00000, ips: 377.87682 images/sec
INFO:root:loss 44.7185, lr: 0.025000, epoch: 0, step: 13200, eta: 94.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33892 sec, avg_samples: 64.00000, ips: 377.66693 images/sec
Training: 2023-10-17 13:07:49,093 - loss 44.7185, lr: 0.025000, epoch: 0, step: 13200, eta: 94.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33892 sec, avg_samples: 64.00000, ips: 377.66693 images/sec
INFO:root:loss 44.6933, lr: 0.025000, epoch: 0, step: 13300, eta: 94.15 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33874 sec, avg_samples: 64.00000, ips: 377.86620 images/sec
Training: 2023-10-17 13:08:22,973 - loss 44.6933, lr: 0.025000, epoch: 0, step: 13300, eta: 94.15 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33874 sec, avg_samples: 64.00000, ips: 377.86620 images/sec
INFO:root:loss 44.6555, lr: 0.025000, epoch: 0, step: 13400, eta: 94.06 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33900 sec, avg_samples: 64.00000, ips: 377.57804 images/sec
Training: 2023-10-17 13:08:56,878 - loss 44.6555, lr: 0.025000, epoch: 0, step: 13400, eta: 94.06 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33900 sec, avg_samples: 64.00000, ips: 377.57804 images/sec
INFO:root:loss 44.6142, lr: 0.025000, epoch: 0, step: 13500, eta: 93.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33891 sec, avg_samples: 64.00000, ips: 377.68630 images/sec
Training: 2023-10-17 13:09:30,774 - loss 44.6142, lr: 0.025000, epoch: 0, step: 13500, eta: 93.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33891 sec, avg_samples: 64.00000, ips: 377.68630 images/sec
INFO:root:loss 44.5787, lr: 0.025000, epoch: 0, step: 13600, eta: 93.87 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33877 sec, avg_samples: 64.00000, ips: 377.83937 images/sec
Training: 2023-10-17 13:10:04,657 - loss 44.5787, lr: 0.025000, epoch: 0, step: 13600, eta: 93.87 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33877 sec, avg_samples: 64.00000, ips: 377.83937 images/sec
INFO:root:loss 44.5509, lr: 0.025000, epoch: 0, step: 13700, eta: 93.78 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33874 sec, avg_samples: 64.00000, ips: 377.87647 images/sec
Training: 2023-10-17 13:10:38,536 - loss 44.5509, lr: 0.025000, epoch: 0, step: 13700, eta: 93.78 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33874 sec, avg_samples: 64.00000, ips: 377.87647 images/sec
INFO:root:loss 44.5821, lr: 0.025000, epoch: 0, step: 13800, eta: 93.69 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33893 sec, avg_samples: 64.00000, ips: 377.66006 images/sec
Training: 2023-10-17 13:11:12,434 - loss 44.5821, lr: 0.025000, epoch: 0, step: 13800, eta: 93.69 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33893 sec, avg_samples: 64.00000, ips: 377.66006 images/sec
INFO:root:loss 44.4769, lr: 0.025000, epoch: 0, step: 13900, eta: 93.60 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33877 sec, avg_samples: 64.00000, ips: 377.83903 images/sec
Training: 2023-10-17 13:11:46,316 - loss 44.4769, lr: 0.025000, epoch: 0, step: 13900, eta: 93.60 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33877 sec, avg_samples: 64.00000, ips: 377.83903 images/sec
INFO:root:loss 44.4542, lr: 0.025000, epoch: 0, step: 14000, eta: 93.51 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33878 sec, avg_samples: 64.00000, ips: 377.82783 images/sec
Training: 2023-10-17 13:12:20,208 - loss 44.4542, lr: 0.025000, epoch: 0, step: 14000, eta: 93.51 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33878 sec, avg_samples: 64.00000, ips: 377.82783 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][14000]XNorm: 7.995233
Training: 2023-10-17 13:12:52,335 - [lfw][14000]XNorm: 7.995233
INFO:root:[lfw][14000]Accuracy-Flip: 0.77250+-0.01564
Training: 2023-10-17 13:12:52,335 - [lfw][14000]Accuracy-Flip: 0.77250+-0.01564
INFO:root:[lfw][14000]Accuracy-Highest: 0.77250
Training: 2023-10-17 13:12:52,335 - [lfw][14000]Accuracy-Highest: 0.77250
INFO:root:test time: 32.1273
Training: 2023-10-17 13:12:52,335 - test time: 32.1273
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][14000]XNorm: 8.038375
Training: 2023-10-17 13:13:29,462 - [cfp_fp][14000]XNorm: 8.038375
INFO:root:[cfp_fp][14000]Accuracy-Flip: 0.60186+-0.01430
Training: 2023-10-17 13:13:29,462 - [cfp_fp][14000]Accuracy-Flip: 0.60186+-0.01430
INFO:root:[cfp_fp][14000]Accuracy-Highest: 0.60186
Training: 2023-10-17 13:13:29,462 - [cfp_fp][14000]Accuracy-Highest: 0.60186
INFO:root:test time: 37.1271
Training: 2023-10-17 13:13:29,462 - test time: 37.1271
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][14000]XNorm: 8.098069
Training: 2023-10-17 13:14:01,465 - [agedb_30][14000]XNorm: 8.098069
INFO:root:[agedb_30][14000]Accuracy-Flip: 0.55267+-0.01786
Training: 2023-10-17 13:14:01,465 - [agedb_30][14000]Accuracy-Flip: 0.55267+-0.01786
INFO:root:[agedb_30][14000]Accuracy-Highest: 0.56350
Training: 2023-10-17 13:14:01,465 - [agedb_30][14000]Accuracy-Highest: 0.56350
INFO:root:test time: 32.0027
Training: 2023-10-17 13:14:01,465 - test time: 32.0027
INFO:root:loss 44.3867, lr: 0.025000, epoch: 0, step: 14100, eta: 95.18 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67082 images/sec
Training: 2023-10-17 13:14:35,273 - loss 44.3867, lr: 0.025000, epoch: 0, step: 14100, eta: 95.18 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67082 images/sec
INFO:root:loss 44.4506, lr: 0.025000, epoch: 0, step: 14200, eta: 95.08 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33867 sec, avg_samples: 64.00000, ips: 377.94975 images/sec
Training: 2023-10-17 13:15:09,146 - loss 44.4506, lr: 0.025000, epoch: 0, step: 14200, eta: 95.08 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33867 sec, avg_samples: 64.00000, ips: 377.94975 images/sec
INFO:root:loss 44.3619, lr: 0.025000, epoch: 0, step: 14300, eta: 94.98 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33917 sec, avg_samples: 64.00000, ips: 377.38824 images/sec
Training: 2023-10-17 13:15:43,070 - loss 44.3619, lr: 0.025000, epoch: 0, step: 14300, eta: 94.98 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33917 sec, avg_samples: 64.00000, ips: 377.38824 images/sec
INFO:root:loss 44.3500, lr: 0.025000, epoch: 0, step: 14400, eta: 94.89 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33916 sec, avg_samples: 64.00000, ips: 377.40495 images/sec
Training: 2023-10-17 13:16:16,992 - loss 44.3500, lr: 0.025000, epoch: 0, step: 14400, eta: 94.89 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33916 sec, avg_samples: 64.00000, ips: 377.40495 images/sec
INFO:root:loss 44.2857, lr: 0.025000, epoch: 0, step: 14500, eta: 94.79 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33919 sec, avg_samples: 64.00000, ips: 377.37111 images/sec
Training: 2023-10-17 13:16:50,917 - loss 44.2857, lr: 0.025000, epoch: 0, step: 14500, eta: 94.79 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33919 sec, avg_samples: 64.00000, ips: 377.37111 images/sec
INFO:root:loss 44.2778, lr: 0.025000, epoch: 0, step: 14600, eta: 94.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33926 sec, avg_samples: 64.00000, ips: 377.29323 images/sec
Training: 2023-10-17 13:17:24,849 - loss 44.2778, lr: 0.025000, epoch: 0, step: 14600, eta: 94.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33926 sec, avg_samples: 64.00000, ips: 377.29323 images/sec
INFO:root:loss 44.2301, lr: 0.025000, epoch: 0, step: 14700, eta: 94.61 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33917 sec, avg_samples: 64.00000, ips: 377.38996 images/sec
Training: 2023-10-17 13:17:58,772 - loss 44.2301, lr: 0.025000, epoch: 0, step: 14700, eta: 94.61 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33917 sec, avg_samples: 64.00000, ips: 377.38996 images/sec
INFO:root:loss 44.1800, lr: 0.025000, epoch: 0, step: 14800, eta: 94.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33923 sec, avg_samples: 64.00000, ips: 377.33046 images/sec
Training: 2023-10-17 13:18:32,700 - loss 44.1800, lr: 0.025000, epoch: 0, step: 14800, eta: 94.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33923 sec, avg_samples: 64.00000, ips: 377.33046 images/sec
INFO:root:loss 44.1818, lr: 0.025000, epoch: 0, step: 14900, eta: 94.43 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33929 sec, avg_samples: 64.00000, ips: 377.26186 images/sec
Training: 2023-10-17 13:19:06,635 - loss 44.1818, lr: 0.025000, epoch: 0, step: 14900, eta: 94.43 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33929 sec, avg_samples: 64.00000, ips: 377.26186 images/sec
INFO:root:loss 44.1563, lr: 0.025000, epoch: 0, step: 15000, eta: 94.34 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33937 sec, avg_samples: 64.00000, ips: 377.17120 images/sec
Training: 2023-10-17 13:19:40,578 - loss 44.1563, lr: 0.025000, epoch: 0, step: 15000, eta: 94.34 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33937 sec, avg_samples: 64.00000, ips: 377.17120 images/sec
INFO:root:loss 44.0657, lr: 0.025000, epoch: 0, step: 15100, eta: 94.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33923 sec, avg_samples: 64.00000, ips: 377.32412 images/sec
Training: 2023-10-17 13:20:14,507 - loss 44.0657, lr: 0.025000, epoch: 0, step: 15100, eta: 94.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33923 sec, avg_samples: 64.00000, ips: 377.32412 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 15200, eta: 94.17 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33933 sec, avg_samples: 64.00000, ips: 377.21195 images/sec
Training: 2023-10-17 13:20:48,447 - loss nan, lr: 0.025000, epoch: 0, step: 15200, eta: 94.17 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33933 sec, avg_samples: 64.00000, ips: 377.21195 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 15300, eta: 94.08 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33915 sec, avg_samples: 64.00000, ips: 377.41176 images/sec
Training: 2023-10-17 13:21:22,368 - loss nan, lr: 0.025000, epoch: 0, step: 15300, eta: 94.08 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33915 sec, avg_samples: 64.00000, ips: 377.41176 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 15400, eta: 94.00 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33870 sec, avg_samples: 64.00000, ips: 377.91917 images/sec
Training: 2023-10-17 13:21:56,244 - loss nan, lr: 0.025000, epoch: 0, step: 15400, eta: 94.00 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33870 sec, avg_samples: 64.00000, ips: 377.91917 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 15500, eta: 93.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33881 sec, avg_samples: 64.00000, ips: 377.79696 images/sec
Training: 2023-10-17 13:22:30,130 - loss nan, lr: 0.025000, epoch: 0, step: 15500, eta: 93.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33881 sec, avg_samples: 64.00000, ips: 377.79696 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 15600, eta: 93.83 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33854 sec, avg_samples: 64.00000, ips: 378.09208 images/sec
Training: 2023-10-17 13:23:03,990 - loss nan, lr: 0.025000, epoch: 0, step: 15600, eta: 93.83 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33854 sec, avg_samples: 64.00000, ips: 378.09208 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 15700, eta: 93.75 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33879 sec, avg_samples: 64.00000, ips: 377.81877 images/sec
Training: 2023-10-17 13:23:37,875 - loss nan, lr: 0.025000, epoch: 0, step: 15700, eta: 93.75 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33879 sec, avg_samples: 64.00000, ips: 377.81877 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 15800, eta: 93.67 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33877 sec, avg_samples: 64.00000, ips: 377.83304 images/sec
Training: 2023-10-17 13:24:11,758 - loss nan, lr: 0.025000, epoch: 0, step: 15800, eta: 93.67 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33877 sec, avg_samples: 64.00000, ips: 377.83304 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 15900, eta: 93.59 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33883 sec, avg_samples: 64.00000, ips: 377.76958 images/sec
Training: 2023-10-17 13:24:45,647 - loss nan, lr: 0.025000, epoch: 0, step: 15900, eta: 93.59 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33883 sec, avg_samples: 64.00000, ips: 377.76958 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 16000, eta: 93.51 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33876 sec, avg_samples: 64.00000, ips: 377.85188 images/sec
Training: 2023-10-17 13:25:19,528 - loss nan, lr: 0.025000, epoch: 0, step: 16000, eta: 93.51 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33876 sec, avg_samples: 64.00000, ips: 377.85188 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][16000]XNorm: 7.073230
Training: 2023-10-17 13:25:51,599 - [lfw][16000]XNorm: 7.073230
INFO:root:[lfw][16000]Accuracy-Flip: 0.79833+-0.01319
Training: 2023-10-17 13:25:51,600 - [lfw][16000]Accuracy-Flip: 0.79833+-0.01319
INFO:root:[lfw][16000]Accuracy-Highest: 0.79833
Training: 2023-10-17 13:25:51,600 - [lfw][16000]Accuracy-Highest: 0.79833
INFO:root:test time: 32.0715
Training: 2023-10-17 13:25:51,600 - test time: 32.0715
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][16000]XNorm: 6.797601
Training: 2023-10-17 13:26:28,752 - [cfp_fp][16000]XNorm: 6.797601
INFO:root:[cfp_fp][16000]Accuracy-Flip: 0.61300+-0.01487
Training: 2023-10-17 13:26:28,753 - [cfp_fp][16000]Accuracy-Flip: 0.61300+-0.01487
INFO:root:[cfp_fp][16000]Accuracy-Highest: 0.61300
Training: 2023-10-17 13:26:28,753 - [cfp_fp][16000]Accuracy-Highest: 0.61300
INFO:root:test time: 37.1529
Training: 2023-10-17 13:26:28,753 - test time: 37.1529
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][16000]XNorm: 7.149530
Training: 2023-10-17 13:27:00,810 - [agedb_30][16000]XNorm: 7.149530
INFO:root:[agedb_30][16000]Accuracy-Flip: 0.57683+-0.01779
Training: 2023-10-17 13:27:00,810 - [agedb_30][16000]Accuracy-Flip: 0.57683+-0.01779
INFO:root:[agedb_30][16000]Accuracy-Highest: 0.57683
Training: 2023-10-17 13:27:00,810 - [agedb_30][16000]Accuracy-Highest: 0.57683
INFO:root:test time: 32.0577
Training: 2023-10-17 13:27:00,810 - test time: 32.0577
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 16100, eta: 94.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33767 sec, avg_samples: 64.00000, ips: 379.07090 images/sec
Training: 2023-10-17 13:27:34,583 - loss nan, lr: 0.025000, epoch: 0, step: 16100, eta: 94.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33767 sec, avg_samples: 64.00000, ips: 379.07090 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 16200, eta: 94.87 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68736 images/sec
Training: 2023-10-17 13:28:08,390 - loss nan, lr: 0.025000, epoch: 0, step: 16200, eta: 94.87 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68736 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 16300, eta: 94.78 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.60410 images/sec
Training: 2023-10-17 13:28:42,205 - loss nan, lr: 0.025000, epoch: 0, step: 16300, eta: 94.78 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.60410 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 16400, eta: 94.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68241 images/sec
Training: 2023-10-17 13:29:16,012 - loss nan, lr: 0.025000, epoch: 0, step: 16400, eta: 94.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68241 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 16500, eta: 94.61 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.70054 images/sec
Training: 2023-10-17 13:29:49,818 - loss nan, lr: 0.025000, epoch: 0, step: 16500, eta: 94.61 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.70054 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 16600, eta: 94.53 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67056 images/sec
Training: 2023-10-17 13:30:23,627 - loss nan, lr: 0.025000, epoch: 0, step: 16600, eta: 94.53 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67056 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 16700, eta: 94.44 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68517 images/sec
Training: 2023-10-17 13:30:57,434 - loss nan, lr: 0.025000, epoch: 0, step: 16700, eta: 94.44 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68517 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 16800, eta: 94.36 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33797 sec, avg_samples: 64.00000, ips: 378.72794 images/sec
Training: 2023-10-17 13:31:31,237 - loss nan, lr: 0.025000, epoch: 0, step: 16800, eta: 94.36 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33797 sec, avg_samples: 64.00000, ips: 378.72794 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 16900, eta: 94.28 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.85406 images/sec
Training: 2023-10-17 13:32:05,029 - loss nan, lr: 0.025000, epoch: 0, step: 16900, eta: 94.28 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.85406 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 17000, eta: 94.20 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.82721 images/sec
Training: 2023-10-17 13:32:38,824 - loss nan, lr: 0.025000, epoch: 0, step: 17000, eta: 94.20 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.82721 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 17100, eta: 94.12 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33795 sec, avg_samples: 64.00000, ips: 378.75821 images/sec
Training: 2023-10-17 13:33:12,625 - loss nan, lr: 0.025000, epoch: 0, step: 17100, eta: 94.12 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33795 sec, avg_samples: 64.00000, ips: 378.75821 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 17200, eta: 94.04 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67548 images/sec
Training: 2023-10-17 13:33:46,433 - loss nan, lr: 0.025000, epoch: 0, step: 17200, eta: 94.04 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67548 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 17300, eta: 93.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.81493 images/sec
Training: 2023-10-17 13:34:20,229 - loss nan, lr: 0.025000, epoch: 0, step: 17300, eta: 93.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.81493 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 17400, eta: 93.88 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66631 images/sec
Training: 2023-10-17 13:34:54,038 - loss nan, lr: 0.025000, epoch: 0, step: 17400, eta: 93.88 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66631 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 17500, eta: 93.80 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33797 sec, avg_samples: 64.00000, ips: 378.73528 images/sec
Training: 2023-10-17 13:35:27,840 - loss nan, lr: 0.025000, epoch: 0, step: 17500, eta: 93.80 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33797 sec, avg_samples: 64.00000, ips: 378.73528 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 17600, eta: 93.73 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76813 images/sec
Training: 2023-10-17 13:36:01,639 - loss nan, lr: 0.025000, epoch: 0, step: 17600, eta: 93.73 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76813 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 17700, eta: 93.65 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33804 sec, avg_samples: 64.00000, ips: 378.65074 images/sec
Training: 2023-10-17 13:36:35,449 - loss nan, lr: 0.025000, epoch: 0, step: 17700, eta: 93.65 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33804 sec, avg_samples: 64.00000, ips: 378.65074 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 17800, eta: 93.58 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33806 sec, avg_samples: 64.00000, ips: 378.63188 images/sec
Training: 2023-10-17 13:37:09,261 - loss nan, lr: 0.025000, epoch: 0, step: 17800, eta: 93.58 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33806 sec, avg_samples: 64.00000, ips: 378.63188 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 17900, eta: 93.50 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33796 sec, avg_samples: 64.00000, ips: 378.73938 images/sec
Training: 2023-10-17 13:37:43,063 - loss nan, lr: 0.025000, epoch: 0, step: 17900, eta: 93.50 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33796 sec, avg_samples: 64.00000, ips: 378.73938 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 18000, eta: 93.43 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33810 sec, avg_samples: 64.00000, ips: 378.58421 images/sec
Training: 2023-10-17 13:38:16,879 - loss nan, lr: 0.025000, epoch: 0, step: 18000, eta: 93.43 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33810 sec, avg_samples: 64.00000, ips: 378.58421 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][18000]XNorm: 5.505737
Training: 2023-10-17 13:38:48,950 - [lfw][18000]XNorm: 5.505737
INFO:root:[lfw][18000]Accuracy-Flip: 0.79733+-0.01179
Training: 2023-10-17 13:38:48,950 - [lfw][18000]Accuracy-Flip: 0.79733+-0.01179
INFO:root:[lfw][18000]Accuracy-Highest: 0.79833
Training: 2023-10-17 13:38:48,951 - [lfw][18000]Accuracy-Highest: 0.79833
INFO:root:test time: 32.0718
Training: 2023-10-17 13:38:48,951 - test time: 32.0718
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][18000]XNorm: 5.295990
Training: 2023-10-17 13:39:26,149 - [cfp_fp][18000]XNorm: 5.295990
INFO:root:[cfp_fp][18000]Accuracy-Flip: 0.61371+-0.01346
Training: 2023-10-17 13:39:26,149 - [cfp_fp][18000]Accuracy-Flip: 0.61371+-0.01346
INFO:root:[cfp_fp][18000]Accuracy-Highest: 0.61371
Training: 2023-10-17 13:39:26,149 - [cfp_fp][18000]Accuracy-Highest: 0.61371
INFO:root:test time: 37.1988
Training: 2023-10-17 13:39:26,149 - test time: 37.1988
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][18000]XNorm: 5.571899
Training: 2023-10-17 13:39:58,180 - [agedb_30][18000]XNorm: 5.571899
INFO:root:[agedb_30][18000]Accuracy-Flip: 0.57767+-0.01847
Training: 2023-10-17 13:39:58,180 - [agedb_30][18000]Accuracy-Flip: 0.57767+-0.01847
INFO:root:[agedb_30][18000]Accuracy-Highest: 0.57767
Training: 2023-10-17 13:39:58,180 - [agedb_30][18000]Accuracy-Highest: 0.57767
INFO:root:test time: 32.0305
Training: 2023-10-17 13:39:58,180 - test time: 32.0305
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 18100, eta: 94.72 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33745 sec, avg_samples: 64.00000, ips: 379.31416 images/sec
Training: 2023-10-17 13:40:31,931 - loss nan, lr: 0.025000, epoch: 0, step: 18100, eta: 94.72 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33745 sec, avg_samples: 64.00000, ips: 379.31416 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 18200, eta: 94.64 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33775 sec, avg_samples: 64.00000, ips: 378.97585 images/sec
Training: 2023-10-17 13:41:05,712 - loss nan, lr: 0.025000, epoch: 0, step: 18200, eta: 94.64 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33775 sec, avg_samples: 64.00000, ips: 378.97585 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 18300, eta: 94.56 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33781 sec, avg_samples: 64.00000, ips: 378.91599 images/sec
Training: 2023-10-17 13:41:39,499 - loss nan, lr: 0.025000, epoch: 0, step: 18300, eta: 94.56 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33781 sec, avg_samples: 64.00000, ips: 378.91599 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 18400, eta: 94.48 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.82205 images/sec
Training: 2023-10-17 13:42:13,294 - loss nan, lr: 0.025000, epoch: 0, step: 18400, eta: 94.48 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.82205 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 18500, eta: 94.40 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33768 sec, avg_samples: 64.00000, ips: 379.05408 images/sec
Training: 2023-10-17 13:42:47,069 - loss nan, lr: 0.025000, epoch: 0, step: 18500, eta: 94.40 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33768 sec, avg_samples: 64.00000, ips: 379.05408 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 18600, eta: 94.32 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33774 sec, avg_samples: 64.00000, ips: 378.99173 images/sec
Training: 2023-10-17 13:43:20,848 - loss nan, lr: 0.025000, epoch: 0, step: 18600, eta: 94.32 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33774 sec, avg_samples: 64.00000, ips: 378.99173 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 18700, eta: 94.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.86169 images/sec
Training: 2023-10-17 13:43:54,640 - loss nan, lr: 0.025000, epoch: 0, step: 18700, eta: 94.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.86169 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 18800, eta: 94.17 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33764 sec, avg_samples: 64.00000, ips: 379.09977 images/sec
Training: 2023-10-17 13:44:28,410 - loss nan, lr: 0.025000, epoch: 0, step: 18800, eta: 94.17 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33764 sec, avg_samples: 64.00000, ips: 379.09977 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 18900, eta: 94.10 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33774 sec, avg_samples: 64.00000, ips: 378.99042 images/sec
Training: 2023-10-17 13:45:02,190 - loss nan, lr: 0.025000, epoch: 0, step: 18900, eta: 94.10 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33774 sec, avg_samples: 64.00000, ips: 378.99042 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 19000, eta: 94.02 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33775 sec, avg_samples: 64.00000, ips: 378.97601 images/sec
Training: 2023-10-17 13:45:35,972 - loss nan, lr: 0.025000, epoch: 0, step: 19000, eta: 94.02 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33775 sec, avg_samples: 64.00000, ips: 378.97601 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 19100, eta: 93.95 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33783 sec, avg_samples: 64.00000, ips: 378.88884 images/sec
Training: 2023-10-17 13:46:09,761 - loss nan, lr: 0.025000, epoch: 0, step: 19100, eta: 93.95 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33783 sec, avg_samples: 64.00000, ips: 378.88884 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 19200, eta: 93.88 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.79118 images/sec
Training: 2023-10-17 13:46:43,559 - loss nan, lr: 0.025000, epoch: 0, step: 19200, eta: 93.88 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.79118 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 19300, eta: 93.81 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33777 sec, avg_samples: 64.00000, ips: 378.95161 images/sec
Training: 2023-10-17 13:47:17,342 - loss nan, lr: 0.025000, epoch: 0, step: 19300, eta: 93.81 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33777 sec, avg_samples: 64.00000, ips: 378.95161 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 19400, eta: 93.73 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33776 sec, avg_samples: 64.00000, ips: 378.96297 images/sec
Training: 2023-10-17 13:47:51,124 - loss nan, lr: 0.025000, epoch: 0, step: 19400, eta: 93.73 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33776 sec, avg_samples: 64.00000, ips: 378.96297 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 19500, eta: 93.66 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33779 sec, avg_samples: 64.00000, ips: 378.93893 images/sec
Training: 2023-10-17 13:48:24,909 - loss nan, lr: 0.025000, epoch: 0, step: 19500, eta: 93.66 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33779 sec, avg_samples: 64.00000, ips: 378.93893 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 19600, eta: 93.59 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.86239 images/sec
Training: 2023-10-17 13:48:58,700 - loss nan, lr: 0.025000, epoch: 0, step: 19600, eta: 93.59 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.86239 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 19700, eta: 93.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.83310 images/sec
Training: 2023-10-17 13:49:32,494 - loss nan, lr: 0.025000, epoch: 0, step: 19700, eta: 93.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.83310 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 19800, eta: 93.46 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33774 sec, avg_samples: 64.00000, ips: 378.98802 images/sec
Training: 2023-10-17 13:50:06,275 - loss nan, lr: 0.025000, epoch: 0, step: 19800, eta: 93.46 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33774 sec, avg_samples: 64.00000, ips: 378.98802 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 19900, eta: 93.39 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33773 sec, avg_samples: 64.00000, ips: 379.00543 images/sec
Training: 2023-10-17 13:50:40,053 - loss nan, lr: 0.025000, epoch: 0, step: 19900, eta: 93.39 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33773 sec, avg_samples: 64.00000, ips: 379.00543 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 20000, eta: 93.32 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33780 sec, avg_samples: 64.00000, ips: 378.92127 images/sec
Training: 2023-10-17 13:51:13,848 - loss nan, lr: 0.025000, epoch: 0, step: 20000, eta: 93.32 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33780 sec, avg_samples: 64.00000, ips: 378.92127 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][20000]XNorm: 4.291414
Training: 2023-10-17 13:51:45,904 - [lfw][20000]XNorm: 4.291414
INFO:root:[lfw][20000]Accuracy-Flip: 0.80017+-0.00956
Training: 2023-10-17 13:51:45,904 - [lfw][20000]Accuracy-Flip: 0.80017+-0.00956
INFO:root:[lfw][20000]Accuracy-Highest: 0.80017
Training: 2023-10-17 13:51:45,904 - [lfw][20000]Accuracy-Highest: 0.80017
INFO:root:test time: 32.0563
Training: 2023-10-17 13:51:45,904 - test time: 32.0563
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][20000]XNorm: 4.128390
Training: 2023-10-17 13:52:23,124 - [cfp_fp][20000]XNorm: 4.128390
INFO:root:[cfp_fp][20000]Accuracy-Flip: 0.61043+-0.01378
Training: 2023-10-17 13:52:23,124 - [cfp_fp][20000]Accuracy-Flip: 0.61043+-0.01378
INFO:root:[cfp_fp][20000]Accuracy-Highest: 0.61371
Training: 2023-10-17 13:52:23,124 - [cfp_fp][20000]Accuracy-Highest: 0.61371
INFO:root:test time: 37.2192
Training: 2023-10-17 13:52:23,124 - test time: 37.2192
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][20000]XNorm: 4.344388
Training: 2023-10-17 13:52:55,139 - [agedb_30][20000]XNorm: 4.344388
INFO:root:[agedb_30][20000]Accuracy-Flip: 0.57600+-0.01911
Training: 2023-10-17 13:52:55,140 - [agedb_30][20000]Accuracy-Flip: 0.57600+-0.01911
INFO:root:[agedb_30][20000]Accuracy-Highest: 0.57767
Training: 2023-10-17 13:52:55,140 - [agedb_30][20000]Accuracy-Highest: 0.57767
INFO:root:test time: 32.0158
Training: 2023-10-17 13:52:55,140 - test time: 32.0158
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 20100, eta: 94.47 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33722 sec, avg_samples: 64.00000, ips: 379.56948 images/sec
Training: 2023-10-17 13:53:28,868 - loss nan, lr: 0.025000, epoch: 0, step: 20100, eta: 94.47 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33722 sec, avg_samples: 64.00000, ips: 379.56948 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 20200, eta: 94.40 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33793 sec, avg_samples: 64.00000, ips: 378.77336 images/sec
Training: 2023-10-17 13:54:02,667 - loss nan, lr: 0.025000, epoch: 0, step: 20200, eta: 94.40 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33793 sec, avg_samples: 64.00000, ips: 378.77336 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 20300, eta: 94.33 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67952 images/sec
Training: 2023-10-17 13:54:36,475 - loss nan, lr: 0.025000, epoch: 0, step: 20300, eta: 94.33 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67952 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 20400, eta: 94.26 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33836 sec, avg_samples: 64.00000, ips: 378.29547 images/sec
Training: 2023-10-17 13:55:10,317 - loss nan, lr: 0.025000, epoch: 0, step: 20400, eta: 94.26 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33836 sec, avg_samples: 64.00000, ips: 378.29547 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 20500, eta: 94.19 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33840 sec, avg_samples: 64.00000, ips: 378.25610 images/sec
Training: 2023-10-17 13:55:44,163 - loss nan, lr: 0.025000, epoch: 0, step: 20500, eta: 94.19 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33840 sec, avg_samples: 64.00000, ips: 378.25610 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 20600, eta: 94.12 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33840 sec, avg_samples: 64.00000, ips: 378.24938 images/sec
Training: 2023-10-17 13:56:18,009 - loss nan, lr: 0.025000, epoch: 0, step: 20600, eta: 94.12 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33840 sec, avg_samples: 64.00000, ips: 378.24938 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 20700, eta: 94.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33839 sec, avg_samples: 64.00000, ips: 378.26571 images/sec
Training: 2023-10-17 13:56:51,853 - loss nan, lr: 0.025000, epoch: 0, step: 20700, eta: 94.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33839 sec, avg_samples: 64.00000, ips: 378.26571 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 20800, eta: 93.98 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33846 sec, avg_samples: 64.00000, ips: 378.17845 images/sec
Training: 2023-10-17 13:57:25,706 - loss nan, lr: 0.025000, epoch: 0, step: 20800, eta: 93.98 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33846 sec, avg_samples: 64.00000, ips: 378.17845 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 20900, eta: 93.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33850 sec, avg_samples: 64.00000, ips: 378.13779 images/sec
Training: 2023-10-17 13:57:59,562 - loss nan, lr: 0.025000, epoch: 0, step: 20900, eta: 93.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33850 sec, avg_samples: 64.00000, ips: 378.13779 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 21000, eta: 93.85 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33850 sec, avg_samples: 64.00000, ips: 378.13432 images/sec
Training: 2023-10-17 13:58:33,419 - loss nan, lr: 0.025000, epoch: 0, step: 21000, eta: 93.85 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33850 sec, avg_samples: 64.00000, ips: 378.13432 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 21100, eta: 93.78 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33842 sec, avg_samples: 64.00000, ips: 378.23382 images/sec
Training: 2023-10-17 13:59:07,266 - loss nan, lr: 0.025000, epoch: 0, step: 21100, eta: 93.78 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33842 sec, avg_samples: 64.00000, ips: 378.23382 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 21200, eta: 93.71 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33845 sec, avg_samples: 64.00000, ips: 378.19115 images/sec
Training: 2023-10-17 13:59:41,118 - loss nan, lr: 0.025000, epoch: 0, step: 21200, eta: 93.71 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33845 sec, avg_samples: 64.00000, ips: 378.19115 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 21300, eta: 93.65 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33845 sec, avg_samples: 64.00000, ips: 378.19848 images/sec
Training: 2023-10-17 14:00:14,968 - loss nan, lr: 0.025000, epoch: 0, step: 21300, eta: 93.65 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33845 sec, avg_samples: 64.00000, ips: 378.19848 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 21400, eta: 93.58 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33845 sec, avg_samples: 64.00000, ips: 378.19838 images/sec
Training: 2023-10-17 14:00:48,828 - loss nan, lr: 0.025000, epoch: 0, step: 21400, eta: 93.58 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33845 sec, avg_samples: 64.00000, ips: 378.19838 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 21500, eta: 93.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33842 sec, avg_samples: 64.00000, ips: 378.23340 images/sec
Training: 2023-10-17 14:01:22,675 - loss nan, lr: 0.025000, epoch: 0, step: 21500, eta: 93.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33842 sec, avg_samples: 64.00000, ips: 378.23340 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 21600, eta: 93.45 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33854 sec, avg_samples: 64.00000, ips: 378.09542 images/sec
Training: 2023-10-17 14:01:56,535 - loss nan, lr: 0.025000, epoch: 0, step: 21600, eta: 93.45 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33854 sec, avg_samples: 64.00000, ips: 378.09542 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 21700, eta: 93.39 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33833 sec, avg_samples: 64.00000, ips: 378.33282 images/sec
Training: 2023-10-17 14:02:30,382 - loss nan, lr: 0.025000, epoch: 0, step: 21700, eta: 93.39 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33833 sec, avg_samples: 64.00000, ips: 378.33282 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 21800, eta: 93.33 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33830 sec, avg_samples: 64.00000, ips: 378.36445 images/sec
Training: 2023-10-17 14:03:04,218 - loss nan, lr: 0.025000, epoch: 0, step: 21800, eta: 93.33 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33830 sec, avg_samples: 64.00000, ips: 378.36445 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 21900, eta: 93.26 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33847 sec, avg_samples: 64.00000, ips: 378.16784 images/sec
Training: 2023-10-17 14:03:38,072 - loss nan, lr: 0.025000, epoch: 0, step: 21900, eta: 93.26 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33847 sec, avg_samples: 64.00000, ips: 378.16784 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 22000, eta: 93.20 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33833 sec, avg_samples: 64.00000, ips: 378.32993 images/sec
Training: 2023-10-17 14:04:11,920 - loss nan, lr: 0.025000, epoch: 0, step: 22000, eta: 93.20 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33833 sec, avg_samples: 64.00000, ips: 378.32993 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][22000]XNorm: 3.334586
Training: 2023-10-17 14:04:43,931 - [lfw][22000]XNorm: 3.334586
INFO:root:[lfw][22000]Accuracy-Flip: 0.79600+-0.01012
Training: 2023-10-17 14:04:43,931 - [lfw][22000]Accuracy-Flip: 0.79600+-0.01012
INFO:root:[lfw][22000]Accuracy-Highest: 0.80017
Training: 2023-10-17 14:04:43,931 - [lfw][22000]Accuracy-Highest: 0.80017
INFO:root:test time: 32.0117
Training: 2023-10-17 14:04:43,931 - test time: 32.0117
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][22000]XNorm: 3.209013
Training: 2023-10-17 14:05:21,102 - [cfp_fp][22000]XNorm: 3.209013
INFO:root:[cfp_fp][22000]Accuracy-Flip: 0.61143+-0.01500
Training: 2023-10-17 14:05:21,102 - [cfp_fp][22000]Accuracy-Flip: 0.61143+-0.01500
INFO:root:[cfp_fp][22000]Accuracy-Highest: 0.61371
Training: 2023-10-17 14:05:21,102 - [cfp_fp][22000]Accuracy-Highest: 0.61371
INFO:root:test time: 37.1704
Training: 2023-10-17 14:05:21,102 - test time: 37.1704
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][22000]XNorm: 3.373157
Training: 2023-10-17 14:05:53,137 - [agedb_30][22000]XNorm: 3.373157
INFO:root:[agedb_30][22000]Accuracy-Flip: 0.58350+-0.01674
Training: 2023-10-17 14:05:53,137 - [agedb_30][22000]Accuracy-Flip: 0.58350+-0.01674
INFO:root:[agedb_30][22000]Accuracy-Highest: 0.58350
Training: 2023-10-17 14:05:53,137 - [agedb_30][22000]Accuracy-Highest: 0.58350
INFO:root:test time: 32.0351
Training: 2023-10-17 14:05:53,137 - test time: 32.0351
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 22100, eta: 94.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33711 sec, avg_samples: 64.00000, ips: 379.69549 images/sec
Training: 2023-10-17 14:06:26,854 - loss nan, lr: 0.025000, epoch: 0, step: 22100, eta: 94.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33711 sec, avg_samples: 64.00000, ips: 379.69549 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 22200, eta: 94.18 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33797 sec, avg_samples: 64.00000, ips: 378.72733 images/sec
Training: 2023-10-17 14:07:00,657 - loss nan, lr: 0.025000, epoch: 0, step: 22200, eta: 94.18 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33797 sec, avg_samples: 64.00000, ips: 378.72733 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 22300, eta: 94.11 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.70868 images/sec
Training: 2023-10-17 14:07:34,462 - loss nan, lr: 0.025000, epoch: 0, step: 22300, eta: 94.11 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.70868 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 22400, eta: 94.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33809 sec, avg_samples: 64.00000, ips: 378.59878 images/sec
Training: 2023-10-17 14:08:08,277 - loss nan, lr: 0.025000, epoch: 0, step: 22400, eta: 94.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33809 sec, avg_samples: 64.00000, ips: 378.59878 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 22500, eta: 93.98 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.63881 images/sec
Training: 2023-10-17 14:08:42,089 - loss nan, lr: 0.025000, epoch: 0, step: 22500, eta: 93.98 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.63881 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 22600, eta: 93.92 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33854 sec, avg_samples: 64.00000, ips: 378.09128 images/sec
Training: 2023-10-17 14:09:15,949 - loss nan, lr: 0.025000, epoch: 0, step: 22600, eta: 93.92 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33854 sec, avg_samples: 64.00000, ips: 378.09128 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 22700, eta: 93.86 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33866 sec, avg_samples: 64.00000, ips: 377.96187 images/sec
Training: 2023-10-17 14:09:49,821 - loss nan, lr: 0.025000, epoch: 0, step: 22700, eta: 93.86 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33866 sec, avg_samples: 64.00000, ips: 377.96187 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 22800, eta: 93.79 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33844 sec, avg_samples: 64.00000, ips: 378.20624 images/sec
Training: 2023-10-17 14:10:23,671 - loss nan, lr: 0.025000, epoch: 0, step: 22800, eta: 93.79 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33844 sec, avg_samples: 64.00000, ips: 378.20624 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 22900, eta: 93.73 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33834 sec, avg_samples: 64.00000, ips: 378.32263 images/sec
Training: 2023-10-17 14:10:57,511 - loss nan, lr: 0.025000, epoch: 0, step: 22900, eta: 93.73 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33834 sec, avg_samples: 64.00000, ips: 378.32263 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 23000, eta: 93.67 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33859 sec, avg_samples: 64.00000, ips: 378.03371 images/sec
Training: 2023-10-17 14:11:31,376 - loss nan, lr: 0.025000, epoch: 0, step: 23000, eta: 93.67 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33859 sec, avg_samples: 64.00000, ips: 378.03371 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 23100, eta: 93.61 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33856 sec, avg_samples: 64.00000, ips: 378.06781 images/sec
Training: 2023-10-17 14:12:05,239 - loss nan, lr: 0.025000, epoch: 0, step: 23100, eta: 93.61 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33856 sec, avg_samples: 64.00000, ips: 378.06781 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 23200, eta: 93.54 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33849 sec, avg_samples: 64.00000, ips: 378.14463 images/sec
Training: 2023-10-17 14:12:39,094 - loss nan, lr: 0.025000, epoch: 0, step: 23200, eta: 93.54 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33849 sec, avg_samples: 64.00000, ips: 378.14463 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 23300, eta: 93.48 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33854 sec, avg_samples: 64.00000, ips: 378.09585 images/sec
Training: 2023-10-17 14:13:12,954 - loss nan, lr: 0.025000, epoch: 0, step: 23300, eta: 93.48 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33854 sec, avg_samples: 64.00000, ips: 378.09585 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 23400, eta: 93.42 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33850 sec, avg_samples: 64.00000, ips: 378.14021 images/sec
Training: 2023-10-17 14:13:46,810 - loss nan, lr: 0.025000, epoch: 0, step: 23400, eta: 93.42 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33850 sec, avg_samples: 64.00000, ips: 378.14021 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 23500, eta: 93.36 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33852 sec, avg_samples: 64.00000, ips: 378.12175 images/sec
Training: 2023-10-17 14:14:20,668 - loss nan, lr: 0.025000, epoch: 0, step: 23500, eta: 93.36 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33852 sec, avg_samples: 64.00000, ips: 378.12175 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 23600, eta: 93.30 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33864 sec, avg_samples: 64.00000, ips: 377.98578 images/sec
Training: 2023-10-17 14:14:54,538 - loss nan, lr: 0.025000, epoch: 0, step: 23600, eta: 93.30 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33864 sec, avg_samples: 64.00000, ips: 377.98578 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 23700, eta: 93.24 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33844 sec, avg_samples: 64.00000, ips: 378.20692 images/sec
Training: 2023-10-17 14:15:28,388 - loss nan, lr: 0.025000, epoch: 0, step: 23700, eta: 93.24 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33844 sec, avg_samples: 64.00000, ips: 378.20692 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 23800, eta: 93.18 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33853 sec, avg_samples: 64.00000, ips: 378.11015 images/sec
Training: 2023-10-17 14:16:02,246 - loss nan, lr: 0.025000, epoch: 0, step: 23800, eta: 93.18 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33853 sec, avg_samples: 64.00000, ips: 378.11015 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 23900, eta: 93.13 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33847 sec, avg_samples: 64.00000, ips: 378.16998 images/sec
Training: 2023-10-17 14:16:36,100 - loss nan, lr: 0.025000, epoch: 0, step: 23900, eta: 93.13 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33847 sec, avg_samples: 64.00000, ips: 378.16998 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 24000, eta: 93.07 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33840 sec, avg_samples: 64.00000, ips: 378.24994 images/sec
Training: 2023-10-17 14:17:09,950 - loss nan, lr: 0.025000, epoch: 0, step: 24000, eta: 93.07 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33840 sec, avg_samples: 64.00000, ips: 378.24994 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][24000]XNorm: 2.599059
Training: 2023-10-17 14:17:41,977 - [lfw][24000]XNorm: 2.599059
INFO:root:[lfw][24000]Accuracy-Flip: 0.79633+-0.01288
Training: 2023-10-17 14:17:41,977 - [lfw][24000]Accuracy-Flip: 0.79633+-0.01288
INFO:root:[lfw][24000]Accuracy-Highest: 0.80017
Training: 2023-10-17 14:17:41,977 - [lfw][24000]Accuracy-Highest: 0.80017
INFO:root:test time: 32.0277
Training: 2023-10-17 14:17:41,978 - test time: 32.0277
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][24000]XNorm: 2.501024
Training: 2023-10-17 14:18:19,190 - [cfp_fp][24000]XNorm: 2.501024
INFO:root:[cfp_fp][24000]Accuracy-Flip: 0.61357+-0.01278
Training: 2023-10-17 14:18:19,190 - [cfp_fp][24000]Accuracy-Flip: 0.61357+-0.01278
INFO:root:[cfp_fp][24000]Accuracy-Highest: 0.61371
Training: 2023-10-17 14:18:19,190 - [cfp_fp][24000]Accuracy-Highest: 0.61371
INFO:root:test time: 37.2122
Training: 2023-10-17 14:18:19,190 - test time: 37.2122
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][24000]XNorm: 2.629560
Training: 2023-10-17 14:18:51,219 - [agedb_30][24000]XNorm: 2.629560
INFO:root:[agedb_30][24000]Accuracy-Flip: 0.58150+-0.01905
Training: 2023-10-17 14:18:51,219 - [agedb_30][24000]Accuracy-Flip: 0.58150+-0.01905
INFO:root:[agedb_30][24000]Accuracy-Highest: 0.58350
Training: 2023-10-17 14:18:51,219 - [agedb_30][24000]Accuracy-Highest: 0.58350
INFO:root:test time: 32.0291
Training: 2023-10-17 14:18:51,219 - test time: 32.0291
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 24100, eta: 94.02 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33752 sec, avg_samples: 64.00000, ips: 379.23831 images/sec
Training: 2023-10-17 14:19:24,977 - loss nan, lr: 0.025000, epoch: 0, step: 24100, eta: 94.02 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33752 sec, avg_samples: 64.00000, ips: 379.23831 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 24200, eta: 93.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33809 sec, avg_samples: 64.00000, ips: 378.59304 images/sec
Training: 2023-10-17 14:19:58,792 - loss nan, lr: 0.025000, epoch: 0, step: 24200, eta: 93.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33809 sec, avg_samples: 64.00000, ips: 378.59304 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 24300, eta: 93.90 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68868 images/sec
Training: 2023-10-17 14:20:32,599 - loss nan, lr: 0.025000, epoch: 0, step: 24300, eta: 93.90 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68868 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 24400, eta: 93.84 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33813 sec, avg_samples: 64.00000, ips: 378.55786 images/sec
Training: 2023-10-17 14:21:06,417 - loss nan, lr: 0.025000, epoch: 0, step: 24400, eta: 93.84 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33813 sec, avg_samples: 64.00000, ips: 378.55786 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 24500, eta: 93.78 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67245 images/sec
Training: 2023-10-17 14:21:40,225 - loss nan, lr: 0.025000, epoch: 0, step: 24500, eta: 93.78 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67245 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 24600, eta: 93.72 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.61145 images/sec
Training: 2023-10-17 14:22:14,039 - loss nan, lr: 0.025000, epoch: 0, step: 24600, eta: 93.72 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.61145 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 24700, eta: 93.66 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33809 sec, avg_samples: 64.00000, ips: 378.59358 images/sec
Training: 2023-10-17 14:22:47,854 - loss nan, lr: 0.025000, epoch: 0, step: 24700, eta: 93.66 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33809 sec, avg_samples: 64.00000, ips: 378.59358 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 24800, eta: 93.60 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33804 sec, avg_samples: 64.00000, ips: 378.65264 images/sec
Training: 2023-10-17 14:23:21,664 - loss nan, lr: 0.025000, epoch: 0, step: 24800, eta: 93.60 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33804 sec, avg_samples: 64.00000, ips: 378.65264 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 24900, eta: 93.54 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33819 sec, avg_samples: 64.00000, ips: 378.48376 images/sec
Training: 2023-10-17 14:23:55,489 - loss nan, lr: 0.025000, epoch: 0, step: 24900, eta: 93.54 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33819 sec, avg_samples: 64.00000, ips: 378.48376 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 25000, eta: 93.48 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33809 sec, avg_samples: 64.00000, ips: 378.59743 images/sec
Training: 2023-10-17 14:24:29,304 - loss nan, lr: 0.025000, epoch: 0, step: 25000, eta: 93.48 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33809 sec, avg_samples: 64.00000, ips: 378.59743 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 25100, eta: 93.42 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33824 sec, avg_samples: 64.00000, ips: 378.42887 images/sec
Training: 2023-10-17 14:25:03,134 - loss nan, lr: 0.025000, epoch: 0, step: 25100, eta: 93.42 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33824 sec, avg_samples: 64.00000, ips: 378.42887 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 25200, eta: 93.37 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.60851 images/sec
Training: 2023-10-17 14:25:36,948 - loss nan, lr: 0.025000, epoch: 0, step: 25200, eta: 93.37 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.60851 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 25300, eta: 93.31 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.71584 images/sec
Training: 2023-10-17 14:26:10,752 - loss nan, lr: 0.025000, epoch: 0, step: 25300, eta: 93.31 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.71584 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 25400, eta: 93.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33797 sec, avg_samples: 64.00000, ips: 378.73647 images/sec
Training: 2023-10-17 14:26:44,555 - loss nan, lr: 0.025000, epoch: 0, step: 25400, eta: 93.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33797 sec, avg_samples: 64.00000, ips: 378.73647 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 25500, eta: 93.19 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66164 images/sec
Training: 2023-10-17 14:27:18,364 - loss nan, lr: 0.025000, epoch: 0, step: 25500, eta: 93.19 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66164 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 25600, eta: 93.14 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.63967 images/sec
Training: 2023-10-17 14:27:52,175 - loss nan, lr: 0.025000, epoch: 0, step: 25600, eta: 93.14 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.63967 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 25700, eta: 93.08 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33821 sec, avg_samples: 64.00000, ips: 378.46256 images/sec
Training: 2023-10-17 14:28:26,002 - loss nan, lr: 0.025000, epoch: 0, step: 25700, eta: 93.08 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33821 sec, avg_samples: 64.00000, ips: 378.46256 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 25800, eta: 93.03 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67326 images/sec
Training: 2023-10-17 14:28:59,810 - loss nan, lr: 0.025000, epoch: 0, step: 25800, eta: 93.03 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67326 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 25900, eta: 92.97 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33815 sec, avg_samples: 64.00000, ips: 378.53166 images/sec
Training: 2023-10-17 14:29:33,631 - loss nan, lr: 0.025000, epoch: 0, step: 25900, eta: 92.97 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33815 sec, avg_samples: 64.00000, ips: 378.53166 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 26000, eta: 92.92 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33822 sec, avg_samples: 64.00000, ips: 378.45086 images/sec
Training: 2023-10-17 14:30:07,459 - loss nan, lr: 0.025000, epoch: 0, step: 26000, eta: 92.92 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33822 sec, avg_samples: 64.00000, ips: 378.45086 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][26000]XNorm: 2.020726
Training: 2023-10-17 14:30:39,507 - [lfw][26000]XNorm: 2.020726
INFO:root:[lfw][26000]Accuracy-Flip: 0.80183+-0.01045
Training: 2023-10-17 14:30:39,508 - [lfw][26000]Accuracy-Flip: 0.80183+-0.01045
INFO:root:[lfw][26000]Accuracy-Highest: 0.80183
Training: 2023-10-17 14:30:39,508 - [lfw][26000]Accuracy-Highest: 0.80183
INFO:root:test time: 32.0485
Training: 2023-10-17 14:30:39,508 - test time: 32.0485
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][26000]XNorm: 1.944218
Training: 2023-10-17 14:31:16,696 - [cfp_fp][26000]XNorm: 1.944218
INFO:root:[cfp_fp][26000]Accuracy-Flip: 0.61114+-0.01255
Training: 2023-10-17 14:31:16,696 - [cfp_fp][26000]Accuracy-Flip: 0.61114+-0.01255
INFO:root:[cfp_fp][26000]Accuracy-Highest: 0.61371
Training: 2023-10-17 14:31:16,696 - [cfp_fp][26000]Accuracy-Highest: 0.61371
INFO:root:test time: 37.1883
Training: 2023-10-17 14:31:16,696 - test time: 37.1883
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][26000]XNorm: 2.045158
Training: 2023-10-17 14:31:48,697 - [agedb_30][26000]XNorm: 2.045158
INFO:root:[agedb_30][26000]Accuracy-Flip: 0.57633+-0.01622
Training: 2023-10-17 14:31:48,697 - [agedb_30][26000]Accuracy-Flip: 0.57633+-0.01622
INFO:root:[agedb_30][26000]Accuracy-Highest: 0.58350
Training: 2023-10-17 14:31:48,697 - [agedb_30][26000]Accuracy-Highest: 0.58350
INFO:root:test time: 32.0008
Training: 2023-10-17 14:31:48,697 - test time: 32.0008
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 26100, eta: 93.80 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33716 sec, avg_samples: 64.00000, ips: 379.63908 images/sec
Training: 2023-10-17 14:32:22,419 - loss nan, lr: 0.025000, epoch: 0, step: 26100, eta: 93.80 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33716 sec, avg_samples: 64.00000, ips: 379.63908 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 26200, eta: 93.74 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33809 sec, avg_samples: 64.00000, ips: 378.59630 images/sec
Training: 2023-10-17 14:32:56,234 - loss nan, lr: 0.025000, epoch: 0, step: 26200, eta: 93.74 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33809 sec, avg_samples: 64.00000, ips: 378.59630 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 26300, eta: 93.68 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33812 sec, avg_samples: 64.00000, ips: 378.56000 images/sec
Training: 2023-10-17 14:33:30,052 - loss nan, lr: 0.025000, epoch: 0, step: 26300, eta: 93.68 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33812 sec, avg_samples: 64.00000, ips: 378.56000 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 26400, eta: 93.62 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.65927 images/sec
Training: 2023-10-17 14:34:03,862 - loss nan, lr: 0.025000, epoch: 0, step: 26400, eta: 93.62 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.65927 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 26500, eta: 93.57 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68965 images/sec
Training: 2023-10-17 14:34:37,668 - loss nan, lr: 0.025000, epoch: 0, step: 26500, eta: 93.57 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68965 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 26600, eta: 93.51 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33826 sec, avg_samples: 64.00000, ips: 378.40994 images/sec
Training: 2023-10-17 14:35:11,500 - loss nan, lr: 0.025000, epoch: 0, step: 26600, eta: 93.51 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33826 sec, avg_samples: 64.00000, ips: 378.40994 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 26700, eta: 93.45 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33854 sec, avg_samples: 64.00000, ips: 378.09094 images/sec
Training: 2023-10-17 14:35:45,360 - loss nan, lr: 0.025000, epoch: 0, step: 26700, eta: 93.45 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33854 sec, avg_samples: 64.00000, ips: 378.09094 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 26800, eta: 93.40 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33856 sec, avg_samples: 64.00000, ips: 378.07554 images/sec
Training: 2023-10-17 14:36:19,222 - loss nan, lr: 0.025000, epoch: 0, step: 26800, eta: 93.40 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33856 sec, avg_samples: 64.00000, ips: 378.07554 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 26900, eta: 93.34 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33846 sec, avg_samples: 64.00000, ips: 378.18891 images/sec
Training: 2023-10-17 14:36:53,074 - loss nan, lr: 0.025000, epoch: 0, step: 26900, eta: 93.34 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33846 sec, avg_samples: 64.00000, ips: 378.18891 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 27000, eta: 93.29 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33846 sec, avg_samples: 64.00000, ips: 378.18199 images/sec
Training: 2023-10-17 14:37:26,926 - loss nan, lr: 0.025000, epoch: 0, step: 27000, eta: 93.29 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33846 sec, avg_samples: 64.00000, ips: 378.18199 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 27100, eta: 93.24 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33826 sec, avg_samples: 64.00000, ips: 378.40283 images/sec
Training: 2023-10-17 14:38:00,758 - loss nan, lr: 0.025000, epoch: 0, step: 27100, eta: 93.24 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33826 sec, avg_samples: 64.00000, ips: 378.40283 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 27200, eta: 93.18 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33841 sec, avg_samples: 64.00000, ips: 378.24233 images/sec
Training: 2023-10-17 14:38:34,605 - loss nan, lr: 0.025000, epoch: 0, step: 27200, eta: 93.18 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33841 sec, avg_samples: 64.00000, ips: 378.24233 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 27300, eta: 93.13 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33854 sec, avg_samples: 64.00000, ips: 378.09319 images/sec
Training: 2023-10-17 14:39:08,465 - loss nan, lr: 0.025000, epoch: 0, step: 27300, eta: 93.13 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33854 sec, avg_samples: 64.00000, ips: 378.09319 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 27400, eta: 93.07 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33857 sec, avg_samples: 64.00000, ips: 378.06029 images/sec
Training: 2023-10-17 14:39:42,328 - loss nan, lr: 0.025000, epoch: 0, step: 27400, eta: 93.07 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33857 sec, avg_samples: 64.00000, ips: 378.06029 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 27500, eta: 93.02 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33856 sec, avg_samples: 64.00000, ips: 378.07111 images/sec
Training: 2023-10-17 14:40:16,191 - loss nan, lr: 0.025000, epoch: 0, step: 27500, eta: 93.02 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33856 sec, avg_samples: 64.00000, ips: 378.07111 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 27600, eta: 92.97 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33851 sec, avg_samples: 64.00000, ips: 378.12369 images/sec
Training: 2023-10-17 14:40:50,048 - loss nan, lr: 0.025000, epoch: 0, step: 27600, eta: 92.97 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33851 sec, avg_samples: 64.00000, ips: 378.12369 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 27700, eta: 92.92 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33854 sec, avg_samples: 64.00000, ips: 378.09221 images/sec
Training: 2023-10-17 14:41:23,908 - loss nan, lr: 0.025000, epoch: 0, step: 27700, eta: 92.92 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33854 sec, avg_samples: 64.00000, ips: 378.09221 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 27800, eta: 92.86 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33857 sec, avg_samples: 64.00000, ips: 378.06146 images/sec
Training: 2023-10-17 14:41:57,771 - loss nan, lr: 0.025000, epoch: 0, step: 27800, eta: 92.86 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33857 sec, avg_samples: 64.00000, ips: 378.06146 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 27900, eta: 92.81 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33868 sec, avg_samples: 64.00000, ips: 377.94180 images/sec
Training: 2023-10-17 14:42:31,645 - loss nan, lr: 0.025000, epoch: 0, step: 27900, eta: 92.81 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33868 sec, avg_samples: 64.00000, ips: 377.94180 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 28000, eta: 92.76 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33849 sec, avg_samples: 64.00000, ips: 378.14821 images/sec
Training: 2023-10-17 14:43:05,500 - loss nan, lr: 0.025000, epoch: 0, step: 28000, eta: 92.76 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33849 sec, avg_samples: 64.00000, ips: 378.14821 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][28000]XNorm: 1.572488
Training: 2023-10-17 14:43:37,503 - [lfw][28000]XNorm: 1.572488
INFO:root:[lfw][28000]Accuracy-Flip: 0.79800+-0.01161
Training: 2023-10-17 14:43:37,503 - [lfw][28000]Accuracy-Flip: 0.79800+-0.01161
INFO:root:[lfw][28000]Accuracy-Highest: 0.80183
Training: 2023-10-17 14:43:37,503 - [lfw][28000]Accuracy-Highest: 0.80183
INFO:root:test time: 32.0029
Training: 2023-10-17 14:43:37,503 - test time: 32.0029
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][28000]XNorm: 1.515568
Training: 2023-10-17 14:44:14,704 - [cfp_fp][28000]XNorm: 1.515568
INFO:root:[cfp_fp][28000]Accuracy-Flip: 0.61400+-0.01290
Training: 2023-10-17 14:44:14,704 - [cfp_fp][28000]Accuracy-Flip: 0.61400+-0.01290
INFO:root:[cfp_fp][28000]Accuracy-Highest: 0.61400
Training: 2023-10-17 14:44:14,705 - [cfp_fp][28000]Accuracy-Highest: 0.61400
INFO:root:test time: 37.2013
Training: 2023-10-17 14:44:14,705 - test time: 37.2013
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][28000]XNorm: 1.592035
Training: 2023-10-17 14:44:46,724 - [agedb_30][28000]XNorm: 1.592035
INFO:root:[agedb_30][28000]Accuracy-Flip: 0.57750+-0.01674
Training: 2023-10-17 14:44:46,724 - [agedb_30][28000]Accuracy-Flip: 0.57750+-0.01674
INFO:root:[agedb_30][28000]Accuracy-Highest: 0.58350
Training: 2023-10-17 14:44:46,724 - [agedb_30][28000]Accuracy-Highest: 0.58350
INFO:root:test time: 32.0191
Training: 2023-10-17 14:44:46,724 - test time: 32.0191
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 28100, eta: 93.57 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33728 sec, avg_samples: 64.00000, ips: 379.50728 images/sec
Training: 2023-10-17 14:45:20,457 - loss nan, lr: 0.025000, epoch: 0, step: 28100, eta: 93.57 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33728 sec, avg_samples: 64.00000, ips: 379.50728 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 28200, eta: 93.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.67004 images/sec
Training: 2023-10-17 14:45:54,266 - loss nan, lr: 0.025000, epoch: 0, step: 28200, eta: 93.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.67004 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 28300, eta: 93.47 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33811 sec, avg_samples: 64.00000, ips: 378.57203 images/sec
Training: 2023-10-17 14:46:28,083 - loss nan, lr: 0.025000, epoch: 0, step: 28300, eta: 93.47 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33811 sec, avg_samples: 64.00000, ips: 378.57203 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 28400, eta: 93.41 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.71135 images/sec
Training: 2023-10-17 14:47:01,888 - loss nan, lr: 0.025000, epoch: 0, step: 28400, eta: 93.41 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.71135 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 28500, eta: 93.36 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33850 sec, avg_samples: 64.00000, ips: 378.13468 images/sec
Training: 2023-10-17 14:47:35,744 - loss nan, lr: 0.025000, epoch: 0, step: 28500, eta: 93.36 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33850 sec, avg_samples: 64.00000, ips: 378.13468 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 28600, eta: 93.31 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33859 sec, avg_samples: 64.00000, ips: 378.03896 images/sec
Training: 2023-10-17 14:48:09,609 - loss nan, lr: 0.025000, epoch: 0, step: 28600, eta: 93.31 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33859 sec, avg_samples: 64.00000, ips: 378.03896 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 28700, eta: 93.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33844 sec, avg_samples: 64.00000, ips: 378.20387 images/sec
Training: 2023-10-17 14:48:43,460 - loss nan, lr: 0.025000, epoch: 0, step: 28700, eta: 93.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33844 sec, avg_samples: 64.00000, ips: 378.20387 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 28800, eta: 93.20 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33842 sec, avg_samples: 64.00000, ips: 378.22468 images/sec
Training: 2023-10-17 14:49:17,308 - loss nan, lr: 0.025000, epoch: 0, step: 28800, eta: 93.20 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33842 sec, avg_samples: 64.00000, ips: 378.22468 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 28900, eta: 93.15 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33855 sec, avg_samples: 64.00000, ips: 378.08640 images/sec
Training: 2023-10-17 14:49:51,169 - loss nan, lr: 0.025000, epoch: 0, step: 28900, eta: 93.15 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33855 sec, avg_samples: 64.00000, ips: 378.08640 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 29000, eta: 93.10 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33839 sec, avg_samples: 64.00000, ips: 378.26184 images/sec
Training: 2023-10-17 14:50:25,014 - loss nan, lr: 0.025000, epoch: 0, step: 29000, eta: 93.10 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33839 sec, avg_samples: 64.00000, ips: 378.26184 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 29100, eta: 93.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33859 sec, avg_samples: 64.00000, ips: 378.04218 images/sec
Training: 2023-10-17 14:50:58,879 - loss nan, lr: 0.025000, epoch: 0, step: 29100, eta: 93.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33859 sec, avg_samples: 64.00000, ips: 378.04218 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 29200, eta: 93.00 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33851 sec, avg_samples: 64.00000, ips: 378.12842 images/sec
Training: 2023-10-17 14:51:32,736 - loss nan, lr: 0.025000, epoch: 0, step: 29200, eta: 93.00 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33851 sec, avg_samples: 64.00000, ips: 378.12842 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 29300, eta: 92.94 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33839 sec, avg_samples: 64.00000, ips: 378.26023 images/sec
Training: 2023-10-17 14:52:06,581 - loss nan, lr: 0.025000, epoch: 0, step: 29300, eta: 92.94 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33839 sec, avg_samples: 64.00000, ips: 378.26023 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 29400, eta: 92.89 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33857 sec, avg_samples: 64.00000, ips: 378.05574 images/sec
Training: 2023-10-17 14:52:40,445 - loss nan, lr: 0.025000, epoch: 0, step: 29400, eta: 92.89 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33857 sec, avg_samples: 64.00000, ips: 378.05574 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 29500, eta: 92.84 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33858 sec, avg_samples: 64.00000, ips: 378.05274 images/sec
Training: 2023-10-17 14:53:14,308 - loss nan, lr: 0.025000, epoch: 0, step: 29500, eta: 92.84 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33858 sec, avg_samples: 64.00000, ips: 378.05274 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 29600, eta: 92.79 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33850 sec, avg_samples: 64.00000, ips: 378.14135 images/sec
Training: 2023-10-17 14:53:48,164 - loss nan, lr: 0.025000, epoch: 0, step: 29600, eta: 92.79 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33850 sec, avg_samples: 64.00000, ips: 378.14135 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 29700, eta: 92.74 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33853 sec, avg_samples: 64.00000, ips: 378.10912 images/sec
Training: 2023-10-17 14:54:22,023 - loss nan, lr: 0.025000, epoch: 0, step: 29700, eta: 92.74 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33853 sec, avg_samples: 64.00000, ips: 378.10912 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 29800, eta: 92.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33872 sec, avg_samples: 64.00000, ips: 377.89733 images/sec
Training: 2023-10-17 14:54:55,901 - loss nan, lr: 0.025000, epoch: 0, step: 29800, eta: 92.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33872 sec, avg_samples: 64.00000, ips: 377.89733 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 29900, eta: 92.65 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33863 sec, avg_samples: 64.00000, ips: 377.99823 images/sec
Training: 2023-10-17 14:55:29,769 - loss nan, lr: 0.025000, epoch: 0, step: 29900, eta: 92.65 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33863 sec, avg_samples: 64.00000, ips: 377.99823 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 30000, eta: 92.60 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33845 sec, avg_samples: 64.00000, ips: 378.19131 images/sec
Training: 2023-10-17 14:56:03,629 - loss nan, lr: 0.025000, epoch: 0, step: 30000, eta: 92.60 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33845 sec, avg_samples: 64.00000, ips: 378.19131 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][30000]XNorm: 1.226136
Training: 2023-10-17 14:56:35,631 - [lfw][30000]XNorm: 1.226136
INFO:root:[lfw][30000]Accuracy-Flip: 0.80367+-0.01074
Training: 2023-10-17 14:56:35,631 - [lfw][30000]Accuracy-Flip: 0.80367+-0.01074
INFO:root:[lfw][30000]Accuracy-Highest: 0.80367
Training: 2023-10-17 14:56:35,631 - [lfw][30000]Accuracy-Highest: 0.80367
INFO:root:test time: 32.0019
Training: 2023-10-17 14:56:35,631 - test time: 32.0019
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][30000]XNorm: 1.178712
Training: 2023-10-17 14:57:12,831 - [cfp_fp][30000]XNorm: 1.178712
INFO:root:[cfp_fp][30000]Accuracy-Flip: 0.61443+-0.01395
Training: 2023-10-17 14:57:12,831 - [cfp_fp][30000]Accuracy-Flip: 0.61443+-0.01395
INFO:root:[cfp_fp][30000]Accuracy-Highest: 0.61443
Training: 2023-10-17 14:57:12,831 - [cfp_fp][30000]Accuracy-Highest: 0.61443
INFO:root:test time: 37.2004
Training: 2023-10-17 14:57:12,831 - test time: 37.2004
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][30000]XNorm: 1.240483
Training: 2023-10-17 14:57:44,841 - [agedb_30][30000]XNorm: 1.240483
INFO:root:[agedb_30][30000]Accuracy-Flip: 0.57450+-0.01381
Training: 2023-10-17 14:57:44,842 - [agedb_30][30000]Accuracy-Flip: 0.57450+-0.01381
INFO:root:[agedb_30][30000]Accuracy-Highest: 0.58350
Training: 2023-10-17 14:57:44,842 - [agedb_30][30000]Accuracy-Highest: 0.58350
INFO:root:test time: 32.0102
Training: 2023-10-17 14:57:44,842 - test time: 32.0102
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 30100, eta: 93.35 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33713 sec, avg_samples: 64.00000, ips: 379.67192 images/sec
Training: 2023-10-17 14:58:18,561 - loss nan, lr: 0.025000, epoch: 0, step: 30100, eta: 93.35 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33713 sec, avg_samples: 64.00000, ips: 379.67192 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 30200, eta: 93.30 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33806 sec, avg_samples: 64.00000, ips: 378.63398 images/sec
Training: 2023-10-17 14:58:52,372 - loss nan, lr: 0.025000, epoch: 0, step: 30200, eta: 93.30 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33806 sec, avg_samples: 64.00000, ips: 378.63398 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 30300, eta: 93.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33825 sec, avg_samples: 64.00000, ips: 378.41406 images/sec
Training: 2023-10-17 14:59:26,204 - loss nan, lr: 0.025000, epoch: 0, step: 30300, eta: 93.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33825 sec, avg_samples: 64.00000, ips: 378.41406 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 30400, eta: 93.20 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33866 sec, avg_samples: 64.00000, ips: 377.96462 images/sec
Training: 2023-10-17 15:00:00,075 - loss nan, lr: 0.025000, epoch: 0, step: 30400, eta: 93.20 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33866 sec, avg_samples: 64.00000, ips: 377.96462 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 30500, eta: 93.15 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33832 sec, avg_samples: 64.00000, ips: 378.33758 images/sec
Training: 2023-10-17 15:00:33,914 - loss nan, lr: 0.025000, epoch: 0, step: 30500, eta: 93.15 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33832 sec, avg_samples: 64.00000, ips: 378.33758 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 30600, eta: 93.10 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33839 sec, avg_samples: 64.00000, ips: 378.26280 images/sec
Training: 2023-10-17 15:01:07,759 - loss nan, lr: 0.025000, epoch: 0, step: 30600, eta: 93.10 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33839 sec, avg_samples: 64.00000, ips: 378.26280 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 30700, eta: 93.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33865 sec, avg_samples: 64.00000, ips: 377.97488 images/sec
Training: 2023-10-17 15:01:41,630 - loss nan, lr: 0.025000, epoch: 0, step: 30700, eta: 93.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33865 sec, avg_samples: 64.00000, ips: 377.97488 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 30800, eta: 93.00 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33842 sec, avg_samples: 64.00000, ips: 378.23100 images/sec
Training: 2023-10-17 15:02:15,477 - loss nan, lr: 0.025000, epoch: 0, step: 30800, eta: 93.00 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33842 sec, avg_samples: 64.00000, ips: 378.23100 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 30900, eta: 92.95 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33839 sec, avg_samples: 64.00000, ips: 378.25690 images/sec
Training: 2023-10-17 15:02:49,323 - loss nan, lr: 0.025000, epoch: 0, step: 30900, eta: 92.95 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33839 sec, avg_samples: 64.00000, ips: 378.25690 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 31000, eta: 92.90 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33860 sec, avg_samples: 64.00000, ips: 378.02581 images/sec
Training: 2023-10-17 15:03:23,189 - loss nan, lr: 0.025000, epoch: 0, step: 31000, eta: 92.90 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33860 sec, avg_samples: 64.00000, ips: 378.02581 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 31100, eta: 92.85 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33849 sec, avg_samples: 64.00000, ips: 378.15082 images/sec
Training: 2023-10-17 15:03:57,044 - loss nan, lr: 0.025000, epoch: 0, step: 31100, eta: 92.85 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33849 sec, avg_samples: 64.00000, ips: 378.15082 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 31200, eta: 92.81 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33857 sec, avg_samples: 64.00000, ips: 378.06138 images/sec
Training: 2023-10-17 15:04:30,907 - loss nan, lr: 0.025000, epoch: 0, step: 31200, eta: 92.81 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33857 sec, avg_samples: 64.00000, ips: 378.06138 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 31300, eta: 92.76 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33844 sec, avg_samples: 64.00000, ips: 378.21137 images/sec
Training: 2023-10-17 15:05:04,757 - loss nan, lr: 0.025000, epoch: 0, step: 31300, eta: 92.76 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33844 sec, avg_samples: 64.00000, ips: 378.21137 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 31400, eta: 92.71 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33848 sec, avg_samples: 64.00000, ips: 378.15880 images/sec
Training: 2023-10-17 15:05:38,611 - loss nan, lr: 0.025000, epoch: 0, step: 31400, eta: 92.71 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33848 sec, avg_samples: 64.00000, ips: 378.15880 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 31500, eta: 92.66 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33855 sec, avg_samples: 64.00000, ips: 378.07847 images/sec
Training: 2023-10-17 15:06:12,473 - loss nan, lr: 0.025000, epoch: 0, step: 31500, eta: 92.66 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33855 sec, avg_samples: 64.00000, ips: 378.07847 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 31600, eta: 92.61 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33864 sec, avg_samples: 64.00000, ips: 377.98534 images/sec
Training: 2023-10-17 15:06:46,342 - loss nan, lr: 0.025000, epoch: 0, step: 31600, eta: 92.61 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33864 sec, avg_samples: 64.00000, ips: 377.98534 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 31700, eta: 92.57 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33856 sec, avg_samples: 64.00000, ips: 378.07726 images/sec
Training: 2023-10-17 15:07:20,204 - loss nan, lr: 0.025000, epoch: 0, step: 31700, eta: 92.57 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33856 sec, avg_samples: 64.00000, ips: 378.07726 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 31800, eta: 92.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33846 sec, avg_samples: 64.00000, ips: 378.17907 images/sec
Training: 2023-10-17 15:07:54,057 - loss nan, lr: 0.025000, epoch: 0, step: 31800, eta: 92.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33846 sec, avg_samples: 64.00000, ips: 378.17907 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 31900, eta: 92.47 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33855 sec, avg_samples: 64.00000, ips: 378.08184 images/sec
Training: 2023-10-17 15:08:27,918 - loss nan, lr: 0.025000, epoch: 0, step: 31900, eta: 92.47 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33855 sec, avg_samples: 64.00000, ips: 378.08184 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 32000, eta: 92.43 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33849 sec, avg_samples: 64.00000, ips: 378.15241 images/sec
Training: 2023-10-17 15:09:01,781 - loss nan, lr: 0.025000, epoch: 0, step: 32000, eta: 92.43 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33849 sec, avg_samples: 64.00000, ips: 378.15241 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][32000]XNorm: 0.954259
Training: 2023-10-17 15:09:33,783 - [lfw][32000]XNorm: 0.954259
INFO:root:[lfw][32000]Accuracy-Flip: 0.80233+-0.01346
Training: 2023-10-17 15:09:33,783 - [lfw][32000]Accuracy-Flip: 0.80233+-0.01346
INFO:root:[lfw][32000]Accuracy-Highest: 0.80367
Training: 2023-10-17 15:09:33,784 - [lfw][32000]Accuracy-Highest: 0.80367
INFO:root:test time: 32.0025
Training: 2023-10-17 15:09:33,784 - test time: 32.0025
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][32000]XNorm: 0.916954
Training: 2023-10-17 15:10:10,940 - [cfp_fp][32000]XNorm: 0.916954
INFO:root:[cfp_fp][32000]Accuracy-Flip: 0.61171+-0.01268
Training: 2023-10-17 15:10:10,940 - [cfp_fp][32000]Accuracy-Flip: 0.61171+-0.01268
INFO:root:[cfp_fp][32000]Accuracy-Highest: 0.61443
Training: 2023-10-17 15:10:10,940 - [cfp_fp][32000]Accuracy-Highest: 0.61443
INFO:root:test time: 37.1563
Training: 2023-10-17 15:10:10,940 - test time: 37.1563
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][32000]XNorm: 0.964541
Training: 2023-10-17 15:10:42,949 - [agedb_30][32000]XNorm: 0.964541
INFO:root:[agedb_30][32000]Accuracy-Flip: 0.57783+-0.01602
Training: 2023-10-17 15:10:42,949 - [agedb_30][32000]Accuracy-Flip: 0.57783+-0.01602
INFO:root:[agedb_30][32000]Accuracy-Highest: 0.58350
Training: 2023-10-17 15:10:42,949 - [agedb_30][32000]Accuracy-Highest: 0.58350
INFO:root:test time: 32.0095
Training: 2023-10-17 15:10:42,949 - test time: 32.0095
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 32100, eta: 93.13 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33730 sec, avg_samples: 64.00000, ips: 379.48869 images/sec
Training: 2023-10-17 15:11:16,685 - loss nan, lr: 0.025000, epoch: 0, step: 32100, eta: 93.13 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33730 sec, avg_samples: 64.00000, ips: 379.48869 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 32200, eta: 93.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33760 sec, avg_samples: 64.00000, ips: 379.14169 images/sec
Training: 2023-10-17 15:11:50,451 - loss nan, lr: 0.025000, epoch: 0, step: 32200, eta: 93.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33760 sec, avg_samples: 64.00000, ips: 379.14169 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 32300, eta: 93.04 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33760 sec, avg_samples: 64.00000, ips: 379.14363 images/sec
Training: 2023-10-17 15:12:24,218 - loss nan, lr: 0.025000, epoch: 0, step: 32300, eta: 93.04 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33760 sec, avg_samples: 64.00000, ips: 379.14363 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 32400, eta: 92.99 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33763 sec, avg_samples: 64.00000, ips: 379.10931 images/sec
Training: 2023-10-17 15:12:57,987 - loss nan, lr: 0.025000, epoch: 0, step: 32400, eta: 92.99 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33763 sec, avg_samples: 64.00000, ips: 379.10931 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 32500, eta: 92.94 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.60416 images/sec
Training: 2023-10-17 15:13:31,801 - loss nan, lr: 0.025000, epoch: 0, step: 32500, eta: 92.94 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.60416 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 32600, eta: 92.89 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33813 sec, avg_samples: 64.00000, ips: 378.55489 images/sec
Training: 2023-10-17 15:14:05,620 - loss nan, lr: 0.025000, epoch: 0, step: 32600, eta: 92.89 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33813 sec, avg_samples: 64.00000, ips: 378.55489 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 32700, eta: 92.84 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33809 sec, avg_samples: 64.00000, ips: 378.59441 images/sec
Training: 2023-10-17 15:14:39,436 - loss nan, lr: 0.025000, epoch: 0, step: 32700, eta: 92.84 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33809 sec, avg_samples: 64.00000, ips: 378.59441 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 32800, eta: 92.80 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66206 images/sec
Training: 2023-10-17 15:15:13,245 - loss nan, lr: 0.025000, epoch: 0, step: 32800, eta: 92.80 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66206 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 32900, eta: 92.75 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.81520 images/sec
Training: 2023-10-17 15:15:47,040 - loss nan, lr: 0.025000, epoch: 0, step: 32900, eta: 92.75 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.81520 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 33000, eta: 92.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.70337 images/sec
Training: 2023-10-17 15:16:20,846 - loss nan, lr: 0.025000, epoch: 0, step: 33000, eta: 92.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.70337 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 33100, eta: 92.66 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.60706 images/sec
Training: 2023-10-17 15:16:54,660 - loss nan, lr: 0.025000, epoch: 0, step: 33100, eta: 92.66 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.60706 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 33200, eta: 92.61 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.64524 images/sec
Training: 2023-10-17 15:17:28,471 - loss nan, lr: 0.025000, epoch: 0, step: 33200, eta: 92.61 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.64524 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 33300, eta: 92.56 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33816 sec, avg_samples: 64.00000, ips: 378.52008 images/sec
Training: 2023-10-17 15:18:02,293 - loss nan, lr: 0.025000, epoch: 0, step: 33300, eta: 92.56 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33816 sec, avg_samples: 64.00000, ips: 378.52008 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 33400, eta: 92.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68486 images/sec
Training: 2023-10-17 15:18:36,100 - loss nan, lr: 0.025000, epoch: 0, step: 33400, eta: 92.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68486 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 33500, eta: 92.47 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.71078 images/sec
Training: 2023-10-17 15:19:09,905 - loss nan, lr: 0.025000, epoch: 0, step: 33500, eta: 92.47 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.71078 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 33600, eta: 92.43 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33828 sec, avg_samples: 64.00000, ips: 378.38398 images/sec
Training: 2023-10-17 15:19:43,739 - loss nan, lr: 0.025000, epoch: 0, step: 33600, eta: 92.43 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33828 sec, avg_samples: 64.00000, ips: 378.38398 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 33700, eta: 92.38 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33810 sec, avg_samples: 64.00000, ips: 378.59090 images/sec
Training: 2023-10-17 15:20:17,555 - loss nan, lr: 0.025000, epoch: 0, step: 33700, eta: 92.38 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33810 sec, avg_samples: 64.00000, ips: 378.59090 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 33800, eta: 92.34 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.69002 images/sec
Training: 2023-10-17 15:20:51,362 - loss nan, lr: 0.025000, epoch: 0, step: 33800, eta: 92.34 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.69002 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 33900, eta: 92.29 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33810 sec, avg_samples: 64.00000, ips: 378.59052 images/sec
Training: 2023-10-17 15:21:25,178 - loss nan, lr: 0.025000, epoch: 0, step: 33900, eta: 92.29 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33810 sec, avg_samples: 64.00000, ips: 378.59052 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 34000, eta: 92.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33804 sec, avg_samples: 64.00000, ips: 378.65850 images/sec
Training: 2023-10-17 15:21:58,995 - loss nan, lr: 0.025000, epoch: 0, step: 34000, eta: 92.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33804 sec, avg_samples: 64.00000, ips: 378.65850 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][34000]XNorm: 0.743175
Training: 2023-10-17 15:22:31,009 - [lfw][34000]XNorm: 0.743175
INFO:root:[lfw][34000]Accuracy-Flip: 0.80333+-0.00952
Training: 2023-10-17 15:22:31,009 - [lfw][34000]Accuracy-Flip: 0.80333+-0.00952
INFO:root:[lfw][34000]Accuracy-Highest: 0.80367
Training: 2023-10-17 15:22:31,009 - [lfw][34000]Accuracy-Highest: 0.80367
INFO:root:test time: 32.0136
Training: 2023-10-17 15:22:31,009 - test time: 32.0136
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][34000]XNorm: 0.715787
Training: 2023-10-17 15:23:08,205 - [cfp_fp][34000]XNorm: 0.715787
INFO:root:[cfp_fp][34000]Accuracy-Flip: 0.61414+-0.01262
Training: 2023-10-17 15:23:08,205 - [cfp_fp][34000]Accuracy-Flip: 0.61414+-0.01262
INFO:root:[cfp_fp][34000]Accuracy-Highest: 0.61443
Training: 2023-10-17 15:23:08,205 - [cfp_fp][34000]Accuracy-Highest: 0.61443
INFO:root:test time: 37.1955
Training: 2023-10-17 15:23:08,205 - test time: 37.1955
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][34000]XNorm: 0.753059
Training: 2023-10-17 15:23:40,216 - [agedb_30][34000]XNorm: 0.753059
INFO:root:[agedb_30][34000]Accuracy-Flip: 0.57550+-0.01807
Training: 2023-10-17 15:23:40,216 - [agedb_30][34000]Accuracy-Flip: 0.57550+-0.01807
INFO:root:[agedb_30][34000]Accuracy-Highest: 0.58350
Training: 2023-10-17 15:23:40,216 - [agedb_30][34000]Accuracy-Highest: 0.58350
INFO:root:test time: 32.0113
Training: 2023-10-17 15:23:40,216 - test time: 32.0113
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 34100, eta: 92.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33715 sec, avg_samples: 64.00000, ips: 379.65127 images/sec
Training: 2023-10-17 15:24:13,937 - loss nan, lr: 0.025000, epoch: 0, step: 34100, eta: 92.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33715 sec, avg_samples: 64.00000, ips: 379.65127 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 34200, eta: 92.86 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33754 sec, avg_samples: 64.00000, ips: 379.21866 images/sec
Training: 2023-10-17 15:24:47,697 - loss nan, lr: 0.025000, epoch: 0, step: 34200, eta: 92.86 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33754 sec, avg_samples: 64.00000, ips: 379.21866 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 34300, eta: 92.82 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33768 sec, avg_samples: 64.00000, ips: 379.05994 images/sec
Training: 2023-10-17 15:25:21,471 - loss nan, lr: 0.025000, epoch: 0, step: 34300, eta: 92.82 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33768 sec, avg_samples: 64.00000, ips: 379.05994 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 34400, eta: 92.77 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33755 sec, avg_samples: 64.00000, ips: 379.20827 images/sec
Training: 2023-10-17 15:25:55,231 - loss nan, lr: 0.025000, epoch: 0, step: 34400, eta: 92.77 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33755 sec, avg_samples: 64.00000, ips: 379.20827 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 34500, eta: 92.72 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33761 sec, avg_samples: 64.00000, ips: 379.13628 images/sec
Training: 2023-10-17 15:26:28,998 - loss nan, lr: 0.025000, epoch: 0, step: 34500, eta: 92.72 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33761 sec, avg_samples: 64.00000, ips: 379.13628 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 34600, eta: 92.68 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33796 sec, avg_samples: 64.00000, ips: 378.73894 images/sec
Training: 2023-10-17 15:27:02,801 - loss nan, lr: 0.025000, epoch: 0, step: 34600, eta: 92.68 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33796 sec, avg_samples: 64.00000, ips: 378.73894 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 34700, eta: 92.63 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33796 sec, avg_samples: 64.00000, ips: 378.74418 images/sec
Training: 2023-10-17 15:27:36,603 - loss nan, lr: 0.025000, epoch: 0, step: 34700, eta: 92.63 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33796 sec, avg_samples: 64.00000, ips: 378.74418 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 34800, eta: 92.59 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33797 sec, avg_samples: 64.00000, ips: 378.73243 images/sec
Training: 2023-10-17 15:28:10,406 - loss nan, lr: 0.025000, epoch: 0, step: 34800, eta: 92.59 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33797 sec, avg_samples: 64.00000, ips: 378.73243 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 34900, eta: 92.54 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33810 sec, avg_samples: 64.00000, ips: 378.58320 images/sec
Training: 2023-10-17 15:28:44,222 - loss nan, lr: 0.025000, epoch: 0, step: 34900, eta: 92.54 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33810 sec, avg_samples: 64.00000, ips: 378.58320 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 35000, eta: 92.50 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.64387 images/sec
Training: 2023-10-17 15:29:18,033 - loss nan, lr: 0.025000, epoch: 0, step: 35000, eta: 92.50 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.64387 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 35100, eta: 92.45 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33807 sec, avg_samples: 64.00000, ips: 378.61971 images/sec
Training: 2023-10-17 15:29:51,846 - loss nan, lr: 0.025000, epoch: 0, step: 35100, eta: 92.45 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33807 sec, avg_samples: 64.00000, ips: 378.61971 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 35200, eta: 92.41 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33814 sec, avg_samples: 64.00000, ips: 378.54387 images/sec
Training: 2023-10-17 15:30:25,666 - loss nan, lr: 0.025000, epoch: 0, step: 35200, eta: 92.41 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33814 sec, avg_samples: 64.00000, ips: 378.54387 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 35300, eta: 92.36 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33811 sec, avg_samples: 64.00000, ips: 378.57628 images/sec
Training: 2023-10-17 15:30:59,483 - loss nan, lr: 0.025000, epoch: 0, step: 35300, eta: 92.36 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33811 sec, avg_samples: 64.00000, ips: 378.57628 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 35400, eta: 92.32 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33810 sec, avg_samples: 64.00000, ips: 378.58948 images/sec
Training: 2023-10-17 15:31:33,299 - loss nan, lr: 0.025000, epoch: 0, step: 35400, eta: 92.32 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33810 sec, avg_samples: 64.00000, ips: 378.58948 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 35500, eta: 92.28 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.64102 images/sec
Training: 2023-10-17 15:32:07,110 - loss nan, lr: 0.025000, epoch: 0, step: 35500, eta: 92.28 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.64102 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 35600, eta: 92.23 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33814 sec, avg_samples: 64.00000, ips: 378.53654 images/sec
Training: 2023-10-17 15:32:40,930 - loss nan, lr: 0.025000, epoch: 0, step: 35600, eta: 92.23 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33814 sec, avg_samples: 64.00000, ips: 378.53654 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 0, step: 35700, eta: 92.19 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33810 sec, avg_samples: 64.00000, ips: 378.58720 images/sec
Training: 2023-10-17 15:33:14,747 - loss nan, lr: 0.025000, epoch: 0, step: 35700, eta: 92.19 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33810 sec, avg_samples: 64.00000, ips: 378.58720 images/sec
/usr/local/lib/python3.7/dist-packages/paddle/framework/io.py:412: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working
  if isinstance(obj, collections.Iterable) and not isinstance(obj, (
INFO:root:Save model to model/FresResNet101/0.
Training: 2023-10-17 15:33:27,986 - Save model to model/FresResNet101/0.
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 35800, eta: 92.16 hours, avg_reader_cost: 0.00900 sec, avg_batch_cost: 0.22517 sec, avg_samples: 40.96000, ips: 363.81719 images/sec
Training: 2023-10-17 15:33:50,507 - loss nan, lr: 0.025000, epoch: 1, step: 35800, eta: 92.16 hours, avg_reader_cost: 0.00900 sec, avg_batch_cost: 0.22517 sec, avg_samples: 40.96000, ips: 363.81719 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 35900, eta: 92.12 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.83660 images/sec
Training: 2023-10-17 15:34:24,300 - loss nan, lr: 0.025000, epoch: 1, step: 35900, eta: 92.12 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.83660 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 36000, eta: 92.07 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33817 sec, avg_samples: 64.00000, ips: 378.50774 images/sec
Training: 2023-10-17 15:34:58,123 - loss nan, lr: 0.025000, epoch: 1, step: 36000, eta: 92.07 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33817 sec, avg_samples: 64.00000, ips: 378.50774 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][36000]XNorm: 0.577674
Training: 2023-10-17 15:35:30,148 - [lfw][36000]XNorm: 0.577674
INFO:root:[lfw][36000]Accuracy-Flip: 0.80033+-0.01115
Training: 2023-10-17 15:35:30,148 - [lfw][36000]Accuracy-Flip: 0.80033+-0.01115
INFO:root:[lfw][36000]Accuracy-Highest: 0.80367
Training: 2023-10-17 15:35:30,148 - [lfw][36000]Accuracy-Highest: 0.80367
INFO:root:test time: 32.0243
Training: 2023-10-17 15:35:30,148 - test time: 32.0243
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][36000]XNorm: 0.556460
Training: 2023-10-17 15:36:07,289 - [cfp_fp][36000]XNorm: 0.556460
INFO:root:[cfp_fp][36000]Accuracy-Flip: 0.61114+-0.01285
Training: 2023-10-17 15:36:07,290 - [cfp_fp][36000]Accuracy-Flip: 0.61114+-0.01285
INFO:root:[cfp_fp][36000]Accuracy-Highest: 0.61443
Training: 2023-10-17 15:36:07,290 - [cfp_fp][36000]Accuracy-Highest: 0.61443
INFO:root:test time: 37.1415
Training: 2023-10-17 15:36:07,290 - test time: 37.1415
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][36000]XNorm: 0.584652
Training: 2023-10-17 15:36:39,290 - [agedb_30][36000]XNorm: 0.584652
INFO:root:[agedb_30][36000]Accuracy-Flip: 0.57883+-0.01790
Training: 2023-10-17 15:36:39,290 - [agedb_30][36000]Accuracy-Flip: 0.57883+-0.01790
INFO:root:[agedb_30][36000]Accuracy-Highest: 0.58350
Training: 2023-10-17 15:36:39,290 - [agedb_30][36000]Accuracy-Highest: 0.58350
INFO:root:test time: 32.0007
Training: 2023-10-17 15:36:39,290 - test time: 32.0007
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 36100, eta: 92.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33710 sec, avg_samples: 64.00000, ips: 379.71416 images/sec
Training: 2023-10-17 15:37:13,006 - loss nan, lr: 0.025000, epoch: 1, step: 36100, eta: 92.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33710 sec, avg_samples: 64.00000, ips: 379.71416 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 36200, eta: 92.65 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33745 sec, avg_samples: 64.00000, ips: 379.31113 images/sec
Training: 2023-10-17 15:37:46,757 - loss nan, lr: 0.025000, epoch: 1, step: 36200, eta: 92.65 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33745 sec, avg_samples: 64.00000, ips: 379.31113 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 36300, eta: 92.61 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33769 sec, avg_samples: 64.00000, ips: 379.05085 images/sec
Training: 2023-10-17 15:38:20,532 - loss nan, lr: 0.025000, epoch: 1, step: 36300, eta: 92.61 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33769 sec, avg_samples: 64.00000, ips: 379.05085 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 36400, eta: 92.56 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.71672 images/sec
Training: 2023-10-17 15:38:54,336 - loss nan, lr: 0.025000, epoch: 1, step: 36400, eta: 92.56 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.71672 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 36500, eta: 92.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.78570 images/sec
Training: 2023-10-17 15:39:28,134 - loss nan, lr: 0.025000, epoch: 1, step: 36500, eta: 92.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.78570 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 36600, eta: 92.48 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68160 images/sec
Training: 2023-10-17 15:40:01,942 - loss nan, lr: 0.025000, epoch: 1, step: 36600, eta: 92.48 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68160 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 36700, eta: 92.43 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.64114 images/sec
Training: 2023-10-17 15:40:35,753 - loss nan, lr: 0.025000, epoch: 1, step: 36700, eta: 92.43 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.64114 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 36800, eta: 92.39 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33783 sec, avg_samples: 64.00000, ips: 378.89354 images/sec
Training: 2023-10-17 15:41:09,542 - loss nan, lr: 0.025000, epoch: 1, step: 36800, eta: 92.39 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33783 sec, avg_samples: 64.00000, ips: 378.89354 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 36900, eta: 92.35 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67831 images/sec
Training: 2023-10-17 15:41:43,350 - loss nan, lr: 0.025000, epoch: 1, step: 36900, eta: 92.35 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67831 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 37000, eta: 92.30 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.79865 images/sec
Training: 2023-10-17 15:42:17,147 - loss nan, lr: 0.025000, epoch: 1, step: 37000, eta: 92.30 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.79865 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 37100, eta: 92.26 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66639 images/sec
Training: 2023-10-17 15:42:50,956 - loss nan, lr: 0.025000, epoch: 1, step: 37100, eta: 92.26 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66639 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 37200, eta: 92.22 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.79532 images/sec
Training: 2023-10-17 15:43:24,753 - loss nan, lr: 0.025000, epoch: 1, step: 37200, eta: 92.22 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.79532 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 37300, eta: 92.17 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33784 sec, avg_samples: 64.00000, ips: 378.87605 images/sec
Training: 2023-10-17 15:43:58,544 - loss nan, lr: 0.025000, epoch: 1, step: 37300, eta: 92.17 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33784 sec, avg_samples: 64.00000, ips: 378.87605 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 37400, eta: 92.13 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33807 sec, avg_samples: 64.00000, ips: 378.61826 images/sec
Training: 2023-10-17 15:44:32,357 - loss nan, lr: 0.025000, epoch: 1, step: 37400, eta: 92.13 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33807 sec, avg_samples: 64.00000, ips: 378.61826 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 37500, eta: 92.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.71967 images/sec
Training: 2023-10-17 15:45:06,161 - loss nan, lr: 0.025000, epoch: 1, step: 37500, eta: 92.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.71967 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 37600, eta: 92.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76537 images/sec
Training: 2023-10-17 15:45:39,961 - loss nan, lr: 0.025000, epoch: 1, step: 37600, eta: 92.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76537 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 37700, eta: 92.01 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33795 sec, avg_samples: 64.00000, ips: 378.74969 images/sec
Training: 2023-10-17 15:46:13,763 - loss nan, lr: 0.025000, epoch: 1, step: 37700, eta: 92.01 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33795 sec, avg_samples: 64.00000, ips: 378.74969 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 37800, eta: 91.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66695 images/sec
Training: 2023-10-17 15:46:47,571 - loss nan, lr: 0.025000, epoch: 1, step: 37800, eta: 91.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66695 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 37900, eta: 91.92 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33834 sec, avg_samples: 64.00000, ips: 378.32002 images/sec
Training: 2023-10-17 15:47:21,411 - loss nan, lr: 0.025000, epoch: 1, step: 37900, eta: 91.92 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33834 sec, avg_samples: 64.00000, ips: 378.32002 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 38000, eta: 91.88 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76267 images/sec
Training: 2023-10-17 15:47:55,212 - loss nan, lr: 0.025000, epoch: 1, step: 38000, eta: 91.88 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76267 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][38000]XNorm: 0.450090
Training: 2023-10-17 15:48:27,205 - [lfw][38000]XNorm: 0.450090
INFO:root:[lfw][38000]Accuracy-Flip: 0.79883+-0.01052
Training: 2023-10-17 15:48:27,205 - [lfw][38000]Accuracy-Flip: 0.79883+-0.01052
INFO:root:[lfw][38000]Accuracy-Highest: 0.80367
Training: 2023-10-17 15:48:27,205 - [lfw][38000]Accuracy-Highest: 0.80367
INFO:root:test time: 31.9933
Training: 2023-10-17 15:48:27,205 - test time: 31.9933
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][38000]XNorm: 0.434445
Training: 2023-10-17 15:49:04,329 - [cfp_fp][38000]XNorm: 0.434445
INFO:root:[cfp_fp][38000]Accuracy-Flip: 0.61343+-0.01244
Training: 2023-10-17 15:49:04,329 - [cfp_fp][38000]Accuracy-Flip: 0.61343+-0.01244
INFO:root:[cfp_fp][38000]Accuracy-Highest: 0.61443
Training: 2023-10-17 15:49:04,329 - [cfp_fp][38000]Accuracy-Highest: 0.61443
INFO:root:test time: 37.1240
Training: 2023-10-17 15:49:04,329 - test time: 37.1240
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][38000]XNorm: 0.455800
Training: 2023-10-17 15:49:36,334 - [agedb_30][38000]XNorm: 0.455800
INFO:root:[agedb_30][38000]Accuracy-Flip: 0.57633+-0.01975
Training: 2023-10-17 15:49:36,334 - [agedb_30][38000]Accuracy-Flip: 0.57633+-0.01975
INFO:root:[agedb_30][38000]Accuracy-Highest: 0.58350
Training: 2023-10-17 15:49:36,334 - [agedb_30][38000]Accuracy-Highest: 0.58350
INFO:root:test time: 32.0050
Training: 2023-10-17 15:49:36,334 - test time: 32.0050
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 38100, eta: 92.47 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33692 sec, avg_samples: 64.00000, ips: 379.91202 images/sec
Training: 2023-10-17 15:50:10,032 - loss nan, lr: 0.025000, epoch: 1, step: 38100, eta: 92.47 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33692 sec, avg_samples: 64.00000, ips: 379.91202 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 38200, eta: 92.43 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33725 sec, avg_samples: 64.00000, ips: 379.54278 images/sec
Training: 2023-10-17 15:50:43,763 - loss nan, lr: 0.025000, epoch: 1, step: 38200, eta: 92.43 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33725 sec, avg_samples: 64.00000, ips: 379.54278 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 38300, eta: 92.39 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33779 sec, avg_samples: 64.00000, ips: 378.93730 images/sec
Training: 2023-10-17 15:51:17,547 - loss nan, lr: 0.025000, epoch: 1, step: 38300, eta: 92.39 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33779 sec, avg_samples: 64.00000, ips: 378.93730 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 38400, eta: 92.34 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33787 sec, avg_samples: 64.00000, ips: 378.84109 images/sec
Training: 2023-10-17 15:51:51,341 - loss nan, lr: 0.025000, epoch: 1, step: 38400, eta: 92.34 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33787 sec, avg_samples: 64.00000, ips: 378.84109 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 38500, eta: 92.30 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33782 sec, avg_samples: 64.00000, ips: 378.90191 images/sec
Training: 2023-10-17 15:52:25,129 - loss nan, lr: 0.025000, epoch: 1, step: 38500, eta: 92.30 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33782 sec, avg_samples: 64.00000, ips: 378.90191 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 38600, eta: 92.26 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.78547 images/sec
Training: 2023-10-17 15:52:58,927 - loss nan, lr: 0.025000, epoch: 1, step: 38600, eta: 92.26 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.78547 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 38700, eta: 92.22 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.71844 images/sec
Training: 2023-10-17 15:53:32,731 - loss nan, lr: 0.025000, epoch: 1, step: 38700, eta: 92.22 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.71844 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 38800, eta: 92.18 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.70354 images/sec
Training: 2023-10-17 15:54:06,537 - loss nan, lr: 0.025000, epoch: 1, step: 38800, eta: 92.18 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.70354 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 38900, eta: 92.13 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33766 sec, avg_samples: 64.00000, ips: 379.08389 images/sec
Training: 2023-10-17 15:54:40,309 - loss nan, lr: 0.025000, epoch: 1, step: 38900, eta: 92.13 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33766 sec, avg_samples: 64.00000, ips: 379.08389 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 39000, eta: 92.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33782 sec, avg_samples: 64.00000, ips: 378.89615 images/sec
Training: 2023-10-17 15:55:14,097 - loss nan, lr: 0.025000, epoch: 1, step: 39000, eta: 92.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33782 sec, avg_samples: 64.00000, ips: 378.89615 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 39100, eta: 92.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76555 images/sec
Training: 2023-10-17 15:55:47,897 - loss nan, lr: 0.025000, epoch: 1, step: 39100, eta: 92.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76555 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 39200, eta: 92.01 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33797 sec, avg_samples: 64.00000, ips: 378.73143 images/sec
Training: 2023-10-17 15:56:21,700 - loss nan, lr: 0.025000, epoch: 1, step: 39200, eta: 92.01 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33797 sec, avg_samples: 64.00000, ips: 378.73143 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 39300, eta: 91.97 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.87061 images/sec
Training: 2023-10-17 15:56:55,491 - loss nan, lr: 0.025000, epoch: 1, step: 39300, eta: 91.97 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.87061 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 39400, eta: 91.93 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33782 sec, avg_samples: 64.00000, ips: 378.90150 images/sec
Training: 2023-10-17 15:57:29,279 - loss nan, lr: 0.025000, epoch: 1, step: 39400, eta: 91.93 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33782 sec, avg_samples: 64.00000, ips: 378.90150 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 39500, eta: 91.89 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.80254 images/sec
Training: 2023-10-17 15:58:03,076 - loss nan, lr: 0.025000, epoch: 1, step: 39500, eta: 91.89 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.80254 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 39600, eta: 91.85 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.82051 images/sec
Training: 2023-10-17 15:58:36,871 - loss nan, lr: 0.025000, epoch: 1, step: 39600, eta: 91.85 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.82051 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 39700, eta: 91.81 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.82303 images/sec
Training: 2023-10-17 15:59:10,666 - loss nan, lr: 0.025000, epoch: 1, step: 39700, eta: 91.81 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.82303 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 39800, eta: 91.77 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33782 sec, avg_samples: 64.00000, ips: 378.89800 images/sec
Training: 2023-10-17 15:59:44,454 - loss nan, lr: 0.025000, epoch: 1, step: 39800, eta: 91.77 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33782 sec, avg_samples: 64.00000, ips: 378.89800 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 39900, eta: 91.73 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68339 images/sec
Training: 2023-10-17 16:00:18,261 - loss nan, lr: 0.025000, epoch: 1, step: 39900, eta: 91.73 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68339 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 40000, eta: 91.69 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.80577 images/sec
Training: 2023-10-17 16:00:52,058 - loss nan, lr: 0.025000, epoch: 1, step: 40000, eta: 91.69 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.80577 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][40000]XNorm: 0.350042
Training: 2023-10-17 16:01:24,048 - [lfw][40000]XNorm: 0.350042
INFO:root:[lfw][40000]Accuracy-Flip: 0.80483+-0.00855
Training: 2023-10-17 16:01:24,048 - [lfw][40000]Accuracy-Flip: 0.80483+-0.00855
INFO:root:[lfw][40000]Accuracy-Highest: 0.80483
Training: 2023-10-17 16:01:24,048 - [lfw][40000]Accuracy-Highest: 0.80483
INFO:root:test time: 31.9901
Training: 2023-10-17 16:01:24,048 - test time: 31.9901
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][40000]XNorm: 0.338407
Training: 2023-10-17 16:02:01,174 - [cfp_fp][40000]XNorm: 0.338407
INFO:root:[cfp_fp][40000]Accuracy-Flip: 0.61300+-0.01138
Training: 2023-10-17 16:02:01,174 - [cfp_fp][40000]Accuracy-Flip: 0.61300+-0.01138
INFO:root:[cfp_fp][40000]Accuracy-Highest: 0.61443
Training: 2023-10-17 16:02:01,174 - [cfp_fp][40000]Accuracy-Highest: 0.61443
INFO:root:test time: 37.1262
Training: 2023-10-17 16:02:01,174 - test time: 37.1262
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][40000]XNorm: 0.354928
Training: 2023-10-17 16:02:33,177 - [agedb_30][40000]XNorm: 0.354928
INFO:root:[agedb_30][40000]Accuracy-Flip: 0.57900+-0.01818
Training: 2023-10-17 16:02:33,177 - [agedb_30][40000]Accuracy-Flip: 0.57900+-0.01818
INFO:root:[agedb_30][40000]Accuracy-Highest: 0.58350
Training: 2023-10-17 16:02:33,177 - [agedb_30][40000]Accuracy-Highest: 0.58350
INFO:root:test time: 32.0028
Training: 2023-10-17 16:02:33,177 - test time: 32.0028
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 40100, eta: 92.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33724 sec, avg_samples: 64.00000, ips: 379.55033 images/sec
Training: 2023-10-17 16:03:06,907 - loss nan, lr: 0.025000, epoch: 1, step: 40100, eta: 92.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33724 sec, avg_samples: 64.00000, ips: 379.55033 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 40200, eta: 92.20 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.80812 images/sec
Training: 2023-10-17 16:03:40,703 - loss nan, lr: 0.025000, epoch: 1, step: 40200, eta: 92.20 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.80812 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 40300, eta: 92.16 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33780 sec, avg_samples: 64.00000, ips: 378.91885 images/sec
Training: 2023-10-17 16:04:14,489 - loss nan, lr: 0.025000, epoch: 1, step: 40300, eta: 92.16 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33780 sec, avg_samples: 64.00000, ips: 378.91885 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 40400, eta: 92.12 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33793 sec, avg_samples: 64.00000, ips: 378.77888 images/sec
Training: 2023-10-17 16:04:48,288 - loss nan, lr: 0.025000, epoch: 1, step: 40400, eta: 92.12 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33793 sec, avg_samples: 64.00000, ips: 378.77888 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 40500, eta: 92.08 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33835 sec, avg_samples: 64.00000, ips: 378.30631 images/sec
Training: 2023-10-17 16:05:22,129 - loss nan, lr: 0.025000, epoch: 1, step: 40500, eta: 92.08 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33835 sec, avg_samples: 64.00000, ips: 378.30631 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 40600, eta: 92.04 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33842 sec, avg_samples: 64.00000, ips: 378.22900 images/sec
Training: 2023-10-17 16:05:55,977 - loss nan, lr: 0.025000, epoch: 1, step: 40600, eta: 92.04 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33842 sec, avg_samples: 64.00000, ips: 378.22900 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 40700, eta: 92.00 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33840 sec, avg_samples: 64.00000, ips: 378.25321 images/sec
Training: 2023-10-17 16:06:29,823 - loss nan, lr: 0.025000, epoch: 1, step: 40700, eta: 92.00 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33840 sec, avg_samples: 64.00000, ips: 378.25321 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 40800, eta: 91.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33847 sec, avg_samples: 64.00000, ips: 378.16696 images/sec
Training: 2023-10-17 16:07:03,676 - loss nan, lr: 0.025000, epoch: 1, step: 40800, eta: 91.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33847 sec, avg_samples: 64.00000, ips: 378.16696 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 40900, eta: 91.92 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33848 sec, avg_samples: 64.00000, ips: 378.16661 images/sec
Training: 2023-10-17 16:07:37,530 - loss nan, lr: 0.025000, epoch: 1, step: 40900, eta: 91.92 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33848 sec, avg_samples: 64.00000, ips: 378.16661 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 41000, eta: 91.88 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33853 sec, avg_samples: 64.00000, ips: 378.10889 images/sec
Training: 2023-10-17 16:08:11,389 - loss nan, lr: 0.025000, epoch: 1, step: 41000, eta: 91.88 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33853 sec, avg_samples: 64.00000, ips: 378.10889 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 41100, eta: 91.84 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33830 sec, avg_samples: 64.00000, ips: 378.36484 images/sec
Training: 2023-10-17 16:08:45,224 - loss nan, lr: 0.025000, epoch: 1, step: 41100, eta: 91.84 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33830 sec, avg_samples: 64.00000, ips: 378.36484 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 41200, eta: 91.81 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33832 sec, avg_samples: 64.00000, ips: 378.33540 images/sec
Training: 2023-10-17 16:09:19,063 - loss nan, lr: 0.025000, epoch: 1, step: 41200, eta: 91.81 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33832 sec, avg_samples: 64.00000, ips: 378.33540 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 41300, eta: 91.77 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33837 sec, avg_samples: 64.00000, ips: 378.28033 images/sec
Training: 2023-10-17 16:09:52,906 - loss nan, lr: 0.025000, epoch: 1, step: 41300, eta: 91.77 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33837 sec, avg_samples: 64.00000, ips: 378.28033 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 41400, eta: 91.73 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33854 sec, avg_samples: 64.00000, ips: 378.09160 images/sec
Training: 2023-10-17 16:10:26,767 - loss nan, lr: 0.025000, epoch: 1, step: 41400, eta: 91.73 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33854 sec, avg_samples: 64.00000, ips: 378.09160 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 41500, eta: 91.69 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33835 sec, avg_samples: 64.00000, ips: 378.30354 images/sec
Training: 2023-10-17 16:11:00,608 - loss nan, lr: 0.025000, epoch: 1, step: 41500, eta: 91.69 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33835 sec, avg_samples: 64.00000, ips: 378.30354 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 41600, eta: 91.65 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33842 sec, avg_samples: 64.00000, ips: 378.22842 images/sec
Training: 2023-10-17 16:11:34,456 - loss nan, lr: 0.025000, epoch: 1, step: 41600, eta: 91.65 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33842 sec, avg_samples: 64.00000, ips: 378.22842 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 41700, eta: 91.61 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33824 sec, avg_samples: 64.00000, ips: 378.42393 images/sec
Training: 2023-10-17 16:12:08,287 - loss nan, lr: 0.025000, epoch: 1, step: 41700, eta: 91.61 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33824 sec, avg_samples: 64.00000, ips: 378.42393 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 41800, eta: 91.57 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33837 sec, avg_samples: 64.00000, ips: 378.28504 images/sec
Training: 2023-10-17 16:12:42,130 - loss nan, lr: 0.025000, epoch: 1, step: 41800, eta: 91.57 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33837 sec, avg_samples: 64.00000, ips: 378.28504 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 41900, eta: 91.53 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33841 sec, avg_samples: 64.00000, ips: 378.24264 images/sec
Training: 2023-10-17 16:13:15,976 - loss nan, lr: 0.025000, epoch: 1, step: 41900, eta: 91.53 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33841 sec, avg_samples: 64.00000, ips: 378.24264 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 42000, eta: 91.50 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33859 sec, avg_samples: 64.00000, ips: 378.03697 images/sec
Training: 2023-10-17 16:13:49,842 - loss nan, lr: 0.025000, epoch: 1, step: 42000, eta: 91.50 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33859 sec, avg_samples: 64.00000, ips: 378.03697 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][42000]XNorm: 0.272183
Training: 2023-10-17 16:14:21,889 - [lfw][42000]XNorm: 0.272183
INFO:root:[lfw][42000]Accuracy-Flip: 0.80800+-0.01169
Training: 2023-10-17 16:14:21,889 - [lfw][42000]Accuracy-Flip: 0.80800+-0.01169
INFO:root:[lfw][42000]Accuracy-Highest: 0.80800
Training: 2023-10-17 16:14:21,889 - [lfw][42000]Accuracy-Highest: 0.80800
INFO:root:test time: 32.0472
Training: 2023-10-17 16:14:21,889 - test time: 32.0472
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][42000]XNorm: 0.264205
Training: 2023-10-17 16:14:59,082 - [cfp_fp][42000]XNorm: 0.264205
INFO:root:[cfp_fp][42000]Accuracy-Flip: 0.61214+-0.01236
Training: 2023-10-17 16:14:59,083 - [cfp_fp][42000]Accuracy-Flip: 0.61214+-0.01236
INFO:root:[cfp_fp][42000]Accuracy-Highest: 0.61443
Training: 2023-10-17 16:14:59,083 - [cfp_fp][42000]Accuracy-Highest: 0.61443
INFO:root:test time: 37.1935
Training: 2023-10-17 16:14:59,083 - test time: 37.1935
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][42000]XNorm: 0.275820
Training: 2023-10-17 16:15:31,128 - [agedb_30][42000]XNorm: 0.275820
INFO:root:[agedb_30][42000]Accuracy-Flip: 0.58100+-0.01960
Training: 2023-10-17 16:15:31,128 - [agedb_30][42000]Accuracy-Flip: 0.58100+-0.01960
INFO:root:[agedb_30][42000]Accuracy-Highest: 0.58350
Training: 2023-10-17 16:15:31,128 - [agedb_30][42000]Accuracy-Highest: 0.58350
INFO:root:test time: 32.0457
Training: 2023-10-17 16:15:31,128 - test time: 32.0457
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 42100, eta: 92.03 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33693 sec, avg_samples: 64.00000, ips: 379.90508 images/sec
Training: 2023-10-17 16:16:04,827 - loss nan, lr: 0.025000, epoch: 1, step: 42100, eta: 92.03 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33693 sec, avg_samples: 64.00000, ips: 379.90508 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 42200, eta: 91.99 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.72161 images/sec
Training: 2023-10-17 16:16:38,631 - loss nan, lr: 0.025000, epoch: 1, step: 42200, eta: 91.99 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.72161 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 42300, eta: 91.95 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33839 sec, avg_samples: 64.00000, ips: 378.26188 images/sec
Training: 2023-10-17 16:17:12,475 - loss nan, lr: 0.025000, epoch: 1, step: 42300, eta: 91.95 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33839 sec, avg_samples: 64.00000, ips: 378.26188 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 42400, eta: 91.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33836 sec, avg_samples: 64.00000, ips: 378.28981 images/sec
Training: 2023-10-17 16:17:46,318 - loss nan, lr: 0.025000, epoch: 1, step: 42400, eta: 91.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33836 sec, avg_samples: 64.00000, ips: 378.28981 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 42500, eta: 91.87 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33842 sec, avg_samples: 64.00000, ips: 378.22786 images/sec
Training: 2023-10-17 16:18:20,166 - loss nan, lr: 0.025000, epoch: 1, step: 42500, eta: 91.87 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33842 sec, avg_samples: 64.00000, ips: 378.22786 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 42600, eta: 91.83 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33826 sec, avg_samples: 64.00000, ips: 378.40909 images/sec
Training: 2023-10-17 16:18:53,998 - loss nan, lr: 0.025000, epoch: 1, step: 42600, eta: 91.83 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33826 sec, avg_samples: 64.00000, ips: 378.40909 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 42700, eta: 91.79 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33843 sec, avg_samples: 64.00000, ips: 378.21726 images/sec
Training: 2023-10-17 16:19:27,847 - loss nan, lr: 0.025000, epoch: 1, step: 42700, eta: 91.79 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33843 sec, avg_samples: 64.00000, ips: 378.21726 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 42800, eta: 91.76 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33844 sec, avg_samples: 64.00000, ips: 378.20689 images/sec
Training: 2023-10-17 16:20:01,696 - loss nan, lr: 0.025000, epoch: 1, step: 42800, eta: 91.76 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33844 sec, avg_samples: 64.00000, ips: 378.20689 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 42900, eta: 91.72 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33835 sec, avg_samples: 64.00000, ips: 378.30710 images/sec
Training: 2023-10-17 16:20:35,537 - loss nan, lr: 0.025000, epoch: 1, step: 42900, eta: 91.72 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33835 sec, avg_samples: 64.00000, ips: 378.30710 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 43000, eta: 91.68 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33842 sec, avg_samples: 64.00000, ips: 378.22633 images/sec
Training: 2023-10-17 16:21:09,385 - loss nan, lr: 0.025000, epoch: 1, step: 43000, eta: 91.68 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33842 sec, avg_samples: 64.00000, ips: 378.22633 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 43100, eta: 91.64 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33836 sec, avg_samples: 64.00000, ips: 378.29875 images/sec
Training: 2023-10-17 16:21:43,227 - loss nan, lr: 0.025000, epoch: 1, step: 43100, eta: 91.64 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33836 sec, avg_samples: 64.00000, ips: 378.29875 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 43200, eta: 91.60 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33840 sec, avg_samples: 64.00000, ips: 378.25182 images/sec
Training: 2023-10-17 16:22:17,073 - loss nan, lr: 0.025000, epoch: 1, step: 43200, eta: 91.60 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33840 sec, avg_samples: 64.00000, ips: 378.25182 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 43300, eta: 91.57 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33837 sec, avg_samples: 64.00000, ips: 378.28617 images/sec
Training: 2023-10-17 16:22:50,916 - loss nan, lr: 0.025000, epoch: 1, step: 43300, eta: 91.57 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33837 sec, avg_samples: 64.00000, ips: 378.28617 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 43400, eta: 91.53 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33842 sec, avg_samples: 64.00000, ips: 378.22906 images/sec
Training: 2023-10-17 16:23:24,763 - loss nan, lr: 0.025000, epoch: 1, step: 43400, eta: 91.53 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33842 sec, avg_samples: 64.00000, ips: 378.22906 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 43500, eta: 91.49 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33838 sec, avg_samples: 64.00000, ips: 378.26929 images/sec
Training: 2023-10-17 16:23:58,608 - loss nan, lr: 0.025000, epoch: 1, step: 43500, eta: 91.49 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33838 sec, avg_samples: 64.00000, ips: 378.26929 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 43600, eta: 91.45 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33829 sec, avg_samples: 64.00000, ips: 378.37684 images/sec
Training: 2023-10-17 16:24:32,442 - loss nan, lr: 0.025000, epoch: 1, step: 43600, eta: 91.45 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33829 sec, avg_samples: 64.00000, ips: 378.37684 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 43700, eta: 91.42 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33824 sec, avg_samples: 64.00000, ips: 378.43326 images/sec
Training: 2023-10-17 16:25:06,272 - loss nan, lr: 0.025000, epoch: 1, step: 43700, eta: 91.42 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33824 sec, avg_samples: 64.00000, ips: 378.43326 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 43800, eta: 91.38 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33816 sec, avg_samples: 64.00000, ips: 378.52120 images/sec
Training: 2023-10-17 16:25:40,094 - loss nan, lr: 0.025000, epoch: 1, step: 43800, eta: 91.38 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33816 sec, avg_samples: 64.00000, ips: 378.52120 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 43900, eta: 91.34 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33848 sec, avg_samples: 64.00000, ips: 378.15665 images/sec
Training: 2023-10-17 16:26:13,948 - loss nan, lr: 0.025000, epoch: 1, step: 43900, eta: 91.34 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33848 sec, avg_samples: 64.00000, ips: 378.15665 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 44000, eta: 91.30 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33828 sec, avg_samples: 64.00000, ips: 378.38983 images/sec
Training: 2023-10-17 16:26:47,781 - loss nan, lr: 0.025000, epoch: 1, step: 44000, eta: 91.30 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33828 sec, avg_samples: 64.00000, ips: 378.38983 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][44000]XNorm: 0.211749
Training: 2023-10-17 16:27:19,828 - [lfw][44000]XNorm: 0.211749
INFO:root:[lfw][44000]Accuracy-Flip: 0.80967+-0.01337
Training: 2023-10-17 16:27:19,828 - [lfw][44000]Accuracy-Flip: 0.80967+-0.01337
INFO:root:[lfw][44000]Accuracy-Highest: 0.80967
Training: 2023-10-17 16:27:19,828 - [lfw][44000]Accuracy-Highest: 0.80967
INFO:root:test time: 32.0466
Training: 2023-10-17 16:27:19,828 - test time: 32.0466
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][44000]XNorm: 0.207147
Training: 2023-10-17 16:27:56,975 - [cfp_fp][44000]XNorm: 0.207147
INFO:root:[cfp_fp][44000]Accuracy-Flip: 0.60929+-0.01118
Training: 2023-10-17 16:27:56,975 - [cfp_fp][44000]Accuracy-Flip: 0.60929+-0.01118
INFO:root:[cfp_fp][44000]Accuracy-Highest: 0.61443
Training: 2023-10-17 16:27:56,975 - [cfp_fp][44000]Accuracy-Highest: 0.61443
INFO:root:test time: 37.1471
Training: 2023-10-17 16:27:56,975 - test time: 37.1471
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][44000]XNorm: 0.215320
Training: 2023-10-17 16:28:28,976 - [agedb_30][44000]XNorm: 0.215320
INFO:root:[agedb_30][44000]Accuracy-Flip: 0.57833+-0.01462
Training: 2023-10-17 16:28:28,976 - [agedb_30][44000]Accuracy-Flip: 0.57833+-0.01462
INFO:root:[agedb_30][44000]Accuracy-Highest: 0.58350
Training: 2023-10-17 16:28:28,976 - [agedb_30][44000]Accuracy-Highest: 0.58350
INFO:root:test time: 32.0006
Training: 2023-10-17 16:28:28,976 - test time: 32.0006
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 44100, eta: 91.81 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33721 sec, avg_samples: 64.00000, ips: 379.58039 images/sec
Training: 2023-10-17 16:29:02,703 - loss nan, lr: 0.025000, epoch: 1, step: 44100, eta: 91.81 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33721 sec, avg_samples: 64.00000, ips: 379.58039 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 44200, eta: 91.77 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.79315 images/sec
Training: 2023-10-17 16:29:36,501 - loss nan, lr: 0.025000, epoch: 1, step: 44200, eta: 91.77 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.79315 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 44300, eta: 91.73 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33787 sec, avg_samples: 64.00000, ips: 378.84684 images/sec
Training: 2023-10-17 16:30:10,293 - loss nan, lr: 0.025000, epoch: 1, step: 44300, eta: 91.73 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33787 sec, avg_samples: 64.00000, ips: 378.84684 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 44400, eta: 91.69 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.79858 images/sec
Training: 2023-10-17 16:30:44,090 - loss nan, lr: 0.025000, epoch: 1, step: 44400, eta: 91.69 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.79858 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 44500, eta: 91.66 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.61006 images/sec
Training: 2023-10-17 16:31:17,904 - loss nan, lr: 0.025000, epoch: 1, step: 44500, eta: 91.66 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.61006 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 44600, eta: 91.62 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33839 sec, avg_samples: 64.00000, ips: 378.25920 images/sec
Training: 2023-10-17 16:31:51,749 - loss nan, lr: 0.025000, epoch: 1, step: 44600, eta: 91.62 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33839 sec, avg_samples: 64.00000, ips: 378.25920 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 44700, eta: 91.58 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33835 sec, avg_samples: 64.00000, ips: 378.30847 images/sec
Training: 2023-10-17 16:32:25,590 - loss nan, lr: 0.025000, epoch: 1, step: 44700, eta: 91.58 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33835 sec, avg_samples: 64.00000, ips: 378.30847 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 44800, eta: 91.54 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33842 sec, avg_samples: 64.00000, ips: 378.22703 images/sec
Training: 2023-10-17 16:32:59,438 - loss nan, lr: 0.025000, epoch: 1, step: 44800, eta: 91.54 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33842 sec, avg_samples: 64.00000, ips: 378.22703 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 44900, eta: 91.51 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33836 sec, avg_samples: 64.00000, ips: 378.29505 images/sec
Training: 2023-10-17 16:33:33,280 - loss nan, lr: 0.025000, epoch: 1, step: 44900, eta: 91.51 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33836 sec, avg_samples: 64.00000, ips: 378.29505 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 45000, eta: 91.47 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33855 sec, avg_samples: 64.00000, ips: 378.08761 images/sec
Training: 2023-10-17 16:34:07,141 - loss nan, lr: 0.025000, epoch: 1, step: 45000, eta: 91.47 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33855 sec, avg_samples: 64.00000, ips: 378.08761 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 45100, eta: 91.43 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33846 sec, avg_samples: 64.00000, ips: 378.18098 images/sec
Training: 2023-10-17 16:34:40,993 - loss nan, lr: 0.025000, epoch: 1, step: 45100, eta: 91.43 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33846 sec, avg_samples: 64.00000, ips: 378.18098 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 45200, eta: 91.40 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33847 sec, avg_samples: 64.00000, ips: 378.17162 images/sec
Training: 2023-10-17 16:35:14,846 - loss nan, lr: 0.025000, epoch: 1, step: 45200, eta: 91.40 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33847 sec, avg_samples: 64.00000, ips: 378.17162 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 45300, eta: 91.36 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33840 sec, avg_samples: 64.00000, ips: 378.25590 images/sec
Training: 2023-10-17 16:35:48,692 - loss nan, lr: 0.025000, epoch: 1, step: 45300, eta: 91.36 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33840 sec, avg_samples: 64.00000, ips: 378.25590 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 45400, eta: 91.33 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33838 sec, avg_samples: 64.00000, ips: 378.27389 images/sec
Training: 2023-10-17 16:36:22,536 - loss nan, lr: 0.025000, epoch: 1, step: 45400, eta: 91.33 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33838 sec, avg_samples: 64.00000, ips: 378.27389 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 45500, eta: 91.29 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33851 sec, avg_samples: 64.00000, ips: 378.13076 images/sec
Training: 2023-10-17 16:36:56,393 - loss nan, lr: 0.025000, epoch: 1, step: 45500, eta: 91.29 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33851 sec, avg_samples: 64.00000, ips: 378.13076 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 45600, eta: 91.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33857 sec, avg_samples: 64.00000, ips: 378.06380 images/sec
Training: 2023-10-17 16:37:30,255 - loss nan, lr: 0.025000, epoch: 1, step: 45600, eta: 91.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33857 sec, avg_samples: 64.00000, ips: 378.06380 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 45700, eta: 91.22 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33855 sec, avg_samples: 64.00000, ips: 378.08858 images/sec
Training: 2023-10-17 16:38:04,116 - loss nan, lr: 0.025000, epoch: 1, step: 45700, eta: 91.22 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33855 sec, avg_samples: 64.00000, ips: 378.08858 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 45800, eta: 91.18 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33843 sec, avg_samples: 64.00000, ips: 378.21574 images/sec
Training: 2023-10-17 16:38:37,965 - loss nan, lr: 0.025000, epoch: 1, step: 45800, eta: 91.18 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33843 sec, avg_samples: 64.00000, ips: 378.21574 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 45900, eta: 91.15 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33835 sec, avg_samples: 64.00000, ips: 378.30448 images/sec
Training: 2023-10-17 16:39:11,806 - loss nan, lr: 0.025000, epoch: 1, step: 45900, eta: 91.15 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33835 sec, avg_samples: 64.00000, ips: 378.30448 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 46000, eta: 91.11 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33842 sec, avg_samples: 64.00000, ips: 378.22469 images/sec
Training: 2023-10-17 16:39:45,655 - loss nan, lr: 0.025000, epoch: 1, step: 46000, eta: 91.11 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33842 sec, avg_samples: 64.00000, ips: 378.22469 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][46000]XNorm: 0.164989
Training: 2023-10-17 16:40:17,669 - [lfw][46000]XNorm: 0.164989
INFO:root:[lfw][46000]Accuracy-Flip: 0.80917+-0.01160
Training: 2023-10-17 16:40:17,669 - [lfw][46000]Accuracy-Flip: 0.80917+-0.01160
INFO:root:[lfw][46000]Accuracy-Highest: 0.80967
Training: 2023-10-17 16:40:17,669 - [lfw][46000]Accuracy-Highest: 0.80967
INFO:root:test time: 32.0138
Training: 2023-10-17 16:40:17,669 - test time: 32.0138
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][46000]XNorm: 0.162577
Training: 2023-10-17 16:40:54,804 - [cfp_fp][46000]XNorm: 0.162577
INFO:root:[cfp_fp][46000]Accuracy-Flip: 0.60986+-0.01466
Training: 2023-10-17 16:40:54,804 - [cfp_fp][46000]Accuracy-Flip: 0.60986+-0.01466
INFO:root:[cfp_fp][46000]Accuracy-Highest: 0.61443
Training: 2023-10-17 16:40:54,804 - [cfp_fp][46000]Accuracy-Highest: 0.61443
INFO:root:test time: 37.1355
Training: 2023-10-17 16:40:54,804 - test time: 37.1355
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][46000]XNorm: 0.168559
Training: 2023-10-17 16:41:26,817 - [agedb_30][46000]XNorm: 0.168559
INFO:root:[agedb_30][46000]Accuracy-Flip: 0.57100+-0.01944
Training: 2023-10-17 16:41:26,817 - [agedb_30][46000]Accuracy-Flip: 0.57100+-0.01944
INFO:root:[agedb_30][46000]Accuracy-Highest: 0.58350
Training: 2023-10-17 16:41:26,817 - [agedb_30][46000]Accuracy-Highest: 0.58350
INFO:root:test time: 32.0130
Training: 2023-10-17 16:41:26,817 - test time: 32.0130
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 46100, eta: 91.59 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33721 sec, avg_samples: 64.00000, ips: 379.58606 images/sec
Training: 2023-10-17 16:42:00,544 - loss nan, lr: 0.025000, epoch: 1, step: 46100, eta: 91.59 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33721 sec, avg_samples: 64.00000, ips: 379.58606 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 46200, eta: 91.55 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.60744 images/sec
Training: 2023-10-17 16:42:34,358 - loss nan, lr: 0.025000, epoch: 1, step: 46200, eta: 91.55 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.60744 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 46300, eta: 91.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.63664 images/sec
Training: 2023-10-17 16:43:08,169 - loss nan, lr: 0.025000, epoch: 1, step: 46300, eta: 91.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.63664 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 46400, eta: 91.48 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.82311 images/sec
Training: 2023-10-17 16:43:41,964 - loss nan, lr: 0.025000, epoch: 1, step: 46400, eta: 91.48 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.82311 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 46500, eta: 91.44 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.80962 images/sec
Training: 2023-10-17 16:44:15,760 - loss nan, lr: 0.025000, epoch: 1, step: 46500, eta: 91.44 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.80962 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 46600, eta: 91.41 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.78340 images/sec
Training: 2023-10-17 16:44:49,558 - loss nan, lr: 0.025000, epoch: 1, step: 46600, eta: 91.41 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.78340 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 46700, eta: 91.37 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33812 sec, avg_samples: 64.00000, ips: 378.55936 images/sec
Training: 2023-10-17 16:45:23,376 - loss nan, lr: 0.025000, epoch: 1, step: 46700, eta: 91.37 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33812 sec, avg_samples: 64.00000, ips: 378.55936 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 46800, eta: 91.33 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33816 sec, avg_samples: 64.00000, ips: 378.51642 images/sec
Training: 2023-10-17 16:45:57,199 - loss nan, lr: 0.025000, epoch: 1, step: 46800, eta: 91.33 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33816 sec, avg_samples: 64.00000, ips: 378.51642 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 46900, eta: 91.30 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33811 sec, avg_samples: 64.00000, ips: 378.56944 images/sec
Training: 2023-10-17 16:46:31,016 - loss nan, lr: 0.025000, epoch: 1, step: 46900, eta: 91.30 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33811 sec, avg_samples: 64.00000, ips: 378.56944 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 47000, eta: 91.26 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.80772 images/sec
Training: 2023-10-17 16:47:04,812 - loss nan, lr: 0.025000, epoch: 1, step: 47000, eta: 91.26 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.80772 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 47100, eta: 91.23 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68189 images/sec
Training: 2023-10-17 16:47:38,619 - loss nan, lr: 0.025000, epoch: 1, step: 47100, eta: 91.23 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68189 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 47200, eta: 91.19 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33811 sec, avg_samples: 64.00000, ips: 378.57515 images/sec
Training: 2023-10-17 16:48:12,436 - loss nan, lr: 0.025000, epoch: 1, step: 47200, eta: 91.19 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33811 sec, avg_samples: 64.00000, ips: 378.57515 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 47300, eta: 91.16 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33804 sec, avg_samples: 64.00000, ips: 378.65564 images/sec
Training: 2023-10-17 16:48:46,246 - loss nan, lr: 0.025000, epoch: 1, step: 47300, eta: 91.16 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33804 sec, avg_samples: 64.00000, ips: 378.65564 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 47400, eta: 91.12 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33813 sec, avg_samples: 64.00000, ips: 378.54815 images/sec
Training: 2023-10-17 16:49:20,065 - loss nan, lr: 0.025000, epoch: 1, step: 47400, eta: 91.12 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33813 sec, avg_samples: 64.00000, ips: 378.54815 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 47500, eta: 91.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33871 sec, avg_samples: 64.00000, ips: 377.90472 images/sec
Training: 2023-10-17 16:49:53,942 - loss nan, lr: 0.025000, epoch: 1, step: 47500, eta: 91.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33871 sec, avg_samples: 64.00000, ips: 377.90472 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 47600, eta: 91.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33851 sec, avg_samples: 64.00000, ips: 378.13300 images/sec
Training: 2023-10-17 16:50:27,799 - loss nan, lr: 0.025000, epoch: 1, step: 47600, eta: 91.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33851 sec, avg_samples: 64.00000, ips: 378.13300 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 47700, eta: 91.02 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33853 sec, avg_samples: 64.00000, ips: 378.10862 images/sec
Training: 2023-10-17 16:51:01,658 - loss nan, lr: 0.025000, epoch: 1, step: 47700, eta: 91.02 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33853 sec, avg_samples: 64.00000, ips: 378.10862 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 47800, eta: 90.98 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33848 sec, avg_samples: 64.00000, ips: 378.16154 images/sec
Training: 2023-10-17 16:51:35,512 - loss nan, lr: 0.025000, epoch: 1, step: 47800, eta: 90.98 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33848 sec, avg_samples: 64.00000, ips: 378.16154 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 47900, eta: 90.95 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33842 sec, avg_samples: 64.00000, ips: 378.22643 images/sec
Training: 2023-10-17 16:52:09,360 - loss nan, lr: 0.025000, epoch: 1, step: 47900, eta: 90.95 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33842 sec, avg_samples: 64.00000, ips: 378.22643 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 48000, eta: 90.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33850 sec, avg_samples: 64.00000, ips: 378.13549 images/sec
Training: 2023-10-17 16:52:43,217 - loss nan, lr: 0.025000, epoch: 1, step: 48000, eta: 90.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33850 sec, avg_samples: 64.00000, ips: 378.13549 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][48000]XNorm: 0.127538
Training: 2023-10-17 16:53:15,286 - [lfw][48000]XNorm: 0.127538
INFO:root:[lfw][48000]Accuracy-Flip: 0.81183+-0.01363
Training: 2023-10-17 16:53:15,286 - [lfw][48000]Accuracy-Flip: 0.81183+-0.01363
INFO:root:[lfw][48000]Accuracy-Highest: 0.81183
Training: 2023-10-17 16:53:15,286 - [lfw][48000]Accuracy-Highest: 0.81183
INFO:root:test time: 32.0688
Training: 2023-10-17 16:53:15,286 - test time: 32.0688
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][48000]XNorm: 0.128073
Training: 2023-10-17 16:53:52,498 - [cfp_fp][48000]XNorm: 0.128073
INFO:root:[cfp_fp][48000]Accuracy-Flip: 0.60243+-0.01390
Training: 2023-10-17 16:53:52,498 - [cfp_fp][48000]Accuracy-Flip: 0.60243+-0.01390
INFO:root:[cfp_fp][48000]Accuracy-Highest: 0.61443
Training: 2023-10-17 16:53:52,498 - [cfp_fp][48000]Accuracy-Highest: 0.61443
INFO:root:test time: 37.2122
Training: 2023-10-17 16:53:52,498 - test time: 37.2122
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][48000]XNorm: 0.130698
Training: 2023-10-17 16:54:24,549 - [agedb_30][48000]XNorm: 0.130698
INFO:root:[agedb_30][48000]Accuracy-Flip: 0.57950+-0.01515
Training: 2023-10-17 16:54:24,549 - [agedb_30][48000]Accuracy-Flip: 0.57950+-0.01515
INFO:root:[agedb_30][48000]Accuracy-Highest: 0.58350
Training: 2023-10-17 16:54:24,549 - [agedb_30][48000]Accuracy-Highest: 0.58350
INFO:root:test time: 32.0510
Training: 2023-10-17 16:54:24,549 - test time: 32.0510
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 48100, eta: 91.37 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33727 sec, avg_samples: 64.00000, ips: 379.51581 images/sec
Training: 2023-10-17 16:54:58,282 - loss nan, lr: 0.025000, epoch: 1, step: 48100, eta: 91.37 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33727 sec, avg_samples: 64.00000, ips: 379.51581 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 48200, eta: 91.34 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33807 sec, avg_samples: 64.00000, ips: 378.61559 images/sec
Training: 2023-10-17 16:55:32,095 - loss nan, lr: 0.025000, epoch: 1, step: 48200, eta: 91.34 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33807 sec, avg_samples: 64.00000, ips: 378.61559 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 48300, eta: 91.30 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.64555 images/sec
Training: 2023-10-17 16:56:05,906 - loss nan, lr: 0.025000, epoch: 1, step: 48300, eta: 91.30 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.64555 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 48400, eta: 91.26 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.85465 images/sec
Training: 2023-10-17 16:56:39,698 - loss nan, lr: 0.025000, epoch: 1, step: 48400, eta: 91.26 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.85465 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 48500, eta: 91.23 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33778 sec, avg_samples: 64.00000, ips: 378.94678 images/sec
Training: 2023-10-17 16:57:13,482 - loss nan, lr: 0.025000, epoch: 1, step: 48500, eta: 91.23 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33778 sec, avg_samples: 64.00000, ips: 378.94678 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 48600, eta: 91.19 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33814 sec, avg_samples: 64.00000, ips: 378.54102 images/sec
Training: 2023-10-17 16:57:47,302 - loss nan, lr: 0.025000, epoch: 1, step: 48600, eta: 91.19 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33814 sec, avg_samples: 64.00000, ips: 378.54102 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 48700, eta: 91.16 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33804 sec, avg_samples: 64.00000, ips: 378.65830 images/sec
Training: 2023-10-17 16:58:21,111 - loss nan, lr: 0.025000, epoch: 1, step: 48700, eta: 91.16 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33804 sec, avg_samples: 64.00000, ips: 378.65830 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 48800, eta: 91.12 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33797 sec, avg_samples: 64.00000, ips: 378.73184 images/sec
Training: 2023-10-17 16:58:54,914 - loss nan, lr: 0.025000, epoch: 1, step: 48800, eta: 91.12 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33797 sec, avg_samples: 64.00000, ips: 378.73184 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 48900, eta: 91.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.72523 images/sec
Training: 2023-10-17 16:59:28,717 - loss nan, lr: 0.025000, epoch: 1, step: 48900, eta: 91.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.72523 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 49000, eta: 91.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33852 sec, avg_samples: 64.00000, ips: 378.11094 images/sec
Training: 2023-10-17 17:00:02,576 - loss nan, lr: 0.025000, epoch: 1, step: 49000, eta: 91.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33852 sec, avg_samples: 64.00000, ips: 378.11094 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 49100, eta: 91.02 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33858 sec, avg_samples: 64.00000, ips: 378.05189 images/sec
Training: 2023-10-17 17:00:36,440 - loss nan, lr: 0.025000, epoch: 1, step: 49100, eta: 91.02 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33858 sec, avg_samples: 64.00000, ips: 378.05189 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 49200, eta: 90.98 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33851 sec, avg_samples: 64.00000, ips: 378.13171 images/sec
Training: 2023-10-17 17:01:10,297 - loss nan, lr: 0.025000, epoch: 1, step: 49200, eta: 90.98 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33851 sec, avg_samples: 64.00000, ips: 378.13171 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 49300, eta: 90.95 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33844 sec, avg_samples: 64.00000, ips: 378.20504 images/sec
Training: 2023-10-17 17:01:44,147 - loss nan, lr: 0.025000, epoch: 1, step: 49300, eta: 90.95 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33844 sec, avg_samples: 64.00000, ips: 378.20504 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 49400, eta: 90.92 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33844 sec, avg_samples: 64.00000, ips: 378.20882 images/sec
Training: 2023-10-17 17:02:17,997 - loss nan, lr: 0.025000, epoch: 1, step: 49400, eta: 90.92 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33844 sec, avg_samples: 64.00000, ips: 378.20882 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 49500, eta: 90.88 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33836 sec, avg_samples: 64.00000, ips: 378.29030 images/sec
Training: 2023-10-17 17:02:51,839 - loss nan, lr: 0.025000, epoch: 1, step: 49500, eta: 90.88 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33836 sec, avg_samples: 64.00000, ips: 378.29030 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 49600, eta: 90.85 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33867 sec, avg_samples: 64.00000, ips: 377.94787 images/sec
Training: 2023-10-17 17:03:25,713 - loss nan, lr: 0.025000, epoch: 1, step: 49600, eta: 90.85 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33867 sec, avg_samples: 64.00000, ips: 377.94787 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 49700, eta: 90.81 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33855 sec, avg_samples: 64.00000, ips: 378.07792 images/sec
Training: 2023-10-17 17:03:59,574 - loss nan, lr: 0.025000, epoch: 1, step: 49700, eta: 90.81 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33855 sec, avg_samples: 64.00000, ips: 378.07792 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 49800, eta: 90.78 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33850 sec, avg_samples: 64.00000, ips: 378.14134 images/sec
Training: 2023-10-17 17:04:33,430 - loss nan, lr: 0.025000, epoch: 1, step: 49800, eta: 90.78 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33850 sec, avg_samples: 64.00000, ips: 378.14134 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 49900, eta: 90.75 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33854 sec, avg_samples: 64.00000, ips: 378.09525 images/sec
Training: 2023-10-17 17:05:07,290 - loss nan, lr: 0.025000, epoch: 1, step: 49900, eta: 90.75 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33854 sec, avg_samples: 64.00000, ips: 378.09525 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 50000, eta: 90.71 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33852 sec, avg_samples: 64.00000, ips: 378.12061 images/sec
Training: 2023-10-17 17:05:41,148 - loss nan, lr: 0.025000, epoch: 1, step: 50000, eta: 90.71 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33852 sec, avg_samples: 64.00000, ips: 378.12061 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][50000]XNorm: 0.097574
Training: 2023-10-17 17:06:13,189 - [lfw][50000]XNorm: 0.097574
INFO:root:[lfw][50000]Accuracy-Flip: 0.80533+-0.01757
Training: 2023-10-17 17:06:13,190 - [lfw][50000]Accuracy-Flip: 0.80533+-0.01757
INFO:root:[lfw][50000]Accuracy-Highest: 0.81183
Training: 2023-10-17 17:06:13,190 - [lfw][50000]Accuracy-Highest: 0.81183
INFO:root:test time: 32.0415
Training: 2023-10-17 17:06:13,190 - test time: 32.0415
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][50000]XNorm: 0.099774
Training: 2023-10-17 17:06:50,412 - [cfp_fp][50000]XNorm: 0.099774
INFO:root:[cfp_fp][50000]Accuracy-Flip: 0.60600+-0.01477
Training: 2023-10-17 17:06:50,412 - [cfp_fp][50000]Accuracy-Flip: 0.60600+-0.01477
INFO:root:[cfp_fp][50000]Accuracy-Highest: 0.61443
Training: 2023-10-17 17:06:50,412 - [cfp_fp][50000]Accuracy-Highest: 0.61443
INFO:root:test time: 37.2227
Training: 2023-10-17 17:06:50,412 - test time: 37.2227
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][50000]XNorm: 0.100765
Training: 2023-10-17 17:07:22,395 - [agedb_30][50000]XNorm: 0.100765
INFO:root:[agedb_30][50000]Accuracy-Flip: 0.58167+-0.01781
Training: 2023-10-17 17:07:22,396 - [agedb_30][50000]Accuracy-Flip: 0.58167+-0.01781
INFO:root:[agedb_30][50000]Accuracy-Highest: 0.58350
Training: 2023-10-17 17:07:22,396 - [agedb_30][50000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.9832
Training: 2023-10-17 17:07:22,396 - test time: 31.9832
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 50100, eta: 91.15 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33738 sec, avg_samples: 64.00000, ips: 379.39967 images/sec
Training: 2023-10-17 17:07:56,139 - loss nan, lr: 0.025000, epoch: 1, step: 50100, eta: 91.15 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33738 sec, avg_samples: 64.00000, ips: 379.39967 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 50200, eta: 91.12 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.79847 images/sec
Training: 2023-10-17 17:08:29,936 - loss nan, lr: 0.025000, epoch: 1, step: 50200, eta: 91.12 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.79847 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 50300, eta: 91.08 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.61051 images/sec
Training: 2023-10-17 17:09:03,750 - loss nan, lr: 0.025000, epoch: 1, step: 50300, eta: 91.08 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.61051 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 50400, eta: 91.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33815 sec, avg_samples: 64.00000, ips: 378.53333 images/sec
Training: 2023-10-17 17:09:37,570 - loss nan, lr: 0.025000, epoch: 1, step: 50400, eta: 91.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33815 sec, avg_samples: 64.00000, ips: 378.53333 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 50500, eta: 91.01 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33818 sec, avg_samples: 64.00000, ips: 378.49165 images/sec
Training: 2023-10-17 17:10:11,395 - loss nan, lr: 0.025000, epoch: 1, step: 50500, eta: 91.01 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33818 sec, avg_samples: 64.00000, ips: 378.49165 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 50600, eta: 90.98 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33809 sec, avg_samples: 64.00000, ips: 378.59473 images/sec
Training: 2023-10-17 17:10:45,210 - loss nan, lr: 0.025000, epoch: 1, step: 50600, eta: 90.98 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33809 sec, avg_samples: 64.00000, ips: 378.59473 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 50700, eta: 90.95 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33797 sec, avg_samples: 64.00000, ips: 378.73353 images/sec
Training: 2023-10-17 17:11:19,013 - loss nan, lr: 0.025000, epoch: 1, step: 50700, eta: 90.95 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33797 sec, avg_samples: 64.00000, ips: 378.73353 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 50800, eta: 90.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33817 sec, avg_samples: 64.00000, ips: 378.50418 images/sec
Training: 2023-10-17 17:11:52,836 - loss nan, lr: 0.025000, epoch: 1, step: 50800, eta: 90.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33817 sec, avg_samples: 64.00000, ips: 378.50418 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 50900, eta: 90.88 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33817 sec, avg_samples: 64.00000, ips: 378.50971 images/sec
Training: 2023-10-17 17:12:26,658 - loss nan, lr: 0.025000, epoch: 1, step: 50900, eta: 90.88 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33817 sec, avg_samples: 64.00000, ips: 378.50971 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 51000, eta: 90.84 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.70920 images/sec
Training: 2023-10-17 17:13:00,463 - loss nan, lr: 0.025000, epoch: 1, step: 51000, eta: 90.84 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.70920 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 51100, eta: 90.81 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68424 images/sec
Training: 2023-10-17 17:13:34,271 - loss nan, lr: 0.025000, epoch: 1, step: 51100, eta: 90.81 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68424 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 51200, eta: 90.78 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33810 sec, avg_samples: 64.00000, ips: 378.58195 images/sec
Training: 2023-10-17 17:14:08,087 - loss nan, lr: 0.025000, epoch: 1, step: 51200, eta: 90.78 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33810 sec, avg_samples: 64.00000, ips: 378.58195 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 51300, eta: 90.74 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.60716 images/sec
Training: 2023-10-17 17:14:41,901 - loss nan, lr: 0.025000, epoch: 1, step: 51300, eta: 90.74 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.60716 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 51400, eta: 90.71 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33829 sec, avg_samples: 64.00000, ips: 378.37081 images/sec
Training: 2023-10-17 17:15:15,736 - loss nan, lr: 0.025000, epoch: 1, step: 51400, eta: 90.71 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33829 sec, avg_samples: 64.00000, ips: 378.37081 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 51500, eta: 90.68 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33807 sec, avg_samples: 64.00000, ips: 378.62239 images/sec
Training: 2023-10-17 17:15:49,549 - loss nan, lr: 0.025000, epoch: 1, step: 51500, eta: 90.68 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33807 sec, avg_samples: 64.00000, ips: 378.62239 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 51600, eta: 90.64 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33811 sec, avg_samples: 64.00000, ips: 378.57357 images/sec
Training: 2023-10-17 17:16:23,366 - loss nan, lr: 0.025000, epoch: 1, step: 51600, eta: 90.64 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33811 sec, avg_samples: 64.00000, ips: 378.57357 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 51700, eta: 90.61 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76676 images/sec
Training: 2023-10-17 17:16:57,165 - loss nan, lr: 0.025000, epoch: 1, step: 51700, eta: 90.61 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76676 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 51800, eta: 90.58 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68471 images/sec
Training: 2023-10-17 17:17:30,972 - loss nan, lr: 0.025000, epoch: 1, step: 51800, eta: 90.58 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68471 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 51900, eta: 90.54 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33806 sec, avg_samples: 64.00000, ips: 378.62854 images/sec
Training: 2023-10-17 17:18:04,785 - loss nan, lr: 0.025000, epoch: 1, step: 51900, eta: 90.54 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33806 sec, avg_samples: 64.00000, ips: 378.62854 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 52000, eta: 90.51 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33815 sec, avg_samples: 64.00000, ips: 378.53026 images/sec
Training: 2023-10-17 17:18:38,605 - loss nan, lr: 0.025000, epoch: 1, step: 52000, eta: 90.51 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33815 sec, avg_samples: 64.00000, ips: 378.53026 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][52000]XNorm: 0.072676
Training: 2023-10-17 17:19:10,680 - [lfw][52000]XNorm: 0.072676
INFO:root:[lfw][52000]Accuracy-Flip: 0.80000+-0.01526
Training: 2023-10-17 17:19:10,680 - [lfw][52000]Accuracy-Flip: 0.80000+-0.01526
INFO:root:[lfw][52000]Accuracy-Highest: 0.81183
Training: 2023-10-17 17:19:10,680 - [lfw][52000]Accuracy-Highest: 0.81183
INFO:root:test time: 32.0741
Training: 2023-10-17 17:19:10,680 - test time: 32.0741
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][52000]XNorm: 0.075455
Training: 2023-10-17 17:19:47,900 - [cfp_fp][52000]XNorm: 0.075455
INFO:root:[cfp_fp][52000]Accuracy-Flip: 0.60643+-0.01120
Training: 2023-10-17 17:19:47,900 - [cfp_fp][52000]Accuracy-Flip: 0.60643+-0.01120
INFO:root:[cfp_fp][52000]Accuracy-Highest: 0.61443
Training: 2023-10-17 17:19:47,900 - [cfp_fp][52000]Accuracy-Highest: 0.61443
INFO:root:test time: 37.2202
Training: 2023-10-17 17:19:47,900 - test time: 37.2202
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][52000]XNorm: 0.075807
Training: 2023-10-17 17:20:19,940 - [agedb_30][52000]XNorm: 0.075807
INFO:root:[agedb_30][52000]Accuracy-Flip: 0.57467+-0.01651
Training: 2023-10-17 17:20:19,940 - [agedb_30][52000]Accuracy-Flip: 0.57467+-0.01651
INFO:root:[agedb_30][52000]Accuracy-Highest: 0.58350
Training: 2023-10-17 17:20:19,940 - [agedb_30][52000]Accuracy-Highest: 0.58350
INFO:root:test time: 32.0396
Training: 2023-10-17 17:20:19,940 - test time: 32.0396
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 52100, eta: 90.93 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33724 sec, avg_samples: 64.00000, ips: 379.55556 images/sec
Training: 2023-10-17 17:20:53,669 - loss nan, lr: 0.025000, epoch: 1, step: 52100, eta: 90.93 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33724 sec, avg_samples: 64.00000, ips: 379.55556 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 52200, eta: 90.90 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33814 sec, avg_samples: 64.00000, ips: 378.54675 images/sec
Training: 2023-10-17 17:21:27,489 - loss nan, lr: 0.025000, epoch: 1, step: 52200, eta: 90.90 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33814 sec, avg_samples: 64.00000, ips: 378.54675 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 52300, eta: 90.87 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33825 sec, avg_samples: 64.00000, ips: 378.41630 images/sec
Training: 2023-10-17 17:22:01,320 - loss nan, lr: 0.025000, epoch: 1, step: 52300, eta: 90.87 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33825 sec, avg_samples: 64.00000, ips: 378.41630 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 52400, eta: 90.83 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33815 sec, avg_samples: 64.00000, ips: 378.52803 images/sec
Training: 2023-10-17 17:22:35,141 - loss nan, lr: 0.025000, epoch: 1, step: 52400, eta: 90.83 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33815 sec, avg_samples: 64.00000, ips: 378.52803 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 52500, eta: 90.80 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.64170 images/sec
Training: 2023-10-17 17:23:08,952 - loss nan, lr: 0.025000, epoch: 1, step: 52500, eta: 90.80 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.64170 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 52600, eta: 90.77 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33812 sec, avg_samples: 64.00000, ips: 378.56556 images/sec
Training: 2023-10-17 17:23:42,770 - loss nan, lr: 0.025000, epoch: 1, step: 52600, eta: 90.77 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33812 sec, avg_samples: 64.00000, ips: 378.56556 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 52700, eta: 90.73 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33813 sec, avg_samples: 64.00000, ips: 378.55427 images/sec
Training: 2023-10-17 17:24:16,588 - loss nan, lr: 0.025000, epoch: 1, step: 52700, eta: 90.73 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33813 sec, avg_samples: 64.00000, ips: 378.55427 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 52800, eta: 90.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33812 sec, avg_samples: 64.00000, ips: 378.56599 images/sec
Training: 2023-10-17 17:24:50,406 - loss nan, lr: 0.025000, epoch: 1, step: 52800, eta: 90.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33812 sec, avg_samples: 64.00000, ips: 378.56599 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 52900, eta: 90.67 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.60944 images/sec
Training: 2023-10-17 17:25:24,220 - loss nan, lr: 0.025000, epoch: 1, step: 52900, eta: 90.67 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.60944 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 53000, eta: 90.63 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.61181 images/sec
Training: 2023-10-17 17:25:58,033 - loss nan, lr: 0.025000, epoch: 1, step: 53000, eta: 90.63 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.61181 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 53100, eta: 90.60 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33823 sec, avg_samples: 64.00000, ips: 378.44409 images/sec
Training: 2023-10-17 17:26:31,862 - loss nan, lr: 0.025000, epoch: 1, step: 53100, eta: 90.60 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33823 sec, avg_samples: 64.00000, ips: 378.44409 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 53200, eta: 90.57 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33804 sec, avg_samples: 64.00000, ips: 378.65243 images/sec
Training: 2023-10-17 17:27:05,672 - loss nan, lr: 0.025000, epoch: 1, step: 53200, eta: 90.57 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33804 sec, avg_samples: 64.00000, ips: 378.65243 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 53300, eta: 90.54 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33812 sec, avg_samples: 64.00000, ips: 378.56669 images/sec
Training: 2023-10-17 17:27:39,490 - loss nan, lr: 0.025000, epoch: 1, step: 53300, eta: 90.54 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33812 sec, avg_samples: 64.00000, ips: 378.56669 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 53400, eta: 90.50 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.71610 images/sec
Training: 2023-10-17 17:28:13,294 - loss nan, lr: 0.025000, epoch: 1, step: 53400, eta: 90.50 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.71610 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 53500, eta: 90.47 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33825 sec, avg_samples: 64.00000, ips: 378.41692 images/sec
Training: 2023-10-17 17:28:47,125 - loss nan, lr: 0.025000, epoch: 1, step: 53500, eta: 90.47 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33825 sec, avg_samples: 64.00000, ips: 378.41692 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 53600, eta: 90.44 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33813 sec, avg_samples: 64.00000, ips: 378.55478 images/sec
Training: 2023-10-17 17:29:20,944 - loss nan, lr: 0.025000, epoch: 1, step: 53600, eta: 90.44 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33813 sec, avg_samples: 64.00000, ips: 378.55478 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 53700, eta: 90.41 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33814 sec, avg_samples: 64.00000, ips: 378.53845 images/sec
Training: 2023-10-17 17:29:54,764 - loss nan, lr: 0.025000, epoch: 1, step: 53700, eta: 90.41 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33814 sec, avg_samples: 64.00000, ips: 378.53845 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 53800, eta: 90.37 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33817 sec, avg_samples: 64.00000, ips: 378.51048 images/sec
Training: 2023-10-17 17:30:28,586 - loss nan, lr: 0.025000, epoch: 1, step: 53800, eta: 90.37 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33817 sec, avg_samples: 64.00000, ips: 378.51048 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 53900, eta: 90.34 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66632 images/sec
Training: 2023-10-17 17:31:02,395 - loss nan, lr: 0.025000, epoch: 1, step: 53900, eta: 90.34 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66632 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 54000, eta: 90.31 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33809 sec, avg_samples: 64.00000, ips: 378.59859 images/sec
Training: 2023-10-17 17:31:36,210 - loss nan, lr: 0.025000, epoch: 1, step: 54000, eta: 90.31 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33809 sec, avg_samples: 64.00000, ips: 378.59859 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][54000]XNorm: 0.049435
Training: 2023-10-17 17:32:08,270 - [lfw][54000]XNorm: 0.049435
INFO:root:[lfw][54000]Accuracy-Flip: 0.79133+-0.01633
Training: 2023-10-17 17:32:08,270 - [lfw][54000]Accuracy-Flip: 0.79133+-0.01633
INFO:root:[lfw][54000]Accuracy-Highest: 0.81183
Training: 2023-10-17 17:32:08,271 - [lfw][54000]Accuracy-Highest: 0.81183
INFO:root:test time: 32.0604
Training: 2023-10-17 17:32:08,271 - test time: 32.0604
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][54000]XNorm: 0.052306
Training: 2023-10-17 17:32:45,482 - [cfp_fp][54000]XNorm: 0.052306
INFO:root:[cfp_fp][54000]Accuracy-Flip: 0.60057+-0.01166
Training: 2023-10-17 17:32:45,482 - [cfp_fp][54000]Accuracy-Flip: 0.60057+-0.01166
INFO:root:[cfp_fp][54000]Accuracy-Highest: 0.61443
Training: 2023-10-17 17:32:45,482 - [cfp_fp][54000]Accuracy-Highest: 0.61443
INFO:root:test time: 37.2119
Training: 2023-10-17 17:32:45,482 - test time: 37.2119
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][54000]XNorm: 0.052694
Training: 2023-10-17 17:33:17,519 - [agedb_30][54000]XNorm: 0.052694
INFO:root:[agedb_30][54000]Accuracy-Flip: 0.55850+-0.02093
Training: 2023-10-17 17:33:17,519 - [agedb_30][54000]Accuracy-Flip: 0.55850+-0.02093
INFO:root:[agedb_30][54000]Accuracy-Highest: 0.58350
Training: 2023-10-17 17:33:17,519 - [agedb_30][54000]Accuracy-Highest: 0.58350
INFO:root:test time: 32.0368
Training: 2023-10-17 17:33:17,519 - test time: 32.0368
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 54100, eta: 90.71 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33727 sec, avg_samples: 64.00000, ips: 379.52302 images/sec
Training: 2023-10-17 17:33:51,252 - loss nan, lr: 0.025000, epoch: 1, step: 54100, eta: 90.71 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33727 sec, avg_samples: 64.00000, ips: 379.52302 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 54200, eta: 90.68 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.80839 images/sec
Training: 2023-10-17 17:34:25,048 - loss nan, lr: 0.025000, epoch: 1, step: 54200, eta: 90.68 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.80839 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 54300, eta: 90.65 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.82520 images/sec
Training: 2023-10-17 17:34:58,842 - loss nan, lr: 0.025000, epoch: 1, step: 54300, eta: 90.65 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.82520 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 54400, eta: 90.62 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.83094 images/sec
Training: 2023-10-17 17:35:32,636 - loss nan, lr: 0.025000, epoch: 1, step: 54400, eta: 90.62 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.83094 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 54500, eta: 90.58 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33847 sec, avg_samples: 64.00000, ips: 378.17712 images/sec
Training: 2023-10-17 17:36:06,489 - loss nan, lr: 0.025000, epoch: 1, step: 54500, eta: 90.58 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33847 sec, avg_samples: 64.00000, ips: 378.17712 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 54600, eta: 90.55 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33822 sec, avg_samples: 64.00000, ips: 378.44751 images/sec
Training: 2023-10-17 17:36:40,318 - loss nan, lr: 0.025000, epoch: 1, step: 54600, eta: 90.55 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33822 sec, avg_samples: 64.00000, ips: 378.44751 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 54700, eta: 90.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33838 sec, avg_samples: 64.00000, ips: 378.26971 images/sec
Training: 2023-10-17 17:37:14,162 - loss nan, lr: 0.025000, epoch: 1, step: 54700, eta: 90.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33838 sec, avg_samples: 64.00000, ips: 378.26971 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 54800, eta: 90.49 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33854 sec, avg_samples: 64.00000, ips: 378.09200 images/sec
Training: 2023-10-17 17:37:48,022 - loss nan, lr: 0.025000, epoch: 1, step: 54800, eta: 90.49 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33854 sec, avg_samples: 64.00000, ips: 378.09200 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 54900, eta: 90.46 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33845 sec, avg_samples: 64.00000, ips: 378.19787 images/sec
Training: 2023-10-17 17:38:21,873 - loss nan, lr: 0.025000, epoch: 1, step: 54900, eta: 90.46 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33845 sec, avg_samples: 64.00000, ips: 378.19787 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 55000, eta: 90.42 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33843 sec, avg_samples: 64.00000, ips: 378.21867 images/sec
Training: 2023-10-17 17:38:55,722 - loss nan, lr: 0.025000, epoch: 1, step: 55000, eta: 90.42 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33843 sec, avg_samples: 64.00000, ips: 378.21867 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 55100, eta: 90.39 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33855 sec, avg_samples: 64.00000, ips: 378.08378 images/sec
Training: 2023-10-17 17:39:29,583 - loss nan, lr: 0.025000, epoch: 1, step: 55100, eta: 90.39 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33855 sec, avg_samples: 64.00000, ips: 378.08378 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 55200, eta: 90.36 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33850 sec, avg_samples: 64.00000, ips: 378.13766 images/sec
Training: 2023-10-17 17:40:03,439 - loss nan, lr: 0.025000, epoch: 1, step: 55200, eta: 90.36 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33850 sec, avg_samples: 64.00000, ips: 378.13766 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 55300, eta: 90.33 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33844 sec, avg_samples: 64.00000, ips: 378.21123 images/sec
Training: 2023-10-17 17:40:37,289 - loss nan, lr: 0.025000, epoch: 1, step: 55300, eta: 90.33 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33844 sec, avg_samples: 64.00000, ips: 378.21123 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 55400, eta: 90.30 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33836 sec, avg_samples: 64.00000, ips: 378.29651 images/sec
Training: 2023-10-17 17:41:11,131 - loss nan, lr: 0.025000, epoch: 1, step: 55400, eta: 90.30 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33836 sec, avg_samples: 64.00000, ips: 378.29651 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 55500, eta: 90.27 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33849 sec, avg_samples: 64.00000, ips: 378.14684 images/sec
Training: 2023-10-17 17:41:44,986 - loss nan, lr: 0.025000, epoch: 1, step: 55500, eta: 90.27 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33849 sec, avg_samples: 64.00000, ips: 378.14684 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 55600, eta: 90.23 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33845 sec, avg_samples: 64.00000, ips: 378.19179 images/sec
Training: 2023-10-17 17:42:18,838 - loss nan, lr: 0.025000, epoch: 1, step: 55600, eta: 90.23 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33845 sec, avg_samples: 64.00000, ips: 378.19179 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 55700, eta: 90.20 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33832 sec, avg_samples: 64.00000, ips: 378.34219 images/sec
Training: 2023-10-17 17:42:52,676 - loss nan, lr: 0.025000, epoch: 1, step: 55700, eta: 90.20 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33832 sec, avg_samples: 64.00000, ips: 378.34219 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 55800, eta: 90.17 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33860 sec, avg_samples: 64.00000, ips: 378.03081 images/sec
Training: 2023-10-17 17:43:26,542 - loss nan, lr: 0.025000, epoch: 1, step: 55800, eta: 90.17 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33860 sec, avg_samples: 64.00000, ips: 378.03081 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 55900, eta: 90.14 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33856 sec, avg_samples: 64.00000, ips: 378.07012 images/sec
Training: 2023-10-17 17:44:00,404 - loss nan, lr: 0.025000, epoch: 1, step: 55900, eta: 90.14 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33856 sec, avg_samples: 64.00000, ips: 378.07012 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 56000, eta: 90.11 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33842 sec, avg_samples: 64.00000, ips: 378.22801 images/sec
Training: 2023-10-17 17:44:34,252 - loss nan, lr: 0.025000, epoch: 1, step: 56000, eta: 90.11 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33842 sec, avg_samples: 64.00000, ips: 378.22801 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][56000]XNorm: 0.024257
Training: 2023-10-17 17:45:06,286 - [lfw][56000]XNorm: 0.024257
INFO:root:[lfw][56000]Accuracy-Flip: 0.77633+-0.01420
Training: 2023-10-17 17:45:06,286 - [lfw][56000]Accuracy-Flip: 0.77633+-0.01420
INFO:root:[lfw][56000]Accuracy-Highest: 0.81183
Training: 2023-10-17 17:45:06,286 - [lfw][56000]Accuracy-Highest: 0.81183
INFO:root:test time: 32.0344
Training: 2023-10-17 17:45:06,287 - test time: 32.0344
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][56000]XNorm: 0.026056
Training: 2023-10-17 17:45:43,472 - [cfp_fp][56000]XNorm: 0.026056
INFO:root:[cfp_fp][56000]Accuracy-Flip: 0.59886+-0.01356
Training: 2023-10-17 17:45:43,472 - [cfp_fp][56000]Accuracy-Flip: 0.59886+-0.01356
INFO:root:[cfp_fp][56000]Accuracy-Highest: 0.61443
Training: 2023-10-17 17:45:43,472 - [cfp_fp][56000]Accuracy-Highest: 0.61443
INFO:root:test time: 37.1853
Training: 2023-10-17 17:45:43,472 - test time: 37.1853
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][56000]XNorm: 0.026299
Training: 2023-10-17 17:46:15,514 - [agedb_30][56000]XNorm: 0.026299
INFO:root:[agedb_30][56000]Accuracy-Flip: 0.54900+-0.01924
Training: 2023-10-17 17:46:15,514 - [agedb_30][56000]Accuracy-Flip: 0.54900+-0.01924
INFO:root:[agedb_30][56000]Accuracy-Highest: 0.58350
Training: 2023-10-17 17:46:15,514 - [agedb_30][56000]Accuracy-Highest: 0.58350
INFO:root:test time: 32.0423
Training: 2023-10-17 17:46:15,514 - test time: 32.0423
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 56100, eta: 90.50 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33711 sec, avg_samples: 64.00000, ips: 379.69784 images/sec
Training: 2023-10-17 17:46:49,231 - loss nan, lr: 0.025000, epoch: 1, step: 56100, eta: 90.50 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33711 sec, avg_samples: 64.00000, ips: 379.69784 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 56200, eta: 90.47 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33804 sec, avg_samples: 64.00000, ips: 378.65158 images/sec
Training: 2023-10-17 17:47:23,041 - loss nan, lr: 0.025000, epoch: 1, step: 56200, eta: 90.47 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33804 sec, avg_samples: 64.00000, ips: 378.65158 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 56300, eta: 90.43 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33806 sec, avg_samples: 64.00000, ips: 378.63659 images/sec
Training: 2023-10-17 17:47:56,853 - loss nan, lr: 0.025000, epoch: 1, step: 56300, eta: 90.43 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33806 sec, avg_samples: 64.00000, ips: 378.63659 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 56400, eta: 90.40 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33809 sec, avg_samples: 64.00000, ips: 378.60289 images/sec
Training: 2023-10-17 17:48:30,667 - loss nan, lr: 0.025000, epoch: 1, step: 56400, eta: 90.40 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33809 sec, avg_samples: 64.00000, ips: 378.60289 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 56500, eta: 90.37 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33819 sec, avg_samples: 64.00000, ips: 378.48325 images/sec
Training: 2023-10-17 17:49:04,492 - loss nan, lr: 0.025000, epoch: 1, step: 56500, eta: 90.37 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33819 sec, avg_samples: 64.00000, ips: 378.48325 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 56600, eta: 90.34 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33812 sec, avg_samples: 64.00000, ips: 378.55842 images/sec
Training: 2023-10-17 17:49:38,310 - loss nan, lr: 0.025000, epoch: 1, step: 56600, eta: 90.34 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33812 sec, avg_samples: 64.00000, ips: 378.55842 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 56700, eta: 90.31 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33856 sec, avg_samples: 64.00000, ips: 378.06771 images/sec
Training: 2023-10-17 17:50:12,173 - loss nan, lr: 0.025000, epoch: 1, step: 56700, eta: 90.31 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33856 sec, avg_samples: 64.00000, ips: 378.06771 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 56800, eta: 90.28 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33854 sec, avg_samples: 64.00000, ips: 378.09331 images/sec
Training: 2023-10-17 17:50:46,033 - loss nan, lr: 0.025000, epoch: 1, step: 56800, eta: 90.28 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33854 sec, avg_samples: 64.00000, ips: 378.09331 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 56900, eta: 90.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33862 sec, avg_samples: 64.00000, ips: 378.00539 images/sec
Training: 2023-10-17 17:51:19,901 - loss nan, lr: 0.025000, epoch: 1, step: 56900, eta: 90.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33862 sec, avg_samples: 64.00000, ips: 378.00539 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 57000, eta: 90.21 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33841 sec, avg_samples: 64.00000, ips: 378.23891 images/sec
Training: 2023-10-17 17:51:53,748 - loss nan, lr: 0.025000, epoch: 1, step: 57000, eta: 90.21 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33841 sec, avg_samples: 64.00000, ips: 378.23891 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 57100, eta: 90.18 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33834 sec, avg_samples: 64.00000, ips: 378.31495 images/sec
Training: 2023-10-17 17:52:27,589 - loss nan, lr: 0.025000, epoch: 1, step: 57100, eta: 90.18 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33834 sec, avg_samples: 64.00000, ips: 378.31495 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 57200, eta: 90.15 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33862 sec, avg_samples: 64.00000, ips: 378.00991 images/sec
Training: 2023-10-17 17:53:01,456 - loss nan, lr: 0.025000, epoch: 1, step: 57200, eta: 90.15 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33862 sec, avg_samples: 64.00000, ips: 378.00991 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 57300, eta: 90.12 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33841 sec, avg_samples: 64.00000, ips: 378.23907 images/sec
Training: 2023-10-17 17:53:35,303 - loss nan, lr: 0.025000, epoch: 1, step: 57300, eta: 90.12 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33841 sec, avg_samples: 64.00000, ips: 378.23907 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 57400, eta: 90.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33850 sec, avg_samples: 64.00000, ips: 378.13925 images/sec
Training: 2023-10-17 17:54:09,160 - loss nan, lr: 0.025000, epoch: 1, step: 57400, eta: 90.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33850 sec, avg_samples: 64.00000, ips: 378.13925 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 57500, eta: 90.06 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33845 sec, avg_samples: 64.00000, ips: 378.19755 images/sec
Training: 2023-10-17 17:54:43,010 - loss nan, lr: 0.025000, epoch: 1, step: 57500, eta: 90.06 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33845 sec, avg_samples: 64.00000, ips: 378.19755 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 57600, eta: 90.03 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33852 sec, avg_samples: 64.00000, ips: 378.11176 images/sec
Training: 2023-10-17 17:55:16,869 - loss nan, lr: 0.025000, epoch: 1, step: 57600, eta: 90.03 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33852 sec, avg_samples: 64.00000, ips: 378.11176 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 57700, eta: 90.00 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33854 sec, avg_samples: 64.00000, ips: 378.08920 images/sec
Training: 2023-10-17 17:55:50,729 - loss nan, lr: 0.025000, epoch: 1, step: 57700, eta: 90.00 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33854 sec, avg_samples: 64.00000, ips: 378.08920 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 57800, eta: 89.97 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33845 sec, avg_samples: 64.00000, ips: 378.19871 images/sec
Training: 2023-10-17 17:56:24,580 - loss nan, lr: 0.025000, epoch: 1, step: 57800, eta: 89.97 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33845 sec, avg_samples: 64.00000, ips: 378.19871 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 57900, eta: 89.94 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33844 sec, avg_samples: 64.00000, ips: 378.20437 images/sec
Training: 2023-10-17 17:56:58,430 - loss nan, lr: 0.025000, epoch: 1, step: 57900, eta: 89.94 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33844 sec, avg_samples: 64.00000, ips: 378.20437 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 58000, eta: 89.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33856 sec, avg_samples: 64.00000, ips: 378.07325 images/sec
Training: 2023-10-17 17:57:32,292 - loss nan, lr: 0.025000, epoch: 1, step: 58000, eta: 89.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33856 sec, avg_samples: 64.00000, ips: 378.07325 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][58000]XNorm: 0.006345
Training: 2023-10-17 17:58:04,321 - [lfw][58000]XNorm: 0.006345
INFO:root:[lfw][58000]Accuracy-Flip: 0.74200+-0.01595
Training: 2023-10-17 17:58:04,321 - [lfw][58000]Accuracy-Flip: 0.74200+-0.01595
INFO:root:[lfw][58000]Accuracy-Highest: 0.81183
Training: 2023-10-17 17:58:04,321 - [lfw][58000]Accuracy-Highest: 0.81183
INFO:root:test time: 32.0282
Training: 2023-10-17 17:58:04,321 - test time: 32.0282
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][58000]XNorm: 0.006903
Training: 2023-10-17 17:58:41,559 - [cfp_fp][58000]XNorm: 0.006903
INFO:root:[cfp_fp][58000]Accuracy-Flip: 0.61471+-0.01880
Training: 2023-10-17 17:58:41,559 - [cfp_fp][58000]Accuracy-Flip: 0.61471+-0.01880
INFO:root:[cfp_fp][58000]Accuracy-Highest: 0.61471
Training: 2023-10-17 17:58:41,559 - [cfp_fp][58000]Accuracy-Highest: 0.61471
INFO:root:test time: 37.2380
Training: 2023-10-17 17:58:41,559 - test time: 37.2380
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][58000]XNorm: 0.006811
Training: 2023-10-17 17:59:13,588 - [agedb_30][58000]XNorm: 0.006811
INFO:root:[agedb_30][58000]Accuracy-Flip: 0.56450+-0.01640
Training: 2023-10-17 17:59:13,588 - [agedb_30][58000]Accuracy-Flip: 0.56450+-0.01640
INFO:root:[agedb_30][58000]Accuracy-Highest: 0.58350
Training: 2023-10-17 17:59:13,588 - [agedb_30][58000]Accuracy-Highest: 0.58350
INFO:root:test time: 32.0295
Training: 2023-10-17 17:59:13,588 - test time: 32.0295
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 58100, eta: 90.28 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33719 sec, avg_samples: 64.00000, ips: 379.61205 images/sec
Training: 2023-10-17 17:59:47,313 - loss nan, lr: 0.025000, epoch: 1, step: 58100, eta: 90.28 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33719 sec, avg_samples: 64.00000, ips: 379.61205 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 58200, eta: 90.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33767 sec, avg_samples: 64.00000, ips: 379.06373 images/sec
Training: 2023-10-17 18:00:21,086 - loss nan, lr: 0.025000, epoch: 1, step: 58200, eta: 90.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33767 sec, avg_samples: 64.00000, ips: 379.06373 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 58300, eta: 90.22 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33769 sec, avg_samples: 64.00000, ips: 379.05035 images/sec
Training: 2023-10-17 18:00:54,861 - loss nan, lr: 0.025000, epoch: 1, step: 58300, eta: 90.22 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33769 sec, avg_samples: 64.00000, ips: 379.05035 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 58400, eta: 90.19 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33744 sec, avg_samples: 64.00000, ips: 379.32654 images/sec
Training: 2023-10-17 18:01:28,611 - loss nan, lr: 0.025000, epoch: 1, step: 58400, eta: 90.19 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33744 sec, avg_samples: 64.00000, ips: 379.32654 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 58500, eta: 90.16 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33768 sec, avg_samples: 64.00000, ips: 379.05160 images/sec
Training: 2023-10-17 18:02:02,385 - loss nan, lr: 0.025000, epoch: 1, step: 58500, eta: 90.16 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33768 sec, avg_samples: 64.00000, ips: 379.05160 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 58600, eta: 90.13 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68386 images/sec
Training: 2023-10-17 18:02:36,193 - loss nan, lr: 0.025000, epoch: 1, step: 58600, eta: 90.13 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68386 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 58700, eta: 90.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.70532 images/sec
Training: 2023-10-17 18:03:09,998 - loss nan, lr: 0.025000, epoch: 1, step: 58700, eta: 90.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.70532 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 58800, eta: 90.06 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.78807 images/sec
Training: 2023-10-17 18:03:43,796 - loss nan, lr: 0.025000, epoch: 1, step: 58800, eta: 90.06 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.78807 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 58900, eta: 90.03 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76112 images/sec
Training: 2023-10-17 18:04:17,597 - loss nan, lr: 0.025000, epoch: 1, step: 58900, eta: 90.03 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76112 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 59000, eta: 90.00 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33797 sec, avg_samples: 64.00000, ips: 378.72882 images/sec
Training: 2023-10-17 18:04:51,400 - loss nan, lr: 0.025000, epoch: 1, step: 59000, eta: 90.00 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33797 sec, avg_samples: 64.00000, ips: 378.72882 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 59100, eta: 89.97 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.69505 images/sec
Training: 2023-10-17 18:05:25,206 - loss nan, lr: 0.025000, epoch: 1, step: 59100, eta: 89.97 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.69505 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 59200, eta: 89.94 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.69458 images/sec
Training: 2023-10-17 18:05:59,013 - loss nan, lr: 0.025000, epoch: 1, step: 59200, eta: 89.94 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.69458 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 59300, eta: 89.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.69752 images/sec
Training: 2023-10-17 18:06:32,819 - loss nan, lr: 0.025000, epoch: 1, step: 59300, eta: 89.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.69752 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 59400, eta: 89.88 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33807 sec, avg_samples: 64.00000, ips: 378.61745 images/sec
Training: 2023-10-17 18:07:06,632 - loss nan, lr: 0.025000, epoch: 1, step: 59400, eta: 89.88 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33807 sec, avg_samples: 64.00000, ips: 378.61745 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 59500, eta: 89.85 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33796 sec, avg_samples: 64.00000, ips: 378.74783 images/sec
Training: 2023-10-17 18:07:40,434 - loss nan, lr: 0.025000, epoch: 1, step: 59500, eta: 89.85 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33796 sec, avg_samples: 64.00000, ips: 378.74783 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 59600, eta: 89.82 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33807 sec, avg_samples: 64.00000, ips: 378.62224 images/sec
Training: 2023-10-17 18:08:14,247 - loss nan, lr: 0.025000, epoch: 1, step: 59600, eta: 89.82 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33807 sec, avg_samples: 64.00000, ips: 378.62224 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 59700, eta: 89.79 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67043 images/sec
Training: 2023-10-17 18:08:48,056 - loss nan, lr: 0.025000, epoch: 1, step: 59700, eta: 89.79 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67043 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 59800, eta: 89.76 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33779 sec, avg_samples: 64.00000, ips: 378.92807 images/sec
Training: 2023-10-17 18:09:21,841 - loss nan, lr: 0.025000, epoch: 1, step: 59800, eta: 89.76 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33779 sec, avg_samples: 64.00000, ips: 378.92807 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 59900, eta: 89.73 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76596 images/sec
Training: 2023-10-17 18:09:55,641 - loss nan, lr: 0.025000, epoch: 1, step: 59900, eta: 89.73 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76596 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 60000, eta: 89.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.78543 images/sec
Training: 2023-10-17 18:10:29,440 - loss nan, lr: 0.025000, epoch: 1, step: 60000, eta: 89.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.78543 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][60000]XNorm: 0.002881
Training: 2023-10-17 18:11:01,498 - [lfw][60000]XNorm: 0.002881
INFO:root:[lfw][60000]Accuracy-Flip: 0.69500+-0.01440
Training: 2023-10-17 18:11:01,498 - [lfw][60000]Accuracy-Flip: 0.69500+-0.01440
INFO:root:[lfw][60000]Accuracy-Highest: 0.81183
Training: 2023-10-17 18:11:01,498 - [lfw][60000]Accuracy-Highest: 0.81183
INFO:root:test time: 32.0582
Training: 2023-10-17 18:11:01,498 - test time: 32.0582
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][60000]XNorm: 0.002940
Training: 2023-10-17 18:11:38,747 - [cfp_fp][60000]XNorm: 0.002940
INFO:root:[cfp_fp][60000]Accuracy-Flip: 0.61571+-0.02048
Training: 2023-10-17 18:11:38,747 - [cfp_fp][60000]Accuracy-Flip: 0.61571+-0.02048
INFO:root:[cfp_fp][60000]Accuracy-Highest: 0.61571
Training: 2023-10-17 18:11:38,747 - [cfp_fp][60000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.2487
Training: 2023-10-17 18:11:38,747 - test time: 37.2487
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][60000]XNorm: 0.002894
Training: 2023-10-17 18:12:10,776 - [agedb_30][60000]XNorm: 0.002894
INFO:root:[agedb_30][60000]Accuracy-Flip: 0.56417+-0.01474
Training: 2023-10-17 18:12:10,777 - [agedb_30][60000]Accuracy-Flip: 0.56417+-0.01474
INFO:root:[agedb_30][60000]Accuracy-Highest: 0.58350
Training: 2023-10-17 18:12:10,777 - [agedb_30][60000]Accuracy-Highest: 0.58350
INFO:root:test time: 32.0296
Training: 2023-10-17 18:12:10,777 - test time: 32.0296
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 60100, eta: 90.06 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33724 sec, avg_samples: 64.00000, ips: 379.55381 images/sec
Training: 2023-10-17 18:12:44,506 - loss nan, lr: 0.025000, epoch: 1, step: 60100, eta: 90.06 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33724 sec, avg_samples: 64.00000, ips: 379.55381 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 60200, eta: 90.03 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33750 sec, avg_samples: 64.00000, ips: 379.26217 images/sec
Training: 2023-10-17 18:13:18,262 - loss nan, lr: 0.025000, epoch: 1, step: 60200, eta: 90.03 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33750 sec, avg_samples: 64.00000, ips: 379.26217 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 60300, eta: 90.00 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33776 sec, avg_samples: 64.00000, ips: 378.96953 images/sec
Training: 2023-10-17 18:13:52,044 - loss nan, lr: 0.025000, epoch: 1, step: 60300, eta: 90.00 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33776 sec, avg_samples: 64.00000, ips: 378.96953 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 60400, eta: 89.97 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33763 sec, avg_samples: 64.00000, ips: 379.11695 images/sec
Training: 2023-10-17 18:14:25,813 - loss nan, lr: 0.025000, epoch: 1, step: 60400, eta: 89.97 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33763 sec, avg_samples: 64.00000, ips: 379.11695 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 60500, eta: 89.94 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33761 sec, avg_samples: 64.00000, ips: 379.13566 images/sec
Training: 2023-10-17 18:14:59,579 - loss nan, lr: 0.025000, epoch: 1, step: 60500, eta: 89.94 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33761 sec, avg_samples: 64.00000, ips: 379.13566 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 60600, eta: 89.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33766 sec, avg_samples: 64.00000, ips: 379.07985 images/sec
Training: 2023-10-17 18:15:33,351 - loss nan, lr: 0.025000, epoch: 1, step: 60600, eta: 89.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33766 sec, avg_samples: 64.00000, ips: 379.07985 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 60700, eta: 89.88 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33750 sec, avg_samples: 64.00000, ips: 379.26442 images/sec
Training: 2023-10-17 18:16:07,106 - loss nan, lr: 0.025000, epoch: 1, step: 60700, eta: 89.88 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33750 sec, avg_samples: 64.00000, ips: 379.26442 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 60800, eta: 89.85 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33775 sec, avg_samples: 64.00000, ips: 378.97673 images/sec
Training: 2023-10-17 18:16:40,886 - loss nan, lr: 0.025000, epoch: 1, step: 60800, eta: 89.85 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33775 sec, avg_samples: 64.00000, ips: 378.97673 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 60900, eta: 89.82 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33767 sec, avg_samples: 64.00000, ips: 379.06920 images/sec
Training: 2023-10-17 18:17:14,659 - loss nan, lr: 0.025000, epoch: 1, step: 60900, eta: 89.82 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33767 sec, avg_samples: 64.00000, ips: 379.06920 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 61000, eta: 89.79 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33767 sec, avg_samples: 64.00000, ips: 379.06349 images/sec
Training: 2023-10-17 18:17:48,432 - loss nan, lr: 0.025000, epoch: 1, step: 61000, eta: 89.79 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33767 sec, avg_samples: 64.00000, ips: 379.06349 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 61100, eta: 89.76 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33768 sec, avg_samples: 64.00000, ips: 379.05879 images/sec
Training: 2023-10-17 18:18:22,205 - loss nan, lr: 0.025000, epoch: 1, step: 61100, eta: 89.76 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33768 sec, avg_samples: 64.00000, ips: 379.05879 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 61200, eta: 89.73 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33780 sec, avg_samples: 64.00000, ips: 378.92457 images/sec
Training: 2023-10-17 18:18:55,991 - loss nan, lr: 0.025000, epoch: 1, step: 61200, eta: 89.73 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33780 sec, avg_samples: 64.00000, ips: 378.92457 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 61300, eta: 89.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33758 sec, avg_samples: 64.00000, ips: 379.17246 images/sec
Training: 2023-10-17 18:19:29,754 - loss nan, lr: 0.025000, epoch: 1, step: 61300, eta: 89.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33758 sec, avg_samples: 64.00000, ips: 379.17246 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 61400, eta: 89.67 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33755 sec, avg_samples: 64.00000, ips: 379.20541 images/sec
Training: 2023-10-17 18:20:03,515 - loss nan, lr: 0.025000, epoch: 1, step: 61400, eta: 89.67 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33755 sec, avg_samples: 64.00000, ips: 379.20541 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 61500, eta: 89.64 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33774 sec, avg_samples: 64.00000, ips: 378.98950 images/sec
Training: 2023-10-17 18:20:37,294 - loss nan, lr: 0.025000, epoch: 1, step: 61500, eta: 89.64 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33774 sec, avg_samples: 64.00000, ips: 378.98950 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 61600, eta: 89.61 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33780 sec, avg_samples: 64.00000, ips: 378.92405 images/sec
Training: 2023-10-17 18:21:11,080 - loss nan, lr: 0.025000, epoch: 1, step: 61600, eta: 89.61 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33780 sec, avg_samples: 64.00000, ips: 378.92405 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 61700, eta: 89.58 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33783 sec, avg_samples: 64.00000, ips: 378.88827 images/sec
Training: 2023-10-17 18:21:44,868 - loss nan, lr: 0.025000, epoch: 1, step: 61700, eta: 89.58 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33783 sec, avg_samples: 64.00000, ips: 378.88827 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 61800, eta: 89.55 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33760 sec, avg_samples: 64.00000, ips: 379.15178 images/sec
Training: 2023-10-17 18:22:18,634 - loss nan, lr: 0.025000, epoch: 1, step: 61800, eta: 89.55 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33760 sec, avg_samples: 64.00000, ips: 379.15178 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 61900, eta: 89.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33770 sec, avg_samples: 64.00000, ips: 379.03251 images/sec
Training: 2023-10-17 18:22:52,409 - loss nan, lr: 0.025000, epoch: 1, step: 61900, eta: 89.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33770 sec, avg_samples: 64.00000, ips: 379.03251 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 62000, eta: 89.49 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33763 sec, avg_samples: 64.00000, ips: 379.11181 images/sec
Training: 2023-10-17 18:23:26,178 - loss nan, lr: 0.025000, epoch: 1, step: 62000, eta: 89.49 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33763 sec, avg_samples: 64.00000, ips: 379.11181 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][62000]XNorm: 0.002193
Training: 2023-10-17 18:23:58,265 - [lfw][62000]XNorm: 0.002193
INFO:root:[lfw][62000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 18:23:58,265 - [lfw][62000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][62000]Accuracy-Highest: 0.81183
Training: 2023-10-17 18:23:58,265 - [lfw][62000]Accuracy-Highest: 0.81183
INFO:root:test time: 32.0869
Training: 2023-10-17 18:23:58,266 - test time: 32.0869
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][62000]XNorm: 0.002196
Training: 2023-10-17 18:24:35,495 - [cfp_fp][62000]XNorm: 0.002196
INFO:root:[cfp_fp][62000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 18:24:35,495 - [cfp_fp][62000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][62000]Accuracy-Highest: 0.61571
Training: 2023-10-17 18:24:35,495 - [cfp_fp][62000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.2297
Training: 2023-10-17 18:24:35,495 - test time: 37.2297
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][62000]XNorm: 0.002192
Training: 2023-10-17 18:25:07,516 - [agedb_30][62000]XNorm: 0.002192
INFO:root:[agedb_30][62000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 18:25:07,516 - [agedb_30][62000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][62000]Accuracy-Highest: 0.58350
Training: 2023-10-17 18:25:07,516 - [agedb_30][62000]Accuracy-Highest: 0.58350
INFO:root:test time: 32.0206
Training: 2023-10-17 18:25:07,516 - test time: 32.0206
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 62100, eta: 89.84 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33713 sec, avg_samples: 64.00000, ips: 379.67827 images/sec
Training: 2023-10-17 18:25:41,235 - loss nan, lr: 0.025000, epoch: 1, step: 62100, eta: 89.84 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33713 sec, avg_samples: 64.00000, ips: 379.67827 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 62200, eta: 89.81 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33757 sec, avg_samples: 64.00000, ips: 379.17546 images/sec
Training: 2023-10-17 18:26:14,998 - loss nan, lr: 0.025000, epoch: 1, step: 62200, eta: 89.81 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33757 sec, avg_samples: 64.00000, ips: 379.17546 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 62300, eta: 89.78 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.86005 images/sec
Training: 2023-10-17 18:26:48,789 - loss nan, lr: 0.025000, epoch: 1, step: 62300, eta: 89.78 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.86005 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 62400, eta: 89.75 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33772 sec, avg_samples: 64.00000, ips: 379.00852 images/sec
Training: 2023-10-17 18:27:22,567 - loss nan, lr: 0.025000, epoch: 1, step: 62400, eta: 89.75 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33772 sec, avg_samples: 64.00000, ips: 379.00852 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 62500, eta: 89.72 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33812 sec, avg_samples: 64.00000, ips: 378.56163 images/sec
Training: 2023-10-17 18:27:56,386 - loss nan, lr: 0.025000, epoch: 1, step: 62500, eta: 89.72 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33812 sec, avg_samples: 64.00000, ips: 378.56163 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 62600, eta: 89.69 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33844 sec, avg_samples: 64.00000, ips: 378.21089 images/sec
Training: 2023-10-17 18:28:30,235 - loss nan, lr: 0.025000, epoch: 1, step: 62600, eta: 89.69 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33844 sec, avg_samples: 64.00000, ips: 378.21089 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 62700, eta: 89.66 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33846 sec, avg_samples: 64.00000, ips: 378.17928 images/sec
Training: 2023-10-17 18:29:04,088 - loss nan, lr: 0.025000, epoch: 1, step: 62700, eta: 89.66 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33846 sec, avg_samples: 64.00000, ips: 378.17928 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 62800, eta: 89.63 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33840 sec, avg_samples: 64.00000, ips: 378.25228 images/sec
Training: 2023-10-17 18:29:37,934 - loss nan, lr: 0.025000, epoch: 1, step: 62800, eta: 89.63 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33840 sec, avg_samples: 64.00000, ips: 378.25228 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 62900, eta: 89.60 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33826 sec, avg_samples: 64.00000, ips: 378.40653 images/sec
Training: 2023-10-17 18:30:11,766 - loss nan, lr: 0.025000, epoch: 1, step: 62900, eta: 89.60 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33826 sec, avg_samples: 64.00000, ips: 378.40653 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 63000, eta: 89.57 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33841 sec, avg_samples: 64.00000, ips: 378.23509 images/sec
Training: 2023-10-17 18:30:45,614 - loss nan, lr: 0.025000, epoch: 1, step: 63000, eta: 89.57 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33841 sec, avg_samples: 64.00000, ips: 378.23509 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 63100, eta: 89.55 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33818 sec, avg_samples: 64.00000, ips: 378.49744 images/sec
Training: 2023-10-17 18:31:19,438 - loss nan, lr: 0.025000, epoch: 1, step: 63100, eta: 89.55 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33818 sec, avg_samples: 64.00000, ips: 378.49744 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 63200, eta: 89.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33809 sec, avg_samples: 64.00000, ips: 378.59478 images/sec
Training: 2023-10-17 18:31:53,253 - loss nan, lr: 0.025000, epoch: 1, step: 63200, eta: 89.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33809 sec, avg_samples: 64.00000, ips: 378.59478 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 63300, eta: 89.49 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33827 sec, avg_samples: 64.00000, ips: 378.39162 images/sec
Training: 2023-10-17 18:32:27,087 - loss nan, lr: 0.025000, epoch: 1, step: 63300, eta: 89.49 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33827 sec, avg_samples: 64.00000, ips: 378.39162 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 63400, eta: 89.46 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33836 sec, avg_samples: 64.00000, ips: 378.29365 images/sec
Training: 2023-10-17 18:33:00,929 - loss nan, lr: 0.025000, epoch: 1, step: 63400, eta: 89.46 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33836 sec, avg_samples: 64.00000, ips: 378.29365 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 63500, eta: 89.43 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33826 sec, avg_samples: 64.00000, ips: 378.40342 images/sec
Training: 2023-10-17 18:33:34,761 - loss nan, lr: 0.025000, epoch: 1, step: 63500, eta: 89.43 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33826 sec, avg_samples: 64.00000, ips: 378.40342 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 63600, eta: 89.40 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33825 sec, avg_samples: 64.00000, ips: 378.41451 images/sec
Training: 2023-10-17 18:34:08,593 - loss nan, lr: 0.025000, epoch: 1, step: 63600, eta: 89.40 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33825 sec, avg_samples: 64.00000, ips: 378.41451 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 63700, eta: 89.37 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33846 sec, avg_samples: 64.00000, ips: 378.18491 images/sec
Training: 2023-10-17 18:34:42,445 - loss nan, lr: 0.025000, epoch: 1, step: 63700, eta: 89.37 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33846 sec, avg_samples: 64.00000, ips: 378.18491 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 63800, eta: 89.34 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33825 sec, avg_samples: 64.00000, ips: 378.42222 images/sec
Training: 2023-10-17 18:35:16,276 - loss nan, lr: 0.025000, epoch: 1, step: 63800, eta: 89.34 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33825 sec, avg_samples: 64.00000, ips: 378.42222 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 63900, eta: 89.32 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33839 sec, avg_samples: 64.00000, ips: 378.26736 images/sec
Training: 2023-10-17 18:35:50,121 - loss nan, lr: 0.025000, epoch: 1, step: 63900, eta: 89.32 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33839 sec, avg_samples: 64.00000, ips: 378.26736 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 64000, eta: 89.29 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33847 sec, avg_samples: 64.00000, ips: 378.16807 images/sec
Training: 2023-10-17 18:36:23,974 - loss nan, lr: 0.025000, epoch: 1, step: 64000, eta: 89.29 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33847 sec, avg_samples: 64.00000, ips: 378.16807 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][64000]XNorm: 0.001707
Training: 2023-10-17 18:36:55,991 - [lfw][64000]XNorm: 0.001707
INFO:root:[lfw][64000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 18:36:55,991 - [lfw][64000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][64000]Accuracy-Highest: 0.81183
Training: 2023-10-17 18:36:55,991 - [lfw][64000]Accuracy-Highest: 0.81183
INFO:root:test time: 32.0171
Training: 2023-10-17 18:36:55,991 - test time: 32.0171
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][64000]XNorm: 0.001707
Training: 2023-10-17 18:37:33,149 - [cfp_fp][64000]XNorm: 0.001707
INFO:root:[cfp_fp][64000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 18:37:33,149 - [cfp_fp][64000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][64000]Accuracy-Highest: 0.61571
Training: 2023-10-17 18:37:33,149 - [cfp_fp][64000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.1577
Training: 2023-10-17 18:37:33,149 - test time: 37.1577
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][64000]XNorm: 0.001707
Training: 2023-10-17 18:38:05,147 - [agedb_30][64000]XNorm: 0.001707
INFO:root:[agedb_30][64000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 18:38:05,147 - [agedb_30][64000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][64000]Accuracy-Highest: 0.58350
Training: 2023-10-17 18:38:05,147 - [agedb_30][64000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.9979
Training: 2023-10-17 18:38:05,147 - test time: 31.9979
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 64100, eta: 89.62 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33698 sec, avg_samples: 64.00000, ips: 379.84506 images/sec
Training: 2023-10-17 18:38:38,851 - loss nan, lr: 0.025000, epoch: 1, step: 64100, eta: 89.62 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33698 sec, avg_samples: 64.00000, ips: 379.84506 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 64200, eta: 89.59 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33779 sec, avg_samples: 64.00000, ips: 378.93349 images/sec
Training: 2023-10-17 18:39:12,636 - loss nan, lr: 0.025000, epoch: 1, step: 64200, eta: 89.59 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33779 sec, avg_samples: 64.00000, ips: 378.93349 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 64300, eta: 89.56 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33813 sec, avg_samples: 64.00000, ips: 378.55413 images/sec
Training: 2023-10-17 18:39:46,455 - loss nan, lr: 0.025000, epoch: 1, step: 64300, eta: 89.56 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33813 sec, avg_samples: 64.00000, ips: 378.55413 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 64400, eta: 89.53 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33831 sec, avg_samples: 64.00000, ips: 378.34806 images/sec
Training: 2023-10-17 18:40:20,293 - loss nan, lr: 0.025000, epoch: 1, step: 64400, eta: 89.53 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33831 sec, avg_samples: 64.00000, ips: 378.34806 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 64500, eta: 89.51 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33834 sec, avg_samples: 64.00000, ips: 378.31801 images/sec
Training: 2023-10-17 18:40:54,133 - loss nan, lr: 0.025000, epoch: 1, step: 64500, eta: 89.51 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33834 sec, avg_samples: 64.00000, ips: 378.31801 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 64600, eta: 89.48 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33834 sec, avg_samples: 64.00000, ips: 378.32022 images/sec
Training: 2023-10-17 18:41:27,972 - loss nan, lr: 0.025000, epoch: 1, step: 64600, eta: 89.48 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33834 sec, avg_samples: 64.00000, ips: 378.32022 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 64700, eta: 89.45 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33822 sec, avg_samples: 64.00000, ips: 378.45583 images/sec
Training: 2023-10-17 18:42:01,800 - loss nan, lr: 0.025000, epoch: 1, step: 64700, eta: 89.45 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33822 sec, avg_samples: 64.00000, ips: 378.45583 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 64800, eta: 89.42 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33828 sec, avg_samples: 64.00000, ips: 378.38700 images/sec
Training: 2023-10-17 18:42:35,634 - loss nan, lr: 0.025000, epoch: 1, step: 64800, eta: 89.42 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33828 sec, avg_samples: 64.00000, ips: 378.38700 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 64900, eta: 89.39 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33829 sec, avg_samples: 64.00000, ips: 378.37025 images/sec
Training: 2023-10-17 18:43:09,469 - loss nan, lr: 0.025000, epoch: 1, step: 64900, eta: 89.39 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33829 sec, avg_samples: 64.00000, ips: 378.37025 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 65000, eta: 89.36 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33824 sec, avg_samples: 64.00000, ips: 378.43339 images/sec
Training: 2023-10-17 18:43:43,299 - loss nan, lr: 0.025000, epoch: 1, step: 65000, eta: 89.36 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33824 sec, avg_samples: 64.00000, ips: 378.43339 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 65100, eta: 89.33 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33823 sec, avg_samples: 64.00000, ips: 378.43872 images/sec
Training: 2023-10-17 18:44:17,128 - loss nan, lr: 0.025000, epoch: 1, step: 65100, eta: 89.33 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33823 sec, avg_samples: 64.00000, ips: 378.43872 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 65200, eta: 89.31 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33796 sec, avg_samples: 64.00000, ips: 378.73874 images/sec
Training: 2023-10-17 18:44:50,931 - loss nan, lr: 0.025000, epoch: 1, step: 65200, eta: 89.31 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33796 sec, avg_samples: 64.00000, ips: 378.73874 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 65300, eta: 89.28 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33821 sec, avg_samples: 64.00000, ips: 378.46589 images/sec
Training: 2023-10-17 18:45:24,758 - loss nan, lr: 0.025000, epoch: 1, step: 65300, eta: 89.28 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33821 sec, avg_samples: 64.00000, ips: 378.46589 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 65400, eta: 89.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33832 sec, avg_samples: 64.00000, ips: 378.33989 images/sec
Training: 2023-10-17 18:45:58,596 - loss nan, lr: 0.025000, epoch: 1, step: 65400, eta: 89.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33832 sec, avg_samples: 64.00000, ips: 378.33989 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 65500, eta: 89.22 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33834 sec, avg_samples: 64.00000, ips: 378.32007 images/sec
Training: 2023-10-17 18:46:32,436 - loss nan, lr: 0.025000, epoch: 1, step: 65500, eta: 89.22 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33834 sec, avg_samples: 64.00000, ips: 378.32007 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 65600, eta: 89.19 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33836 sec, avg_samples: 64.00000, ips: 378.29588 images/sec
Training: 2023-10-17 18:47:06,278 - loss nan, lr: 0.025000, epoch: 1, step: 65600, eta: 89.19 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33836 sec, avg_samples: 64.00000, ips: 378.29588 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 65700, eta: 89.16 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33833 sec, avg_samples: 64.00000, ips: 378.32861 images/sec
Training: 2023-10-17 18:47:40,117 - loss nan, lr: 0.025000, epoch: 1, step: 65700, eta: 89.16 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33833 sec, avg_samples: 64.00000, ips: 378.32861 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 65800, eta: 89.14 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33829 sec, avg_samples: 64.00000, ips: 378.37121 images/sec
Training: 2023-10-17 18:48:13,952 - loss nan, lr: 0.025000, epoch: 1, step: 65800, eta: 89.14 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33829 sec, avg_samples: 64.00000, ips: 378.37121 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 65900, eta: 89.11 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33822 sec, avg_samples: 64.00000, ips: 378.44769 images/sec
Training: 2023-10-17 18:48:47,780 - loss nan, lr: 0.025000, epoch: 1, step: 65900, eta: 89.11 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33822 sec, avg_samples: 64.00000, ips: 378.44769 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 66000, eta: 89.08 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33824 sec, avg_samples: 64.00000, ips: 378.42999 images/sec
Training: 2023-10-17 18:49:21,611 - loss nan, lr: 0.025000, epoch: 1, step: 66000, eta: 89.08 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33824 sec, avg_samples: 64.00000, ips: 378.42999 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][66000]XNorm: 0.001329
Training: 2023-10-17 18:49:53,604 - [lfw][66000]XNorm: 0.001329
INFO:root:[lfw][66000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 18:49:53,604 - [lfw][66000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][66000]Accuracy-Highest: 0.81183
Training: 2023-10-17 18:49:53,604 - [lfw][66000]Accuracy-Highest: 0.81183
INFO:root:test time: 31.9937
Training: 2023-10-17 18:49:53,605 - test time: 31.9937
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][66000]XNorm: 0.001329
Training: 2023-10-17 18:50:30,743 - [cfp_fp][66000]XNorm: 0.001329
INFO:root:[cfp_fp][66000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 18:50:30,744 - [cfp_fp][66000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][66000]Accuracy-Highest: 0.61571
Training: 2023-10-17 18:50:30,744 - [cfp_fp][66000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.1391
Training: 2023-10-17 18:50:30,744 - test time: 37.1391
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][66000]XNorm: 0.001329
Training: 2023-10-17 18:51:02,756 - [agedb_30][66000]XNorm: 0.001329
INFO:root:[agedb_30][66000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 18:51:02,756 - [agedb_30][66000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][66000]Accuracy-Highest: 0.58350
Training: 2023-10-17 18:51:02,756 - [agedb_30][66000]Accuracy-Highest: 0.58350
INFO:root:test time: 32.0128
Training: 2023-10-17 18:51:02,757 - test time: 32.0128
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 66100, eta: 89.40 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33680 sec, avg_samples: 64.00000, ips: 380.04890 images/sec
Training: 2023-10-17 18:51:36,442 - loss nan, lr: 0.025000, epoch: 1, step: 66100, eta: 89.40 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33680 sec, avg_samples: 64.00000, ips: 380.04890 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 66200, eta: 89.37 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33780 sec, avg_samples: 64.00000, ips: 378.91754 images/sec
Training: 2023-10-17 18:52:10,229 - loss nan, lr: 0.025000, epoch: 1, step: 66200, eta: 89.37 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33780 sec, avg_samples: 64.00000, ips: 378.91754 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 66300, eta: 89.35 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.60307 images/sec
Training: 2023-10-17 18:52:44,043 - loss nan, lr: 0.025000, epoch: 1, step: 66300, eta: 89.35 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.60307 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 66400, eta: 89.32 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33838 sec, avg_samples: 64.00000, ips: 378.27748 images/sec
Training: 2023-10-17 18:53:17,887 - loss nan, lr: 0.025000, epoch: 1, step: 66400, eta: 89.32 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33838 sec, avg_samples: 64.00000, ips: 378.27748 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 66500, eta: 89.29 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33825 sec, avg_samples: 64.00000, ips: 378.41674 images/sec
Training: 2023-10-17 18:53:51,718 - loss nan, lr: 0.025000, epoch: 1, step: 66500, eta: 89.29 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33825 sec, avg_samples: 64.00000, ips: 378.41674 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 66600, eta: 89.26 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33832 sec, avg_samples: 64.00000, ips: 378.33898 images/sec
Training: 2023-10-17 18:54:25,556 - loss nan, lr: 0.025000, epoch: 1, step: 66600, eta: 89.26 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33832 sec, avg_samples: 64.00000, ips: 378.33898 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 66700, eta: 89.23 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33831 sec, avg_samples: 64.00000, ips: 378.35543 images/sec
Training: 2023-10-17 18:54:59,393 - loss nan, lr: 0.025000, epoch: 1, step: 66700, eta: 89.23 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33831 sec, avg_samples: 64.00000, ips: 378.35543 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 66800, eta: 89.21 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33852 sec, avg_samples: 64.00000, ips: 378.11374 images/sec
Training: 2023-10-17 18:55:33,251 - loss nan, lr: 0.025000, epoch: 1, step: 66800, eta: 89.21 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33852 sec, avg_samples: 64.00000, ips: 378.11374 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 66900, eta: 89.18 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33832 sec, avg_samples: 64.00000, ips: 378.34162 images/sec
Training: 2023-10-17 18:56:07,089 - loss nan, lr: 0.025000, epoch: 1, step: 66900, eta: 89.18 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33832 sec, avg_samples: 64.00000, ips: 378.34162 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 67000, eta: 89.15 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33834 sec, avg_samples: 64.00000, ips: 378.31841 images/sec
Training: 2023-10-17 18:56:40,929 - loss nan, lr: 0.025000, epoch: 1, step: 67000, eta: 89.15 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33834 sec, avg_samples: 64.00000, ips: 378.31841 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 67100, eta: 89.12 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33837 sec, avg_samples: 64.00000, ips: 378.28959 images/sec
Training: 2023-10-17 18:57:14,771 - loss nan, lr: 0.025000, epoch: 1, step: 67100, eta: 89.12 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33837 sec, avg_samples: 64.00000, ips: 378.28959 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 67200, eta: 89.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33838 sec, avg_samples: 64.00000, ips: 378.27165 images/sec
Training: 2023-10-17 18:57:48,615 - loss nan, lr: 0.025000, epoch: 1, step: 67200, eta: 89.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33838 sec, avg_samples: 64.00000, ips: 378.27165 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 67300, eta: 89.07 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33833 sec, avg_samples: 64.00000, ips: 378.33122 images/sec
Training: 2023-10-17 18:58:22,454 - loss nan, lr: 0.025000, epoch: 1, step: 67300, eta: 89.07 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33833 sec, avg_samples: 64.00000, ips: 378.33122 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 67400, eta: 89.04 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33838 sec, avg_samples: 64.00000, ips: 378.26798 images/sec
Training: 2023-10-17 18:58:56,298 - loss nan, lr: 0.025000, epoch: 1, step: 67400, eta: 89.04 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33838 sec, avg_samples: 64.00000, ips: 378.26798 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 67500, eta: 89.01 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33821 sec, avg_samples: 64.00000, ips: 378.45870 images/sec
Training: 2023-10-17 18:59:30,126 - loss nan, lr: 0.025000, epoch: 1, step: 67500, eta: 89.01 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33821 sec, avg_samples: 64.00000, ips: 378.45870 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 67600, eta: 88.98 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33820 sec, avg_samples: 64.00000, ips: 378.47833 images/sec
Training: 2023-10-17 19:00:03,952 - loss nan, lr: 0.025000, epoch: 1, step: 67600, eta: 88.98 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33820 sec, avg_samples: 64.00000, ips: 378.47833 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 67700, eta: 88.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33814 sec, avg_samples: 64.00000, ips: 378.54527 images/sec
Training: 2023-10-17 19:00:37,771 - loss nan, lr: 0.025000, epoch: 1, step: 67700, eta: 88.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33814 sec, avg_samples: 64.00000, ips: 378.54527 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 67800, eta: 88.93 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33823 sec, avg_samples: 64.00000, ips: 378.43668 images/sec
Training: 2023-10-17 19:01:11,601 - loss nan, lr: 0.025000, epoch: 1, step: 67800, eta: 88.93 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33823 sec, avg_samples: 64.00000, ips: 378.43668 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 67900, eta: 88.90 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33834 sec, avg_samples: 64.00000, ips: 378.31810 images/sec
Training: 2023-10-17 19:01:45,441 - loss nan, lr: 0.025000, epoch: 1, step: 67900, eta: 88.90 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33834 sec, avg_samples: 64.00000, ips: 378.31810 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 68000, eta: 88.87 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33850 sec, avg_samples: 64.00000, ips: 378.14260 images/sec
Training: 2023-10-17 19:02:19,296 - loss nan, lr: 0.025000, epoch: 1, step: 68000, eta: 88.87 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33850 sec, avg_samples: 64.00000, ips: 378.14260 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][68000]XNorm: 0.001035
Training: 2023-10-17 19:02:51,314 - [lfw][68000]XNorm: 0.001035
INFO:root:[lfw][68000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 19:02:51,314 - [lfw][68000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][68000]Accuracy-Highest: 0.81183
Training: 2023-10-17 19:02:51,314 - [lfw][68000]Accuracy-Highest: 0.81183
INFO:root:test time: 32.0177
Training: 2023-10-17 19:02:51,314 - test time: 32.0177
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][68000]XNorm: 0.001035
Training: 2023-10-17 19:03:28,471 - [cfp_fp][68000]XNorm: 0.001035
INFO:root:[cfp_fp][68000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 19:03:28,471 - [cfp_fp][68000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][68000]Accuracy-Highest: 0.61571
Training: 2023-10-17 19:03:28,471 - [cfp_fp][68000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.1572
Training: 2023-10-17 19:03:28,471 - test time: 37.1572
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][68000]XNorm: 0.001035
Training: 2023-10-17 19:04:00,459 - [agedb_30][68000]XNorm: 0.001035
INFO:root:[agedb_30][68000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 19:04:00,459 - [agedb_30][68000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][68000]Accuracy-Highest: 0.58350
Training: 2023-10-17 19:04:00,459 - [agedb_30][68000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.9877
Training: 2023-10-17 19:04:00,459 - test time: 31.9877
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 68100, eta: 89.19 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33701 sec, avg_samples: 64.00000, ips: 379.81238 images/sec
Training: 2023-10-17 19:04:34,166 - loss nan, lr: 0.025000, epoch: 1, step: 68100, eta: 89.19 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33701 sec, avg_samples: 64.00000, ips: 379.81238 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 68200, eta: 89.16 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33770 sec, avg_samples: 64.00000, ips: 379.03879 images/sec
Training: 2023-10-17 19:05:07,941 - loss nan, lr: 0.025000, epoch: 1, step: 68200, eta: 89.16 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33770 sec, avg_samples: 64.00000, ips: 379.03879 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 68300, eta: 89.13 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33811 sec, avg_samples: 64.00000, ips: 378.58033 images/sec
Training: 2023-10-17 19:05:41,758 - loss nan, lr: 0.025000, epoch: 1, step: 68300, eta: 89.13 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33811 sec, avg_samples: 64.00000, ips: 378.58033 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 68400, eta: 89.10 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33826 sec, avg_samples: 64.00000, ips: 378.40545 images/sec
Training: 2023-10-17 19:06:15,590 - loss nan, lr: 0.025000, epoch: 1, step: 68400, eta: 89.10 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33826 sec, avg_samples: 64.00000, ips: 378.40545 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 68500, eta: 89.07 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33812 sec, avg_samples: 64.00000, ips: 378.56465 images/sec
Training: 2023-10-17 19:06:49,408 - loss nan, lr: 0.025000, epoch: 1, step: 68500, eta: 89.07 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33812 sec, avg_samples: 64.00000, ips: 378.56465 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 68600, eta: 89.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33823 sec, avg_samples: 64.00000, ips: 378.44406 images/sec
Training: 2023-10-17 19:07:23,237 - loss nan, lr: 0.025000, epoch: 1, step: 68600, eta: 89.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33823 sec, avg_samples: 64.00000, ips: 378.44406 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 68700, eta: 89.02 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33826 sec, avg_samples: 64.00000, ips: 378.41060 images/sec
Training: 2023-10-17 19:07:57,069 - loss nan, lr: 0.025000, epoch: 1, step: 68700, eta: 89.02 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33826 sec, avg_samples: 64.00000, ips: 378.41060 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 68800, eta: 88.99 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33815 sec, avg_samples: 64.00000, ips: 378.52956 images/sec
Training: 2023-10-17 19:08:30,890 - loss nan, lr: 0.025000, epoch: 1, step: 68800, eta: 88.99 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33815 sec, avg_samples: 64.00000, ips: 378.52956 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 68900, eta: 88.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33817 sec, avg_samples: 64.00000, ips: 378.51220 images/sec
Training: 2023-10-17 19:09:04,712 - loss nan, lr: 0.025000, epoch: 1, step: 68900, eta: 88.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33817 sec, avg_samples: 64.00000, ips: 378.51220 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 69000, eta: 88.94 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33823 sec, avg_samples: 64.00000, ips: 378.44142 images/sec
Training: 2023-10-17 19:09:38,541 - loss nan, lr: 0.025000, epoch: 1, step: 69000, eta: 88.94 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33823 sec, avg_samples: 64.00000, ips: 378.44142 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 69100, eta: 88.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33817 sec, avg_samples: 64.00000, ips: 378.50678 images/sec
Training: 2023-10-17 19:10:12,364 - loss nan, lr: 0.025000, epoch: 1, step: 69100, eta: 88.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33817 sec, avg_samples: 64.00000, ips: 378.50678 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 69200, eta: 88.88 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33829 sec, avg_samples: 64.00000, ips: 378.37497 images/sec
Training: 2023-10-17 19:10:46,199 - loss nan, lr: 0.025000, epoch: 1, step: 69200, eta: 88.88 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33829 sec, avg_samples: 64.00000, ips: 378.37497 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 69300, eta: 88.85 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33827 sec, avg_samples: 64.00000, ips: 378.39310 images/sec
Training: 2023-10-17 19:11:20,033 - loss nan, lr: 0.025000, epoch: 1, step: 69300, eta: 88.85 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33827 sec, avg_samples: 64.00000, ips: 378.39310 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 69400, eta: 88.83 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33811 sec, avg_samples: 64.00000, ips: 378.57147 images/sec
Training: 2023-10-17 19:11:53,850 - loss nan, lr: 0.025000, epoch: 1, step: 69400, eta: 88.83 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33811 sec, avg_samples: 64.00000, ips: 378.57147 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 69500, eta: 88.80 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33823 sec, avg_samples: 64.00000, ips: 378.44495 images/sec
Training: 2023-10-17 19:12:27,679 - loss nan, lr: 0.025000, epoch: 1, step: 69500, eta: 88.80 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33823 sec, avg_samples: 64.00000, ips: 378.44495 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 69600, eta: 88.77 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33825 sec, avg_samples: 64.00000, ips: 378.42192 images/sec
Training: 2023-10-17 19:13:01,510 - loss nan, lr: 0.025000, epoch: 1, step: 69600, eta: 88.77 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33825 sec, avg_samples: 64.00000, ips: 378.42192 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 69700, eta: 88.75 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33818 sec, avg_samples: 64.00000, ips: 378.49644 images/sec
Training: 2023-10-17 19:13:35,334 - loss nan, lr: 0.025000, epoch: 1, step: 69700, eta: 88.75 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33818 sec, avg_samples: 64.00000, ips: 378.49644 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 69800, eta: 88.72 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33844 sec, avg_samples: 64.00000, ips: 378.20084 images/sec
Training: 2023-10-17 19:14:09,184 - loss nan, lr: 0.025000, epoch: 1, step: 69800, eta: 88.72 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33844 sec, avg_samples: 64.00000, ips: 378.20084 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 69900, eta: 88.69 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33811 sec, avg_samples: 64.00000, ips: 378.57140 images/sec
Training: 2023-10-17 19:14:43,002 - loss nan, lr: 0.025000, epoch: 1, step: 69900, eta: 88.69 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33811 sec, avg_samples: 64.00000, ips: 378.57140 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 70000, eta: 88.67 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33815 sec, avg_samples: 64.00000, ips: 378.53145 images/sec
Training: 2023-10-17 19:15:16,823 - loss nan, lr: 0.025000, epoch: 1, step: 70000, eta: 88.67 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33815 sec, avg_samples: 64.00000, ips: 378.53145 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][70000]XNorm: 0.000806
Training: 2023-10-17 19:15:48,820 - [lfw][70000]XNorm: 0.000806
INFO:root:[lfw][70000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 19:15:48,820 - [lfw][70000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][70000]Accuracy-Highest: 0.81183
Training: 2023-10-17 19:15:48,820 - [lfw][70000]Accuracy-Highest: 0.81183
INFO:root:test time: 31.9973
Training: 2023-10-17 19:15:48,820 - test time: 31.9973
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][70000]XNorm: 0.000806
Training: 2023-10-17 19:16:25,977 - [cfp_fp][70000]XNorm: 0.000806
INFO:root:[cfp_fp][70000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 19:16:25,977 - [cfp_fp][70000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][70000]Accuracy-Highest: 0.61571
Training: 2023-10-17 19:16:25,977 - [cfp_fp][70000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.1569
Training: 2023-10-17 19:16:25,977 - test time: 37.1569
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][70000]XNorm: 0.000806
Training: 2023-10-17 19:16:57,971 - [agedb_30][70000]XNorm: 0.000806
INFO:root:[agedb_30][70000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 19:16:57,971 - [agedb_30][70000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][70000]Accuracy-Highest: 0.58350
Training: 2023-10-17 19:16:57,971 - [agedb_30][70000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.9935
Training: 2023-10-17 19:16:57,971 - test time: 31.9935
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 70100, eta: 88.97 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33697 sec, avg_samples: 64.00000, ips: 379.85061 images/sec
Training: 2023-10-17 19:17:31,674 - loss nan, lr: 0.025000, epoch: 1, step: 70100, eta: 88.97 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33697 sec, avg_samples: 64.00000, ips: 379.85061 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 70200, eta: 88.94 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.85553 images/sec
Training: 2023-10-17 19:18:05,466 - loss nan, lr: 0.025000, epoch: 1, step: 70200, eta: 88.94 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.85553 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 70300, eta: 88.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68201 images/sec
Training: 2023-10-17 19:18:39,273 - loss nan, lr: 0.025000, epoch: 1, step: 70300, eta: 88.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68201 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 70400, eta: 88.89 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33845 sec, avg_samples: 64.00000, ips: 378.19415 images/sec
Training: 2023-10-17 19:19:13,124 - loss nan, lr: 0.025000, epoch: 1, step: 70400, eta: 88.89 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33845 sec, avg_samples: 64.00000, ips: 378.19415 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 70500, eta: 88.86 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33824 sec, avg_samples: 64.00000, ips: 378.43160 images/sec
Training: 2023-10-17 19:19:46,954 - loss nan, lr: 0.025000, epoch: 1, step: 70500, eta: 88.86 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33824 sec, avg_samples: 64.00000, ips: 378.43160 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 70600, eta: 88.83 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33831 sec, avg_samples: 64.00000, ips: 378.34974 images/sec
Training: 2023-10-17 19:20:20,791 - loss nan, lr: 0.025000, epoch: 1, step: 70600, eta: 88.83 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33831 sec, avg_samples: 64.00000, ips: 378.34974 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 70700, eta: 88.80 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33811 sec, avg_samples: 64.00000, ips: 378.57951 images/sec
Training: 2023-10-17 19:20:54,608 - loss nan, lr: 0.025000, epoch: 1, step: 70700, eta: 88.80 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33811 sec, avg_samples: 64.00000, ips: 378.57951 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 70800, eta: 88.78 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33827 sec, avg_samples: 64.00000, ips: 378.39878 images/sec
Training: 2023-10-17 19:21:28,441 - loss nan, lr: 0.025000, epoch: 1, step: 70800, eta: 88.78 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33827 sec, avg_samples: 64.00000, ips: 378.39878 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 70900, eta: 88.75 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33826 sec, avg_samples: 64.00000, ips: 378.40835 images/sec
Training: 2023-10-17 19:22:02,273 - loss nan, lr: 0.025000, epoch: 1, step: 70900, eta: 88.75 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33826 sec, avg_samples: 64.00000, ips: 378.40835 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 71000, eta: 88.72 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33822 sec, avg_samples: 64.00000, ips: 378.45577 images/sec
Training: 2023-10-17 19:22:36,100 - loss nan, lr: 0.025000, epoch: 1, step: 71000, eta: 88.72 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33822 sec, avg_samples: 64.00000, ips: 378.45577 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 71100, eta: 88.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33844 sec, avg_samples: 64.00000, ips: 378.21032 images/sec
Training: 2023-10-17 19:23:09,950 - loss nan, lr: 0.025000, epoch: 1, step: 71100, eta: 88.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33844 sec, avg_samples: 64.00000, ips: 378.21032 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 71200, eta: 88.67 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.60713 images/sec
Training: 2023-10-17 19:23:43,764 - loss nan, lr: 0.025000, epoch: 1, step: 71200, eta: 88.67 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.60713 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 71300, eta: 88.64 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33831 sec, avg_samples: 64.00000, ips: 378.35306 images/sec
Training: 2023-10-17 19:24:17,601 - loss nan, lr: 0.025000, epoch: 1, step: 71300, eta: 88.64 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33831 sec, avg_samples: 64.00000, ips: 378.35306 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 1, step: 71400, eta: 88.62 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33844 sec, avg_samples: 64.00000, ips: 378.20781 images/sec
Training: 2023-10-17 19:24:51,451 - loss nan, lr: 0.025000, epoch: 1, step: 71400, eta: 88.62 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33844 sec, avg_samples: 64.00000, ips: 378.20781 images/sec
INFO:root:Save model to model/FresResNet101/1.
Training: 2023-10-17 19:25:16,905 - Save model to model/FresResNet101/1.
INFO:root:Remove checkpoint model/FresResNet101/0.
Training: 2023-10-17 19:25:16,905 - Remove checkpoint model/FresResNet101/0.
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 71500, eta: 88.60 hours, avg_reader_cost: 0.00898 sec, avg_batch_cost: 0.10348 sec, avg_samples: 17.92000, ips: 346.35375 images/sec
Training: 2023-10-17 19:25:27,390 - loss nan, lr: 0.025000, epoch: 2, step: 71500, eta: 88.60 hours, avg_reader_cost: 0.00898 sec, avg_batch_cost: 0.10348 sec, avg_samples: 17.92000, ips: 346.35375 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 71600, eta: 88.57 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33781 sec, avg_samples: 64.00000, ips: 378.91453 images/sec
Training: 2023-10-17 19:26:01,177 - loss nan, lr: 0.025000, epoch: 2, step: 71600, eta: 88.57 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33781 sec, avg_samples: 64.00000, ips: 378.91453 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 71700, eta: 88.54 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.82183 images/sec
Training: 2023-10-17 19:26:34,972 - loss nan, lr: 0.025000, epoch: 2, step: 71700, eta: 88.54 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.82183 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 71800, eta: 88.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33787 sec, avg_samples: 64.00000, ips: 378.83873 images/sec
Training: 2023-10-17 19:27:08,766 - loss nan, lr: 0.025000, epoch: 2, step: 71800, eta: 88.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33787 sec, avg_samples: 64.00000, ips: 378.83873 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 71900, eta: 88.49 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.64030 images/sec
Training: 2023-10-17 19:27:42,577 - loss nan, lr: 0.025000, epoch: 2, step: 71900, eta: 88.49 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.64030 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 72000, eta: 88.46 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.71530 images/sec
Training: 2023-10-17 19:28:16,381 - loss nan, lr: 0.025000, epoch: 2, step: 72000, eta: 88.46 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.71530 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][72000]XNorm: 0.000627
Training: 2023-10-17 19:28:48,415 - [lfw][72000]XNorm: 0.000627
INFO:root:[lfw][72000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 19:28:48,415 - [lfw][72000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][72000]Accuracy-Highest: 0.81183
Training: 2023-10-17 19:28:48,415 - [lfw][72000]Accuracy-Highest: 0.81183
INFO:root:test time: 32.0332
Training: 2023-10-17 19:28:48,415 - test time: 32.0332
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][72000]XNorm: 0.000627
Training: 2023-10-17 19:29:25,584 - [cfp_fp][72000]XNorm: 0.000627
INFO:root:[cfp_fp][72000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 19:29:25,584 - [cfp_fp][72000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][72000]Accuracy-Highest: 0.61571
Training: 2023-10-17 19:29:25,584 - [cfp_fp][72000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.1695
Training: 2023-10-17 19:29:25,584 - test time: 37.1695
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][72000]XNorm: 0.000627
Training: 2023-10-17 19:29:57,606 - [agedb_30][72000]XNorm: 0.000627
INFO:root:[agedb_30][72000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 19:29:57,606 - [agedb_30][72000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][72000]Accuracy-Highest: 0.58350
Training: 2023-10-17 19:29:57,606 - [agedb_30][72000]Accuracy-Highest: 0.58350
INFO:root:test time: 32.0218
Training: 2023-10-17 19:29:57,606 - test time: 32.0218
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 72100, eta: 88.76 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33700 sec, avg_samples: 64.00000, ips: 379.82003 images/sec
Training: 2023-10-17 19:30:31,312 - loss nan, lr: 0.025000, epoch: 2, step: 72100, eta: 88.76 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33700 sec, avg_samples: 64.00000, ips: 379.82003 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 72200, eta: 88.73 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33783 sec, avg_samples: 64.00000, ips: 378.89370 images/sec
Training: 2023-10-17 19:31:05,101 - loss nan, lr: 0.025000, epoch: 2, step: 72200, eta: 88.73 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33783 sec, avg_samples: 64.00000, ips: 378.89370 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 72300, eta: 88.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33775 sec, avg_samples: 64.00000, ips: 378.97700 images/sec
Training: 2023-10-17 19:31:38,882 - loss nan, lr: 0.025000, epoch: 2, step: 72300, eta: 88.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33775 sec, avg_samples: 64.00000, ips: 378.97700 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 72400, eta: 88.68 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33770 sec, avg_samples: 64.00000, ips: 379.03624 images/sec
Training: 2023-10-17 19:32:12,657 - loss nan, lr: 0.025000, epoch: 2, step: 72400, eta: 88.68 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33770 sec, avg_samples: 64.00000, ips: 379.03624 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 72500, eta: 88.65 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33780 sec, avg_samples: 64.00000, ips: 378.91899 images/sec
Training: 2023-10-17 19:32:46,443 - loss nan, lr: 0.025000, epoch: 2, step: 72500, eta: 88.65 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33780 sec, avg_samples: 64.00000, ips: 378.91899 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 72600, eta: 88.62 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33763 sec, avg_samples: 64.00000, ips: 379.11771 images/sec
Training: 2023-10-17 19:33:20,212 - loss nan, lr: 0.025000, epoch: 2, step: 72600, eta: 88.62 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33763 sec, avg_samples: 64.00000, ips: 379.11771 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 72700, eta: 88.60 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.60841 images/sec
Training: 2023-10-17 19:33:54,026 - loss nan, lr: 0.025000, epoch: 2, step: 72700, eta: 88.60 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.60841 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 72800, eta: 88.57 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33829 sec, avg_samples: 64.00000, ips: 378.37895 images/sec
Training: 2023-10-17 19:34:27,860 - loss nan, lr: 0.025000, epoch: 2, step: 72800, eta: 88.57 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33829 sec, avg_samples: 64.00000, ips: 378.37895 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 72900, eta: 88.54 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33841 sec, avg_samples: 64.00000, ips: 378.24476 images/sec
Training: 2023-10-17 19:35:01,707 - loss nan, lr: 0.025000, epoch: 2, step: 72900, eta: 88.54 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33841 sec, avg_samples: 64.00000, ips: 378.24476 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 73000, eta: 88.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33846 sec, avg_samples: 64.00000, ips: 378.18589 images/sec
Training: 2023-10-17 19:35:35,559 - loss nan, lr: 0.025000, epoch: 2, step: 73000, eta: 88.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33846 sec, avg_samples: 64.00000, ips: 378.18589 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 73100, eta: 88.49 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33839 sec, avg_samples: 64.00000, ips: 378.26198 images/sec
Training: 2023-10-17 19:36:09,404 - loss nan, lr: 0.025000, epoch: 2, step: 73100, eta: 88.49 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33839 sec, avg_samples: 64.00000, ips: 378.26198 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 73200, eta: 88.46 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33833 sec, avg_samples: 64.00000, ips: 378.32394 images/sec
Training: 2023-10-17 19:36:43,243 - loss nan, lr: 0.025000, epoch: 2, step: 73200, eta: 88.46 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33833 sec, avg_samples: 64.00000, ips: 378.32394 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 73300, eta: 88.44 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33842 sec, avg_samples: 64.00000, ips: 378.22644 images/sec
Training: 2023-10-17 19:37:17,091 - loss nan, lr: 0.025000, epoch: 2, step: 73300, eta: 88.44 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33842 sec, avg_samples: 64.00000, ips: 378.22644 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 73400, eta: 88.41 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33838 sec, avg_samples: 64.00000, ips: 378.26853 images/sec
Training: 2023-10-17 19:37:50,936 - loss nan, lr: 0.025000, epoch: 2, step: 73400, eta: 88.41 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33838 sec, avg_samples: 64.00000, ips: 378.26853 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 73500, eta: 88.39 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33830 sec, avg_samples: 64.00000, ips: 378.36500 images/sec
Training: 2023-10-17 19:38:24,772 - loss nan, lr: 0.025000, epoch: 2, step: 73500, eta: 88.39 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33830 sec, avg_samples: 64.00000, ips: 378.36500 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 73600, eta: 88.36 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33835 sec, avg_samples: 64.00000, ips: 378.30809 images/sec
Training: 2023-10-17 19:38:58,613 - loss nan, lr: 0.025000, epoch: 2, step: 73600, eta: 88.36 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33835 sec, avg_samples: 64.00000, ips: 378.30809 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 73700, eta: 88.33 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33834 sec, avg_samples: 64.00000, ips: 378.31909 images/sec
Training: 2023-10-17 19:39:32,453 - loss nan, lr: 0.025000, epoch: 2, step: 73700, eta: 88.33 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33834 sec, avg_samples: 64.00000, ips: 378.31909 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 73800, eta: 88.31 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33826 sec, avg_samples: 64.00000, ips: 378.41035 images/sec
Training: 2023-10-17 19:40:06,284 - loss nan, lr: 0.025000, epoch: 2, step: 73800, eta: 88.31 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33826 sec, avg_samples: 64.00000, ips: 378.41035 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 73900, eta: 88.28 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33829 sec, avg_samples: 64.00000, ips: 378.37914 images/sec
Training: 2023-10-17 19:40:40,119 - loss nan, lr: 0.025000, epoch: 2, step: 73900, eta: 88.28 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33829 sec, avg_samples: 64.00000, ips: 378.37914 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 74000, eta: 88.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.60336 images/sec
Training: 2023-10-17 19:41:13,933 - loss nan, lr: 0.025000, epoch: 2, step: 74000, eta: 88.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.60336 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][74000]XNorm: 0.000488
Training: 2023-10-17 19:41:45,954 - [lfw][74000]XNorm: 0.000488
INFO:root:[lfw][74000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 19:41:45,955 - [lfw][74000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][74000]Accuracy-Highest: 0.81183
Training: 2023-10-17 19:41:45,955 - [lfw][74000]Accuracy-Highest: 0.81183
INFO:root:test time: 32.0212
Training: 2023-10-17 19:41:45,955 - test time: 32.0212
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][74000]XNorm: 0.000488
Training: 2023-10-17 19:42:23,120 - [cfp_fp][74000]XNorm: 0.000488
INFO:root:[cfp_fp][74000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 19:42:23,120 - [cfp_fp][74000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][74000]Accuracy-Highest: 0.61571
Training: 2023-10-17 19:42:23,120 - [cfp_fp][74000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.1656
Training: 2023-10-17 19:42:23,120 - test time: 37.1656
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][74000]XNorm: 0.000488
Training: 2023-10-17 19:42:55,017 - [agedb_30][74000]XNorm: 0.000488
INFO:root:[agedb_30][74000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 19:42:55,017 - [agedb_30][74000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][74000]Accuracy-Highest: 0.58350
Training: 2023-10-17 19:42:55,017 - [agedb_30][74000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.8969
Training: 2023-10-17 19:42:55,017 - test time: 31.8969
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 74100, eta: 88.54 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33697 sec, avg_samples: 64.00000, ips: 379.85542 images/sec
Training: 2023-10-17 19:43:28,720 - loss nan, lr: 0.025000, epoch: 2, step: 74100, eta: 88.54 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33697 sec, avg_samples: 64.00000, ips: 379.85542 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 74200, eta: 88.51 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33781 sec, avg_samples: 64.00000, ips: 378.90928 images/sec
Training: 2023-10-17 19:44:02,507 - loss nan, lr: 0.025000, epoch: 2, step: 74200, eta: 88.51 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33781 sec, avg_samples: 64.00000, ips: 378.90928 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 74300, eta: 88.49 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33773 sec, avg_samples: 64.00000, ips: 379.00592 images/sec
Training: 2023-10-17 19:44:36,286 - loss nan, lr: 0.025000, epoch: 2, step: 74300, eta: 88.49 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33773 sec, avg_samples: 64.00000, ips: 379.00592 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 74400, eta: 88.46 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33777 sec, avg_samples: 64.00000, ips: 378.95595 images/sec
Training: 2023-10-17 19:45:10,068 - loss nan, lr: 0.025000, epoch: 2, step: 74400, eta: 88.46 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33777 sec, avg_samples: 64.00000, ips: 378.95595 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 74500, eta: 88.43 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33793 sec, avg_samples: 64.00000, ips: 378.77455 images/sec
Training: 2023-10-17 19:45:43,867 - loss nan, lr: 0.025000, epoch: 2, step: 74500, eta: 88.43 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33793 sec, avg_samples: 64.00000, ips: 378.77455 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 74600, eta: 88.41 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33782 sec, avg_samples: 64.00000, ips: 378.90011 images/sec
Training: 2023-10-17 19:46:17,655 - loss nan, lr: 0.025000, epoch: 2, step: 74600, eta: 88.41 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33782 sec, avg_samples: 64.00000, ips: 378.90011 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 74700, eta: 88.38 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.85838 images/sec
Training: 2023-10-17 19:46:51,447 - loss nan, lr: 0.025000, epoch: 2, step: 74700, eta: 88.38 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.85838 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 74800, eta: 88.35 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.87107 images/sec
Training: 2023-10-17 19:47:25,237 - loss nan, lr: 0.025000, epoch: 2, step: 74800, eta: 88.35 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.87107 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 74900, eta: 88.33 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33787 sec, avg_samples: 64.00000, ips: 378.84905 images/sec
Training: 2023-10-17 19:47:59,029 - loss nan, lr: 0.025000, epoch: 2, step: 74900, eta: 88.33 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33787 sec, avg_samples: 64.00000, ips: 378.84905 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 75000, eta: 88.30 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.81333 images/sec
Training: 2023-10-17 19:48:32,825 - loss nan, lr: 0.025000, epoch: 2, step: 75000, eta: 88.30 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.81333 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 75100, eta: 88.28 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.82264 images/sec
Training: 2023-10-17 19:49:06,619 - loss nan, lr: 0.025000, epoch: 2, step: 75100, eta: 88.28 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.82264 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 75200, eta: 88.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33787 sec, avg_samples: 64.00000, ips: 378.84072 images/sec
Training: 2023-10-17 19:49:40,413 - loss nan, lr: 0.025000, epoch: 2, step: 75200, eta: 88.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33787 sec, avg_samples: 64.00000, ips: 378.84072 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 75300, eta: 88.22 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.85067 images/sec
Training: 2023-10-17 19:50:14,205 - loss nan, lr: 0.025000, epoch: 2, step: 75300, eta: 88.22 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.85067 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 75400, eta: 88.20 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33793 sec, avg_samples: 64.00000, ips: 378.77799 images/sec
Training: 2023-10-17 19:50:48,003 - loss nan, lr: 0.025000, epoch: 2, step: 75400, eta: 88.20 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33793 sec, avg_samples: 64.00000, ips: 378.77799 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 75500, eta: 88.17 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.71638 images/sec
Training: 2023-10-17 19:51:21,807 - loss nan, lr: 0.025000, epoch: 2, step: 75500, eta: 88.17 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.71638 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 75600, eta: 88.15 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33784 sec, avg_samples: 64.00000, ips: 378.88166 images/sec
Training: 2023-10-17 19:51:55,597 - loss nan, lr: 0.025000, epoch: 2, step: 75600, eta: 88.15 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33784 sec, avg_samples: 64.00000, ips: 378.88166 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 75700, eta: 88.12 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.71890 images/sec
Training: 2023-10-17 19:52:29,401 - loss nan, lr: 0.025000, epoch: 2, step: 75700, eta: 88.12 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.71890 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 75800, eta: 88.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33778 sec, avg_samples: 64.00000, ips: 378.94209 images/sec
Training: 2023-10-17 19:53:03,185 - loss nan, lr: 0.025000, epoch: 2, step: 75800, eta: 88.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33778 sec, avg_samples: 64.00000, ips: 378.94209 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 75900, eta: 88.07 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33795 sec, avg_samples: 64.00000, ips: 378.75455 images/sec
Training: 2023-10-17 19:53:36,986 - loss nan, lr: 0.025000, epoch: 2, step: 75900, eta: 88.07 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33795 sec, avg_samples: 64.00000, ips: 378.75455 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 76000, eta: 88.04 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76259 images/sec
Training: 2023-10-17 19:54:10,786 - loss nan, lr: 0.025000, epoch: 2, step: 76000, eta: 88.04 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76259 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][76000]XNorm: 0.000380
Training: 2023-10-17 19:54:42,813 - [lfw][76000]XNorm: 0.000380
INFO:root:[lfw][76000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 19:54:42,813 - [lfw][76000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][76000]Accuracy-Highest: 0.81183
Training: 2023-10-17 19:54:42,813 - [lfw][76000]Accuracy-Highest: 0.81183
INFO:root:test time: 32.0270
Training: 2023-10-17 19:54:42,813 - test time: 32.0270
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][76000]XNorm: 0.000380
Training: 2023-10-17 19:55:19,972 - [cfp_fp][76000]XNorm: 0.000380
INFO:root:[cfp_fp][76000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 19:55:19,972 - [cfp_fp][76000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][76000]Accuracy-Highest: 0.61571
Training: 2023-10-17 19:55:19,972 - [cfp_fp][76000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.1596
Training: 2023-10-17 19:55:19,973 - test time: 37.1596
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][76000]XNorm: 0.000380
Training: 2023-10-17 19:55:51,896 - [agedb_30][76000]XNorm: 0.000380
INFO:root:[agedb_30][76000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 19:55:51,896 - [agedb_30][76000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][76000]Accuracy-Highest: 0.58350
Training: 2023-10-17 19:55:51,896 - [agedb_30][76000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.9232
Training: 2023-10-17 19:55:51,896 - test time: 31.9232
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 76100, eta: 88.32 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33702 sec, avg_samples: 64.00000, ips: 379.79553 images/sec
Training: 2023-10-17 19:56:25,604 - loss nan, lr: 0.025000, epoch: 2, step: 76100, eta: 88.32 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33702 sec, avg_samples: 64.00000, ips: 379.79553 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 76200, eta: 88.29 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33795 sec, avg_samples: 64.00000, ips: 378.75179 images/sec
Training: 2023-10-17 19:56:59,405 - loss nan, lr: 0.025000, epoch: 2, step: 76200, eta: 88.29 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33795 sec, avg_samples: 64.00000, ips: 378.75179 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 76300, eta: 88.27 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.70686 images/sec
Training: 2023-10-17 19:57:33,210 - loss nan, lr: 0.025000, epoch: 2, step: 76300, eta: 88.27 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.70686 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 76400, eta: 88.24 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.79401 images/sec
Training: 2023-10-17 19:58:07,007 - loss nan, lr: 0.025000, epoch: 2, step: 76400, eta: 88.24 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.79401 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 76500, eta: 88.22 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67995 images/sec
Training: 2023-10-17 19:58:40,815 - loss nan, lr: 0.025000, epoch: 2, step: 76500, eta: 88.22 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67995 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 76600, eta: 88.19 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33795 sec, avg_samples: 64.00000, ips: 378.75253 images/sec
Training: 2023-10-17 19:59:14,616 - loss nan, lr: 0.025000, epoch: 2, step: 76600, eta: 88.19 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33795 sec, avg_samples: 64.00000, ips: 378.75253 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 76700, eta: 88.16 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33770 sec, avg_samples: 64.00000, ips: 379.03346 images/sec
Training: 2023-10-17 19:59:48,392 - loss nan, lr: 0.025000, epoch: 2, step: 76700, eta: 88.16 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33770 sec, avg_samples: 64.00000, ips: 379.03346 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 76800, eta: 88.14 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33795 sec, avg_samples: 64.00000, ips: 378.75274 images/sec
Training: 2023-10-17 20:00:22,192 - loss nan, lr: 0.025000, epoch: 2, step: 76800, eta: 88.14 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33795 sec, avg_samples: 64.00000, ips: 378.75274 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 76900, eta: 88.11 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.83501 images/sec
Training: 2023-10-17 20:00:55,986 - loss nan, lr: 0.025000, epoch: 2, step: 76900, eta: 88.11 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.83501 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 77000, eta: 88.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.69914 images/sec
Training: 2023-10-17 20:01:29,792 - loss nan, lr: 0.025000, epoch: 2, step: 77000, eta: 88.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.69914 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 77100, eta: 88.06 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.83281 images/sec
Training: 2023-10-17 20:02:03,585 - loss nan, lr: 0.025000, epoch: 2, step: 77100, eta: 88.06 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.83281 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 77200, eta: 88.04 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33804 sec, avg_samples: 64.00000, ips: 378.65375 images/sec
Training: 2023-10-17 20:02:37,395 - loss nan, lr: 0.025000, epoch: 2, step: 77200, eta: 88.04 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33804 sec, avg_samples: 64.00000, ips: 378.65375 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 77300, eta: 88.01 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.69252 images/sec
Training: 2023-10-17 20:03:11,202 - loss nan, lr: 0.025000, epoch: 2, step: 77300, eta: 88.01 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.69252 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 77400, eta: 87.98 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33809 sec, avg_samples: 64.00000, ips: 378.59845 images/sec
Training: 2023-10-17 20:03:45,016 - loss nan, lr: 0.025000, epoch: 2, step: 77400, eta: 87.98 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33809 sec, avg_samples: 64.00000, ips: 378.59845 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 77500, eta: 87.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.64377 images/sec
Training: 2023-10-17 20:04:18,827 - loss nan, lr: 0.025000, epoch: 2, step: 77500, eta: 87.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.64377 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 77600, eta: 87.93 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66670 images/sec
Training: 2023-10-17 20:04:52,635 - loss nan, lr: 0.025000, epoch: 2, step: 77600, eta: 87.93 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66670 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 77700, eta: 87.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68887 images/sec
Training: 2023-10-17 20:05:26,442 - loss nan, lr: 0.025000, epoch: 2, step: 77700, eta: 87.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68887 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 77800, eta: 87.88 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33797 sec, avg_samples: 64.00000, ips: 378.73663 images/sec
Training: 2023-10-17 20:06:00,245 - loss nan, lr: 0.025000, epoch: 2, step: 77800, eta: 87.88 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33797 sec, avg_samples: 64.00000, ips: 378.73663 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 77900, eta: 87.86 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33793 sec, avg_samples: 64.00000, ips: 378.77381 images/sec
Training: 2023-10-17 20:06:34,044 - loss nan, lr: 0.025000, epoch: 2, step: 77900, eta: 87.86 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33793 sec, avg_samples: 64.00000, ips: 378.77381 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 78000, eta: 87.83 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33814 sec, avg_samples: 64.00000, ips: 378.53774 images/sec
Training: 2023-10-17 20:07:07,864 - loss nan, lr: 0.025000, epoch: 2, step: 78000, eta: 87.83 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33814 sec, avg_samples: 64.00000, ips: 378.53774 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][78000]XNorm: 0.000296
Training: 2023-10-17 20:07:39,882 - [lfw][78000]XNorm: 0.000296
INFO:root:[lfw][78000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 20:07:39,882 - [lfw][78000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][78000]Accuracy-Highest: 0.81183
Training: 2023-10-17 20:07:39,882 - [lfw][78000]Accuracy-Highest: 0.81183
INFO:root:test time: 32.0182
Training: 2023-10-17 20:07:39,882 - test time: 32.0182
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][78000]XNorm: 0.000296
Training: 2023-10-17 20:08:16,943 - [cfp_fp][78000]XNorm: 0.000296
INFO:root:[cfp_fp][78000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 20:08:16,944 - [cfp_fp][78000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][78000]Accuracy-Highest: 0.61571
Training: 2023-10-17 20:08:16,944 - [cfp_fp][78000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.0615
Training: 2023-10-17 20:08:16,944 - test time: 37.0615
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][78000]XNorm: 0.000296
Training: 2023-10-17 20:08:48,872 - [agedb_30][78000]XNorm: 0.000296
INFO:root:[agedb_30][78000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 20:08:48,872 - [agedb_30][78000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][78000]Accuracy-Highest: 0.58350
Training: 2023-10-17 20:08:48,872 - [agedb_30][78000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.9287
Training: 2023-10-17 20:08:48,872 - test time: 31.9287
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 78100, eta: 88.10 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33726 sec, avg_samples: 64.00000, ips: 379.53116 images/sec
Training: 2023-10-17 20:09:22,604 - loss nan, lr: 0.025000, epoch: 2, step: 78100, eta: 88.10 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33726 sec, avg_samples: 64.00000, ips: 379.53116 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 78200, eta: 88.07 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.71863 images/sec
Training: 2023-10-17 20:09:56,408 - loss nan, lr: 0.025000, epoch: 2, step: 78200, eta: 88.07 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.71863 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 78300, eta: 88.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33804 sec, avg_samples: 64.00000, ips: 378.64992 images/sec
Training: 2023-10-17 20:10:30,218 - loss nan, lr: 0.025000, epoch: 2, step: 78300, eta: 88.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33804 sec, avg_samples: 64.00000, ips: 378.64992 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 78400, eta: 88.02 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.61045 images/sec
Training: 2023-10-17 20:11:04,032 - loss nan, lr: 0.025000, epoch: 2, step: 78400, eta: 88.02 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.61045 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 78500, eta: 88.00 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33816 sec, avg_samples: 64.00000, ips: 378.51488 images/sec
Training: 2023-10-17 20:11:37,854 - loss nan, lr: 0.025000, epoch: 2, step: 78500, eta: 88.00 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33816 sec, avg_samples: 64.00000, ips: 378.51488 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 78600, eta: 87.97 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.72184 images/sec
Training: 2023-10-17 20:12:11,658 - loss nan, lr: 0.025000, epoch: 2, step: 78600, eta: 87.97 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.72184 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 78700, eta: 87.95 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33810 sec, avg_samples: 64.00000, ips: 378.58394 images/sec
Training: 2023-10-17 20:12:45,474 - loss nan, lr: 0.025000, epoch: 2, step: 78700, eta: 87.95 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33810 sec, avg_samples: 64.00000, ips: 378.58394 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 78800, eta: 87.92 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33806 sec, avg_samples: 64.00000, ips: 378.62688 images/sec
Training: 2023-10-17 20:13:19,286 - loss nan, lr: 0.025000, epoch: 2, step: 78800, eta: 87.92 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33806 sec, avg_samples: 64.00000, ips: 378.62688 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 78900, eta: 87.90 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33797 sec, avg_samples: 64.00000, ips: 378.72744 images/sec
Training: 2023-10-17 20:13:53,089 - loss nan, lr: 0.025000, epoch: 2, step: 78900, eta: 87.90 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33797 sec, avg_samples: 64.00000, ips: 378.72744 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 79000, eta: 87.87 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33804 sec, avg_samples: 64.00000, ips: 378.64918 images/sec
Training: 2023-10-17 20:14:26,899 - loss nan, lr: 0.025000, epoch: 2, step: 79000, eta: 87.87 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33804 sec, avg_samples: 64.00000, ips: 378.64918 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 79100, eta: 87.85 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76668 images/sec
Training: 2023-10-17 20:15:00,699 - loss nan, lr: 0.025000, epoch: 2, step: 79100, eta: 87.85 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76668 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 79200, eta: 87.82 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.60568 images/sec
Training: 2023-10-17 20:15:34,513 - loss nan, lr: 0.025000, epoch: 2, step: 79200, eta: 87.82 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.60568 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 79300, eta: 87.80 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.69034 images/sec
Training: 2023-10-17 20:16:08,319 - loss nan, lr: 0.025000, epoch: 2, step: 79300, eta: 87.80 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.69034 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 79400, eta: 87.77 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33815 sec, avg_samples: 64.00000, ips: 378.53084 images/sec
Training: 2023-10-17 20:16:42,140 - loss nan, lr: 0.025000, epoch: 2, step: 79400, eta: 87.77 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33815 sec, avg_samples: 64.00000, ips: 378.53084 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 79500, eta: 87.75 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33815 sec, avg_samples: 64.00000, ips: 378.53067 images/sec
Training: 2023-10-17 20:17:15,961 - loss nan, lr: 0.025000, epoch: 2, step: 79500, eta: 87.75 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33815 sec, avg_samples: 64.00000, ips: 378.53067 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 79600, eta: 87.72 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.67005 images/sec
Training: 2023-10-17 20:17:49,769 - loss nan, lr: 0.025000, epoch: 2, step: 79600, eta: 87.72 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.67005 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 79700, eta: 87.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33807 sec, avg_samples: 64.00000, ips: 378.62367 images/sec
Training: 2023-10-17 20:18:23,582 - loss nan, lr: 0.025000, epoch: 2, step: 79700, eta: 87.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33807 sec, avg_samples: 64.00000, ips: 378.62367 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 79800, eta: 87.67 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68827 images/sec
Training: 2023-10-17 20:18:57,389 - loss nan, lr: 0.025000, epoch: 2, step: 79800, eta: 87.67 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68827 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 79900, eta: 87.65 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33812 sec, avg_samples: 64.00000, ips: 378.55926 images/sec
Training: 2023-10-17 20:19:31,207 - loss nan, lr: 0.025000, epoch: 2, step: 79900, eta: 87.65 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33812 sec, avg_samples: 64.00000, ips: 378.55926 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 80000, eta: 87.62 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.71955 images/sec
Training: 2023-10-17 20:20:05,011 - loss nan, lr: 0.025000, epoch: 2, step: 80000, eta: 87.62 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.71955 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][80000]XNorm: 0.000230
Training: 2023-10-17 20:20:37,032 - [lfw][80000]XNorm: 0.000230
INFO:root:[lfw][80000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 20:20:37,032 - [lfw][80000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][80000]Accuracy-Highest: 0.81183
Training: 2023-10-17 20:20:37,032 - [lfw][80000]Accuracy-Highest: 0.81183
INFO:root:test time: 32.0212
Training: 2023-10-17 20:20:37,032 - test time: 32.0212
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][80000]XNorm: 0.000230
Training: 2023-10-17 20:21:14,086 - [cfp_fp][80000]XNorm: 0.000230
INFO:root:[cfp_fp][80000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 20:21:14,087 - [cfp_fp][80000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][80000]Accuracy-Highest: 0.61571
Training: 2023-10-17 20:21:14,087 - [cfp_fp][80000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.0545
Training: 2023-10-17 20:21:14,087 - test time: 37.0545
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][80000]XNorm: 0.000230
Training: 2023-10-17 20:21:46,015 - [agedb_30][80000]XNorm: 0.000230
INFO:root:[agedb_30][80000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 20:21:46,015 - [agedb_30][80000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][80000]Accuracy-Highest: 0.58350
Training: 2023-10-17 20:21:46,015 - [agedb_30][80000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.9284
Training: 2023-10-17 20:21:46,015 - test time: 31.9284
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 80100, eta: 87.88 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33719 sec, avg_samples: 64.00000, ips: 379.60657 images/sec
Training: 2023-10-17 20:22:19,740 - loss nan, lr: 0.025000, epoch: 2, step: 80100, eta: 87.88 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33719 sec, avg_samples: 64.00000, ips: 379.60657 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 80200, eta: 87.86 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66176 images/sec
Training: 2023-10-17 20:22:53,549 - loss nan, lr: 0.025000, epoch: 2, step: 80200, eta: 87.86 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66176 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 80300, eta: 87.83 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.82890 images/sec
Training: 2023-10-17 20:23:27,343 - loss nan, lr: 0.025000, epoch: 2, step: 80300, eta: 87.83 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.82890 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 80400, eta: 87.81 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.70069 images/sec
Training: 2023-10-17 20:24:01,149 - loss nan, lr: 0.025000, epoch: 2, step: 80400, eta: 87.81 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.70069 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 80500, eta: 87.78 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.72037 images/sec
Training: 2023-10-17 20:24:34,953 - loss nan, lr: 0.025000, epoch: 2, step: 80500, eta: 87.78 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.72037 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 80600, eta: 87.76 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33809 sec, avg_samples: 64.00000, ips: 378.59682 images/sec
Training: 2023-10-17 20:25:08,768 - loss nan, lr: 0.025000, epoch: 2, step: 80600, eta: 87.76 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33809 sec, avg_samples: 64.00000, ips: 378.59682 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 80700, eta: 87.73 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33796 sec, avg_samples: 64.00000, ips: 378.73978 images/sec
Training: 2023-10-17 20:25:42,570 - loss nan, lr: 0.025000, epoch: 2, step: 80700, eta: 87.73 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33796 sec, avg_samples: 64.00000, ips: 378.73978 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 80800, eta: 87.71 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67051 images/sec
Training: 2023-10-17 20:26:16,378 - loss nan, lr: 0.025000, epoch: 2, step: 80800, eta: 87.71 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67051 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 80900, eta: 87.68 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33795 sec, avg_samples: 64.00000, ips: 378.75740 images/sec
Training: 2023-10-17 20:26:50,178 - loss nan, lr: 0.025000, epoch: 2, step: 80900, eta: 87.68 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33795 sec, avg_samples: 64.00000, ips: 378.75740 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 81000, eta: 87.66 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76733 images/sec
Training: 2023-10-17 20:27:23,978 - loss nan, lr: 0.025000, epoch: 2, step: 81000, eta: 87.66 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76733 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 81100, eta: 87.63 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33804 sec, avg_samples: 64.00000, ips: 378.64798 images/sec
Training: 2023-10-17 20:27:57,788 - loss nan, lr: 0.025000, epoch: 2, step: 81100, eta: 87.63 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33804 sec, avg_samples: 64.00000, ips: 378.64798 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 81200, eta: 87.61 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67878 images/sec
Training: 2023-10-17 20:28:31,596 - loss nan, lr: 0.025000, epoch: 2, step: 81200, eta: 87.61 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67878 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 81300, eta: 87.58 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67351 images/sec
Training: 2023-10-17 20:29:05,404 - loss nan, lr: 0.025000, epoch: 2, step: 81300, eta: 87.58 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67351 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 81400, eta: 87.56 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.86187 images/sec
Training: 2023-10-17 20:29:39,195 - loss nan, lr: 0.025000, epoch: 2, step: 81400, eta: 87.56 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.86187 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 81500, eta: 87.53 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.70639 images/sec
Training: 2023-10-17 20:30:13,000 - loss nan, lr: 0.025000, epoch: 2, step: 81500, eta: 87.53 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.70639 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 81600, eta: 87.51 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.70596 images/sec
Training: 2023-10-17 20:30:46,805 - loss nan, lr: 0.025000, epoch: 2, step: 81600, eta: 87.51 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.70596 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 81700, eta: 87.48 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.72491 images/sec
Training: 2023-10-17 20:31:20,609 - loss nan, lr: 0.025000, epoch: 2, step: 81700, eta: 87.48 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.72491 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 81800, eta: 87.46 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66128 images/sec
Training: 2023-10-17 20:31:54,418 - loss nan, lr: 0.025000, epoch: 2, step: 81800, eta: 87.46 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66128 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 81900, eta: 87.43 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68216 images/sec
Training: 2023-10-17 20:32:28,225 - loss nan, lr: 0.025000, epoch: 2, step: 81900, eta: 87.43 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68216 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 82000, eta: 87.41 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33781 sec, avg_samples: 64.00000, ips: 378.91087 images/sec
Training: 2023-10-17 20:33:02,012 - loss nan, lr: 0.025000, epoch: 2, step: 82000, eta: 87.41 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33781 sec, avg_samples: 64.00000, ips: 378.91087 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][82000]XNorm: 0.000179
Training: 2023-10-17 20:33:34,052 - [lfw][82000]XNorm: 0.000179
INFO:root:[lfw][82000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 20:33:34,052 - [lfw][82000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][82000]Accuracy-Highest: 0.81183
Training: 2023-10-17 20:33:34,052 - [lfw][82000]Accuracy-Highest: 0.81183
INFO:root:test time: 32.0396
Training: 2023-10-17 20:33:34,052 - test time: 32.0396
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][82000]XNorm: 0.000179
Training: 2023-10-17 20:34:11,172 - [cfp_fp][82000]XNorm: 0.000179
INFO:root:[cfp_fp][82000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 20:34:11,172 - [cfp_fp][82000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][82000]Accuracy-Highest: 0.61571
Training: 2023-10-17 20:34:11,172 - [cfp_fp][82000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.1197
Training: 2023-10-17 20:34:11,172 - test time: 37.1197
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][82000]XNorm: 0.000179
Training: 2023-10-17 20:34:43,102 - [agedb_30][82000]XNorm: 0.000179
INFO:root:[agedb_30][82000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 20:34:43,102 - [agedb_30][82000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][82000]Accuracy-Highest: 0.58350
Training: 2023-10-17 20:34:43,102 - [agedb_30][82000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.9306
Training: 2023-10-17 20:34:43,103 - test time: 31.9306
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 82100, eta: 87.66 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33712 sec, avg_samples: 64.00000, ips: 379.68633 images/sec
Training: 2023-10-17 20:35:16,820 - loss nan, lr: 0.025000, epoch: 2, step: 82100, eta: 87.66 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33712 sec, avg_samples: 64.00000, ips: 379.68633 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 82200, eta: 87.64 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.69413 images/sec
Training: 2023-10-17 20:35:50,627 - loss nan, lr: 0.025000, epoch: 2, step: 82200, eta: 87.64 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.69413 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 82300, eta: 87.61 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33793 sec, avg_samples: 64.00000, ips: 378.77505 images/sec
Training: 2023-10-17 20:36:24,426 - loss nan, lr: 0.025000, epoch: 2, step: 82300, eta: 87.61 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33793 sec, avg_samples: 64.00000, ips: 378.77505 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 82400, eta: 87.59 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33781 sec, avg_samples: 64.00000, ips: 378.91248 images/sec
Training: 2023-10-17 20:36:58,213 - loss nan, lr: 0.025000, epoch: 2, step: 82400, eta: 87.59 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33781 sec, avg_samples: 64.00000, ips: 378.91248 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 82500, eta: 87.56 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67535 images/sec
Training: 2023-10-17 20:37:32,021 - loss nan, lr: 0.025000, epoch: 2, step: 82500, eta: 87.56 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67535 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 82600, eta: 87.54 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33793 sec, avg_samples: 64.00000, ips: 378.77733 images/sec
Training: 2023-10-17 20:38:05,819 - loss nan, lr: 0.025000, epoch: 2, step: 82600, eta: 87.54 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33793 sec, avg_samples: 64.00000, ips: 378.77733 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 82700, eta: 87.51 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.82163 images/sec
Training: 2023-10-17 20:38:39,614 - loss nan, lr: 0.025000, epoch: 2, step: 82700, eta: 87.51 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.82163 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 82800, eta: 87.49 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66886 images/sec
Training: 2023-10-17 20:39:13,423 - loss nan, lr: 0.025000, epoch: 2, step: 82800, eta: 87.49 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66886 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 82900, eta: 87.47 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33793 sec, avg_samples: 64.00000, ips: 378.77463 images/sec
Training: 2023-10-17 20:39:47,222 - loss nan, lr: 0.025000, epoch: 2, step: 82900, eta: 87.47 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33793 sec, avg_samples: 64.00000, ips: 378.77463 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 83000, eta: 87.44 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.85684 images/sec
Training: 2023-10-17 20:40:21,014 - loss nan, lr: 0.025000, epoch: 2, step: 83000, eta: 87.44 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.85684 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 83100, eta: 87.42 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33807 sec, avg_samples: 64.00000, ips: 378.61509 images/sec
Training: 2023-10-17 20:40:54,827 - loss nan, lr: 0.025000, epoch: 2, step: 83100, eta: 87.42 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33807 sec, avg_samples: 64.00000, ips: 378.61509 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 83200, eta: 87.39 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33784 sec, avg_samples: 64.00000, ips: 378.87235 images/sec
Training: 2023-10-17 20:41:28,617 - loss nan, lr: 0.025000, epoch: 2, step: 83200, eta: 87.39 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33784 sec, avg_samples: 64.00000, ips: 378.87235 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 83300, eta: 87.37 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33814 sec, avg_samples: 64.00000, ips: 378.53767 images/sec
Training: 2023-10-17 20:42:02,438 - loss nan, lr: 0.025000, epoch: 2, step: 83300, eta: 87.37 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33814 sec, avg_samples: 64.00000, ips: 378.53767 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 83400, eta: 87.34 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33804 sec, avg_samples: 64.00000, ips: 378.65528 images/sec
Training: 2023-10-17 20:42:36,247 - loss nan, lr: 0.025000, epoch: 2, step: 83400, eta: 87.34 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33804 sec, avg_samples: 64.00000, ips: 378.65528 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 83500, eta: 87.32 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.65958 images/sec
Training: 2023-10-17 20:43:10,057 - loss nan, lr: 0.025000, epoch: 2, step: 83500, eta: 87.32 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.65958 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 83600, eta: 87.29 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33814 sec, avg_samples: 64.00000, ips: 378.54364 images/sec
Training: 2023-10-17 20:43:43,876 - loss nan, lr: 0.025000, epoch: 2, step: 83600, eta: 87.29 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33814 sec, avg_samples: 64.00000, ips: 378.54364 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 83700, eta: 87.27 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.70732 images/sec
Training: 2023-10-17 20:44:17,681 - loss nan, lr: 0.025000, epoch: 2, step: 83700, eta: 87.27 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.70732 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 83800, eta: 87.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33796 sec, avg_samples: 64.00000, ips: 378.74309 images/sec
Training: 2023-10-17 20:44:51,483 - loss nan, lr: 0.025000, epoch: 2, step: 83800, eta: 87.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33796 sec, avg_samples: 64.00000, ips: 378.74309 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 83900, eta: 87.22 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33783 sec, avg_samples: 64.00000, ips: 378.88595 images/sec
Training: 2023-10-17 20:45:25,272 - loss nan, lr: 0.025000, epoch: 2, step: 83900, eta: 87.22 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33783 sec, avg_samples: 64.00000, ips: 378.88595 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 84000, eta: 87.20 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76215 images/sec
Training: 2023-10-17 20:45:59,073 - loss nan, lr: 0.025000, epoch: 2, step: 84000, eta: 87.20 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76215 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][84000]XNorm: 0.000140
Training: 2023-10-17 20:46:31,101 - [lfw][84000]XNorm: 0.000140
INFO:root:[lfw][84000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 20:46:31,101 - [lfw][84000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][84000]Accuracy-Highest: 0.81183
Training: 2023-10-17 20:46:31,101 - [lfw][84000]Accuracy-Highest: 0.81183
INFO:root:test time: 32.0284
Training: 2023-10-17 20:46:31,101 - test time: 32.0284
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][84000]XNorm: 0.000140
Training: 2023-10-17 20:47:08,148 - [cfp_fp][84000]XNorm: 0.000140
INFO:root:[cfp_fp][84000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 20:47:08,148 - [cfp_fp][84000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][84000]Accuracy-Highest: 0.61571
Training: 2023-10-17 20:47:08,148 - [cfp_fp][84000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.0471
Training: 2023-10-17 20:47:08,148 - test time: 37.0471
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][84000]XNorm: 0.000140
Training: 2023-10-17 20:47:40,083 - [agedb_30][84000]XNorm: 0.000140
INFO:root:[agedb_30][84000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 20:47:40,083 - [agedb_30][84000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][84000]Accuracy-Highest: 0.58350
Training: 2023-10-17 20:47:40,083 - [agedb_30][84000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.9346
Training: 2023-10-17 20:47:40,083 - test time: 31.9346
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 84100, eta: 87.44 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33713 sec, avg_samples: 64.00000, ips: 379.67201 images/sec
Training: 2023-10-17 20:48:13,802 - loss nan, lr: 0.025000, epoch: 2, step: 84100, eta: 87.44 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33713 sec, avg_samples: 64.00000, ips: 379.67201 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 84200, eta: 87.42 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.78877 images/sec
Training: 2023-10-17 20:48:47,600 - loss nan, lr: 0.025000, epoch: 2, step: 84200, eta: 87.42 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.78877 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 84300, eta: 87.40 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33806 sec, avg_samples: 64.00000, ips: 378.62665 images/sec
Training: 2023-10-17 20:49:21,412 - loss nan, lr: 0.025000, epoch: 2, step: 84300, eta: 87.40 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33806 sec, avg_samples: 64.00000, ips: 378.62665 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 84400, eta: 87.37 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.60435 images/sec
Training: 2023-10-17 20:49:55,227 - loss nan, lr: 0.025000, epoch: 2, step: 84400, eta: 87.37 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.60435 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 84500, eta: 87.35 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33809 sec, avg_samples: 64.00000, ips: 378.59823 images/sec
Training: 2023-10-17 20:50:29,041 - loss nan, lr: 0.025000, epoch: 2, step: 84500, eta: 87.35 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33809 sec, avg_samples: 64.00000, ips: 378.59823 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 84600, eta: 87.32 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.80967 images/sec
Training: 2023-10-17 20:51:02,837 - loss nan, lr: 0.025000, epoch: 2, step: 84600, eta: 87.32 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.80967 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 84700, eta: 87.30 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33795 sec, avg_samples: 64.00000, ips: 378.75750 images/sec
Training: 2023-10-17 20:51:36,638 - loss nan, lr: 0.025000, epoch: 2, step: 84700, eta: 87.30 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33795 sec, avg_samples: 64.00000, ips: 378.75750 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 84800, eta: 87.27 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33811 sec, avg_samples: 64.00000, ips: 378.57951 images/sec
Training: 2023-10-17 20:52:10,454 - loss nan, lr: 0.025000, epoch: 2, step: 84800, eta: 87.27 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33811 sec, avg_samples: 64.00000, ips: 378.57951 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 84900, eta: 87.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33820 sec, avg_samples: 64.00000, ips: 378.47873 images/sec
Training: 2023-10-17 20:52:44,280 - loss nan, lr: 0.025000, epoch: 2, step: 84900, eta: 87.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33820 sec, avg_samples: 64.00000, ips: 378.47873 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 85000, eta: 87.23 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.61414 images/sec
Training: 2023-10-17 20:53:18,093 - loss nan, lr: 0.025000, epoch: 2, step: 85000, eta: 87.23 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.61414 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 85100, eta: 87.20 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66422 images/sec
Training: 2023-10-17 20:53:51,902 - loss nan, lr: 0.025000, epoch: 2, step: 85100, eta: 87.20 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66422 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 85200, eta: 87.18 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33813 sec, avg_samples: 64.00000, ips: 378.55198 images/sec
Training: 2023-10-17 20:54:25,721 - loss nan, lr: 0.025000, epoch: 2, step: 85200, eta: 87.18 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33813 sec, avg_samples: 64.00000, ips: 378.55198 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 85300, eta: 87.15 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.70067 images/sec
Training: 2023-10-17 20:54:59,527 - loss nan, lr: 0.025000, epoch: 2, step: 85300, eta: 87.15 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.70067 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 85400, eta: 87.13 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33797 sec, avg_samples: 64.00000, ips: 378.73264 images/sec
Training: 2023-10-17 20:55:33,330 - loss nan, lr: 0.025000, epoch: 2, step: 85400, eta: 87.13 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33797 sec, avg_samples: 64.00000, ips: 378.73264 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 85500, eta: 87.11 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33816 sec, avg_samples: 64.00000, ips: 378.52159 images/sec
Training: 2023-10-17 20:56:07,151 - loss nan, lr: 0.025000, epoch: 2, step: 85500, eta: 87.11 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33816 sec, avg_samples: 64.00000, ips: 378.52159 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 85600, eta: 87.08 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33813 sec, avg_samples: 64.00000, ips: 378.55252 images/sec
Training: 2023-10-17 20:56:40,970 - loss nan, lr: 0.025000, epoch: 2, step: 85600, eta: 87.08 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33813 sec, avg_samples: 64.00000, ips: 378.55252 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 85700, eta: 87.06 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76869 images/sec
Training: 2023-10-17 20:57:14,770 - loss nan, lr: 0.025000, epoch: 2, step: 85700, eta: 87.06 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76869 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 85800, eta: 87.03 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33811 sec, avg_samples: 64.00000, ips: 378.57063 images/sec
Training: 2023-10-17 20:57:48,587 - loss nan, lr: 0.025000, epoch: 2, step: 85800, eta: 87.03 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33811 sec, avg_samples: 64.00000, ips: 378.57063 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 85900, eta: 87.01 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.63676 images/sec
Training: 2023-10-17 20:58:22,398 - loss nan, lr: 0.025000, epoch: 2, step: 85900, eta: 87.01 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.63676 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 86000, eta: 86.99 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33812 sec, avg_samples: 64.00000, ips: 378.55920 images/sec
Training: 2023-10-17 20:58:56,217 - loss nan, lr: 0.025000, epoch: 2, step: 86000, eta: 86.99 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33812 sec, avg_samples: 64.00000, ips: 378.55920 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][86000]XNorm: 0.000109
Training: 2023-10-17 20:59:28,238 - [lfw][86000]XNorm: 0.000109
INFO:root:[lfw][86000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 20:59:28,238 - [lfw][86000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][86000]Accuracy-Highest: 0.81183
Training: 2023-10-17 20:59:28,238 - [lfw][86000]Accuracy-Highest: 0.81183
INFO:root:test time: 32.0215
Training: 2023-10-17 20:59:28,238 - test time: 32.0215
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][86000]XNorm: 0.000109
Training: 2023-10-17 21:00:05,329 - [cfp_fp][86000]XNorm: 0.000109
INFO:root:[cfp_fp][86000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 21:00:05,330 - [cfp_fp][86000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][86000]Accuracy-Highest: 0.61571
Training: 2023-10-17 21:00:05,330 - [cfp_fp][86000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.0913
Training: 2023-10-17 21:00:05,330 - test time: 37.0913
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][86000]XNorm: 0.000109
Training: 2023-10-17 21:00:37,313 - [agedb_30][86000]XNorm: 0.000109
INFO:root:[agedb_30][86000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 21:00:37,313 - [agedb_30][86000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][86000]Accuracy-Highest: 0.58350
Training: 2023-10-17 21:00:37,313 - [agedb_30][86000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.9837
Training: 2023-10-17 21:00:37,313 - test time: 31.9837
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 86100, eta: 87.23 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33715 sec, avg_samples: 64.00000, ips: 379.65160 images/sec
Training: 2023-10-17 21:01:11,034 - loss nan, lr: 0.025000, epoch: 2, step: 86100, eta: 87.23 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33715 sec, avg_samples: 64.00000, ips: 379.65160 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 86200, eta: 87.20 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33770 sec, avg_samples: 64.00000, ips: 379.03180 images/sec
Training: 2023-10-17 21:01:44,811 - loss nan, lr: 0.025000, epoch: 2, step: 86200, eta: 87.20 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33770 sec, avg_samples: 64.00000, ips: 379.03180 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 86300, eta: 87.18 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33797 sec, avg_samples: 64.00000, ips: 378.73174 images/sec
Training: 2023-10-17 21:02:18,614 - loss nan, lr: 0.025000, epoch: 2, step: 86300, eta: 87.18 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33797 sec, avg_samples: 64.00000, ips: 378.73174 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 86400, eta: 87.15 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33778 sec, avg_samples: 64.00000, ips: 378.94054 images/sec
Training: 2023-10-17 21:02:52,398 - loss nan, lr: 0.025000, epoch: 2, step: 86400, eta: 87.15 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33778 sec, avg_samples: 64.00000, ips: 378.94054 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 86500, eta: 87.13 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33787 sec, avg_samples: 64.00000, ips: 378.84441 images/sec
Training: 2023-10-17 21:03:26,191 - loss nan, lr: 0.025000, epoch: 2, step: 86500, eta: 87.13 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33787 sec, avg_samples: 64.00000, ips: 378.84441 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 86600, eta: 87.11 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.79405 images/sec
Training: 2023-10-17 21:03:59,988 - loss nan, lr: 0.025000, epoch: 2, step: 86600, eta: 87.11 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.79405 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 86700, eta: 87.08 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33796 sec, avg_samples: 64.00000, ips: 378.74490 images/sec
Training: 2023-10-17 21:04:33,790 - loss nan, lr: 0.025000, epoch: 2, step: 86700, eta: 87.08 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33796 sec, avg_samples: 64.00000, ips: 378.74490 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 86800, eta: 87.06 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33796 sec, avg_samples: 64.00000, ips: 378.73902 images/sec
Training: 2023-10-17 21:05:07,592 - loss nan, lr: 0.025000, epoch: 2, step: 86800, eta: 87.06 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33796 sec, avg_samples: 64.00000, ips: 378.73902 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 86900, eta: 87.03 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.82676 images/sec
Training: 2023-10-17 21:05:41,386 - loss nan, lr: 0.025000, epoch: 2, step: 86900, eta: 87.03 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.82676 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 87000, eta: 87.01 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.80727 images/sec
Training: 2023-10-17 21:06:15,183 - loss nan, lr: 0.025000, epoch: 2, step: 87000, eta: 87.01 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.80727 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 87100, eta: 86.99 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33797 sec, avg_samples: 64.00000, ips: 378.73284 images/sec
Training: 2023-10-17 21:06:48,985 - loss nan, lr: 0.025000, epoch: 2, step: 87100, eta: 86.99 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33797 sec, avg_samples: 64.00000, ips: 378.73284 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 87200, eta: 86.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.79760 images/sec
Training: 2023-10-17 21:07:22,782 - loss nan, lr: 0.025000, epoch: 2, step: 87200, eta: 86.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.79760 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 87300, eta: 86.94 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.86699 images/sec
Training: 2023-10-17 21:07:56,573 - loss nan, lr: 0.025000, epoch: 2, step: 87300, eta: 86.94 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.86699 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 87400, eta: 86.92 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.72396 images/sec
Training: 2023-10-17 21:08:30,377 - loss nan, lr: 0.025000, epoch: 2, step: 87400, eta: 86.92 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.72396 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 87500, eta: 86.89 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33809 sec, avg_samples: 64.00000, ips: 378.60194 images/sec
Training: 2023-10-17 21:09:04,191 - loss nan, lr: 0.025000, epoch: 2, step: 87500, eta: 86.89 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33809 sec, avg_samples: 64.00000, ips: 378.60194 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 87600, eta: 86.87 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33815 sec, avg_samples: 64.00000, ips: 378.53468 images/sec
Training: 2023-10-17 21:09:38,011 - loss nan, lr: 0.025000, epoch: 2, step: 87600, eta: 86.87 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33815 sec, avg_samples: 64.00000, ips: 378.53468 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 87700, eta: 86.84 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.70853 images/sec
Training: 2023-10-17 21:10:11,816 - loss nan, lr: 0.025000, epoch: 2, step: 87700, eta: 86.84 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.70853 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 87800, eta: 86.82 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.69707 images/sec
Training: 2023-10-17 21:10:45,621 - loss nan, lr: 0.025000, epoch: 2, step: 87800, eta: 86.82 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.69707 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 87900, eta: 86.80 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33806 sec, avg_samples: 64.00000, ips: 378.63413 images/sec
Training: 2023-10-17 21:11:19,433 - loss nan, lr: 0.025000, epoch: 2, step: 87900, eta: 86.80 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33806 sec, avg_samples: 64.00000, ips: 378.63413 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 88000, eta: 86.77 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33812 sec, avg_samples: 64.00000, ips: 378.56476 images/sec
Training: 2023-10-17 21:11:53,250 - loss nan, lr: 0.025000, epoch: 2, step: 88000, eta: 86.77 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33812 sec, avg_samples: 64.00000, ips: 378.56476 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][88000]XNorm: 0.000085
Training: 2023-10-17 21:12:25,293 - [lfw][88000]XNorm: 0.000085
INFO:root:[lfw][88000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 21:12:25,293 - [lfw][88000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][88000]Accuracy-Highest: 0.81183
Training: 2023-10-17 21:12:25,293 - [lfw][88000]Accuracy-Highest: 0.81183
INFO:root:test time: 32.0428
Training: 2023-10-17 21:12:25,293 - test time: 32.0428
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][88000]XNorm: 0.000085
Training: 2023-10-17 21:13:02,379 - [cfp_fp][88000]XNorm: 0.000085
INFO:root:[cfp_fp][88000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 21:13:02,379 - [cfp_fp][88000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][88000]Accuracy-Highest: 0.61571
Training: 2023-10-17 21:13:02,379 - [cfp_fp][88000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.0865
Training: 2023-10-17 21:13:02,379 - test time: 37.0865
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][88000]XNorm: 0.000085
Training: 2023-10-17 21:13:34,316 - [agedb_30][88000]XNorm: 0.000085
INFO:root:[agedb_30][88000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 21:13:34,316 - [agedb_30][88000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][88000]Accuracy-Highest: 0.58350
Training: 2023-10-17 21:13:34,316 - [agedb_30][88000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.9369
Training: 2023-10-17 21:13:34,316 - test time: 31.9369
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 88100, eta: 87.01 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33724 sec, avg_samples: 64.00000, ips: 379.55510 images/sec
Training: 2023-10-17 21:14:08,046 - loss nan, lr: 0.025000, epoch: 2, step: 88100, eta: 87.01 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33724 sec, avg_samples: 64.00000, ips: 379.55510 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 88200, eta: 86.98 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.71463 images/sec
Training: 2023-10-17 21:14:41,850 - loss nan, lr: 0.025000, epoch: 2, step: 88200, eta: 86.98 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.71463 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 88300, eta: 86.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.85589 images/sec
Training: 2023-10-17 21:15:15,642 - loss nan, lr: 0.025000, epoch: 2, step: 88300, eta: 86.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.85589 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 88400, eta: 86.94 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76786 images/sec
Training: 2023-10-17 21:15:49,442 - loss nan, lr: 0.025000, epoch: 2, step: 88400, eta: 86.94 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76786 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 88500, eta: 86.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33795 sec, avg_samples: 64.00000, ips: 378.75704 images/sec
Training: 2023-10-17 21:16:23,242 - loss nan, lr: 0.025000, epoch: 2, step: 88500, eta: 86.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33795 sec, avg_samples: 64.00000, ips: 378.75704 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 88600, eta: 86.89 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.78941 images/sec
Training: 2023-10-17 21:16:57,040 - loss nan, lr: 0.025000, epoch: 2, step: 88600, eta: 86.89 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.78941 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 88700, eta: 86.87 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33806 sec, avg_samples: 64.00000, ips: 378.63044 images/sec
Training: 2023-10-17 21:17:30,852 - loss nan, lr: 0.025000, epoch: 2, step: 88700, eta: 86.87 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33806 sec, avg_samples: 64.00000, ips: 378.63044 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 88800, eta: 86.84 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33804 sec, avg_samples: 64.00000, ips: 378.65534 images/sec
Training: 2023-10-17 21:18:04,662 - loss nan, lr: 0.025000, epoch: 2, step: 88800, eta: 86.84 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33804 sec, avg_samples: 64.00000, ips: 378.65534 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 88900, eta: 86.82 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33806 sec, avg_samples: 64.00000, ips: 378.63215 images/sec
Training: 2023-10-17 21:18:38,474 - loss nan, lr: 0.025000, epoch: 2, step: 88900, eta: 86.82 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33806 sec, avg_samples: 64.00000, ips: 378.63215 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 89000, eta: 86.80 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33807 sec, avg_samples: 64.00000, ips: 378.62042 images/sec
Training: 2023-10-17 21:19:12,286 - loss nan, lr: 0.025000, epoch: 2, step: 89000, eta: 86.80 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33807 sec, avg_samples: 64.00000, ips: 378.62042 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 89100, eta: 86.77 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67198 images/sec
Training: 2023-10-17 21:19:46,095 - loss nan, lr: 0.025000, epoch: 2, step: 89100, eta: 86.77 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67198 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 89200, eta: 86.75 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33809 sec, avg_samples: 64.00000, ips: 378.59377 images/sec
Training: 2023-10-17 21:20:19,910 - loss nan, lr: 0.025000, epoch: 2, step: 89200, eta: 86.75 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33809 sec, avg_samples: 64.00000, ips: 378.59377 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 89300, eta: 86.73 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33809 sec, avg_samples: 64.00000, ips: 378.60112 images/sec
Training: 2023-10-17 21:20:53,724 - loss nan, lr: 0.025000, epoch: 2, step: 89300, eta: 86.73 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33809 sec, avg_samples: 64.00000, ips: 378.60112 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 89400, eta: 86.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.78429 images/sec
Training: 2023-10-17 21:21:27,523 - loss nan, lr: 0.025000, epoch: 2, step: 89400, eta: 86.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.78429 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 89500, eta: 86.68 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66514 images/sec
Training: 2023-10-17 21:22:01,331 - loss nan, lr: 0.025000, epoch: 2, step: 89500, eta: 86.68 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66514 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 89600, eta: 86.66 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68286 images/sec
Training: 2023-10-17 21:22:35,139 - loss nan, lr: 0.025000, epoch: 2, step: 89600, eta: 86.66 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68286 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 89700, eta: 86.63 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33804 sec, avg_samples: 64.00000, ips: 378.64916 images/sec
Training: 2023-10-17 21:23:08,949 - loss nan, lr: 0.025000, epoch: 2, step: 89700, eta: 86.63 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33804 sec, avg_samples: 64.00000, ips: 378.64916 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 89800, eta: 86.61 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33809 sec, avg_samples: 64.00000, ips: 378.59283 images/sec
Training: 2023-10-17 21:23:42,764 - loss nan, lr: 0.025000, epoch: 2, step: 89800, eta: 86.61 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33809 sec, avg_samples: 64.00000, ips: 378.59283 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 89900, eta: 86.59 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.72190 images/sec
Training: 2023-10-17 21:24:16,568 - loss nan, lr: 0.025000, epoch: 2, step: 89900, eta: 86.59 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.72190 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 90000, eta: 86.56 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67555 images/sec
Training: 2023-10-17 21:24:50,376 - loss nan, lr: 0.025000, epoch: 2, step: 90000, eta: 86.56 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67555 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][90000]XNorm: 0.000066
Training: 2023-10-17 21:25:22,381 - [lfw][90000]XNorm: 0.000066
INFO:root:[lfw][90000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 21:25:22,381 - [lfw][90000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][90000]Accuracy-Highest: 0.81183
Training: 2023-10-17 21:25:22,381 - [lfw][90000]Accuracy-Highest: 0.81183
INFO:root:test time: 32.0049
Training: 2023-10-17 21:25:22,381 - test time: 32.0049
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][90000]XNorm: 0.000066
Training: 2023-10-17 21:25:59,423 - [cfp_fp][90000]XNorm: 0.000066
INFO:root:[cfp_fp][90000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 21:25:59,423 - [cfp_fp][90000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][90000]Accuracy-Highest: 0.61571
Training: 2023-10-17 21:25:59,423 - [cfp_fp][90000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.0418
Training: 2023-10-17 21:25:59,423 - test time: 37.0418
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][90000]XNorm: 0.000066
Training: 2023-10-17 21:26:31,344 - [agedb_30][90000]XNorm: 0.000066
INFO:root:[agedb_30][90000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 21:26:31,344 - [agedb_30][90000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][90000]Accuracy-Highest: 0.58350
Training: 2023-10-17 21:26:31,344 - [agedb_30][90000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.9215
Training: 2023-10-17 21:26:31,344 - test time: 31.9215
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 90100, eta: 86.79 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33719 sec, avg_samples: 64.00000, ips: 379.60382 images/sec
Training: 2023-10-17 21:27:05,070 - loss nan, lr: 0.025000, epoch: 2, step: 90100, eta: 86.79 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33719 sec, avg_samples: 64.00000, ips: 379.60382 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 90200, eta: 86.77 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.63672 images/sec
Training: 2023-10-17 21:27:38,881 - loss nan, lr: 0.025000, epoch: 2, step: 90200, eta: 86.77 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.63672 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 90300, eta: 86.74 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33814 sec, avg_samples: 64.00000, ips: 378.54341 images/sec
Training: 2023-10-17 21:28:12,701 - loss nan, lr: 0.025000, epoch: 2, step: 90300, eta: 86.74 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33814 sec, avg_samples: 64.00000, ips: 378.54341 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 90400, eta: 86.72 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68646 images/sec
Training: 2023-10-17 21:28:46,508 - loss nan, lr: 0.025000, epoch: 2, step: 90400, eta: 86.72 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68646 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 90500, eta: 86.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66011 images/sec
Training: 2023-10-17 21:29:20,317 - loss nan, lr: 0.025000, epoch: 2, step: 90500, eta: 86.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66011 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 90600, eta: 86.67 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.64570 images/sec
Training: 2023-10-17 21:29:54,127 - loss nan, lr: 0.025000, epoch: 2, step: 90600, eta: 86.67 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.64570 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 90700, eta: 86.65 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33807 sec, avg_samples: 64.00000, ips: 378.61858 images/sec
Training: 2023-10-17 21:30:27,940 - loss nan, lr: 0.025000, epoch: 2, step: 90700, eta: 86.65 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33807 sec, avg_samples: 64.00000, ips: 378.61858 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 90800, eta: 86.63 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33813 sec, avg_samples: 64.00000, ips: 378.55276 images/sec
Training: 2023-10-17 21:31:01,759 - loss nan, lr: 0.025000, epoch: 2, step: 90800, eta: 86.63 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33813 sec, avg_samples: 64.00000, ips: 378.55276 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 90900, eta: 86.60 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66983 images/sec
Training: 2023-10-17 21:31:35,568 - loss nan, lr: 0.025000, epoch: 2, step: 90900, eta: 86.60 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66983 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 91000, eta: 86.58 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33797 sec, avg_samples: 64.00000, ips: 378.73225 images/sec
Training: 2023-10-17 21:32:09,371 - loss nan, lr: 0.025000, epoch: 2, step: 91000, eta: 86.58 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33797 sec, avg_samples: 64.00000, ips: 378.73225 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 91100, eta: 86.56 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33815 sec, avg_samples: 64.00000, ips: 378.53476 images/sec
Training: 2023-10-17 21:32:43,191 - loss nan, lr: 0.025000, epoch: 2, step: 91100, eta: 86.56 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33815 sec, avg_samples: 64.00000, ips: 378.53476 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 91200, eta: 86.53 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.60487 images/sec
Training: 2023-10-17 21:33:17,006 - loss nan, lr: 0.025000, epoch: 2, step: 91200, eta: 86.53 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.60487 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 91300, eta: 86.51 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68981 images/sec
Training: 2023-10-17 21:33:50,812 - loss nan, lr: 0.025000, epoch: 2, step: 91300, eta: 86.51 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68981 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 91400, eta: 86.49 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33813 sec, avg_samples: 64.00000, ips: 378.55019 images/sec
Training: 2023-10-17 21:34:24,631 - loss nan, lr: 0.025000, epoch: 2, step: 91400, eta: 86.49 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33813 sec, avg_samples: 64.00000, ips: 378.55019 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 91500, eta: 86.46 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.70484 images/sec
Training: 2023-10-17 21:34:58,437 - loss nan, lr: 0.025000, epoch: 2, step: 91500, eta: 86.46 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.70484 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 91600, eta: 86.44 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66058 images/sec
Training: 2023-10-17 21:35:32,246 - loss nan, lr: 0.025000, epoch: 2, step: 91600, eta: 86.44 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66058 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 91700, eta: 86.42 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76476 images/sec
Training: 2023-10-17 21:36:06,046 - loss nan, lr: 0.025000, epoch: 2, step: 91700, eta: 86.42 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76476 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 91800, eta: 86.40 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.71184 images/sec
Training: 2023-10-17 21:36:39,851 - loss nan, lr: 0.025000, epoch: 2, step: 91800, eta: 86.40 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.71184 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 91900, eta: 86.37 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33795 sec, avg_samples: 64.00000, ips: 378.75685 images/sec
Training: 2023-10-17 21:37:13,651 - loss nan, lr: 0.025000, epoch: 2, step: 91900, eta: 86.37 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33795 sec, avg_samples: 64.00000, ips: 378.75685 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 92000, eta: 86.35 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33795 sec, avg_samples: 64.00000, ips: 378.75702 images/sec
Training: 2023-10-17 21:37:47,452 - loss nan, lr: 0.025000, epoch: 2, step: 92000, eta: 86.35 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33795 sec, avg_samples: 64.00000, ips: 378.75702 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][92000]XNorm: 0.000051
Training: 2023-10-17 21:38:19,473 - [lfw][92000]XNorm: 0.000051
INFO:root:[lfw][92000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 21:38:19,473 - [lfw][92000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][92000]Accuracy-Highest: 0.81183
Training: 2023-10-17 21:38:19,473 - [lfw][92000]Accuracy-Highest: 0.81183
INFO:root:test time: 32.0207
Training: 2023-10-17 21:38:19,473 - test time: 32.0207
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][92000]XNorm: 0.000051
Training: 2023-10-17 21:38:56,523 - [cfp_fp][92000]XNorm: 0.000051
INFO:root:[cfp_fp][92000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 21:38:56,523 - [cfp_fp][92000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][92000]Accuracy-Highest: 0.61571
Training: 2023-10-17 21:38:56,523 - [cfp_fp][92000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.0507
Training: 2023-10-17 21:38:56,523 - test time: 37.0507
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][92000]XNorm: 0.000051
Training: 2023-10-17 21:39:28,455 - [agedb_30][92000]XNorm: 0.000051
INFO:root:[agedb_30][92000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 21:39:28,455 - [agedb_30][92000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][92000]Accuracy-Highest: 0.58350
Training: 2023-10-17 21:39:28,455 - [agedb_30][92000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.9319
Training: 2023-10-17 21:39:28,455 - test time: 31.9319
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 92100, eta: 86.57 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33701 sec, avg_samples: 64.00000, ips: 379.81330 images/sec
Training: 2023-10-17 21:40:02,162 - loss nan, lr: 0.025000, epoch: 2, step: 92100, eta: 86.57 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33701 sec, avg_samples: 64.00000, ips: 379.81330 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 92200, eta: 86.55 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.80031 images/sec
Training: 2023-10-17 21:40:35,959 - loss nan, lr: 0.025000, epoch: 2, step: 92200, eta: 86.55 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.80031 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 92300, eta: 86.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.82293 images/sec
Training: 2023-10-17 21:41:09,753 - loss nan, lr: 0.025000, epoch: 2, step: 92300, eta: 86.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.82293 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 92400, eta: 86.50 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33775 sec, avg_samples: 64.00000, ips: 378.97768 images/sec
Training: 2023-10-17 21:41:43,534 - loss nan, lr: 0.025000, epoch: 2, step: 92400, eta: 86.50 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33775 sec, avg_samples: 64.00000, ips: 378.97768 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 92500, eta: 86.48 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.79637 images/sec
Training: 2023-10-17 21:42:17,332 - loss nan, lr: 0.025000, epoch: 2, step: 92500, eta: 86.48 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.79637 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 92600, eta: 86.46 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33777 sec, avg_samples: 64.00000, ips: 378.95824 images/sec
Training: 2023-10-17 21:42:51,114 - loss nan, lr: 0.025000, epoch: 2, step: 92600, eta: 86.46 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33777 sec, avg_samples: 64.00000, ips: 378.95824 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 92700, eta: 86.43 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.80948 images/sec
Training: 2023-10-17 21:43:24,910 - loss nan, lr: 0.025000, epoch: 2, step: 92700, eta: 86.43 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.80948 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 92800, eta: 86.41 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33793 sec, avg_samples: 64.00000, ips: 378.78021 images/sec
Training: 2023-10-17 21:43:58,709 - loss nan, lr: 0.025000, epoch: 2, step: 92800, eta: 86.41 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33793 sec, avg_samples: 64.00000, ips: 378.78021 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 92900, eta: 86.39 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33806 sec, avg_samples: 64.00000, ips: 378.63466 images/sec
Training: 2023-10-17 21:44:32,520 - loss nan, lr: 0.025000, epoch: 2, step: 92900, eta: 86.39 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33806 sec, avg_samples: 64.00000, ips: 378.63466 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 93000, eta: 86.36 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.83291 images/sec
Training: 2023-10-17 21:45:06,314 - loss nan, lr: 0.025000, epoch: 2, step: 93000, eta: 86.36 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.83291 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 93100, eta: 86.34 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76963 images/sec
Training: 2023-10-17 21:45:40,114 - loss nan, lr: 0.025000, epoch: 2, step: 93100, eta: 86.34 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76963 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 93200, eta: 86.32 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68233 images/sec
Training: 2023-10-17 21:46:13,921 - loss nan, lr: 0.025000, epoch: 2, step: 93200, eta: 86.32 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68233 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 93300, eta: 86.30 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.72275 images/sec
Training: 2023-10-17 21:46:47,725 - loss nan, lr: 0.025000, epoch: 2, step: 93300, eta: 86.30 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.72275 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 93400, eta: 86.27 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33807 sec, avg_samples: 64.00000, ips: 378.61697 images/sec
Training: 2023-10-17 21:47:21,538 - loss nan, lr: 0.025000, epoch: 2, step: 93400, eta: 86.27 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33807 sec, avg_samples: 64.00000, ips: 378.61697 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 93500, eta: 86.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.82973 images/sec
Training: 2023-10-17 21:47:55,332 - loss nan, lr: 0.025000, epoch: 2, step: 93500, eta: 86.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.82973 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 93600, eta: 86.23 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.71313 images/sec
Training: 2023-10-17 21:48:29,137 - loss nan, lr: 0.025000, epoch: 2, step: 93600, eta: 86.23 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.71313 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 93700, eta: 86.21 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33795 sec, avg_samples: 64.00000, ips: 378.75874 images/sec
Training: 2023-10-17 21:49:02,937 - loss nan, lr: 0.025000, epoch: 2, step: 93700, eta: 86.21 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33795 sec, avg_samples: 64.00000, ips: 378.75874 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 93800, eta: 86.18 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.83384 images/sec
Training: 2023-10-17 21:49:36,731 - loss nan, lr: 0.025000, epoch: 2, step: 93800, eta: 86.18 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.83384 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 93900, eta: 86.16 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.71735 images/sec
Training: 2023-10-17 21:50:10,535 - loss nan, lr: 0.025000, epoch: 2, step: 93900, eta: 86.16 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.71735 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 94000, eta: 86.14 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.72612 images/sec
Training: 2023-10-17 21:50:44,339 - loss nan, lr: 0.025000, epoch: 2, step: 94000, eta: 86.14 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.72612 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][94000]XNorm: 0.000040
Training: 2023-10-17 21:51:16,388 - [lfw][94000]XNorm: 0.000040
INFO:root:[lfw][94000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 21:51:16,388 - [lfw][94000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][94000]Accuracy-Highest: 0.81183
Training: 2023-10-17 21:51:16,388 - [lfw][94000]Accuracy-Highest: 0.81183
INFO:root:test time: 32.0496
Training: 2023-10-17 21:51:16,388 - test time: 32.0496
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][94000]XNorm: 0.000040
Training: 2023-10-17 21:51:53,538 - [cfp_fp][94000]XNorm: 0.000040
INFO:root:[cfp_fp][94000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 21:51:53,539 - [cfp_fp][94000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][94000]Accuracy-Highest: 0.61571
Training: 2023-10-17 21:51:53,539 - [cfp_fp][94000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.1502
Training: 2023-10-17 21:51:53,539 - test time: 37.1502
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][94000]XNorm: 0.000040
Training: 2023-10-17 21:52:25,433 - [agedb_30][94000]XNorm: 0.000040
INFO:root:[agedb_30][94000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 21:52:25,434 - [agedb_30][94000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][94000]Accuracy-Highest: 0.58350
Training: 2023-10-17 21:52:25,434 - [agedb_30][94000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.8950
Training: 2023-10-17 21:52:25,434 - test time: 31.8950
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 94100, eta: 86.35 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33702 sec, avg_samples: 64.00000, ips: 379.79480 images/sec
Training: 2023-10-17 21:52:59,142 - loss nan, lr: 0.025000, epoch: 2, step: 94100, eta: 86.35 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33702 sec, avg_samples: 64.00000, ips: 379.79480 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 94200, eta: 86.33 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33782 sec, avg_samples: 64.00000, ips: 378.89945 images/sec
Training: 2023-10-17 21:53:32,930 - loss nan, lr: 0.025000, epoch: 2, step: 94200, eta: 86.33 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33782 sec, avg_samples: 64.00000, ips: 378.89945 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 94300, eta: 86.31 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.65988 images/sec
Training: 2023-10-17 21:54:06,739 - loss nan, lr: 0.025000, epoch: 2, step: 94300, eta: 86.31 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.65988 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 94400, eta: 86.28 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33795 sec, avg_samples: 64.00000, ips: 378.75976 images/sec
Training: 2023-10-17 21:54:40,539 - loss nan, lr: 0.025000, epoch: 2, step: 94400, eta: 86.28 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33795 sec, avg_samples: 64.00000, ips: 378.75976 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 94500, eta: 86.26 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.72539 images/sec
Training: 2023-10-17 21:55:14,343 - loss nan, lr: 0.025000, epoch: 2, step: 94500, eta: 86.26 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.72539 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 94600, eta: 86.24 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33796 sec, avg_samples: 64.00000, ips: 378.74401 images/sec
Training: 2023-10-17 21:55:48,144 - loss nan, lr: 0.025000, epoch: 2, step: 94600, eta: 86.24 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33796 sec, avg_samples: 64.00000, ips: 378.74401 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 94700, eta: 86.22 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33809 sec, avg_samples: 64.00000, ips: 378.59992 images/sec
Training: 2023-10-17 21:56:21,959 - loss nan, lr: 0.025000, epoch: 2, step: 94700, eta: 86.22 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33809 sec, avg_samples: 64.00000, ips: 378.59992 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 94800, eta: 86.19 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67849 images/sec
Training: 2023-10-17 21:56:55,767 - loss nan, lr: 0.025000, epoch: 2, step: 94800, eta: 86.19 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67849 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 94900, eta: 86.17 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.85126 images/sec
Training: 2023-10-17 21:57:29,559 - loss nan, lr: 0.025000, epoch: 2, step: 94900, eta: 86.17 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.85126 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 95000, eta: 86.15 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.78497 images/sec
Training: 2023-10-17 21:58:03,357 - loss nan, lr: 0.025000, epoch: 2, step: 95000, eta: 86.15 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.78497 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 95100, eta: 86.13 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66331 images/sec
Training: 2023-10-17 21:58:37,166 - loss nan, lr: 0.025000, epoch: 2, step: 95100, eta: 86.13 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66331 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 95200, eta: 86.10 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.78625 images/sec
Training: 2023-10-17 21:59:10,964 - loss nan, lr: 0.025000, epoch: 2, step: 95200, eta: 86.10 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.78625 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 95300, eta: 86.08 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33804 sec, avg_samples: 64.00000, ips: 378.65272 images/sec
Training: 2023-10-17 21:59:44,774 - loss nan, lr: 0.025000, epoch: 2, step: 95300, eta: 86.08 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33804 sec, avg_samples: 64.00000, ips: 378.65272 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 95400, eta: 86.06 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.70188 images/sec
Training: 2023-10-17 22:00:18,579 - loss nan, lr: 0.025000, epoch: 2, step: 95400, eta: 86.06 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.70188 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 95500, eta: 86.04 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.69688 images/sec
Training: 2023-10-17 22:00:52,385 - loss nan, lr: 0.025000, epoch: 2, step: 95500, eta: 86.04 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.69688 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 95600, eta: 86.01 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66724 images/sec
Training: 2023-10-17 22:01:26,194 - loss nan, lr: 0.025000, epoch: 2, step: 95600, eta: 86.01 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66724 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 95700, eta: 85.99 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.85619 images/sec
Training: 2023-10-17 22:01:59,986 - loss nan, lr: 0.025000, epoch: 2, step: 95700, eta: 85.99 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.85619 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 95800, eta: 85.97 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33793 sec, avg_samples: 64.00000, ips: 378.78099 images/sec
Training: 2023-10-17 22:02:33,784 - loss nan, lr: 0.025000, epoch: 2, step: 95800, eta: 85.97 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33793 sec, avg_samples: 64.00000, ips: 378.78099 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 95900, eta: 85.95 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.81535 images/sec
Training: 2023-10-17 22:03:07,579 - loss nan, lr: 0.025000, epoch: 2, step: 95900, eta: 85.95 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.81535 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 96000, eta: 85.92 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.70681 images/sec
Training: 2023-10-17 22:03:41,385 - loss nan, lr: 0.025000, epoch: 2, step: 96000, eta: 85.92 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.70681 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][96000]XNorm: 0.000031
Training: 2023-10-17 22:04:13,412 - [lfw][96000]XNorm: 0.000031
INFO:root:[lfw][96000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 22:04:13,412 - [lfw][96000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][96000]Accuracy-Highest: 0.81183
Training: 2023-10-17 22:04:13,412 - [lfw][96000]Accuracy-Highest: 0.81183
INFO:root:test time: 32.0276
Training: 2023-10-17 22:04:13,412 - test time: 32.0276
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][96000]XNorm: 0.000031
Training: 2023-10-17 22:04:50,556 - [cfp_fp][96000]XNorm: 0.000031
INFO:root:[cfp_fp][96000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 22:04:50,556 - [cfp_fp][96000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][96000]Accuracy-Highest: 0.61571
Training: 2023-10-17 22:04:50,556 - [cfp_fp][96000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.1439
Training: 2023-10-17 22:04:50,556 - test time: 37.1439
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][96000]XNorm: 0.000031
Training: 2023-10-17 22:05:22,440 - [agedb_30][96000]XNorm: 0.000031
INFO:root:[agedb_30][96000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 22:05:22,440 - [agedb_30][96000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][96000]Accuracy-Highest: 0.58350
Training: 2023-10-17 22:05:22,440 - [agedb_30][96000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.8838
Training: 2023-10-17 22:05:22,440 - test time: 31.8838
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 96100, eta: 86.13 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33690 sec, avg_samples: 64.00000, ips: 379.93903 images/sec
Training: 2023-10-17 22:05:56,136 - loss nan, lr: 0.025000, epoch: 2, step: 96100, eta: 86.13 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33690 sec, avg_samples: 64.00000, ips: 379.93903 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 96200, eta: 86.11 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.79334 images/sec
Training: 2023-10-17 22:06:29,933 - loss nan, lr: 0.025000, epoch: 2, step: 96200, eta: 86.11 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.79334 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 96300, eta: 86.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33768 sec, avg_samples: 64.00000, ips: 379.05235 images/sec
Training: 2023-10-17 22:07:03,707 - loss nan, lr: 0.025000, epoch: 2, step: 96300, eta: 86.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33768 sec, avg_samples: 64.00000, ips: 379.05235 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 96400, eta: 86.07 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33763 sec, avg_samples: 64.00000, ips: 379.11480 images/sec
Training: 2023-10-17 22:07:37,476 - loss nan, lr: 0.025000, epoch: 2, step: 96400, eta: 86.07 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33763 sec, avg_samples: 64.00000, ips: 379.11480 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 96500, eta: 86.04 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33771 sec, avg_samples: 64.00000, ips: 379.02223 images/sec
Training: 2023-10-17 22:08:11,253 - loss nan, lr: 0.025000, epoch: 2, step: 96500, eta: 86.04 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33771 sec, avg_samples: 64.00000, ips: 379.02223 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 96600, eta: 86.02 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33772 sec, avg_samples: 64.00000, ips: 379.00817 images/sec
Training: 2023-10-17 22:08:45,031 - loss nan, lr: 0.025000, epoch: 2, step: 96600, eta: 86.02 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33772 sec, avg_samples: 64.00000, ips: 379.00817 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 96700, eta: 86.00 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33765 sec, avg_samples: 64.00000, ips: 379.08632 images/sec
Training: 2023-10-17 22:09:18,803 - loss nan, lr: 0.025000, epoch: 2, step: 96700, eta: 86.00 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33765 sec, avg_samples: 64.00000, ips: 379.08632 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 96800, eta: 85.98 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33780 sec, avg_samples: 64.00000, ips: 378.92641 images/sec
Training: 2023-10-17 22:09:52,588 - loss nan, lr: 0.025000, epoch: 2, step: 96800, eta: 85.98 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33780 sec, avg_samples: 64.00000, ips: 378.92641 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 96900, eta: 85.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.85255 images/sec
Training: 2023-10-17 22:10:26,380 - loss nan, lr: 0.025000, epoch: 2, step: 96900, eta: 85.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.85255 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 97000, eta: 85.93 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.82584 images/sec
Training: 2023-10-17 22:11:00,175 - loss nan, lr: 0.025000, epoch: 2, step: 97000, eta: 85.93 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.82584 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 97100, eta: 85.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33783 sec, avg_samples: 64.00000, ips: 378.88363 images/sec
Training: 2023-10-17 22:11:33,964 - loss nan, lr: 0.025000, epoch: 2, step: 97100, eta: 85.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33783 sec, avg_samples: 64.00000, ips: 378.88363 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 97200, eta: 85.89 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.81662 images/sec
Training: 2023-10-17 22:12:07,759 - loss nan, lr: 0.025000, epoch: 2, step: 97200, eta: 85.89 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.81662 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 97300, eta: 85.87 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.80753 images/sec
Training: 2023-10-17 22:12:41,555 - loss nan, lr: 0.025000, epoch: 2, step: 97300, eta: 85.87 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.80753 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 97400, eta: 85.84 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33778 sec, avg_samples: 64.00000, ips: 378.94156 images/sec
Training: 2023-10-17 22:13:15,339 - loss nan, lr: 0.025000, epoch: 2, step: 97400, eta: 85.84 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33778 sec, avg_samples: 64.00000, ips: 378.94156 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 97500, eta: 85.82 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.82340 images/sec
Training: 2023-10-17 22:13:49,134 - loss nan, lr: 0.025000, epoch: 2, step: 97500, eta: 85.82 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.82340 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 97600, eta: 85.80 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.75999 images/sec
Training: 2023-10-17 22:14:22,935 - loss nan, lr: 0.025000, epoch: 2, step: 97600, eta: 85.80 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.75999 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 97700, eta: 85.78 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33784 sec, avg_samples: 64.00000, ips: 378.88144 images/sec
Training: 2023-10-17 22:14:56,724 - loss nan, lr: 0.025000, epoch: 2, step: 97700, eta: 85.78 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33784 sec, avg_samples: 64.00000, ips: 378.88144 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 97800, eta: 85.76 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33787 sec, avg_samples: 64.00000, ips: 378.84796 images/sec
Training: 2023-10-17 22:15:30,517 - loss nan, lr: 0.025000, epoch: 2, step: 97800, eta: 85.76 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33787 sec, avg_samples: 64.00000, ips: 378.84796 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 97900, eta: 85.73 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.87188 images/sec
Training: 2023-10-17 22:16:04,307 - loss nan, lr: 0.025000, epoch: 2, step: 97900, eta: 85.73 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.87188 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 98000, eta: 85.71 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33796 sec, avg_samples: 64.00000, ips: 378.74765 images/sec
Training: 2023-10-17 22:16:38,108 - loss nan, lr: 0.025000, epoch: 2, step: 98000, eta: 85.71 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33796 sec, avg_samples: 64.00000, ips: 378.74765 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][98000]XNorm: 0.000024
Training: 2023-10-17 22:17:10,160 - [lfw][98000]XNorm: 0.000024
INFO:root:[lfw][98000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 22:17:10,160 - [lfw][98000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][98000]Accuracy-Highest: 0.81183
Training: 2023-10-17 22:17:10,160 - [lfw][98000]Accuracy-Highest: 0.81183
INFO:root:test time: 32.0514
Training: 2023-10-17 22:17:10,160 - test time: 32.0514
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][98000]XNorm: 0.000024
Training: 2023-10-17 22:17:47,313 - [cfp_fp][98000]XNorm: 0.000024
INFO:root:[cfp_fp][98000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 22:17:47,313 - [cfp_fp][98000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][98000]Accuracy-Highest: 0.61571
Training: 2023-10-17 22:17:47,313 - [cfp_fp][98000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.1530
Training: 2023-10-17 22:17:47,313 - test time: 37.1530
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][98000]XNorm: 0.000024
Training: 2023-10-17 22:18:19,196 - [agedb_30][98000]XNorm: 0.000024
INFO:root:[agedb_30][98000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 22:18:19,196 - [agedb_30][98000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][98000]Accuracy-Highest: 0.58350
Training: 2023-10-17 22:18:19,196 - [agedb_30][98000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.8833
Training: 2023-10-17 22:18:19,196 - test time: 31.8833
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 98100, eta: 85.92 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33677 sec, avg_samples: 64.00000, ips: 380.07996 images/sec
Training: 2023-10-17 22:18:52,879 - loss nan, lr: 0.025000, epoch: 2, step: 98100, eta: 85.92 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33677 sec, avg_samples: 64.00000, ips: 380.07996 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 98200, eta: 85.89 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.79665 images/sec
Training: 2023-10-17 22:19:26,676 - loss nan, lr: 0.025000, epoch: 2, step: 98200, eta: 85.89 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.79665 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 98300, eta: 85.87 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33783 sec, avg_samples: 64.00000, ips: 378.89387 images/sec
Training: 2023-10-17 22:20:00,465 - loss nan, lr: 0.025000, epoch: 2, step: 98300, eta: 85.87 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33783 sec, avg_samples: 64.00000, ips: 378.89387 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 98400, eta: 85.85 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76236 images/sec
Training: 2023-10-17 22:20:34,265 - loss nan, lr: 0.025000, epoch: 2, step: 98400, eta: 85.85 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76236 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 98500, eta: 85.83 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33829 sec, avg_samples: 64.00000, ips: 378.37097 images/sec
Training: 2023-10-17 22:21:08,100 - loss nan, lr: 0.025000, epoch: 2, step: 98500, eta: 85.83 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33829 sec, avg_samples: 64.00000, ips: 378.37097 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 98600, eta: 85.81 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33779 sec, avg_samples: 64.00000, ips: 378.93154 images/sec
Training: 2023-10-17 22:21:41,885 - loss nan, lr: 0.025000, epoch: 2, step: 98600, eta: 85.81 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33779 sec, avg_samples: 64.00000, ips: 378.93154 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 98700, eta: 85.78 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33781 sec, avg_samples: 64.00000, ips: 378.90638 images/sec
Training: 2023-10-17 22:22:15,673 - loss nan, lr: 0.025000, epoch: 2, step: 98700, eta: 85.78 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33781 sec, avg_samples: 64.00000, ips: 378.90638 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 98800, eta: 85.76 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.85955 images/sec
Training: 2023-10-17 22:22:49,464 - loss nan, lr: 0.025000, epoch: 2, step: 98800, eta: 85.76 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.85955 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 98900, eta: 85.74 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33783 sec, avg_samples: 64.00000, ips: 378.88763 images/sec
Training: 2023-10-17 22:23:23,253 - loss nan, lr: 0.025000, epoch: 2, step: 98900, eta: 85.74 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33783 sec, avg_samples: 64.00000, ips: 378.88763 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 99000, eta: 85.72 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33784 sec, avg_samples: 64.00000, ips: 378.87712 images/sec
Training: 2023-10-17 22:23:57,043 - loss nan, lr: 0.025000, epoch: 2, step: 99000, eta: 85.72 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33784 sec, avg_samples: 64.00000, ips: 378.87712 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 99100, eta: 85.69 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.78480 images/sec
Training: 2023-10-17 22:24:30,841 - loss nan, lr: 0.025000, epoch: 2, step: 99100, eta: 85.69 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.78480 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 99200, eta: 85.67 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33780 sec, avg_samples: 64.00000, ips: 378.92463 images/sec
Training: 2023-10-17 22:25:04,627 - loss nan, lr: 0.025000, epoch: 2, step: 99200, eta: 85.67 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33780 sec, avg_samples: 64.00000, ips: 378.92463 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 99300, eta: 85.65 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33781 sec, avg_samples: 64.00000, ips: 378.91505 images/sec
Training: 2023-10-17 22:25:38,413 - loss nan, lr: 0.025000, epoch: 2, step: 99300, eta: 85.65 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33781 sec, avg_samples: 64.00000, ips: 378.91505 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 99400, eta: 85.63 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33782 sec, avg_samples: 64.00000, ips: 378.89564 images/sec
Training: 2023-10-17 22:26:12,201 - loss nan, lr: 0.025000, epoch: 2, step: 99400, eta: 85.63 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33782 sec, avg_samples: 64.00000, ips: 378.89564 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 99500, eta: 85.61 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33780 sec, avg_samples: 64.00000, ips: 378.92197 images/sec
Training: 2023-10-17 22:26:45,987 - loss nan, lr: 0.025000, epoch: 2, step: 99500, eta: 85.61 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33780 sec, avg_samples: 64.00000, ips: 378.92197 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 99600, eta: 85.59 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33784 sec, avg_samples: 64.00000, ips: 378.87853 images/sec
Training: 2023-10-17 22:27:19,777 - loss nan, lr: 0.025000, epoch: 2, step: 99600, eta: 85.59 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33784 sec, avg_samples: 64.00000, ips: 378.87853 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 99700, eta: 85.56 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33796 sec, avg_samples: 64.00000, ips: 378.74183 images/sec
Training: 2023-10-17 22:27:53,579 - loss nan, lr: 0.025000, epoch: 2, step: 99700, eta: 85.56 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33796 sec, avg_samples: 64.00000, ips: 378.74183 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 99800, eta: 85.54 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33784 sec, avg_samples: 64.00000, ips: 378.87359 images/sec
Training: 2023-10-17 22:28:27,369 - loss nan, lr: 0.025000, epoch: 2, step: 99800, eta: 85.54 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33784 sec, avg_samples: 64.00000, ips: 378.87359 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 99900, eta: 85.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.82275 images/sec
Training: 2023-10-17 22:29:01,164 - loss nan, lr: 0.025000, epoch: 2, step: 99900, eta: 85.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.82275 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 100000, eta: 85.50 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.78232 images/sec
Training: 2023-10-17 22:29:34,962 - loss nan, lr: 0.025000, epoch: 2, step: 100000, eta: 85.50 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.78232 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][100000]XNorm: 0.000019
Training: 2023-10-17 22:30:07,028 - [lfw][100000]XNorm: 0.000019
INFO:root:[lfw][100000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 22:30:07,028 - [lfw][100000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][100000]Accuracy-Highest: 0.81183
Training: 2023-10-17 22:30:07,028 - [lfw][100000]Accuracy-Highest: 0.81183
INFO:root:test time: 32.0653
Training: 2023-10-17 22:30:07,028 - test time: 32.0653
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][100000]XNorm: 0.000019
Training: 2023-10-17 22:30:44,206 - [cfp_fp][100000]XNorm: 0.000019
INFO:root:[cfp_fp][100000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 22:30:44,206 - [cfp_fp][100000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][100000]Accuracy-Highest: 0.61571
Training: 2023-10-17 22:30:44,206 - [cfp_fp][100000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.1780
Training: 2023-10-17 22:30:44,206 - test time: 37.1780
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][100000]XNorm: 0.000019
Training: 2023-10-17 22:31:16,183 - [agedb_30][100000]XNorm: 0.000019
INFO:root:[agedb_30][100000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 22:31:16,183 - [agedb_30][100000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][100000]Accuracy-Highest: 0.58350
Training: 2023-10-17 22:31:16,183 - [agedb_30][100000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.9773
Training: 2023-10-17 22:31:16,183 - test time: 31.9773
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 100100, eta: 85.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33709 sec, avg_samples: 64.00000, ips: 379.71970 images/sec
Training: 2023-10-17 22:31:49,899 - loss nan, lr: 0.025000, epoch: 2, step: 100100, eta: 85.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33709 sec, avg_samples: 64.00000, ips: 379.71970 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 100200, eta: 85.68 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33807 sec, avg_samples: 64.00000, ips: 378.62105 images/sec
Training: 2023-10-17 22:32:23,711 - loss nan, lr: 0.025000, epoch: 2, step: 100200, eta: 85.68 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33807 sec, avg_samples: 64.00000, ips: 378.62105 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 100300, eta: 85.65 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33777 sec, avg_samples: 64.00000, ips: 378.95410 images/sec
Training: 2023-10-17 22:32:57,494 - loss nan, lr: 0.025000, epoch: 2, step: 100300, eta: 85.65 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33777 sec, avg_samples: 64.00000, ips: 378.95410 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 100400, eta: 85.63 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.70831 images/sec
Training: 2023-10-17 22:33:31,299 - loss nan, lr: 0.025000, epoch: 2, step: 100400, eta: 85.63 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.70831 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 100500, eta: 85.61 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.72579 images/sec
Training: 2023-10-17 22:34:05,103 - loss nan, lr: 0.025000, epoch: 2, step: 100500, eta: 85.61 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.72579 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 100600, eta: 85.59 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33787 sec, avg_samples: 64.00000, ips: 378.84853 images/sec
Training: 2023-10-17 22:34:38,895 - loss nan, lr: 0.025000, epoch: 2, step: 100600, eta: 85.59 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33787 sec, avg_samples: 64.00000, ips: 378.84853 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 100700, eta: 85.57 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68781 images/sec
Training: 2023-10-17 22:35:12,702 - loss nan, lr: 0.025000, epoch: 2, step: 100700, eta: 85.57 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68781 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 100800, eta: 85.55 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33809 sec, avg_samples: 64.00000, ips: 378.60191 images/sec
Training: 2023-10-17 22:35:46,516 - loss nan, lr: 0.025000, epoch: 2, step: 100800, eta: 85.55 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33809 sec, avg_samples: 64.00000, ips: 378.60191 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 100900, eta: 85.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.85504 images/sec
Training: 2023-10-17 22:36:20,308 - loss nan, lr: 0.025000, epoch: 2, step: 100900, eta: 85.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.85504 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 101000, eta: 85.50 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.68089 images/sec
Training: 2023-10-17 22:36:54,115 - loss nan, lr: 0.025000, epoch: 2, step: 101000, eta: 85.50 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.68089 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 101100, eta: 85.48 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33813 sec, avg_samples: 64.00000, ips: 378.55748 images/sec
Training: 2023-10-17 22:37:27,934 - loss nan, lr: 0.025000, epoch: 2, step: 101100, eta: 85.48 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33813 sec, avg_samples: 64.00000, ips: 378.55748 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 101200, eta: 85.46 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.72266 images/sec
Training: 2023-10-17 22:38:01,738 - loss nan, lr: 0.025000, epoch: 2, step: 101200, eta: 85.46 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.72266 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 101300, eta: 85.44 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33797 sec, avg_samples: 64.00000, ips: 378.73199 images/sec
Training: 2023-10-17 22:38:35,541 - loss nan, lr: 0.025000, epoch: 2, step: 101300, eta: 85.44 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33797 sec, avg_samples: 64.00000, ips: 378.73199 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 101400, eta: 85.41 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.78554 images/sec
Training: 2023-10-17 22:39:09,339 - loss nan, lr: 0.025000, epoch: 2, step: 101400, eta: 85.41 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.78554 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 101500, eta: 85.39 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.79765 images/sec
Training: 2023-10-17 22:39:43,136 - loss nan, lr: 0.025000, epoch: 2, step: 101500, eta: 85.39 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.79765 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 101600, eta: 85.37 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.65997 images/sec
Training: 2023-10-17 22:40:16,945 - loss nan, lr: 0.025000, epoch: 2, step: 101600, eta: 85.37 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.65997 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 101700, eta: 85.35 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.82410 images/sec
Training: 2023-10-17 22:40:50,740 - loss nan, lr: 0.025000, epoch: 2, step: 101700, eta: 85.35 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.82410 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 101800, eta: 85.33 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33813 sec, avg_samples: 64.00000, ips: 378.55071 images/sec
Training: 2023-10-17 22:41:24,559 - loss nan, lr: 0.025000, epoch: 2, step: 101800, eta: 85.33 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33813 sec, avg_samples: 64.00000, ips: 378.55071 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 101900, eta: 85.31 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.70151 images/sec
Training: 2023-10-17 22:41:58,364 - loss nan, lr: 0.025000, epoch: 2, step: 101900, eta: 85.31 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.70151 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 102000, eta: 85.29 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.70946 images/sec
Training: 2023-10-17 22:42:32,169 - loss nan, lr: 0.025000, epoch: 2, step: 102000, eta: 85.29 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.70946 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][102000]XNorm: 0.000015
Training: 2023-10-17 22:43:04,211 - [lfw][102000]XNorm: 0.000015
INFO:root:[lfw][102000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 22:43:04,212 - [lfw][102000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][102000]Accuracy-Highest: 0.81183
Training: 2023-10-17 22:43:04,212 - [lfw][102000]Accuracy-Highest: 0.81183
INFO:root:test time: 32.0423
Training: 2023-10-17 22:43:04,212 - test time: 32.0423
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][102000]XNorm: 0.000015
Training: 2023-10-17 22:43:41,270 - [cfp_fp][102000]XNorm: 0.000015
INFO:root:[cfp_fp][102000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 22:43:41,271 - [cfp_fp][102000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][102000]Accuracy-Highest: 0.61571
Training: 2023-10-17 22:43:41,271 - [cfp_fp][102000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.0590
Training: 2023-10-17 22:43:41,271 - test time: 37.0590
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][102000]XNorm: 0.000015
Training: 2023-10-17 22:44:13,216 - [agedb_30][102000]XNorm: 0.000015
INFO:root:[agedb_30][102000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 22:44:13,216 - [agedb_30][102000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][102000]Accuracy-Highest: 0.58350
Training: 2023-10-17 22:44:13,216 - [agedb_30][102000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.9455
Training: 2023-10-17 22:44:13,216 - test time: 31.9455
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 102100, eta: 85.48 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33700 sec, avg_samples: 64.00000, ips: 379.82224 images/sec
Training: 2023-10-17 22:44:46,922 - loss nan, lr: 0.025000, epoch: 2, step: 102100, eta: 85.48 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33700 sec, avg_samples: 64.00000, ips: 379.82224 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 102200, eta: 85.46 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33784 sec, avg_samples: 64.00000, ips: 378.88196 images/sec
Training: 2023-10-17 22:45:20,713 - loss nan, lr: 0.025000, epoch: 2, step: 102200, eta: 85.46 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33784 sec, avg_samples: 64.00000, ips: 378.88196 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 102300, eta: 85.44 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68151 images/sec
Training: 2023-10-17 22:45:54,520 - loss nan, lr: 0.025000, epoch: 2, step: 102300, eta: 85.44 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68151 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 102400, eta: 85.42 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33814 sec, avg_samples: 64.00000, ips: 378.54503 images/sec
Training: 2023-10-17 22:46:28,339 - loss nan, lr: 0.025000, epoch: 2, step: 102400, eta: 85.42 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33814 sec, avg_samples: 64.00000, ips: 378.54503 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 102500, eta: 85.39 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.85019 images/sec
Training: 2023-10-17 22:47:02,132 - loss nan, lr: 0.025000, epoch: 2, step: 102500, eta: 85.39 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.85019 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 102600, eta: 85.37 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33796 sec, avg_samples: 64.00000, ips: 378.74129 images/sec
Training: 2023-10-17 22:47:35,934 - loss nan, lr: 0.025000, epoch: 2, step: 102600, eta: 85.37 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33796 sec, avg_samples: 64.00000, ips: 378.74129 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 102700, eta: 85.35 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.70453 images/sec
Training: 2023-10-17 22:48:09,739 - loss nan, lr: 0.025000, epoch: 2, step: 102700, eta: 85.35 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.70453 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 102800, eta: 85.33 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67116 images/sec
Training: 2023-10-17 22:48:43,547 - loss nan, lr: 0.025000, epoch: 2, step: 102800, eta: 85.33 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67116 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 102900, eta: 85.31 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.82300 images/sec
Training: 2023-10-17 22:49:17,342 - loss nan, lr: 0.025000, epoch: 2, step: 102900, eta: 85.31 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.82300 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 103000, eta: 85.29 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.69901 images/sec
Training: 2023-10-17 22:49:51,148 - loss nan, lr: 0.025000, epoch: 2, step: 103000, eta: 85.29 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.69901 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 103100, eta: 85.26 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33795 sec, avg_samples: 64.00000, ips: 378.75624 images/sec
Training: 2023-10-17 22:50:24,948 - loss nan, lr: 0.025000, epoch: 2, step: 103100, eta: 85.26 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33795 sec, avg_samples: 64.00000, ips: 378.75624 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 103200, eta: 85.24 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67497 images/sec
Training: 2023-10-17 22:50:58,756 - loss nan, lr: 0.025000, epoch: 2, step: 103200, eta: 85.24 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67497 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 103300, eta: 85.22 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.69358 images/sec
Training: 2023-10-17 22:51:32,562 - loss nan, lr: 0.025000, epoch: 2, step: 103300, eta: 85.22 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.69358 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 103400, eta: 85.20 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68397 images/sec
Training: 2023-10-17 22:52:06,369 - loss nan, lr: 0.025000, epoch: 2, step: 103400, eta: 85.20 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68397 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 103500, eta: 85.18 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66555 images/sec
Training: 2023-10-17 22:52:40,178 - loss nan, lr: 0.025000, epoch: 2, step: 103500, eta: 85.18 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66555 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 103600, eta: 85.16 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.63960 images/sec
Training: 2023-10-17 22:53:13,988 - loss nan, lr: 0.025000, epoch: 2, step: 103600, eta: 85.16 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.63960 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 103700, eta: 85.14 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33813 sec, avg_samples: 64.00000, ips: 378.55418 images/sec
Training: 2023-10-17 22:53:47,807 - loss nan, lr: 0.025000, epoch: 2, step: 103700, eta: 85.14 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33813 sec, avg_samples: 64.00000, ips: 378.55418 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 103800, eta: 85.11 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.71864 images/sec
Training: 2023-10-17 22:54:21,611 - loss nan, lr: 0.025000, epoch: 2, step: 103800, eta: 85.11 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.71864 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 103900, eta: 85.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33810 sec, avg_samples: 64.00000, ips: 378.58403 images/sec
Training: 2023-10-17 22:54:55,426 - loss nan, lr: 0.025000, epoch: 2, step: 103900, eta: 85.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33810 sec, avg_samples: 64.00000, ips: 378.58403 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 104000, eta: 85.07 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33813 sec, avg_samples: 64.00000, ips: 378.55746 images/sec
Training: 2023-10-17 22:55:29,245 - loss nan, lr: 0.025000, epoch: 2, step: 104000, eta: 85.07 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33813 sec, avg_samples: 64.00000, ips: 378.55746 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][104000]XNorm: 0.000011
Training: 2023-10-17 22:56:01,331 - [lfw][104000]XNorm: 0.000011
INFO:root:[lfw][104000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 22:56:01,331 - [lfw][104000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][104000]Accuracy-Highest: 0.81183
Training: 2023-10-17 22:56:01,331 - [lfw][104000]Accuracy-Highest: 0.81183
INFO:root:test time: 32.0861
Training: 2023-10-17 22:56:01,331 - test time: 32.0861
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][104000]XNorm: 0.000011
Training: 2023-10-17 22:56:38,416 - [cfp_fp][104000]XNorm: 0.000011
INFO:root:[cfp_fp][104000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 22:56:38,417 - [cfp_fp][104000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][104000]Accuracy-Highest: 0.61571
Training: 2023-10-17 22:56:38,417 - [cfp_fp][104000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.0858
Training: 2023-10-17 22:56:38,417 - test time: 37.0858
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][104000]XNorm: 0.000011
Training: 2023-10-17 22:57:10,423 - [agedb_30][104000]XNorm: 0.000011
INFO:root:[agedb_30][104000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 22:57:10,424 - [agedb_30][104000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][104000]Accuracy-Highest: 0.58350
Training: 2023-10-17 22:57:10,424 - [agedb_30][104000]Accuracy-Highest: 0.58350
INFO:root:test time: 32.0069
Training: 2023-10-17 22:57:10,424 - test time: 32.0069
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 104100, eta: 85.26 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33711 sec, avg_samples: 64.00000, ips: 379.69288 images/sec
Training: 2023-10-17 22:57:44,141 - loss nan, lr: 0.025000, epoch: 2, step: 104100, eta: 85.26 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33711 sec, avg_samples: 64.00000, ips: 379.69288 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 104200, eta: 85.24 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.70875 images/sec
Training: 2023-10-17 22:58:17,946 - loss nan, lr: 0.025000, epoch: 2, step: 104200, eta: 85.24 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.70875 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 104300, eta: 85.22 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33787 sec, avg_samples: 64.00000, ips: 378.84778 images/sec
Training: 2023-10-17 22:58:51,739 - loss nan, lr: 0.025000, epoch: 2, step: 104300, eta: 85.22 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33787 sec, avg_samples: 64.00000, ips: 378.84778 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 104400, eta: 85.20 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.69440 images/sec
Training: 2023-10-17 22:59:25,545 - loss nan, lr: 0.025000, epoch: 2, step: 104400, eta: 85.20 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.69440 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 104500, eta: 85.18 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66416 images/sec
Training: 2023-10-17 22:59:59,354 - loss nan, lr: 0.025000, epoch: 2, step: 104500, eta: 85.18 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66416 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 104600, eta: 85.16 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76076 images/sec
Training: 2023-10-17 23:00:33,154 - loss nan, lr: 0.025000, epoch: 2, step: 104600, eta: 85.16 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76076 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 104700, eta: 85.13 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67487 images/sec
Training: 2023-10-17 23:01:06,962 - loss nan, lr: 0.025000, epoch: 2, step: 104700, eta: 85.13 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67487 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 104800, eta: 85.11 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.79143 images/sec
Training: 2023-10-17 23:01:40,760 - loss nan, lr: 0.025000, epoch: 2, step: 104800, eta: 85.11 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.79143 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 104900, eta: 85.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33807 sec, avg_samples: 64.00000, ips: 378.61445 images/sec
Training: 2023-10-17 23:02:14,573 - loss nan, lr: 0.025000, epoch: 2, step: 104900, eta: 85.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33807 sec, avg_samples: 64.00000, ips: 378.61445 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 105000, eta: 85.07 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33795 sec, avg_samples: 64.00000, ips: 378.75610 images/sec
Training: 2023-10-17 23:02:48,374 - loss nan, lr: 0.025000, epoch: 2, step: 105000, eta: 85.07 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33795 sec, avg_samples: 64.00000, ips: 378.75610 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 105100, eta: 85.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.71818 images/sec
Training: 2023-10-17 23:03:22,178 - loss nan, lr: 0.025000, epoch: 2, step: 105100, eta: 85.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.71818 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 105200, eta: 85.03 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66232 images/sec
Training: 2023-10-17 23:03:55,987 - loss nan, lr: 0.025000, epoch: 2, step: 105200, eta: 85.03 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66232 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 105300, eta: 85.01 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.71557 images/sec
Training: 2023-10-17 23:04:29,792 - loss nan, lr: 0.025000, epoch: 2, step: 105300, eta: 85.01 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.71557 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 105400, eta: 84.99 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.79025 images/sec
Training: 2023-10-17 23:05:03,590 - loss nan, lr: 0.025000, epoch: 2, step: 105400, eta: 84.99 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.79025 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 105500, eta: 84.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.69839 images/sec
Training: 2023-10-17 23:05:37,395 - loss nan, lr: 0.025000, epoch: 2, step: 105500, eta: 84.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.69839 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 105600, eta: 84.94 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33806 sec, avg_samples: 64.00000, ips: 378.62772 images/sec
Training: 2023-10-17 23:06:11,208 - loss nan, lr: 0.025000, epoch: 2, step: 105600, eta: 84.94 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33806 sec, avg_samples: 64.00000, ips: 378.62772 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 105700, eta: 84.92 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33797 sec, avg_samples: 64.00000, ips: 378.72935 images/sec
Training: 2023-10-17 23:06:45,011 - loss nan, lr: 0.025000, epoch: 2, step: 105700, eta: 84.92 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33797 sec, avg_samples: 64.00000, ips: 378.72935 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 105800, eta: 84.90 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.60418 images/sec
Training: 2023-10-17 23:07:18,825 - loss nan, lr: 0.025000, epoch: 2, step: 105800, eta: 84.90 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.60418 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 105900, eta: 84.88 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.86049 images/sec
Training: 2023-10-17 23:07:52,616 - loss nan, lr: 0.025000, epoch: 2, step: 105900, eta: 84.88 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.86049 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 106000, eta: 84.86 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.70277 images/sec
Training: 2023-10-17 23:08:26,422 - loss nan, lr: 0.025000, epoch: 2, step: 106000, eta: 84.86 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.70277 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][106000]XNorm: 0.000009
Training: 2023-10-17 23:08:58,474 - [lfw][106000]XNorm: 0.000009
INFO:root:[lfw][106000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 23:08:58,475 - [lfw][106000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][106000]Accuracy-Highest: 0.81183
Training: 2023-10-17 23:08:58,475 - [lfw][106000]Accuracy-Highest: 0.81183
INFO:root:test time: 32.0526
Training: 2023-10-17 23:08:58,475 - test time: 32.0526
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][106000]XNorm: 0.000009
Training: 2023-10-17 23:09:35,560 - [cfp_fp][106000]XNorm: 0.000009
INFO:root:[cfp_fp][106000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 23:09:35,560 - [cfp_fp][106000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][106000]Accuracy-Highest: 0.61571
Training: 2023-10-17 23:09:35,560 - [cfp_fp][106000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.0853
Training: 2023-10-17 23:09:35,560 - test time: 37.0853
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][106000]XNorm: 0.000009
Training: 2023-10-17 23:10:07,553 - [agedb_30][106000]XNorm: 0.000009
INFO:root:[agedb_30][106000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 23:10:07,553 - [agedb_30][106000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][106000]Accuracy-Highest: 0.58350
Training: 2023-10-17 23:10:07,553 - [agedb_30][106000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.9932
Training: 2023-10-17 23:10:07,553 - test time: 31.9932
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 106100, eta: 85.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33706 sec, avg_samples: 64.00000, ips: 379.75318 images/sec
Training: 2023-10-17 23:10:41,266 - loss nan, lr: 0.025000, epoch: 2, step: 106100, eta: 85.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33706 sec, avg_samples: 64.00000, ips: 379.75318 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 106200, eta: 85.02 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.79710 images/sec
Training: 2023-10-17 23:11:15,063 - loss nan, lr: 0.025000, epoch: 2, step: 106200, eta: 85.02 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.79710 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 106300, eta: 85.00 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.79043 images/sec
Training: 2023-10-17 23:11:48,861 - loss nan, lr: 0.025000, epoch: 2, step: 106300, eta: 85.00 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.79043 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 106400, eta: 84.98 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.79552 images/sec
Training: 2023-10-17 23:12:22,658 - loss nan, lr: 0.025000, epoch: 2, step: 106400, eta: 84.98 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.79552 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 106500, eta: 84.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.70836 images/sec
Training: 2023-10-17 23:12:56,463 - loss nan, lr: 0.025000, epoch: 2, step: 106500, eta: 84.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.70836 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 106600, eta: 84.94 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76431 images/sec
Training: 2023-10-17 23:13:30,263 - loss nan, lr: 0.025000, epoch: 2, step: 106600, eta: 84.94 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76431 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 106700, eta: 84.92 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.64158 images/sec
Training: 2023-10-17 23:14:04,074 - loss nan, lr: 0.025000, epoch: 2, step: 106700, eta: 84.92 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.64158 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 106800, eta: 84.90 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33812 sec, avg_samples: 64.00000, ips: 378.56916 images/sec
Training: 2023-10-17 23:14:37,891 - loss nan, lr: 0.025000, epoch: 2, step: 106800, eta: 84.90 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33812 sec, avg_samples: 64.00000, ips: 378.56916 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 106900, eta: 84.88 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.79158 images/sec
Training: 2023-10-17 23:15:11,689 - loss nan, lr: 0.025000, epoch: 2, step: 106900, eta: 84.88 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.79158 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 107000, eta: 84.86 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33807 sec, avg_samples: 64.00000, ips: 378.61892 images/sec
Training: 2023-10-17 23:15:45,502 - loss nan, lr: 0.025000, epoch: 2, step: 107000, eta: 84.86 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33807 sec, avg_samples: 64.00000, ips: 378.61892 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 107100, eta: 84.83 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33804 sec, avg_samples: 64.00000, ips: 378.65819 images/sec
Training: 2023-10-17 23:16:19,312 - loss nan, lr: 0.025000, epoch: 2, step: 107100, eta: 84.83 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33804 sec, avg_samples: 64.00000, ips: 378.65819 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 2, step: 107200, eta: 84.81 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76145 images/sec
Training: 2023-10-17 23:16:53,112 - loss nan, lr: 0.025000, epoch: 2, step: 107200, eta: 84.81 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76145 images/sec
INFO:root:Save model to model/FresResNet101/2.
Training: 2023-10-17 23:16:56,908 - Save model to model/FresResNet101/2.
INFO:root:Remove checkpoint model/FresResNet101/1.
Training: 2023-10-17 23:16:56,908 - Remove checkpoint model/FresResNet101/1.
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 107300, eta: 84.80 hours, avg_reader_cost: 0.00903 sec, avg_batch_cost: 0.31932 sec, avg_samples: 58.88000, ips: 368.77870 images/sec
Training: 2023-10-17 23:17:28,984 - loss nan, lr: 0.025000, epoch: 3, step: 107300, eta: 84.80 hours, avg_reader_cost: 0.00903 sec, avg_batch_cost: 0.31932 sec, avg_samples: 58.88000, ips: 368.77870 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 107400, eta: 84.78 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33711 sec, avg_samples: 64.00000, ips: 379.69506 images/sec
Training: 2023-10-17 23:18:02,701 - loss nan, lr: 0.025000, epoch: 3, step: 107400, eta: 84.78 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33711 sec, avg_samples: 64.00000, ips: 379.69506 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 107500, eta: 84.75 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33707 sec, avg_samples: 64.00000, ips: 379.74531 images/sec
Training: 2023-10-17 23:18:36,414 - loss nan, lr: 0.025000, epoch: 3, step: 107500, eta: 84.75 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33707 sec, avg_samples: 64.00000, ips: 379.74531 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 107600, eta: 84.73 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33704 sec, avg_samples: 64.00000, ips: 379.77281 images/sec
Training: 2023-10-17 23:19:10,124 - loss nan, lr: 0.025000, epoch: 3, step: 107600, eta: 84.73 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33704 sec, avg_samples: 64.00000, ips: 379.77281 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 107700, eta: 84.71 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33696 sec, avg_samples: 64.00000, ips: 379.86244 images/sec
Training: 2023-10-17 23:19:43,827 - loss nan, lr: 0.025000, epoch: 3, step: 107700, eta: 84.71 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33696 sec, avg_samples: 64.00000, ips: 379.86244 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 107800, eta: 84.69 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33719 sec, avg_samples: 64.00000, ips: 379.60626 images/sec
Training: 2023-10-17 23:20:17,552 - loss nan, lr: 0.025000, epoch: 3, step: 107800, eta: 84.69 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33719 sec, avg_samples: 64.00000, ips: 379.60626 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 107900, eta: 84.67 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33714 sec, avg_samples: 64.00000, ips: 379.66238 images/sec
Training: 2023-10-17 23:20:51,272 - loss nan, lr: 0.025000, epoch: 3, step: 107900, eta: 84.67 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33714 sec, avg_samples: 64.00000, ips: 379.66238 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 108000, eta: 84.65 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33721 sec, avg_samples: 64.00000, ips: 379.58193 images/sec
Training: 2023-10-17 23:21:24,999 - loss nan, lr: 0.025000, epoch: 3, step: 108000, eta: 84.65 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33721 sec, avg_samples: 64.00000, ips: 379.58193 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][108000]XNorm: 0.000007
Training: 2023-10-17 23:21:57,081 - [lfw][108000]XNorm: 0.000007
INFO:root:[lfw][108000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 23:21:57,081 - [lfw][108000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][108000]Accuracy-Highest: 0.81183
Training: 2023-10-17 23:21:57,081 - [lfw][108000]Accuracy-Highest: 0.81183
INFO:root:test time: 32.0820
Training: 2023-10-17 23:21:57,081 - test time: 32.0820
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][108000]XNorm: 0.000007
Training: 2023-10-17 23:22:34,181 - [cfp_fp][108000]XNorm: 0.000007
INFO:root:[cfp_fp][108000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 23:22:34,182 - [cfp_fp][108000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][108000]Accuracy-Highest: 0.61571
Training: 2023-10-17 23:22:34,182 - [cfp_fp][108000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.1004
Training: 2023-10-17 23:22:34,182 - test time: 37.1004
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][108000]XNorm: 0.000007
Training: 2023-10-17 23:23:06,188 - [agedb_30][108000]XNorm: 0.000007
INFO:root:[agedb_30][108000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 23:23:06,188 - [agedb_30][108000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][108000]Accuracy-Highest: 0.58350
Training: 2023-10-17 23:23:06,188 - [agedb_30][108000]Accuracy-Highest: 0.58350
INFO:root:test time: 32.0063
Training: 2023-10-17 23:23:06,188 - test time: 32.0063
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 108100, eta: 84.83 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33709 sec, avg_samples: 64.00000, ips: 379.72122 images/sec
Training: 2023-10-17 23:23:39,903 - loss nan, lr: 0.025000, epoch: 3, step: 108100, eta: 84.83 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33709 sec, avg_samples: 64.00000, ips: 379.72122 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 108200, eta: 84.81 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.69880 images/sec
Training: 2023-10-17 23:24:13,708 - loss nan, lr: 0.025000, epoch: 3, step: 108200, eta: 84.81 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.69880 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 108300, eta: 84.79 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.69038 images/sec
Training: 2023-10-17 23:24:47,515 - loss nan, lr: 0.025000, epoch: 3, step: 108300, eta: 84.79 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.69038 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 108400, eta: 84.77 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66684 images/sec
Training: 2023-10-17 23:25:21,323 - loss nan, lr: 0.025000, epoch: 3, step: 108400, eta: 84.77 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66684 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 108500, eta: 84.75 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.81010 images/sec
Training: 2023-10-17 23:25:55,119 - loss nan, lr: 0.025000, epoch: 3, step: 108500, eta: 84.75 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.81010 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 108600, eta: 84.73 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33796 sec, avg_samples: 64.00000, ips: 378.74196 images/sec
Training: 2023-10-17 23:26:28,921 - loss nan, lr: 0.025000, epoch: 3, step: 108600, eta: 84.73 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33796 sec, avg_samples: 64.00000, ips: 378.74196 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 108700, eta: 84.71 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.81610 images/sec
Training: 2023-10-17 23:27:02,717 - loss nan, lr: 0.025000, epoch: 3, step: 108700, eta: 84.71 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.81610 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 108800, eta: 84.68 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.83277 images/sec
Training: 2023-10-17 23:27:36,511 - loss nan, lr: 0.025000, epoch: 3, step: 108800, eta: 84.68 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.83277 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 108900, eta: 84.66 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68742 images/sec
Training: 2023-10-17 23:28:10,318 - loss nan, lr: 0.025000, epoch: 3, step: 108900, eta: 84.66 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68742 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 109000, eta: 84.64 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.64693 images/sec
Training: 2023-10-17 23:28:44,128 - loss nan, lr: 0.025000, epoch: 3, step: 109000, eta: 84.64 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.64693 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 109100, eta: 84.62 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67377 images/sec
Training: 2023-10-17 23:29:17,936 - loss nan, lr: 0.025000, epoch: 3, step: 109100, eta: 84.62 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67377 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 109200, eta: 84.60 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.69355 images/sec
Training: 2023-10-17 23:29:51,743 - loss nan, lr: 0.025000, epoch: 3, step: 109200, eta: 84.60 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.69355 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 109300, eta: 84.58 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.87051 images/sec
Training: 2023-10-17 23:30:25,533 - loss nan, lr: 0.025000, epoch: 3, step: 109300, eta: 84.58 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.87051 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 109400, eta: 84.56 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.71659 images/sec
Training: 2023-10-17 23:30:59,338 - loss nan, lr: 0.025000, epoch: 3, step: 109400, eta: 84.56 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.71659 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 109500, eta: 84.54 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.86180 images/sec
Training: 2023-10-17 23:31:33,129 - loss nan, lr: 0.025000, epoch: 3, step: 109500, eta: 84.54 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.86180 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 109600, eta: 84.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33811 sec, avg_samples: 64.00000, ips: 378.57181 images/sec
Training: 2023-10-17 23:32:06,946 - loss nan, lr: 0.025000, epoch: 3, step: 109600, eta: 84.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33811 sec, avg_samples: 64.00000, ips: 378.57181 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 109700, eta: 84.50 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33811 sec, avg_samples: 64.00000, ips: 378.57235 images/sec
Training: 2023-10-17 23:32:40,764 - loss nan, lr: 0.025000, epoch: 3, step: 109700, eta: 84.50 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33811 sec, avg_samples: 64.00000, ips: 378.57235 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 109800, eta: 84.48 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.81184 images/sec
Training: 2023-10-17 23:33:14,559 - loss nan, lr: 0.025000, epoch: 3, step: 109800, eta: 84.48 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.81184 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 109900, eta: 84.46 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.70076 images/sec
Training: 2023-10-17 23:33:48,365 - loss nan, lr: 0.025000, epoch: 3, step: 109900, eta: 84.46 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.70076 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 110000, eta: 84.44 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.79230 images/sec
Training: 2023-10-17 23:34:22,163 - loss nan, lr: 0.025000, epoch: 3, step: 110000, eta: 84.44 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.79230 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][110000]XNorm: 0.000005
Training: 2023-10-17 23:34:54,205 - [lfw][110000]XNorm: 0.000005
INFO:root:[lfw][110000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 23:34:54,205 - [lfw][110000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][110000]Accuracy-Highest: 0.81183
Training: 2023-10-17 23:34:54,205 - [lfw][110000]Accuracy-Highest: 0.81183
INFO:root:test time: 32.0425
Training: 2023-10-17 23:34:54,205 - test time: 32.0425
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][110000]XNorm: 0.000005
Training: 2023-10-17 23:35:31,275 - [cfp_fp][110000]XNorm: 0.000005
INFO:root:[cfp_fp][110000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 23:35:31,275 - [cfp_fp][110000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][110000]Accuracy-Highest: 0.61571
Training: 2023-10-17 23:35:31,275 - [cfp_fp][110000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.0698
Training: 2023-10-17 23:35:31,275 - test time: 37.0698
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][110000]XNorm: 0.000005
Training: 2023-10-17 23:36:03,213 - [agedb_30][110000]XNorm: 0.000005
INFO:root:[agedb_30][110000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 23:36:03,214 - [agedb_30][110000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][110000]Accuracy-Highest: 0.58350
Training: 2023-10-17 23:36:03,214 - [agedb_30][110000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.9384
Training: 2023-10-17 23:36:03,214 - test time: 31.9384
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 110100, eta: 84.61 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33703 sec, avg_samples: 64.00000, ips: 379.79041 images/sec
Training: 2023-10-17 23:36:36,922 - loss nan, lr: 0.025000, epoch: 3, step: 110100, eta: 84.61 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33703 sec, avg_samples: 64.00000, ips: 379.79041 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 110200, eta: 84.59 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33773 sec, avg_samples: 64.00000, ips: 379.00244 images/sec
Training: 2023-10-17 23:37:10,701 - loss nan, lr: 0.025000, epoch: 3, step: 110200, eta: 84.59 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33773 sec, avg_samples: 64.00000, ips: 379.00244 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 110300, eta: 84.57 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33773 sec, avg_samples: 64.00000, ips: 379.00067 images/sec
Training: 2023-10-17 23:37:44,480 - loss nan, lr: 0.025000, epoch: 3, step: 110300, eta: 84.57 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33773 sec, avg_samples: 64.00000, ips: 379.00067 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 110400, eta: 84.55 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33772 sec, avg_samples: 64.00000, ips: 379.00753 images/sec
Training: 2023-10-17 23:38:18,258 - loss nan, lr: 0.025000, epoch: 3, step: 110400, eta: 84.55 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33772 sec, avg_samples: 64.00000, ips: 379.00753 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 110500, eta: 84.53 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33776 sec, avg_samples: 64.00000, ips: 378.96368 images/sec
Training: 2023-10-17 23:38:52,041 - loss nan, lr: 0.025000, epoch: 3, step: 110500, eta: 84.53 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33776 sec, avg_samples: 64.00000, ips: 378.96368 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 110600, eta: 84.51 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33775 sec, avg_samples: 64.00000, ips: 378.97903 images/sec
Training: 2023-10-17 23:39:25,822 - loss nan, lr: 0.025000, epoch: 3, step: 110600, eta: 84.51 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33775 sec, avg_samples: 64.00000, ips: 378.97903 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 110700, eta: 84.49 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33783 sec, avg_samples: 64.00000, ips: 378.88939 images/sec
Training: 2023-10-17 23:39:59,610 - loss nan, lr: 0.025000, epoch: 3, step: 110700, eta: 84.49 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33783 sec, avg_samples: 64.00000, ips: 378.88939 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 110800, eta: 84.47 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33777 sec, avg_samples: 64.00000, ips: 378.96141 images/sec
Training: 2023-10-17 23:40:33,393 - loss nan, lr: 0.025000, epoch: 3, step: 110800, eta: 84.47 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33777 sec, avg_samples: 64.00000, ips: 378.96141 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 110900, eta: 84.45 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33779 sec, avg_samples: 64.00000, ips: 378.93180 images/sec
Training: 2023-10-17 23:41:07,178 - loss nan, lr: 0.025000, epoch: 3, step: 110900, eta: 84.45 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33779 sec, avg_samples: 64.00000, ips: 378.93180 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 111000, eta: 84.43 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33771 sec, avg_samples: 64.00000, ips: 379.01837 images/sec
Training: 2023-10-17 23:41:40,955 - loss nan, lr: 0.025000, epoch: 3, step: 111000, eta: 84.43 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33771 sec, avg_samples: 64.00000, ips: 379.01837 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 111100, eta: 84.41 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33771 sec, avg_samples: 64.00000, ips: 379.01978 images/sec
Training: 2023-10-17 23:42:14,733 - loss nan, lr: 0.025000, epoch: 3, step: 111100, eta: 84.41 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33771 sec, avg_samples: 64.00000, ips: 379.01978 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 111200, eta: 84.39 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33778 sec, avg_samples: 64.00000, ips: 378.94412 images/sec
Training: 2023-10-17 23:42:48,517 - loss nan, lr: 0.025000, epoch: 3, step: 111200, eta: 84.39 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33778 sec, avg_samples: 64.00000, ips: 378.94412 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 111300, eta: 84.37 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33768 sec, avg_samples: 64.00000, ips: 379.05349 images/sec
Training: 2023-10-17 23:43:22,291 - loss nan, lr: 0.025000, epoch: 3, step: 111300, eta: 84.37 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33768 sec, avg_samples: 64.00000, ips: 379.05349 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 111400, eta: 84.34 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.83220 images/sec
Training: 2023-10-17 23:43:56,085 - loss nan, lr: 0.025000, epoch: 3, step: 111400, eta: 84.34 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.83220 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 111500, eta: 84.32 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33778 sec, avg_samples: 64.00000, ips: 378.94724 images/sec
Training: 2023-10-17 23:44:29,869 - loss nan, lr: 0.025000, epoch: 3, step: 111500, eta: 84.32 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33778 sec, avg_samples: 64.00000, ips: 378.94724 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 111600, eta: 84.30 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33770 sec, avg_samples: 64.00000, ips: 379.03076 images/sec
Training: 2023-10-17 23:45:03,645 - loss nan, lr: 0.025000, epoch: 3, step: 111600, eta: 84.30 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33770 sec, avg_samples: 64.00000, ips: 379.03076 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 111700, eta: 84.28 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.87133 images/sec
Training: 2023-10-17 23:45:37,436 - loss nan, lr: 0.025000, epoch: 3, step: 111700, eta: 84.28 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.87133 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 111800, eta: 84.26 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.86144 images/sec
Training: 2023-10-17 23:46:11,227 - loss nan, lr: 0.025000, epoch: 3, step: 111800, eta: 84.26 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.86144 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 111900, eta: 84.24 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.82967 images/sec
Training: 2023-10-17 23:46:45,022 - loss nan, lr: 0.025000, epoch: 3, step: 111900, eta: 84.24 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.82967 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 112000, eta: 84.22 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33769 sec, avg_samples: 64.00000, ips: 379.05093 images/sec
Training: 2023-10-17 23:47:18,796 - loss nan, lr: 0.025000, epoch: 3, step: 112000, eta: 84.22 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33769 sec, avg_samples: 64.00000, ips: 379.05093 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][112000]XNorm: 0.000004
Training: 2023-10-17 23:47:50,889 - [lfw][112000]XNorm: 0.000004
INFO:root:[lfw][112000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 23:47:50,889 - [lfw][112000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][112000]Accuracy-Highest: 0.81183
Training: 2023-10-17 23:47:50,889 - [lfw][112000]Accuracy-Highest: 0.81183
INFO:root:test time: 32.0931
Training: 2023-10-17 23:47:50,889 - test time: 32.0931
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][112000]XNorm: 0.000004
Training: 2023-10-17 23:48:27,956 - [cfp_fp][112000]XNorm: 0.000004
INFO:root:[cfp_fp][112000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 23:48:27,956 - [cfp_fp][112000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][112000]Accuracy-Highest: 0.61571
Training: 2023-10-17 23:48:27,956 - [cfp_fp][112000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.0664
Training: 2023-10-17 23:48:27,956 - test time: 37.0664
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][112000]XNorm: 0.000004
Training: 2023-10-17 23:48:59,920 - [agedb_30][112000]XNorm: 0.000004
INFO:root:[agedb_30][112000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-17 23:48:59,920 - [agedb_30][112000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][112000]Accuracy-Highest: 0.58350
Training: 2023-10-17 23:48:59,920 - [agedb_30][112000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.9644
Training: 2023-10-17 23:48:59,920 - test time: 31.9644
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 112100, eta: 84.40 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33701 sec, avg_samples: 64.00000, ips: 379.81433 images/sec
Training: 2023-10-17 23:49:33,627 - loss nan, lr: 0.025000, epoch: 3, step: 112100, eta: 84.40 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33701 sec, avg_samples: 64.00000, ips: 379.81433 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 112200, eta: 84.38 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33775 sec, avg_samples: 64.00000, ips: 378.98230 images/sec
Training: 2023-10-17 23:50:07,407 - loss nan, lr: 0.025000, epoch: 3, step: 112200, eta: 84.38 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33775 sec, avg_samples: 64.00000, ips: 378.98230 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 112300, eta: 84.35 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33774 sec, avg_samples: 64.00000, ips: 378.98690 images/sec
Training: 2023-10-17 23:50:41,188 - loss nan, lr: 0.025000, epoch: 3, step: 112300, eta: 84.35 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33774 sec, avg_samples: 64.00000, ips: 378.98690 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 112400, eta: 84.33 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.86303 images/sec
Training: 2023-10-17 23:51:14,979 - loss nan, lr: 0.025000, epoch: 3, step: 112400, eta: 84.33 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.86303 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 112500, eta: 84.31 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33787 sec, avg_samples: 64.00000, ips: 378.83970 images/sec
Training: 2023-10-17 23:51:48,772 - loss nan, lr: 0.025000, epoch: 3, step: 112500, eta: 84.31 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33787 sec, avg_samples: 64.00000, ips: 378.83970 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 112600, eta: 84.29 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33787 sec, avg_samples: 64.00000, ips: 378.83977 images/sec
Training: 2023-10-17 23:52:22,565 - loss nan, lr: 0.025000, epoch: 3, step: 112600, eta: 84.29 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33787 sec, avg_samples: 64.00000, ips: 378.83977 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 112700, eta: 84.27 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.80609 images/sec
Training: 2023-10-17 23:52:56,362 - loss nan, lr: 0.025000, epoch: 3, step: 112700, eta: 84.27 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.80609 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 112800, eta: 84.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.85214 images/sec
Training: 2023-10-17 23:53:30,154 - loss nan, lr: 0.025000, epoch: 3, step: 112800, eta: 84.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.85214 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 112900, eta: 84.23 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33762 sec, avg_samples: 64.00000, ips: 379.12413 images/sec
Training: 2023-10-17 23:54:03,922 - loss nan, lr: 0.025000, epoch: 3, step: 112900, eta: 84.23 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33762 sec, avg_samples: 64.00000, ips: 379.12413 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 113000, eta: 84.21 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33781 sec, avg_samples: 64.00000, ips: 378.91306 images/sec
Training: 2023-10-17 23:54:37,709 - loss nan, lr: 0.025000, epoch: 3, step: 113000, eta: 84.21 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33781 sec, avg_samples: 64.00000, ips: 378.91306 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 113100, eta: 84.19 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33777 sec, avg_samples: 64.00000, ips: 378.95810 images/sec
Training: 2023-10-17 23:55:11,492 - loss nan, lr: 0.025000, epoch: 3, step: 113100, eta: 84.19 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33777 sec, avg_samples: 64.00000, ips: 378.95810 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 113200, eta: 84.17 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33746 sec, avg_samples: 64.00000, ips: 379.30179 images/sec
Training: 2023-10-17 23:55:45,246 - loss nan, lr: 0.025000, epoch: 3, step: 113200, eta: 84.17 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33746 sec, avg_samples: 64.00000, ips: 379.30179 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 113300, eta: 84.15 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.79406 images/sec
Training: 2023-10-17 23:56:19,043 - loss nan, lr: 0.025000, epoch: 3, step: 113300, eta: 84.15 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.79406 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 113400, eta: 84.13 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33777 sec, avg_samples: 64.00000, ips: 378.95277 images/sec
Training: 2023-10-17 23:56:52,825 - loss nan, lr: 0.025000, epoch: 3, step: 113400, eta: 84.13 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33777 sec, avg_samples: 64.00000, ips: 378.95277 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 113500, eta: 84.11 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33804 sec, avg_samples: 64.00000, ips: 378.65621 images/sec
Training: 2023-10-17 23:57:26,635 - loss nan, lr: 0.025000, epoch: 3, step: 113500, eta: 84.11 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33804 sec, avg_samples: 64.00000, ips: 378.65621 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 113600, eta: 84.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.71129 images/sec
Training: 2023-10-17 23:58:00,439 - loss nan, lr: 0.025000, epoch: 3, step: 113600, eta: 84.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.71129 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 113700, eta: 84.07 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.80290 images/sec
Training: 2023-10-17 23:58:34,236 - loss nan, lr: 0.025000, epoch: 3, step: 113700, eta: 84.07 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.80290 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 113800, eta: 84.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.82044 images/sec
Training: 2023-10-17 23:59:08,030 - loss nan, lr: 0.025000, epoch: 3, step: 113800, eta: 84.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.82044 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 113900, eta: 84.03 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.86332 images/sec
Training: 2023-10-17 23:59:41,821 - loss nan, lr: 0.025000, epoch: 3, step: 113900, eta: 84.03 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.86332 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 114000, eta: 84.01 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.86419 images/sec
Training: 2023-10-18 00:00:15,612 - loss nan, lr: 0.025000, epoch: 3, step: 114000, eta: 84.01 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.86419 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][114000]XNorm: 0.000003
Training: 2023-10-18 00:00:47,689 - [lfw][114000]XNorm: 0.000003
INFO:root:[lfw][114000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 00:00:47,689 - [lfw][114000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][114000]Accuracy-Highest: 0.81183
Training: 2023-10-18 00:00:47,689 - [lfw][114000]Accuracy-Highest: 0.81183
INFO:root:test time: 32.0769
Training: 2023-10-18 00:00:47,689 - test time: 32.0769
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][114000]XNorm: 0.000003
Training: 2023-10-18 00:01:24,750 - [cfp_fp][114000]XNorm: 0.000003
INFO:root:[cfp_fp][114000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 00:01:24,750 - [cfp_fp][114000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][114000]Accuracy-Highest: 0.61571
Training: 2023-10-18 00:01:24,750 - [cfp_fp][114000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.0608
Training: 2023-10-18 00:01:24,750 - test time: 37.0608
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][114000]XNorm: 0.000003
Training: 2023-10-18 00:01:56,698 - [agedb_30][114000]XNorm: 0.000003
INFO:root:[agedb_30][114000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 00:01:56,698 - [agedb_30][114000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][114000]Accuracy-Highest: 0.58350
Training: 2023-10-18 00:01:56,698 - [agedb_30][114000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.9477
Training: 2023-10-18 00:01:56,698 - test time: 31.9477
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 114100, eta: 84.18 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33697 sec, avg_samples: 64.00000, ips: 379.86124 images/sec
Training: 2023-10-18 00:02:30,400 - loss nan, lr: 0.025000, epoch: 3, step: 114100, eta: 84.18 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33697 sec, avg_samples: 64.00000, ips: 379.86124 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 114200, eta: 84.16 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33772 sec, avg_samples: 64.00000, ips: 379.01291 images/sec
Training: 2023-10-18 00:03:04,178 - loss nan, lr: 0.025000, epoch: 3, step: 114200, eta: 84.16 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33772 sec, avg_samples: 64.00000, ips: 379.01291 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 114300, eta: 84.14 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33783 sec, avg_samples: 64.00000, ips: 378.89348 images/sec
Training: 2023-10-18 00:03:37,967 - loss nan, lr: 0.025000, epoch: 3, step: 114300, eta: 84.14 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33783 sec, avg_samples: 64.00000, ips: 378.89348 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 114400, eta: 84.12 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33777 sec, avg_samples: 64.00000, ips: 378.95276 images/sec
Training: 2023-10-18 00:04:11,750 - loss nan, lr: 0.025000, epoch: 3, step: 114400, eta: 84.12 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33777 sec, avg_samples: 64.00000, ips: 378.95276 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 114500, eta: 84.10 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.81154 images/sec
Training: 2023-10-18 00:04:45,546 - loss nan, lr: 0.025000, epoch: 3, step: 114500, eta: 84.10 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.81154 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 114600, eta: 84.08 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.70712 images/sec
Training: 2023-10-18 00:05:19,351 - loss nan, lr: 0.025000, epoch: 3, step: 114600, eta: 84.08 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.70712 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 114700, eta: 84.06 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33771 sec, avg_samples: 64.00000, ips: 379.02117 images/sec
Training: 2023-10-18 00:05:53,128 - loss nan, lr: 0.025000, epoch: 3, step: 114700, eta: 84.06 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33771 sec, avg_samples: 64.00000, ips: 379.02117 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 114800, eta: 84.04 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33778 sec, avg_samples: 64.00000, ips: 378.93937 images/sec
Training: 2023-10-18 00:06:26,912 - loss nan, lr: 0.025000, epoch: 3, step: 114800, eta: 84.04 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33778 sec, avg_samples: 64.00000, ips: 378.93937 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 114900, eta: 84.02 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33783 sec, avg_samples: 64.00000, ips: 378.89215 images/sec
Training: 2023-10-18 00:07:00,701 - loss nan, lr: 0.025000, epoch: 3, step: 114900, eta: 84.02 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33783 sec, avg_samples: 64.00000, ips: 378.89215 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 115000, eta: 83.99 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33782 sec, avg_samples: 64.00000, ips: 378.90340 images/sec
Training: 2023-10-18 00:07:34,489 - loss nan, lr: 0.025000, epoch: 3, step: 115000, eta: 83.99 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33782 sec, avg_samples: 64.00000, ips: 378.90340 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 115100, eta: 83.97 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33782 sec, avg_samples: 64.00000, ips: 378.90043 images/sec
Training: 2023-10-18 00:08:08,277 - loss nan, lr: 0.025000, epoch: 3, step: 115100, eta: 83.97 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33782 sec, avg_samples: 64.00000, ips: 378.90043 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 115200, eta: 83.95 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.86319 images/sec
Training: 2023-10-18 00:08:42,068 - loss nan, lr: 0.025000, epoch: 3, step: 115200, eta: 83.95 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.86319 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 115300, eta: 83.93 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33781 sec, avg_samples: 64.00000, ips: 378.90767 images/sec
Training: 2023-10-18 00:09:15,855 - loss nan, lr: 0.025000, epoch: 3, step: 115300, eta: 83.93 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33781 sec, avg_samples: 64.00000, ips: 378.90767 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 115400, eta: 83.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33787 sec, avg_samples: 64.00000, ips: 378.84322 images/sec
Training: 2023-10-18 00:09:49,648 - loss nan, lr: 0.025000, epoch: 3, step: 115400, eta: 83.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33787 sec, avg_samples: 64.00000, ips: 378.84322 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 115500, eta: 83.89 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.83757 images/sec
Training: 2023-10-18 00:10:23,441 - loss nan, lr: 0.025000, epoch: 3, step: 115500, eta: 83.89 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.83757 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 115600, eta: 83.87 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.85650 images/sec
Training: 2023-10-18 00:10:57,233 - loss nan, lr: 0.025000, epoch: 3, step: 115600, eta: 83.87 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.85650 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 115700, eta: 83.85 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.80249 images/sec
Training: 2023-10-18 00:11:31,030 - loss nan, lr: 0.025000, epoch: 3, step: 115700, eta: 83.85 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.80249 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 115800, eta: 83.83 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33795 sec, avg_samples: 64.00000, ips: 378.75241 images/sec
Training: 2023-10-18 00:12:04,831 - loss nan, lr: 0.025000, epoch: 3, step: 115800, eta: 83.83 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33795 sec, avg_samples: 64.00000, ips: 378.75241 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 115900, eta: 83.81 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.86541 images/sec
Training: 2023-10-18 00:12:38,622 - loss nan, lr: 0.025000, epoch: 3, step: 115900, eta: 83.81 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.86541 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 116000, eta: 83.79 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33781 sec, avg_samples: 64.00000, ips: 378.91283 images/sec
Training: 2023-10-18 00:13:12,408 - loss nan, lr: 0.025000, epoch: 3, step: 116000, eta: 83.79 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33781 sec, avg_samples: 64.00000, ips: 378.91283 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][116000]XNorm: 0.000003
Training: 2023-10-18 00:13:44,471 - [lfw][116000]XNorm: 0.000003
INFO:root:[lfw][116000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 00:13:44,471 - [lfw][116000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][116000]Accuracy-Highest: 0.81183
Training: 2023-10-18 00:13:44,471 - [lfw][116000]Accuracy-Highest: 0.81183
INFO:root:test time: 32.0621
Training: 2023-10-18 00:13:44,471 - test time: 32.0621
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][116000]XNorm: 0.000003
Training: 2023-10-18 00:14:21,520 - [cfp_fp][116000]XNorm: 0.000003
INFO:root:[cfp_fp][116000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 00:14:21,520 - [cfp_fp][116000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][116000]Accuracy-Highest: 0.61571
Training: 2023-10-18 00:14:21,520 - [cfp_fp][116000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.0495
Training: 2023-10-18 00:14:21,520 - test time: 37.0495
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][116000]XNorm: 0.000003
Training: 2023-10-18 00:14:53,453 - [agedb_30][116000]XNorm: 0.000003
INFO:root:[agedb_30][116000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 00:14:53,453 - [agedb_30][116000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][116000]Accuracy-Highest: 0.58350
Training: 2023-10-18 00:14:53,453 - [agedb_30][116000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.9324
Training: 2023-10-18 00:14:53,453 - test time: 31.9324
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 116100, eta: 83.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33684 sec, avg_samples: 64.00000, ips: 380.00321 images/sec
Training: 2023-10-18 00:15:27,143 - loss nan, lr: 0.025000, epoch: 3, step: 116100, eta: 83.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33684 sec, avg_samples: 64.00000, ips: 380.00321 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 116200, eta: 83.94 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33795 sec, avg_samples: 64.00000, ips: 378.75902 images/sec
Training: 2023-10-18 00:16:00,943 - loss nan, lr: 0.025000, epoch: 3, step: 116200, eta: 83.94 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33795 sec, avg_samples: 64.00000, ips: 378.75902 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 116300, eta: 83.92 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.78836 images/sec
Training: 2023-10-18 00:16:34,741 - loss nan, lr: 0.025000, epoch: 3, step: 116300, eta: 83.92 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.78836 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 116400, eta: 83.90 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33782 sec, avg_samples: 64.00000, ips: 378.90478 images/sec
Training: 2023-10-18 00:17:08,529 - loss nan, lr: 0.025000, epoch: 3, step: 116400, eta: 83.90 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33782 sec, avg_samples: 64.00000, ips: 378.90478 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 116500, eta: 83.88 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33811 sec, avg_samples: 64.00000, ips: 378.57975 images/sec
Training: 2023-10-18 00:17:42,345 - loss nan, lr: 0.025000, epoch: 3, step: 116500, eta: 83.88 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33811 sec, avg_samples: 64.00000, ips: 378.57975 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 116600, eta: 83.86 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.63819 images/sec
Training: 2023-10-18 00:18:16,156 - loss nan, lr: 0.025000, epoch: 3, step: 116600, eta: 83.86 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.63819 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 116700, eta: 83.84 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33795 sec, avg_samples: 64.00000, ips: 378.75207 images/sec
Training: 2023-10-18 00:18:49,957 - loss nan, lr: 0.025000, epoch: 3, step: 116700, eta: 83.84 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33795 sec, avg_samples: 64.00000, ips: 378.75207 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 116800, eta: 83.82 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.71919 images/sec
Training: 2023-10-18 00:19:23,762 - loss nan, lr: 0.025000, epoch: 3, step: 116800, eta: 83.82 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.71919 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 116900, eta: 83.80 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.63719 images/sec
Training: 2023-10-18 00:19:57,573 - loss nan, lr: 0.025000, epoch: 3, step: 116900, eta: 83.80 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.63719 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 117000, eta: 83.78 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33804 sec, avg_samples: 64.00000, ips: 378.65033 images/sec
Training: 2023-10-18 00:20:31,383 - loss nan, lr: 0.025000, epoch: 3, step: 117000, eta: 83.78 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33804 sec, avg_samples: 64.00000, ips: 378.65033 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 117100, eta: 83.76 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33807 sec, avg_samples: 64.00000, ips: 378.61960 images/sec
Training: 2023-10-18 00:21:05,196 - loss nan, lr: 0.025000, epoch: 3, step: 117100, eta: 83.76 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33807 sec, avg_samples: 64.00000, ips: 378.61960 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 117200, eta: 83.74 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33807 sec, avg_samples: 64.00000, ips: 378.61862 images/sec
Training: 2023-10-18 00:21:39,009 - loss nan, lr: 0.025000, epoch: 3, step: 117200, eta: 83.74 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33807 sec, avg_samples: 64.00000, ips: 378.61862 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 117300, eta: 83.72 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33806 sec, avg_samples: 64.00000, ips: 378.63381 images/sec
Training: 2023-10-18 00:22:12,821 - loss nan, lr: 0.025000, epoch: 3, step: 117300, eta: 83.72 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33806 sec, avg_samples: 64.00000, ips: 378.63381 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 117400, eta: 83.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.71218 images/sec
Training: 2023-10-18 00:22:46,626 - loss nan, lr: 0.025000, epoch: 3, step: 117400, eta: 83.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.71218 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 117500, eta: 83.68 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33806 sec, avg_samples: 64.00000, ips: 378.63251 images/sec
Training: 2023-10-18 00:23:20,437 - loss nan, lr: 0.025000, epoch: 3, step: 117500, eta: 83.68 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33806 sec, avg_samples: 64.00000, ips: 378.63251 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 117600, eta: 83.66 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33793 sec, avg_samples: 64.00000, ips: 378.77220 images/sec
Training: 2023-10-18 00:23:54,237 - loss nan, lr: 0.025000, epoch: 3, step: 117600, eta: 83.66 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33793 sec, avg_samples: 64.00000, ips: 378.77220 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 117700, eta: 83.64 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.69981 images/sec
Training: 2023-10-18 00:24:28,042 - loss nan, lr: 0.025000, epoch: 3, step: 117700, eta: 83.64 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.69981 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 117800, eta: 83.62 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.69342 images/sec
Training: 2023-10-18 00:25:01,849 - loss nan, lr: 0.025000, epoch: 3, step: 117800, eta: 83.62 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33800 sec, avg_samples: 64.00000, ips: 378.69342 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 117900, eta: 83.60 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68737 images/sec
Training: 2023-10-18 00:25:35,656 - loss nan, lr: 0.025000, epoch: 3, step: 117900, eta: 83.60 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33801 sec, avg_samples: 64.00000, ips: 378.68737 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 118000, eta: 83.58 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66334 images/sec
Training: 2023-10-18 00:26:09,465 - loss nan, lr: 0.025000, epoch: 3, step: 118000, eta: 83.58 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66334 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][118000]XNorm: 0.000002
Training: 2023-10-18 00:26:41,505 - [lfw][118000]XNorm: 0.000002
INFO:root:[lfw][118000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 00:26:41,505 - [lfw][118000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][118000]Accuracy-Highest: 0.81183
Training: 2023-10-18 00:26:41,505 - [lfw][118000]Accuracy-Highest: 0.81183
INFO:root:test time: 32.0402
Training: 2023-10-18 00:26:41,505 - test time: 32.0402
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][118000]XNorm: 0.000002
Training: 2023-10-18 00:27:18,597 - [cfp_fp][118000]XNorm: 0.000002
INFO:root:[cfp_fp][118000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 00:27:18,597 - [cfp_fp][118000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][118000]Accuracy-Highest: 0.61571
Training: 2023-10-18 00:27:18,597 - [cfp_fp][118000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.0921
Training: 2023-10-18 00:27:18,597 - test time: 37.0921
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][118000]XNorm: 0.000002
Training: 2023-10-18 00:27:50,517 - [agedb_30][118000]XNorm: 0.000002
INFO:root:[agedb_30][118000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 00:27:50,517 - [agedb_30][118000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][118000]Accuracy-Highest: 0.58350
Training: 2023-10-18 00:27:50,517 - [agedb_30][118000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.9196
Training: 2023-10-18 00:27:50,517 - test time: 31.9196
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 118100, eta: 83.74 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33696 sec, avg_samples: 64.00000, ips: 379.86204 images/sec
Training: 2023-10-18 00:28:24,219 - loss nan, lr: 0.025000, epoch: 3, step: 118100, eta: 83.74 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33696 sec, avg_samples: 64.00000, ips: 379.86204 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 118200, eta: 83.72 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33772 sec, avg_samples: 64.00000, ips: 379.00902 images/sec
Training: 2023-10-18 00:28:57,997 - loss nan, lr: 0.025000, epoch: 3, step: 118200, eta: 83.72 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33772 sec, avg_samples: 64.00000, ips: 379.00902 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 118300, eta: 83.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.77022 images/sec
Training: 2023-10-18 00:29:31,796 - loss nan, lr: 0.025000, epoch: 3, step: 118300, eta: 83.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.77022 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 118400, eta: 83.68 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33775 sec, avg_samples: 64.00000, ips: 378.97320 images/sec
Training: 2023-10-18 00:30:05,577 - loss nan, lr: 0.025000, epoch: 3, step: 118400, eta: 83.68 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33775 sec, avg_samples: 64.00000, ips: 378.97320 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 118500, eta: 83.66 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.86025 images/sec
Training: 2023-10-18 00:30:39,368 - loss nan, lr: 0.025000, epoch: 3, step: 118500, eta: 83.66 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.86025 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 118600, eta: 83.64 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33777 sec, avg_samples: 64.00000, ips: 378.96058 images/sec
Training: 2023-10-18 00:31:13,150 - loss nan, lr: 0.025000, epoch: 3, step: 118600, eta: 83.64 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33777 sec, avg_samples: 64.00000, ips: 378.96058 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 118700, eta: 83.62 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33784 sec, avg_samples: 64.00000, ips: 378.87978 images/sec
Training: 2023-10-18 00:31:46,939 - loss nan, lr: 0.025000, epoch: 3, step: 118700, eta: 83.62 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33784 sec, avg_samples: 64.00000, ips: 378.87978 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 118800, eta: 83.60 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.71488 images/sec
Training: 2023-10-18 00:32:20,743 - loss nan, lr: 0.025000, epoch: 3, step: 118800, eta: 83.60 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.71488 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 118900, eta: 83.58 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.80703 images/sec
Training: 2023-10-18 00:32:54,539 - loss nan, lr: 0.025000, epoch: 3, step: 118900, eta: 83.58 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.80703 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 119000, eta: 83.56 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33776 sec, avg_samples: 64.00000, ips: 378.96969 images/sec
Training: 2023-10-18 00:33:28,321 - loss nan, lr: 0.025000, epoch: 3, step: 119000, eta: 83.56 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33776 sec, avg_samples: 64.00000, ips: 378.96969 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 119100, eta: 83.54 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33783 sec, avg_samples: 64.00000, ips: 378.88434 images/sec
Training: 2023-10-18 00:34:02,110 - loss nan, lr: 0.025000, epoch: 3, step: 119100, eta: 83.54 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33783 sec, avg_samples: 64.00000, ips: 378.88434 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 119200, eta: 83.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.85044 images/sec
Training: 2023-10-18 00:34:35,902 - loss nan, lr: 0.025000, epoch: 3, step: 119200, eta: 83.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.85044 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 119300, eta: 83.50 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33783 sec, avg_samples: 64.00000, ips: 378.89131 images/sec
Training: 2023-10-18 00:35:09,690 - loss nan, lr: 0.025000, epoch: 3, step: 119300, eta: 83.50 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33783 sec, avg_samples: 64.00000, ips: 378.89131 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 119400, eta: 83.48 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.85357 images/sec
Training: 2023-10-18 00:35:43,482 - loss nan, lr: 0.025000, epoch: 3, step: 119400, eta: 83.48 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.85357 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 119500, eta: 83.46 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.82429 images/sec
Training: 2023-10-18 00:36:17,276 - loss nan, lr: 0.025000, epoch: 3, step: 119500, eta: 83.46 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.82429 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 119600, eta: 83.44 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33782 sec, avg_samples: 64.00000, ips: 378.89852 images/sec
Training: 2023-10-18 00:36:51,064 - loss nan, lr: 0.025000, epoch: 3, step: 119600, eta: 83.44 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33782 sec, avg_samples: 64.00000, ips: 378.89852 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 119700, eta: 83.42 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.82092 images/sec
Training: 2023-10-18 00:37:24,858 - loss nan, lr: 0.025000, epoch: 3, step: 119700, eta: 83.42 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.82092 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 119800, eta: 83.40 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33777 sec, avg_samples: 64.00000, ips: 378.95338 images/sec
Training: 2023-10-18 00:37:58,641 - loss nan, lr: 0.025000, epoch: 3, step: 119800, eta: 83.40 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33777 sec, avg_samples: 64.00000, ips: 378.95338 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 119900, eta: 83.38 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76781 images/sec
Training: 2023-10-18 00:38:32,441 - loss nan, lr: 0.025000, epoch: 3, step: 119900, eta: 83.38 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76781 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 120000, eta: 83.36 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33777 sec, avg_samples: 64.00000, ips: 378.95424 images/sec
Training: 2023-10-18 00:39:06,224 - loss nan, lr: 0.025000, epoch: 3, step: 120000, eta: 83.36 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33777 sec, avg_samples: 64.00000, ips: 378.95424 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][120000]XNorm: 0.000002
Training: 2023-10-18 00:39:38,189 - [lfw][120000]XNorm: 0.000002
INFO:root:[lfw][120000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 00:39:38,189 - [lfw][120000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][120000]Accuracy-Highest: 0.81183
Training: 2023-10-18 00:39:38,189 - [lfw][120000]Accuracy-Highest: 0.81183
INFO:root:test time: 31.9655
Training: 2023-10-18 00:39:38,189 - test time: 31.9655
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][120000]XNorm: 0.000002
Training: 2023-10-18 00:40:15,226 - [cfp_fp][120000]XNorm: 0.000002
INFO:root:[cfp_fp][120000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 00:40:15,226 - [cfp_fp][120000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][120000]Accuracy-Highest: 0.61571
Training: 2023-10-18 00:40:15,226 - [cfp_fp][120000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.0366
Training: 2023-10-18 00:40:15,226 - test time: 37.0366
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][120000]XNorm: 0.000002
Training: 2023-10-18 00:40:47,172 - [agedb_30][120000]XNorm: 0.000002
INFO:root:[agedb_30][120000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 00:40:47,172 - [agedb_30][120000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][120000]Accuracy-Highest: 0.58350
Training: 2023-10-18 00:40:47,172 - [agedb_30][120000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.9458
Training: 2023-10-18 00:40:47,172 - test time: 31.9458
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 120100, eta: 83.53 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33683 sec, avg_samples: 64.00000, ips: 380.01330 images/sec
Training: 2023-10-18 00:41:20,861 - loss nan, lr: 0.025000, epoch: 3, step: 120100, eta: 83.53 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33683 sec, avg_samples: 64.00000, ips: 380.01330 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 120200, eta: 83.51 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.86824 images/sec
Training: 2023-10-18 00:41:54,652 - loss nan, lr: 0.025000, epoch: 3, step: 120200, eta: 83.51 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.86824 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 120300, eta: 83.49 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.71287 images/sec
Training: 2023-10-18 00:42:28,456 - loss nan, lr: 0.025000, epoch: 3, step: 120300, eta: 83.49 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.71287 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 120400, eta: 83.47 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.64339 images/sec
Training: 2023-10-18 00:43:02,267 - loss nan, lr: 0.025000, epoch: 3, step: 120400, eta: 83.47 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33805 sec, avg_samples: 64.00000, ips: 378.64339 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 120500, eta: 83.45 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.82644 images/sec
Training: 2023-10-18 00:43:36,062 - loss nan, lr: 0.025000, epoch: 3, step: 120500, eta: 83.45 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.82644 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 120600, eta: 83.43 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.78687 images/sec
Training: 2023-10-18 00:44:09,860 - loss nan, lr: 0.025000, epoch: 3, step: 120600, eta: 83.43 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.78687 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 120700, eta: 83.41 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.72113 images/sec
Training: 2023-10-18 00:44:43,663 - loss nan, lr: 0.025000, epoch: 3, step: 120700, eta: 83.41 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.72113 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 120800, eta: 83.39 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76528 images/sec
Training: 2023-10-18 00:45:17,463 - loss nan, lr: 0.025000, epoch: 3, step: 120800, eta: 83.39 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76528 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 120900, eta: 83.37 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.86603 images/sec
Training: 2023-10-18 00:45:51,254 - loss nan, lr: 0.025000, epoch: 3, step: 120900, eta: 83.37 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.86603 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 121000, eta: 83.35 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33783 sec, avg_samples: 64.00000, ips: 378.88497 images/sec
Training: 2023-10-18 00:46:25,044 - loss nan, lr: 0.025000, epoch: 3, step: 121000, eta: 83.35 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33783 sec, avg_samples: 64.00000, ips: 378.88497 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 121100, eta: 83.33 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.79009 images/sec
Training: 2023-10-18 00:46:58,841 - loss nan, lr: 0.025000, epoch: 3, step: 121100, eta: 83.33 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.79009 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 121200, eta: 83.31 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.83814 images/sec
Training: 2023-10-18 00:47:32,635 - loss nan, lr: 0.025000, epoch: 3, step: 121200, eta: 83.31 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.83814 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 121300, eta: 83.29 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33793 sec, avg_samples: 64.00000, ips: 378.77177 images/sec
Training: 2023-10-18 00:48:06,434 - loss nan, lr: 0.025000, epoch: 3, step: 121300, eta: 83.29 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33793 sec, avg_samples: 64.00000, ips: 378.77177 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 121400, eta: 83.27 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.86639 images/sec
Training: 2023-10-18 00:48:40,225 - loss nan, lr: 0.025000, epoch: 3, step: 121400, eta: 83.27 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.86639 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 121500, eta: 83.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33796 sec, avg_samples: 64.00000, ips: 378.74259 images/sec
Training: 2023-10-18 00:49:14,027 - loss nan, lr: 0.025000, epoch: 3, step: 121500, eta: 83.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33796 sec, avg_samples: 64.00000, ips: 378.74259 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 121600, eta: 83.23 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33796 sec, avg_samples: 64.00000, ips: 378.74769 images/sec
Training: 2023-10-18 00:49:47,829 - loss nan, lr: 0.025000, epoch: 3, step: 121600, eta: 83.23 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33796 sec, avg_samples: 64.00000, ips: 378.74769 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 121700, eta: 83.21 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33793 sec, avg_samples: 64.00000, ips: 378.77210 images/sec
Training: 2023-10-18 00:50:21,628 - loss nan, lr: 0.025000, epoch: 3, step: 121700, eta: 83.21 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33793 sec, avg_samples: 64.00000, ips: 378.77210 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 121800, eta: 83.19 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66109 images/sec
Training: 2023-10-18 00:50:55,437 - loss nan, lr: 0.025000, epoch: 3, step: 121800, eta: 83.19 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33803 sec, avg_samples: 64.00000, ips: 378.66109 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 121900, eta: 83.17 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33793 sec, avg_samples: 64.00000, ips: 378.77428 images/sec
Training: 2023-10-18 00:51:29,237 - loss nan, lr: 0.025000, epoch: 3, step: 121900, eta: 83.17 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33793 sec, avg_samples: 64.00000, ips: 378.77428 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 122000, eta: 83.15 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33796 sec, avg_samples: 64.00000, ips: 378.74812 images/sec
Training: 2023-10-18 00:52:03,038 - loss nan, lr: 0.025000, epoch: 3, step: 122000, eta: 83.15 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33796 sec, avg_samples: 64.00000, ips: 378.74812 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][122000]XNorm: 0.000001
Training: 2023-10-18 00:52:34,956 - [lfw][122000]XNorm: 0.000001
INFO:root:[lfw][122000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 00:52:34,956 - [lfw][122000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][122000]Accuracy-Highest: 0.81183
Training: 2023-10-18 00:52:34,956 - [lfw][122000]Accuracy-Highest: 0.81183
INFO:root:test time: 31.9175
Training: 2023-10-18 00:52:34,956 - test time: 31.9175
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][122000]XNorm: 0.000001
Training: 2023-10-18 00:53:12,003 - [cfp_fp][122000]XNorm: 0.000001
INFO:root:[cfp_fp][122000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 00:53:12,003 - [cfp_fp][122000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][122000]Accuracy-Highest: 0.61571
Training: 2023-10-18 00:53:12,003 - [cfp_fp][122000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.0477
Training: 2023-10-18 00:53:12,003 - test time: 37.0477
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][122000]XNorm: 0.000001
Training: 2023-10-18 00:53:43,935 - [agedb_30][122000]XNorm: 0.000001
INFO:root:[agedb_30][122000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 00:53:43,935 - [agedb_30][122000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][122000]Accuracy-Highest: 0.58350
Training: 2023-10-18 00:53:43,935 - [agedb_30][122000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.9320
Training: 2023-10-18 00:53:43,935 - test time: 31.9320
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 122100, eta: 83.31 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33678 sec, avg_samples: 64.00000, ips: 380.07350 images/sec
Training: 2023-10-18 00:54:17,619 - loss nan, lr: 0.025000, epoch: 3, step: 122100, eta: 83.31 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33678 sec, avg_samples: 64.00000, ips: 380.07350 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 122200, eta: 83.29 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33781 sec, avg_samples: 64.00000, ips: 378.91659 images/sec
Training: 2023-10-18 00:54:51,406 - loss nan, lr: 0.025000, epoch: 3, step: 122200, eta: 83.29 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33781 sec, avg_samples: 64.00000, ips: 378.91659 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 122300, eta: 83.27 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.83326 images/sec
Training: 2023-10-18 00:55:25,200 - loss nan, lr: 0.025000, epoch: 3, step: 122300, eta: 83.27 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.83326 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 122400, eta: 83.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.81249 images/sec
Training: 2023-10-18 00:55:58,995 - loss nan, lr: 0.025000, epoch: 3, step: 122400, eta: 83.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.81249 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 122500, eta: 83.23 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.80885 images/sec
Training: 2023-10-18 00:56:32,791 - loss nan, lr: 0.025000, epoch: 3, step: 122500, eta: 83.23 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.80885 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 122600, eta: 83.21 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76575 images/sec
Training: 2023-10-18 00:57:06,591 - loss nan, lr: 0.025000, epoch: 3, step: 122600, eta: 83.21 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76575 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 122700, eta: 83.19 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33806 sec, avg_samples: 64.00000, ips: 378.62627 images/sec
Training: 2023-10-18 00:57:40,403 - loss nan, lr: 0.025000, epoch: 3, step: 122700, eta: 83.19 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33806 sec, avg_samples: 64.00000, ips: 378.62627 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 122800, eta: 83.17 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33797 sec, avg_samples: 64.00000, ips: 378.72865 images/sec
Training: 2023-10-18 00:58:14,207 - loss nan, lr: 0.025000, epoch: 3, step: 122800, eta: 83.17 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33797 sec, avg_samples: 64.00000, ips: 378.72865 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 122900, eta: 83.15 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76277 images/sec
Training: 2023-10-18 00:58:48,007 - loss nan, lr: 0.025000, epoch: 3, step: 122900, eta: 83.15 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76277 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 123000, eta: 83.13 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33797 sec, avg_samples: 64.00000, ips: 378.72760 images/sec
Training: 2023-10-18 00:59:21,810 - loss nan, lr: 0.025000, epoch: 3, step: 123000, eta: 83.13 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33797 sec, avg_samples: 64.00000, ips: 378.72760 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 123100, eta: 83.11 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.70751 images/sec
Training: 2023-10-18 00:59:55,615 - loss nan, lr: 0.025000, epoch: 3, step: 123100, eta: 83.11 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.70751 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 123200, eta: 83.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33804 sec, avg_samples: 64.00000, ips: 378.65608 images/sec
Training: 2023-10-18 01:00:29,425 - loss nan, lr: 0.025000, epoch: 3, step: 123200, eta: 83.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33804 sec, avg_samples: 64.00000, ips: 378.65608 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 123300, eta: 83.07 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.72394 images/sec
Training: 2023-10-18 01:01:03,228 - loss nan, lr: 0.025000, epoch: 3, step: 123300, eta: 83.07 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.72394 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 123400, eta: 83.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.78601 images/sec
Training: 2023-10-18 01:01:37,026 - loss nan, lr: 0.025000, epoch: 3, step: 123400, eta: 83.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.78601 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 123500, eta: 83.03 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33793 sec, avg_samples: 64.00000, ips: 378.77132 images/sec
Training: 2023-10-18 01:02:10,826 - loss nan, lr: 0.025000, epoch: 3, step: 123500, eta: 83.03 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33793 sec, avg_samples: 64.00000, ips: 378.77132 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 123600, eta: 83.01 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67866 images/sec
Training: 2023-10-18 01:02:44,633 - loss nan, lr: 0.025000, epoch: 3, step: 123600, eta: 83.01 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33802 sec, avg_samples: 64.00000, ips: 378.67866 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 123700, eta: 82.99 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.81936 images/sec
Training: 2023-10-18 01:03:18,428 - loss nan, lr: 0.025000, epoch: 3, step: 123700, eta: 82.99 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.81936 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 123800, eta: 82.97 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33795 sec, avg_samples: 64.00000, ips: 378.75798 images/sec
Training: 2023-10-18 01:03:52,229 - loss nan, lr: 0.025000, epoch: 3, step: 123800, eta: 82.97 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33795 sec, avg_samples: 64.00000, ips: 378.75798 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 123900, eta: 82.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.79746 images/sec
Training: 2023-10-18 01:04:26,026 - loss nan, lr: 0.025000, epoch: 3, step: 123900, eta: 82.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.79746 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 124000, eta: 82.94 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.70964 images/sec
Training: 2023-10-18 01:04:59,831 - loss nan, lr: 0.025000, epoch: 3, step: 124000, eta: 82.94 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.70964 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][124000]XNorm: 0.000001
Training: 2023-10-18 01:05:31,775 - [lfw][124000]XNorm: 0.000001
INFO:root:[lfw][124000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 01:05:31,775 - [lfw][124000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][124000]Accuracy-Highest: 0.81183
Training: 2023-10-18 01:05:31,775 - [lfw][124000]Accuracy-Highest: 0.81183
INFO:root:test time: 31.9444
Training: 2023-10-18 01:05:31,775 - test time: 31.9444
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][124000]XNorm: 0.000001
Training: 2023-10-18 01:06:08,822 - [cfp_fp][124000]XNorm: 0.000001
INFO:root:[cfp_fp][124000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 01:06:08,822 - [cfp_fp][124000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][124000]Accuracy-Highest: 0.61571
Training: 2023-10-18 01:06:08,822 - [cfp_fp][124000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.0471
Training: 2023-10-18 01:06:08,823 - test time: 37.0471
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][124000]XNorm: 0.000001
Training: 2023-10-18 01:06:40,746 - [agedb_30][124000]XNorm: 0.000001
INFO:root:[agedb_30][124000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 01:06:40,746 - [agedb_30][124000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][124000]Accuracy-Highest: 0.58350
Training: 2023-10-18 01:06:40,746 - [agedb_30][124000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.9234
Training: 2023-10-18 01:06:40,746 - test time: 31.9234
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 124100, eta: 83.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33678 sec, avg_samples: 64.00000, ips: 380.06889 images/sec
Training: 2023-10-18 01:07:14,430 - loss nan, lr: 0.025000, epoch: 3, step: 124100, eta: 83.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33678 sec, avg_samples: 64.00000, ips: 380.06889 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 124200, eta: 83.07 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33784 sec, avg_samples: 64.00000, ips: 378.88302 images/sec
Training: 2023-10-18 01:07:48,220 - loss nan, lr: 0.025000, epoch: 3, step: 124200, eta: 83.07 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33784 sec, avg_samples: 64.00000, ips: 378.88302 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 124300, eta: 83.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33764 sec, avg_samples: 64.00000, ips: 379.10040 images/sec
Training: 2023-10-18 01:08:21,990 - loss nan, lr: 0.025000, epoch: 3, step: 124300, eta: 83.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33764 sec, avg_samples: 64.00000, ips: 379.10040 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 124400, eta: 83.03 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.83392 images/sec
Training: 2023-10-18 01:08:55,784 - loss nan, lr: 0.025000, epoch: 3, step: 124400, eta: 83.03 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.83392 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 124500, eta: 83.01 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.84994 images/sec
Training: 2023-10-18 01:09:29,577 - loss nan, lr: 0.025000, epoch: 3, step: 124500, eta: 83.01 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.84994 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 124600, eta: 82.99 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76546 images/sec
Training: 2023-10-18 01:10:03,376 - loss nan, lr: 0.025000, epoch: 3, step: 124600, eta: 82.99 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76546 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 124700, eta: 82.97 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33787 sec, avg_samples: 64.00000, ips: 378.84136 images/sec
Training: 2023-10-18 01:10:37,169 - loss nan, lr: 0.025000, epoch: 3, step: 124700, eta: 82.97 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33787 sec, avg_samples: 64.00000, ips: 378.84136 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 124800, eta: 82.95 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.78658 images/sec
Training: 2023-10-18 01:11:10,967 - loss nan, lr: 0.025000, epoch: 3, step: 124800, eta: 82.95 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.78658 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 124900, eta: 82.93 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33784 sec, avg_samples: 64.00000, ips: 378.87455 images/sec
Training: 2023-10-18 01:11:44,757 - loss nan, lr: 0.025000, epoch: 3, step: 124900, eta: 82.93 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33784 sec, avg_samples: 64.00000, ips: 378.87455 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 125000, eta: 82.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33806 sec, avg_samples: 64.00000, ips: 378.62578 images/sec
Training: 2023-10-18 01:12:18,569 - loss nan, lr: 0.025000, epoch: 3, step: 125000, eta: 82.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33806 sec, avg_samples: 64.00000, ips: 378.62578 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 125100, eta: 82.90 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33783 sec, avg_samples: 64.00000, ips: 378.89171 images/sec
Training: 2023-10-18 01:12:52,357 - loss nan, lr: 0.025000, epoch: 3, step: 125100, eta: 82.90 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33783 sec, avg_samples: 64.00000, ips: 378.89171 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 125200, eta: 82.88 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.81880 images/sec
Training: 2023-10-18 01:13:26,152 - loss nan, lr: 0.025000, epoch: 3, step: 125200, eta: 82.88 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.81880 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 125300, eta: 82.86 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.80351 images/sec
Training: 2023-10-18 01:13:59,948 - loss nan, lr: 0.025000, epoch: 3, step: 125300, eta: 82.86 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.80351 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 125400, eta: 82.84 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.80751 images/sec
Training: 2023-10-18 01:14:33,744 - loss nan, lr: 0.025000, epoch: 3, step: 125400, eta: 82.84 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.80751 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 125500, eta: 82.82 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.71502 images/sec
Training: 2023-10-18 01:15:07,548 - loss nan, lr: 0.025000, epoch: 3, step: 125500, eta: 82.82 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.71502 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 125600, eta: 82.80 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33784 sec, avg_samples: 64.00000, ips: 378.87551 images/sec
Training: 2023-10-18 01:15:41,338 - loss nan, lr: 0.025000, epoch: 3, step: 125600, eta: 82.80 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33784 sec, avg_samples: 64.00000, ips: 378.87551 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 125700, eta: 82.78 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33806 sec, avg_samples: 64.00000, ips: 378.63455 images/sec
Training: 2023-10-18 01:16:15,149 - loss nan, lr: 0.025000, epoch: 3, step: 125700, eta: 82.78 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33806 sec, avg_samples: 64.00000, ips: 378.63455 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 125800, eta: 82.76 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33796 sec, avg_samples: 64.00000, ips: 378.74818 images/sec
Training: 2023-10-18 01:16:48,951 - loss nan, lr: 0.025000, epoch: 3, step: 125800, eta: 82.76 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33796 sec, avg_samples: 64.00000, ips: 378.74818 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 125900, eta: 82.74 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.71929 images/sec
Training: 2023-10-18 01:17:22,754 - loss nan, lr: 0.025000, epoch: 3, step: 125900, eta: 82.74 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.71929 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 126000, eta: 82.72 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33784 sec, avg_samples: 64.00000, ips: 378.87771 images/sec
Training: 2023-10-18 01:17:56,544 - loss nan, lr: 0.025000, epoch: 3, step: 126000, eta: 82.72 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33784 sec, avg_samples: 64.00000, ips: 378.87771 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][126000]XNorm: 0.000001
Training: 2023-10-18 01:18:28,491 - [lfw][126000]XNorm: 0.000001
INFO:root:[lfw][126000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 01:18:28,491 - [lfw][126000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][126000]Accuracy-Highest: 0.81183
Training: 2023-10-18 01:18:28,491 - [lfw][126000]Accuracy-Highest: 0.81183
INFO:root:test time: 31.9469
Training: 2023-10-18 01:18:28,491 - test time: 31.9469
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][126000]XNorm: 0.000001
Training: 2023-10-18 01:19:05,554 - [cfp_fp][126000]XNorm: 0.000001
INFO:root:[cfp_fp][126000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 01:19:05,554 - [cfp_fp][126000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][126000]Accuracy-Highest: 0.61571
Training: 2023-10-18 01:19:05,554 - [cfp_fp][126000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.0627
Training: 2023-10-18 01:19:05,554 - test time: 37.0627
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][126000]XNorm: 0.000001
Training: 2023-10-18 01:19:37,504 - [agedb_30][126000]XNorm: 0.000001
INFO:root:[agedb_30][126000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 01:19:37,504 - [agedb_30][126000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][126000]Accuracy-Highest: 0.58350
Training: 2023-10-18 01:19:37,504 - [agedb_30][126000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.9502
Training: 2023-10-18 01:19:37,504 - test time: 31.9502
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 126100, eta: 82.87 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33688 sec, avg_samples: 64.00000, ips: 379.96085 images/sec
Training: 2023-10-18 01:20:11,198 - loss nan, lr: 0.025000, epoch: 3, step: 126100, eta: 82.87 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33688 sec, avg_samples: 64.00000, ips: 379.96085 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 126200, eta: 82.85 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33776 sec, avg_samples: 64.00000, ips: 378.96398 images/sec
Training: 2023-10-18 01:20:44,980 - loss nan, lr: 0.025000, epoch: 3, step: 126200, eta: 82.85 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33776 sec, avg_samples: 64.00000, ips: 378.96398 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 126300, eta: 82.83 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33771 sec, avg_samples: 64.00000, ips: 379.02245 images/sec
Training: 2023-10-18 01:21:18,757 - loss nan, lr: 0.025000, epoch: 3, step: 126300, eta: 82.83 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33771 sec, avg_samples: 64.00000, ips: 379.02245 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 126400, eta: 82.81 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33775 sec, avg_samples: 64.00000, ips: 378.97668 images/sec
Training: 2023-10-18 01:21:52,538 - loss nan, lr: 0.025000, epoch: 3, step: 126400, eta: 82.81 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33775 sec, avg_samples: 64.00000, ips: 378.97668 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 126500, eta: 82.80 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33780 sec, avg_samples: 64.00000, ips: 378.92598 images/sec
Training: 2023-10-18 01:22:26,324 - loss nan, lr: 0.025000, epoch: 3, step: 126500, eta: 82.80 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33780 sec, avg_samples: 64.00000, ips: 378.92598 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 126600, eta: 82.78 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33778 sec, avg_samples: 64.00000, ips: 378.94760 images/sec
Training: 2023-10-18 01:23:00,108 - loss nan, lr: 0.025000, epoch: 3, step: 126600, eta: 82.78 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33778 sec, avg_samples: 64.00000, ips: 378.94760 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 126700, eta: 82.76 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.79056 images/sec
Training: 2023-10-18 01:23:33,906 - loss nan, lr: 0.025000, epoch: 3, step: 126700, eta: 82.76 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.79056 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 126800, eta: 82.74 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33778 sec, avg_samples: 64.00000, ips: 378.94015 images/sec
Training: 2023-10-18 01:24:07,690 - loss nan, lr: 0.025000, epoch: 3, step: 126800, eta: 82.74 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33778 sec, avg_samples: 64.00000, ips: 378.94015 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 126900, eta: 82.72 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33779 sec, avg_samples: 64.00000, ips: 378.93918 images/sec
Training: 2023-10-18 01:24:41,474 - loss nan, lr: 0.025000, epoch: 3, step: 126900, eta: 82.72 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33779 sec, avg_samples: 64.00000, ips: 378.93918 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 127000, eta: 82.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.81499 images/sec
Training: 2023-10-18 01:25:15,270 - loss nan, lr: 0.025000, epoch: 3, step: 127000, eta: 82.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.81499 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 127100, eta: 82.68 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33777 sec, avg_samples: 64.00000, ips: 378.95375 images/sec
Training: 2023-10-18 01:25:49,053 - loss nan, lr: 0.025000, epoch: 3, step: 127100, eta: 82.68 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33777 sec, avg_samples: 64.00000, ips: 378.95375 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 127200, eta: 82.66 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33783 sec, avg_samples: 64.00000, ips: 378.88938 images/sec
Training: 2023-10-18 01:26:22,842 - loss nan, lr: 0.025000, epoch: 3, step: 127200, eta: 82.66 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33783 sec, avg_samples: 64.00000, ips: 378.88938 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 127300, eta: 82.64 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33771 sec, avg_samples: 64.00000, ips: 379.02492 images/sec
Training: 2023-10-18 01:26:56,619 - loss nan, lr: 0.025000, epoch: 3, step: 127300, eta: 82.64 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33771 sec, avg_samples: 64.00000, ips: 379.02492 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 127400, eta: 82.62 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33774 sec, avg_samples: 64.00000, ips: 378.99211 images/sec
Training: 2023-10-18 01:27:30,398 - loss nan, lr: 0.025000, epoch: 3, step: 127400, eta: 82.62 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33774 sec, avg_samples: 64.00000, ips: 378.99211 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 127500, eta: 82.60 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.79356 images/sec
Training: 2023-10-18 01:28:04,196 - loss nan, lr: 0.025000, epoch: 3, step: 127500, eta: 82.60 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.79356 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 127600, eta: 82.58 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33779 sec, avg_samples: 64.00000, ips: 378.93310 images/sec
Training: 2023-10-18 01:28:37,981 - loss nan, lr: 0.025000, epoch: 3, step: 127600, eta: 82.58 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33779 sec, avg_samples: 64.00000, ips: 378.93310 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 127700, eta: 82.56 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.83021 images/sec
Training: 2023-10-18 01:29:11,775 - loss nan, lr: 0.025000, epoch: 3, step: 127700, eta: 82.56 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.83021 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 127800, eta: 82.54 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.83763 images/sec
Training: 2023-10-18 01:29:45,569 - loss nan, lr: 0.025000, epoch: 3, step: 127800, eta: 82.54 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.83763 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 127900, eta: 82.53 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33774 sec, avg_samples: 64.00000, ips: 378.99405 images/sec
Training: 2023-10-18 01:30:19,348 - loss nan, lr: 0.025000, epoch: 3, step: 127900, eta: 82.53 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33774 sec, avg_samples: 64.00000, ips: 378.99405 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 128000, eta: 82.51 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.85915 images/sec
Training: 2023-10-18 01:30:53,140 - loss nan, lr: 0.025000, epoch: 3, step: 128000, eta: 82.51 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33786 sec, avg_samples: 64.00000, ips: 378.85915 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][128000]XNorm: 0.000001
Training: 2023-10-18 01:31:25,047 - [lfw][128000]XNorm: 0.000001
INFO:root:[lfw][128000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 01:31:25,047 - [lfw][128000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][128000]Accuracy-Highest: 0.81183
Training: 2023-10-18 01:31:25,047 - [lfw][128000]Accuracy-Highest: 0.81183
INFO:root:test time: 31.9070
Training: 2023-10-18 01:31:25,047 - test time: 31.9070
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][128000]XNorm: 0.000001
Training: 2023-10-18 01:32:02,075 - [cfp_fp][128000]XNorm: 0.000001
INFO:root:[cfp_fp][128000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 01:32:02,075 - [cfp_fp][128000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][128000]Accuracy-Highest: 0.61571
Training: 2023-10-18 01:32:02,075 - [cfp_fp][128000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.0280
Training: 2023-10-18 01:32:02,075 - test time: 37.0280
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][128000]XNorm: 0.000001
Training: 2023-10-18 01:32:33,975 - [agedb_30][128000]XNorm: 0.000001
INFO:root:[agedb_30][128000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 01:32:33,975 - [agedb_30][128000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][128000]Accuracy-Highest: 0.58350
Training: 2023-10-18 01:32:33,975 - [agedb_30][128000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.9000
Training: 2023-10-18 01:32:33,975 - test time: 31.9000
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 128100, eta: 82.65 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33691 sec, avg_samples: 64.00000, ips: 379.92002 images/sec
Training: 2023-10-18 01:33:07,672 - loss nan, lr: 0.025000, epoch: 3, step: 128100, eta: 82.65 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33691 sec, avg_samples: 64.00000, ips: 379.92002 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 128200, eta: 82.64 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33773 sec, avg_samples: 64.00000, ips: 379.00183 images/sec
Training: 2023-10-18 01:33:41,451 - loss nan, lr: 0.025000, epoch: 3, step: 128200, eta: 82.64 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33773 sec, avg_samples: 64.00000, ips: 379.00183 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 128300, eta: 82.62 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33775 sec, avg_samples: 64.00000, ips: 378.98126 images/sec
Training: 2023-10-18 01:34:15,232 - loss nan, lr: 0.025000, epoch: 3, step: 128300, eta: 82.62 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33775 sec, avg_samples: 64.00000, ips: 378.98126 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 128400, eta: 82.60 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33771 sec, avg_samples: 64.00000, ips: 379.02028 images/sec
Training: 2023-10-18 01:34:49,009 - loss nan, lr: 0.025000, epoch: 3, step: 128400, eta: 82.60 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33771 sec, avg_samples: 64.00000, ips: 379.02028 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 128500, eta: 82.58 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.81645 images/sec
Training: 2023-10-18 01:35:22,805 - loss nan, lr: 0.025000, epoch: 3, step: 128500, eta: 82.58 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.81645 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 128600, eta: 82.56 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.80837 images/sec
Training: 2023-10-18 01:35:56,601 - loss nan, lr: 0.025000, epoch: 3, step: 128600, eta: 82.56 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33790 sec, avg_samples: 64.00000, ips: 378.80837 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 128700, eta: 82.54 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33779 sec, avg_samples: 64.00000, ips: 378.92886 images/sec
Training: 2023-10-18 01:36:30,386 - loss nan, lr: 0.025000, epoch: 3, step: 128700, eta: 82.54 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33779 sec, avg_samples: 64.00000, ips: 378.92886 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 128800, eta: 82.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.87030 images/sec
Training: 2023-10-18 01:37:04,176 - loss nan, lr: 0.025000, epoch: 3, step: 128800, eta: 82.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.87030 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 128900, eta: 82.50 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.78730 images/sec
Training: 2023-10-18 01:37:37,973 - loss nan, lr: 0.025000, epoch: 3, step: 128900, eta: 82.50 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.78730 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 129000, eta: 82.48 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.78692 images/sec
Training: 2023-10-18 01:38:11,771 - loss nan, lr: 0.025000, epoch: 3, step: 129000, eta: 82.48 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.78692 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 129100, eta: 82.46 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33784 sec, avg_samples: 64.00000, ips: 378.87350 images/sec
Training: 2023-10-18 01:38:45,560 - loss nan, lr: 0.025000, epoch: 3, step: 129100, eta: 82.46 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33784 sec, avg_samples: 64.00000, ips: 378.87350 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 129200, eta: 82.44 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.71266 images/sec
Training: 2023-10-18 01:39:19,364 - loss nan, lr: 0.025000, epoch: 3, step: 129200, eta: 82.44 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.71266 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 129300, eta: 82.42 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.79698 images/sec
Training: 2023-10-18 01:39:53,160 - loss nan, lr: 0.025000, epoch: 3, step: 129300, eta: 82.42 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33791 sec, avg_samples: 64.00000, ips: 378.79698 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 129400, eta: 82.41 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76874 images/sec
Training: 2023-10-18 01:40:26,959 - loss nan, lr: 0.025000, epoch: 3, step: 129400, eta: 82.41 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33794 sec, avg_samples: 64.00000, ips: 378.76874 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 129500, eta: 82.39 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.83381 images/sec
Training: 2023-10-18 01:41:00,752 - loss nan, lr: 0.025000, epoch: 3, step: 129500, eta: 82.39 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.83381 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 129600, eta: 82.37 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.79224 images/sec
Training: 2023-10-18 01:41:34,549 - loss nan, lr: 0.025000, epoch: 3, step: 129600, eta: 82.37 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33792 sec, avg_samples: 64.00000, ips: 378.79224 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 129700, eta: 82.35 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33777 sec, avg_samples: 64.00000, ips: 378.96162 images/sec
Training: 2023-10-18 01:42:08,331 - loss nan, lr: 0.025000, epoch: 3, step: 129700, eta: 82.35 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33777 sec, avg_samples: 64.00000, ips: 378.96162 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 129800, eta: 82.33 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.81608 images/sec
Training: 2023-10-18 01:42:42,125 - loss nan, lr: 0.025000, epoch: 3, step: 129800, eta: 82.33 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33789 sec, avg_samples: 64.00000, ips: 378.81608 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 129900, eta: 82.31 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.82914 images/sec
Training: 2023-10-18 01:43:15,919 - loss nan, lr: 0.025000, epoch: 3, step: 129900, eta: 82.31 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33788 sec, avg_samples: 64.00000, ips: 378.82914 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 130000, eta: 82.29 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.86682 images/sec
Training: 2023-10-18 01:43:49,709 - loss nan, lr: 0.025000, epoch: 3, step: 130000, eta: 82.29 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33785 sec, avg_samples: 64.00000, ips: 378.86682 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][130000]XNorm: 0.000001
Training: 2023-10-18 01:44:21,637 - [lfw][130000]XNorm: 0.000001
INFO:root:[lfw][130000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 01:44:21,637 - [lfw][130000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][130000]Accuracy-Highest: 0.81183
Training: 2023-10-18 01:44:21,637 - [lfw][130000]Accuracy-Highest: 0.81183
INFO:root:test time: 31.9276
Training: 2023-10-18 01:44:21,637 - test time: 31.9276
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][130000]XNorm: 0.000001
Training: 2023-10-18 01:44:58,681 - [cfp_fp][130000]XNorm: 0.000001
INFO:root:[cfp_fp][130000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 01:44:58,681 - [cfp_fp][130000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][130000]Accuracy-Highest: 0.61571
Training: 2023-10-18 01:44:58,681 - [cfp_fp][130000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.0439
Training: 2023-10-18 01:44:58,681 - test time: 37.0439
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][130000]XNorm: 0.000001
Training: 2023-10-18 01:45:30,590 - [agedb_30][130000]XNorm: 0.000001
INFO:root:[agedb_30][130000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 01:45:30,590 - [agedb_30][130000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][130000]Accuracy-Highest: 0.58350
Training: 2023-10-18 01:45:30,590 - [agedb_30][130000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.9094
Training: 2023-10-18 01:45:30,590 - test time: 31.9094
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 130100, eta: 82.44 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33664 sec, avg_samples: 64.00000, ips: 380.23201 images/sec
Training: 2023-10-18 01:46:04,259 - loss nan, lr: 0.025000, epoch: 3, step: 130100, eta: 82.44 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33664 sec, avg_samples: 64.00000, ips: 380.23201 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 130200, eta: 82.42 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33731 sec, avg_samples: 64.00000, ips: 379.47013 images/sec
Training: 2023-10-18 01:46:37,996 - loss nan, lr: 0.025000, epoch: 3, step: 130200, eta: 82.42 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33731 sec, avg_samples: 64.00000, ips: 379.47013 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 130300, eta: 82.40 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33737 sec, avg_samples: 64.00000, ips: 379.40877 images/sec
Training: 2023-10-18 01:47:11,738 - loss nan, lr: 0.025000, epoch: 3, step: 130300, eta: 82.40 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33737 sec, avg_samples: 64.00000, ips: 379.40877 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 130400, eta: 82.38 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33734 sec, avg_samples: 64.00000, ips: 379.44261 images/sec
Training: 2023-10-18 01:47:45,477 - loss nan, lr: 0.025000, epoch: 3, step: 130400, eta: 82.38 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33734 sec, avg_samples: 64.00000, ips: 379.44261 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 130500, eta: 82.36 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33732 sec, avg_samples: 64.00000, ips: 379.46626 images/sec
Training: 2023-10-18 01:48:19,214 - loss nan, lr: 0.025000, epoch: 3, step: 130500, eta: 82.36 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33732 sec, avg_samples: 64.00000, ips: 379.46626 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 130600, eta: 82.34 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33748 sec, avg_samples: 64.00000, ips: 379.28325 images/sec
Training: 2023-10-18 01:48:52,968 - loss nan, lr: 0.025000, epoch: 3, step: 130600, eta: 82.34 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33748 sec, avg_samples: 64.00000, ips: 379.28325 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 130700, eta: 82.32 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33733 sec, avg_samples: 64.00000, ips: 379.44603 images/sec
Training: 2023-10-18 01:49:26,706 - loss nan, lr: 0.025000, epoch: 3, step: 130700, eta: 82.32 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33733 sec, avg_samples: 64.00000, ips: 379.44603 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 130800, eta: 82.30 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33730 sec, avg_samples: 64.00000, ips: 379.48601 images/sec
Training: 2023-10-18 01:50:00,442 - loss nan, lr: 0.025000, epoch: 3, step: 130800, eta: 82.30 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33730 sec, avg_samples: 64.00000, ips: 379.48601 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 130900, eta: 82.28 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33734 sec, avg_samples: 64.00000, ips: 379.44129 images/sec
Training: 2023-10-18 01:50:34,181 - loss nan, lr: 0.025000, epoch: 3, step: 130900, eta: 82.28 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33734 sec, avg_samples: 64.00000, ips: 379.44129 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 131000, eta: 82.27 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33728 sec, avg_samples: 64.00000, ips: 379.50820 images/sec
Training: 2023-10-18 01:51:07,914 - loss nan, lr: 0.025000, epoch: 3, step: 131000, eta: 82.27 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33728 sec, avg_samples: 64.00000, ips: 379.50820 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 131100, eta: 82.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33724 sec, avg_samples: 64.00000, ips: 379.55650 images/sec
Training: 2023-10-18 01:51:41,643 - loss nan, lr: 0.025000, epoch: 3, step: 131100, eta: 82.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33724 sec, avg_samples: 64.00000, ips: 379.55650 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 131200, eta: 82.23 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33746 sec, avg_samples: 64.00000, ips: 379.30055 images/sec
Training: 2023-10-18 01:52:15,395 - loss nan, lr: 0.025000, epoch: 3, step: 131200, eta: 82.23 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33746 sec, avg_samples: 64.00000, ips: 379.30055 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 131300, eta: 82.21 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33740 sec, avg_samples: 64.00000, ips: 379.37482 images/sec
Training: 2023-10-18 01:52:49,141 - loss nan, lr: 0.025000, epoch: 3, step: 131300, eta: 82.21 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33740 sec, avg_samples: 64.00000, ips: 379.37482 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 131400, eta: 82.19 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33743 sec, avg_samples: 64.00000, ips: 379.34069 images/sec
Training: 2023-10-18 01:53:22,889 - loss nan, lr: 0.025000, epoch: 3, step: 131400, eta: 82.19 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33743 sec, avg_samples: 64.00000, ips: 379.34069 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 131500, eta: 82.17 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33736 sec, avg_samples: 64.00000, ips: 379.41977 images/sec
Training: 2023-10-18 01:53:56,630 - loss nan, lr: 0.025000, epoch: 3, step: 131500, eta: 82.17 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33736 sec, avg_samples: 64.00000, ips: 379.41977 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 131600, eta: 82.15 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33737 sec, avg_samples: 64.00000, ips: 379.40649 images/sec
Training: 2023-10-18 01:54:30,373 - loss nan, lr: 0.025000, epoch: 3, step: 131600, eta: 82.15 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33737 sec, avg_samples: 64.00000, ips: 379.40649 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 131700, eta: 82.13 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33735 sec, avg_samples: 64.00000, ips: 379.42237 images/sec
Training: 2023-10-18 01:55:04,114 - loss nan, lr: 0.025000, epoch: 3, step: 131700, eta: 82.13 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33735 sec, avg_samples: 64.00000, ips: 379.42237 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 131800, eta: 82.11 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33732 sec, avg_samples: 64.00000, ips: 379.46009 images/sec
Training: 2023-10-18 01:55:37,851 - loss nan, lr: 0.025000, epoch: 3, step: 131800, eta: 82.11 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33732 sec, avg_samples: 64.00000, ips: 379.46009 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 131900, eta: 82.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33735 sec, avg_samples: 64.00000, ips: 379.42229 images/sec
Training: 2023-10-18 01:56:11,592 - loss nan, lr: 0.025000, epoch: 3, step: 131900, eta: 82.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33735 sec, avg_samples: 64.00000, ips: 379.42229 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 132000, eta: 82.08 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33751 sec, avg_samples: 64.00000, ips: 379.24866 images/sec
Training: 2023-10-18 01:56:45,349 - loss nan, lr: 0.025000, epoch: 3, step: 132000, eta: 82.08 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33751 sec, avg_samples: 64.00000, ips: 379.24866 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][132000]XNorm: 0.000000
Training: 2023-10-18 01:57:17,276 - [lfw][132000]XNorm: 0.000000
INFO:root:[lfw][132000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 01:57:17,276 - [lfw][132000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][132000]Accuracy-Highest: 0.81183
Training: 2023-10-18 01:57:17,276 - [lfw][132000]Accuracy-Highest: 0.81183
INFO:root:test time: 31.9267
Training: 2023-10-18 01:57:17,276 - test time: 31.9267
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][132000]XNorm: 0.000000
Training: 2023-10-18 01:57:54,307 - [cfp_fp][132000]XNorm: 0.000000
INFO:root:[cfp_fp][132000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 01:57:54,307 - [cfp_fp][132000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][132000]Accuracy-Highest: 0.61571
Training: 2023-10-18 01:57:54,307 - [cfp_fp][132000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.0317
Training: 2023-10-18 01:57:54,308 - test time: 37.0317
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][132000]XNorm: 0.000000
Training: 2023-10-18 01:58:26,216 - [agedb_30][132000]XNorm: 0.000000
INFO:root:[agedb_30][132000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 01:58:26,216 - [agedb_30][132000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][132000]Accuracy-Highest: 0.58350
Training: 2023-10-18 01:58:26,216 - [agedb_30][132000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.9084
Training: 2023-10-18 01:58:26,216 - test time: 31.9084
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 132100, eta: 82.22 hours, avg_reader_cost: 0.00003 sec, avg_batch_cost: 0.33616 sec, avg_samples: 64.00000, ips: 380.76978 images/sec
Training: 2023-10-18 01:58:59,841 - loss nan, lr: 0.025000, epoch: 3, step: 132100, eta: 82.22 hours, avg_reader_cost: 0.00003 sec, avg_batch_cost: 0.33616 sec, avg_samples: 64.00000, ips: 380.76978 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 132200, eta: 82.20 hours, avg_reader_cost: 0.00003 sec, avg_batch_cost: 0.33645 sec, avg_samples: 64.00000, ips: 380.43739 images/sec
Training: 2023-10-18 01:59:33,494 - loss nan, lr: 0.025000, epoch: 3, step: 132200, eta: 82.20 hours, avg_reader_cost: 0.00003 sec, avg_batch_cost: 0.33645 sec, avg_samples: 64.00000, ips: 380.43739 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 132300, eta: 82.18 hours, avg_reader_cost: 0.00003 sec, avg_batch_cost: 0.33655 sec, avg_samples: 64.00000, ips: 380.33310 images/sec
Training: 2023-10-18 02:00:07,158 - loss nan, lr: 0.025000, epoch: 3, step: 132300, eta: 82.18 hours, avg_reader_cost: 0.00003 sec, avg_batch_cost: 0.33655 sec, avg_samples: 64.00000, ips: 380.33310 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 132400, eta: 82.16 hours, avg_reader_cost: 0.00003 sec, avg_batch_cost: 0.33676 sec, avg_samples: 64.00000, ips: 380.08825 images/sec
Training: 2023-10-18 02:00:40,842 - loss nan, lr: 0.025000, epoch: 3, step: 132400, eta: 82.16 hours, avg_reader_cost: 0.00003 sec, avg_batch_cost: 0.33676 sec, avg_samples: 64.00000, ips: 380.08825 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 132500, eta: 82.14 hours, avg_reader_cost: 0.00003 sec, avg_batch_cost: 0.33664 sec, avg_samples: 64.00000, ips: 380.22897 images/sec
Training: 2023-10-18 02:01:14,515 - loss nan, lr: 0.025000, epoch: 3, step: 132500, eta: 82.14 hours, avg_reader_cost: 0.00003 sec, avg_batch_cost: 0.33664 sec, avg_samples: 64.00000, ips: 380.22897 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 132600, eta: 82.12 hours, avg_reader_cost: 0.00003 sec, avg_batch_cost: 0.33664 sec, avg_samples: 64.00000, ips: 380.22707 images/sec
Training: 2023-10-18 02:01:48,187 - loss nan, lr: 0.025000, epoch: 3, step: 132600, eta: 82.12 hours, avg_reader_cost: 0.00003 sec, avg_batch_cost: 0.33664 sec, avg_samples: 64.00000, ips: 380.22707 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 132700, eta: 82.10 hours, avg_reader_cost: 0.00003 sec, avg_batch_cost: 0.33675 sec, avg_samples: 64.00000, ips: 380.10142 images/sec
Training: 2023-10-18 02:02:21,871 - loss nan, lr: 0.025000, epoch: 3, step: 132700, eta: 82.10 hours, avg_reader_cost: 0.00003 sec, avg_batch_cost: 0.33675 sec, avg_samples: 64.00000, ips: 380.10142 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 132800, eta: 82.08 hours, avg_reader_cost: 0.00003 sec, avg_batch_cost: 0.33660 sec, avg_samples: 64.00000, ips: 380.27521 images/sec
Training: 2023-10-18 02:02:55,539 - loss nan, lr: 0.025000, epoch: 3, step: 132800, eta: 82.08 hours, avg_reader_cost: 0.00003 sec, avg_batch_cost: 0.33660 sec, avg_samples: 64.00000, ips: 380.27521 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 132900, eta: 82.07 hours, avg_reader_cost: 0.00003 sec, avg_batch_cost: 0.33675 sec, avg_samples: 64.00000, ips: 380.09866 images/sec
Training: 2023-10-18 02:03:29,223 - loss nan, lr: 0.025000, epoch: 3, step: 132900, eta: 82.07 hours, avg_reader_cost: 0.00003 sec, avg_batch_cost: 0.33675 sec, avg_samples: 64.00000, ips: 380.09866 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 133000, eta: 82.05 hours, avg_reader_cost: 0.00003 sec, avg_batch_cost: 0.33675 sec, avg_samples: 64.00000, ips: 380.10512 images/sec
Training: 2023-10-18 02:04:02,906 - loss nan, lr: 0.025000, epoch: 3, step: 133000, eta: 82.05 hours, avg_reader_cost: 0.00003 sec, avg_batch_cost: 0.33675 sec, avg_samples: 64.00000, ips: 380.10512 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 133100, eta: 82.03 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33735 sec, avg_samples: 64.00000, ips: 379.42793 images/sec
Training: 2023-10-18 02:04:36,647 - loss nan, lr: 0.025000, epoch: 3, step: 133100, eta: 82.03 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33735 sec, avg_samples: 64.00000, ips: 379.42793 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 133200, eta: 82.01 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33762 sec, avg_samples: 64.00000, ips: 379.11945 images/sec
Training: 2023-10-18 02:05:10,415 - loss nan, lr: 0.025000, epoch: 3, step: 133200, eta: 82.01 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33762 sec, avg_samples: 64.00000, ips: 379.11945 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 133300, eta: 81.99 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33746 sec, avg_samples: 64.00000, ips: 379.30472 images/sec
Training: 2023-10-18 02:05:44,167 - loss nan, lr: 0.025000, epoch: 3, step: 133300, eta: 81.99 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33746 sec, avg_samples: 64.00000, ips: 379.30472 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 133400, eta: 81.97 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33741 sec, avg_samples: 64.00000, ips: 379.35603 images/sec
Training: 2023-10-18 02:06:17,914 - loss nan, lr: 0.025000, epoch: 3, step: 133400, eta: 81.97 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33741 sec, avg_samples: 64.00000, ips: 379.35603 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 133500, eta: 81.95 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33747 sec, avg_samples: 64.00000, ips: 379.29106 images/sec
Training: 2023-10-18 02:06:51,666 - loss nan, lr: 0.025000, epoch: 3, step: 133500, eta: 81.95 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33747 sec, avg_samples: 64.00000, ips: 379.29106 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 133600, eta: 81.93 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33740 sec, avg_samples: 64.00000, ips: 379.36772 images/sec
Training: 2023-10-18 02:07:25,412 - loss nan, lr: 0.025000, epoch: 3, step: 133600, eta: 81.93 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33740 sec, avg_samples: 64.00000, ips: 379.36772 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 133700, eta: 81.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33725 sec, avg_samples: 64.00000, ips: 379.54470 images/sec
Training: 2023-10-18 02:07:59,143 - loss nan, lr: 0.025000, epoch: 3, step: 133700, eta: 81.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33725 sec, avg_samples: 64.00000, ips: 379.54470 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 133800, eta: 81.90 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33747 sec, avg_samples: 64.00000, ips: 379.29610 images/sec
Training: 2023-10-18 02:08:32,895 - loss nan, lr: 0.025000, epoch: 3, step: 133800, eta: 81.90 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33747 sec, avg_samples: 64.00000, ips: 379.29610 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 133900, eta: 81.88 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33740 sec, avg_samples: 64.00000, ips: 379.36893 images/sec
Training: 2023-10-18 02:09:06,641 - loss nan, lr: 0.025000, epoch: 3, step: 133900, eta: 81.88 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33740 sec, avg_samples: 64.00000, ips: 379.36893 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 134000, eta: 81.86 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33731 sec, avg_samples: 64.00000, ips: 379.46726 images/sec
Training: 2023-10-18 02:09:40,378 - loss nan, lr: 0.025000, epoch: 3, step: 134000, eta: 81.86 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33731 sec, avg_samples: 64.00000, ips: 379.46726 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][134000]XNorm: 0.000000
Training: 2023-10-18 02:10:12,271 - [lfw][134000]XNorm: 0.000000
INFO:root:[lfw][134000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 02:10:12,271 - [lfw][134000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][134000]Accuracy-Highest: 0.81183
Training: 2023-10-18 02:10:12,271 - [lfw][134000]Accuracy-Highest: 0.81183
INFO:root:test time: 31.8924
Training: 2023-10-18 02:10:12,271 - test time: 31.8924
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][134000]XNorm: 0.000000
Training: 2023-10-18 02:10:49,307 - [cfp_fp][134000]XNorm: 0.000000
INFO:root:[cfp_fp][134000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 02:10:49,307 - [cfp_fp][134000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][134000]Accuracy-Highest: 0.61571
Training: 2023-10-18 02:10:49,307 - [cfp_fp][134000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.0361
Training: 2023-10-18 02:10:49,307 - test time: 37.0361
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][134000]XNorm: 0.000000
Training: 2023-10-18 02:11:21,218 - [agedb_30][134000]XNorm: 0.000000
INFO:root:[agedb_30][134000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 02:11:21,219 - [agedb_30][134000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][134000]Accuracy-Highest: 0.58350
Training: 2023-10-18 02:11:21,219 - [agedb_30][134000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.9116
Training: 2023-10-18 02:11:21,219 - test time: 31.9116
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 134100, eta: 82.00 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33680 sec, avg_samples: 64.00000, ips: 380.04360 images/sec
Training: 2023-10-18 02:11:54,905 - loss nan, lr: 0.025000, epoch: 3, step: 134100, eta: 82.00 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33680 sec, avg_samples: 64.00000, ips: 380.04360 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 134200, eta: 81.98 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33728 sec, avg_samples: 64.00000, ips: 379.50124 images/sec
Training: 2023-10-18 02:12:28,639 - loss nan, lr: 0.025000, epoch: 3, step: 134200, eta: 81.98 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33728 sec, avg_samples: 64.00000, ips: 379.50124 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 134300, eta: 81.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33731 sec, avg_samples: 64.00000, ips: 379.47352 images/sec
Training: 2023-10-18 02:13:02,376 - loss nan, lr: 0.025000, epoch: 3, step: 134300, eta: 81.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33731 sec, avg_samples: 64.00000, ips: 379.47352 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 134400, eta: 81.94 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33739 sec, avg_samples: 64.00000, ips: 379.37964 images/sec
Training: 2023-10-18 02:13:36,122 - loss nan, lr: 0.025000, epoch: 3, step: 134400, eta: 81.94 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33739 sec, avg_samples: 64.00000, ips: 379.37964 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 134500, eta: 81.92 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33741 sec, avg_samples: 64.00000, ips: 379.36035 images/sec
Training: 2023-10-18 02:14:09,869 - loss nan, lr: 0.025000, epoch: 3, step: 134500, eta: 81.92 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33741 sec, avg_samples: 64.00000, ips: 379.36035 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 134600, eta: 81.90 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33717 sec, avg_samples: 64.00000, ips: 379.62778 images/sec
Training: 2023-10-18 02:14:43,592 - loss nan, lr: 0.025000, epoch: 3, step: 134600, eta: 81.90 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33717 sec, avg_samples: 64.00000, ips: 379.62778 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 134700, eta: 81.88 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33737 sec, avg_samples: 64.00000, ips: 379.40394 images/sec
Training: 2023-10-18 02:15:17,335 - loss nan, lr: 0.025000, epoch: 3, step: 134700, eta: 81.88 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33737 sec, avg_samples: 64.00000, ips: 379.40394 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 134800, eta: 81.87 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33714 sec, avg_samples: 64.00000, ips: 379.66476 images/sec
Training: 2023-10-18 02:15:51,055 - loss nan, lr: 0.025000, epoch: 3, step: 134800, eta: 81.87 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33714 sec, avg_samples: 64.00000, ips: 379.66476 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 134900, eta: 81.85 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33728 sec, avg_samples: 64.00000, ips: 379.50967 images/sec
Training: 2023-10-18 02:16:24,789 - loss nan, lr: 0.025000, epoch: 3, step: 134900, eta: 81.85 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33728 sec, avg_samples: 64.00000, ips: 379.50967 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 135000, eta: 81.83 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33718 sec, avg_samples: 64.00000, ips: 379.62461 images/sec
Training: 2023-10-18 02:16:58,512 - loss nan, lr: 0.025000, epoch: 3, step: 135000, eta: 81.83 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33718 sec, avg_samples: 64.00000, ips: 379.62461 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 135100, eta: 81.81 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33742 sec, avg_samples: 64.00000, ips: 379.34719 images/sec
Training: 2023-10-18 02:17:32,260 - loss nan, lr: 0.025000, epoch: 3, step: 135100, eta: 81.81 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33742 sec, avg_samples: 64.00000, ips: 379.34719 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 135200, eta: 81.79 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33741 sec, avg_samples: 64.00000, ips: 379.36340 images/sec
Training: 2023-10-18 02:18:06,007 - loss nan, lr: 0.025000, epoch: 3, step: 135200, eta: 81.79 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33741 sec, avg_samples: 64.00000, ips: 379.36340 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 135300, eta: 81.77 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33734 sec, avg_samples: 64.00000, ips: 379.44265 images/sec
Training: 2023-10-18 02:18:39,747 - loss nan, lr: 0.025000, epoch: 3, step: 135300, eta: 81.77 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33734 sec, avg_samples: 64.00000, ips: 379.44265 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 135400, eta: 81.75 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33729 sec, avg_samples: 64.00000, ips: 379.49042 images/sec
Training: 2023-10-18 02:19:13,482 - loss nan, lr: 0.025000, epoch: 3, step: 135400, eta: 81.75 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33729 sec, avg_samples: 64.00000, ips: 379.49042 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 135500, eta: 81.73 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33729 sec, avg_samples: 64.00000, ips: 379.50079 images/sec
Training: 2023-10-18 02:19:47,217 - loss nan, lr: 0.025000, epoch: 3, step: 135500, eta: 81.73 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33729 sec, avg_samples: 64.00000, ips: 379.50079 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 135600, eta: 81.72 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33732 sec, avg_samples: 64.00000, ips: 379.45774 images/sec
Training: 2023-10-18 02:20:20,955 - loss nan, lr: 0.025000, epoch: 3, step: 135600, eta: 81.72 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33732 sec, avg_samples: 64.00000, ips: 379.45774 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 135700, eta: 81.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33728 sec, avg_samples: 64.00000, ips: 379.50584 images/sec
Training: 2023-10-18 02:20:54,689 - loss nan, lr: 0.025000, epoch: 3, step: 135700, eta: 81.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33728 sec, avg_samples: 64.00000, ips: 379.50584 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 135800, eta: 81.68 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33734 sec, avg_samples: 64.00000, ips: 379.43861 images/sec
Training: 2023-10-18 02:21:28,429 - loss nan, lr: 0.025000, epoch: 3, step: 135800, eta: 81.68 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33734 sec, avg_samples: 64.00000, ips: 379.43861 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 135900, eta: 81.66 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33723 sec, avg_samples: 64.00000, ips: 379.56116 images/sec
Training: 2023-10-18 02:22:02,158 - loss nan, lr: 0.025000, epoch: 3, step: 135900, eta: 81.66 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33723 sec, avg_samples: 64.00000, ips: 379.56116 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 136000, eta: 81.64 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33733 sec, avg_samples: 64.00000, ips: 379.45088 images/sec
Training: 2023-10-18 02:22:35,897 - loss nan, lr: 0.025000, epoch: 3, step: 136000, eta: 81.64 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33733 sec, avg_samples: 64.00000, ips: 379.45088 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][136000]XNorm: 0.000000
Training: 2023-10-18 02:23:07,768 - [lfw][136000]XNorm: 0.000000
INFO:root:[lfw][136000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 02:23:07,768 - [lfw][136000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][136000]Accuracy-Highest: 0.81183
Training: 2023-10-18 02:23:07,768 - [lfw][136000]Accuracy-Highest: 0.81183
INFO:root:test time: 31.8710
Training: 2023-10-18 02:23:07,768 - test time: 31.8710
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][136000]XNorm: 0.000000
Training: 2023-10-18 02:23:44,801 - [cfp_fp][136000]XNorm: 0.000000
INFO:root:[cfp_fp][136000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 02:23:44,801 - [cfp_fp][136000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][136000]Accuracy-Highest: 0.61571
Training: 2023-10-18 02:23:44,801 - [cfp_fp][136000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.0330
Training: 2023-10-18 02:23:44,801 - test time: 37.0330
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][136000]XNorm: 0.000000
Training: 2023-10-18 02:24:16,657 - [agedb_30][136000]XNorm: 0.000000
INFO:root:[agedb_30][136000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 02:24:16,658 - [agedb_30][136000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][136000]Accuracy-Highest: 0.58350
Training: 2023-10-18 02:24:16,658 - [agedb_30][136000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.8564
Training: 2023-10-18 02:24:16,658 - test time: 31.8564
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 136100, eta: 81.78 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33683 sec, avg_samples: 64.00000, ips: 380.01694 images/sec
Training: 2023-10-18 02:24:50,346 - loss nan, lr: 0.025000, epoch: 3, step: 136100, eta: 81.78 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33683 sec, avg_samples: 64.00000, ips: 380.01694 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 136200, eta: 81.76 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33733 sec, avg_samples: 64.00000, ips: 379.45067 images/sec
Training: 2023-10-18 02:25:24,085 - loss nan, lr: 0.025000, epoch: 3, step: 136200, eta: 81.76 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33733 sec, avg_samples: 64.00000, ips: 379.45067 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 136300, eta: 81.74 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33731 sec, avg_samples: 64.00000, ips: 379.47440 images/sec
Training: 2023-10-18 02:25:57,822 - loss nan, lr: 0.025000, epoch: 3, step: 136300, eta: 81.74 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33731 sec, avg_samples: 64.00000, ips: 379.47440 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 136400, eta: 81.72 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33729 sec, avg_samples: 64.00000, ips: 379.49811 images/sec
Training: 2023-10-18 02:26:31,557 - loss nan, lr: 0.025000, epoch: 3, step: 136400, eta: 81.72 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33729 sec, avg_samples: 64.00000, ips: 379.49811 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 136500, eta: 81.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33738 sec, avg_samples: 64.00000, ips: 379.39383 images/sec
Training: 2023-10-18 02:27:05,301 - loss nan, lr: 0.025000, epoch: 3, step: 136500, eta: 81.70 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33738 sec, avg_samples: 64.00000, ips: 379.39383 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 136600, eta: 81.69 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33729 sec, avg_samples: 64.00000, ips: 379.49454 images/sec
Training: 2023-10-18 02:27:39,035 - loss nan, lr: 0.025000, epoch: 3, step: 136600, eta: 81.69 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33729 sec, avg_samples: 64.00000, ips: 379.49454 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 136700, eta: 81.67 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33747 sec, avg_samples: 64.00000, ips: 379.29845 images/sec
Training: 2023-10-18 02:28:12,787 - loss nan, lr: 0.025000, epoch: 3, step: 136700, eta: 81.67 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33747 sec, avg_samples: 64.00000, ips: 379.29845 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 136800, eta: 81.65 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33733 sec, avg_samples: 64.00000, ips: 379.45549 images/sec
Training: 2023-10-18 02:28:46,526 - loss nan, lr: 0.025000, epoch: 3, step: 136800, eta: 81.65 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33733 sec, avg_samples: 64.00000, ips: 379.45549 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 136900, eta: 81.63 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33737 sec, avg_samples: 64.00000, ips: 379.40816 images/sec
Training: 2023-10-18 02:29:20,268 - loss nan, lr: 0.025000, epoch: 3, step: 136900, eta: 81.63 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33737 sec, avg_samples: 64.00000, ips: 379.40816 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 137000, eta: 81.61 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33737 sec, avg_samples: 64.00000, ips: 379.41035 images/sec
Training: 2023-10-18 02:29:54,010 - loss nan, lr: 0.025000, epoch: 3, step: 137000, eta: 81.61 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33737 sec, avg_samples: 64.00000, ips: 379.41035 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 137100, eta: 81.59 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33738 sec, avg_samples: 64.00000, ips: 379.39143 images/sec
Training: 2023-10-18 02:30:27,754 - loss nan, lr: 0.025000, epoch: 3, step: 137100, eta: 81.59 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33738 sec, avg_samples: 64.00000, ips: 379.39143 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 137200, eta: 81.57 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33735 sec, avg_samples: 64.00000, ips: 379.42949 images/sec
Training: 2023-10-18 02:31:01,495 - loss nan, lr: 0.025000, epoch: 3, step: 137200, eta: 81.57 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33735 sec, avg_samples: 64.00000, ips: 379.42949 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 137300, eta: 81.55 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33730 sec, avg_samples: 64.00000, ips: 379.48239 images/sec
Training: 2023-10-18 02:31:35,230 - loss nan, lr: 0.025000, epoch: 3, step: 137300, eta: 81.55 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33730 sec, avg_samples: 64.00000, ips: 379.48239 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 137400, eta: 81.54 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33738 sec, avg_samples: 64.00000, ips: 379.39523 images/sec
Training: 2023-10-18 02:32:08,974 - loss nan, lr: 0.025000, epoch: 3, step: 137400, eta: 81.54 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33738 sec, avg_samples: 64.00000, ips: 379.39523 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 137500, eta: 81.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33718 sec, avg_samples: 64.00000, ips: 379.61745 images/sec
Training: 2023-10-18 02:32:42,698 - loss nan, lr: 0.025000, epoch: 3, step: 137500, eta: 81.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33718 sec, avg_samples: 64.00000, ips: 379.61745 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 137600, eta: 81.50 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33735 sec, avg_samples: 64.00000, ips: 379.42832 images/sec
Training: 2023-10-18 02:33:16,438 - loss nan, lr: 0.025000, epoch: 3, step: 137600, eta: 81.50 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33735 sec, avg_samples: 64.00000, ips: 379.42832 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 137700, eta: 81.48 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33744 sec, avg_samples: 64.00000, ips: 379.32902 images/sec
Training: 2023-10-18 02:33:50,188 - loss nan, lr: 0.025000, epoch: 3, step: 137700, eta: 81.48 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33744 sec, avg_samples: 64.00000, ips: 379.32902 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 137800, eta: 81.46 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33737 sec, avg_samples: 64.00000, ips: 379.40238 images/sec
Training: 2023-10-18 02:34:23,931 - loss nan, lr: 0.025000, epoch: 3, step: 137800, eta: 81.46 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33737 sec, avg_samples: 64.00000, ips: 379.40238 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 137900, eta: 81.44 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33741 sec, avg_samples: 64.00000, ips: 379.36492 images/sec
Training: 2023-10-18 02:34:57,677 - loss nan, lr: 0.025000, epoch: 3, step: 137900, eta: 81.44 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33741 sec, avg_samples: 64.00000, ips: 379.36492 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 138000, eta: 81.43 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33736 sec, avg_samples: 64.00000, ips: 379.41535 images/sec
Training: 2023-10-18 02:35:31,419 - loss nan, lr: 0.025000, epoch: 3, step: 138000, eta: 81.43 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33736 sec, avg_samples: 64.00000, ips: 379.41535 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][138000]XNorm: 0.000000
Training: 2023-10-18 02:36:03,298 - [lfw][138000]XNorm: 0.000000
INFO:root:[lfw][138000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 02:36:03,298 - [lfw][138000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][138000]Accuracy-Highest: 0.81183
Training: 2023-10-18 02:36:03,298 - [lfw][138000]Accuracy-Highest: 0.81183
INFO:root:test time: 31.8791
Training: 2023-10-18 02:36:03,298 - test time: 31.8791
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][138000]XNorm: 0.000000
Training: 2023-10-18 02:36:40,289 - [cfp_fp][138000]XNorm: 0.000000
INFO:root:[cfp_fp][138000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 02:36:40,289 - [cfp_fp][138000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][138000]Accuracy-Highest: 0.61571
Training: 2023-10-18 02:36:40,289 - [cfp_fp][138000]Accuracy-Highest: 0.61571
INFO:root:test time: 36.9909
Training: 2023-10-18 02:36:40,289 - test time: 36.9909
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][138000]XNorm: 0.000000
Training: 2023-10-18 02:37:12,168 - [agedb_30][138000]XNorm: 0.000000
INFO:root:[agedb_30][138000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 02:37:12,168 - [agedb_30][138000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][138000]Accuracy-Highest: 0.58350
Training: 2023-10-18 02:37:12,168 - [agedb_30][138000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.8792
Training: 2023-10-18 02:37:12,168 - test time: 31.8792
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 138100, eta: 81.56 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33669 sec, avg_samples: 64.00000, ips: 380.17325 images/sec
Training: 2023-10-18 02:37:45,843 - loss nan, lr: 0.025000, epoch: 3, step: 138100, eta: 81.56 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33669 sec, avg_samples: 64.00000, ips: 380.17325 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 138200, eta: 81.54 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33728 sec, avg_samples: 64.00000, ips: 379.50362 images/sec
Training: 2023-10-18 02:38:19,578 - loss nan, lr: 0.025000, epoch: 3, step: 138200, eta: 81.54 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33728 sec, avg_samples: 64.00000, ips: 379.50362 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 138300, eta: 81.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33721 sec, avg_samples: 64.00000, ips: 379.58467 images/sec
Training: 2023-10-18 02:38:53,305 - loss nan, lr: 0.025000, epoch: 3, step: 138300, eta: 81.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33721 sec, avg_samples: 64.00000, ips: 379.58467 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 138400, eta: 81.50 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33728 sec, avg_samples: 64.00000, ips: 379.50295 images/sec
Training: 2023-10-18 02:39:27,039 - loss nan, lr: 0.025000, epoch: 3, step: 138400, eta: 81.50 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33728 sec, avg_samples: 64.00000, ips: 379.50295 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 138500, eta: 81.49 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33743 sec, avg_samples: 64.00000, ips: 379.33437 images/sec
Training: 2023-10-18 02:40:00,788 - loss nan, lr: 0.025000, epoch: 3, step: 138500, eta: 81.49 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33743 sec, avg_samples: 64.00000, ips: 379.33437 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 138600, eta: 81.47 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33731 sec, avg_samples: 64.00000, ips: 379.47285 images/sec
Training: 2023-10-18 02:40:34,525 - loss nan, lr: 0.025000, epoch: 3, step: 138600, eta: 81.47 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33731 sec, avg_samples: 64.00000, ips: 379.47285 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 138700, eta: 81.45 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33725 sec, avg_samples: 64.00000, ips: 379.54257 images/sec
Training: 2023-10-18 02:41:08,256 - loss nan, lr: 0.025000, epoch: 3, step: 138700, eta: 81.45 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33725 sec, avg_samples: 64.00000, ips: 379.54257 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 138800, eta: 81.43 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33733 sec, avg_samples: 64.00000, ips: 379.44699 images/sec
Training: 2023-10-18 02:41:41,995 - loss nan, lr: 0.025000, epoch: 3, step: 138800, eta: 81.43 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33733 sec, avg_samples: 64.00000, ips: 379.44699 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 138900, eta: 81.41 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33732 sec, avg_samples: 64.00000, ips: 379.46084 images/sec
Training: 2023-10-18 02:42:15,733 - loss nan, lr: 0.025000, epoch: 3, step: 138900, eta: 81.41 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33732 sec, avg_samples: 64.00000, ips: 379.46084 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 139000, eta: 81.39 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33736 sec, avg_samples: 64.00000, ips: 379.41235 images/sec
Training: 2023-10-18 02:42:49,476 - loss nan, lr: 0.025000, epoch: 3, step: 139000, eta: 81.39 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33736 sec, avg_samples: 64.00000, ips: 379.41235 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 139100, eta: 81.37 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33727 sec, avg_samples: 64.00000, ips: 379.51400 images/sec
Training: 2023-10-18 02:43:23,209 - loss nan, lr: 0.025000, epoch: 3, step: 139100, eta: 81.37 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33727 sec, avg_samples: 64.00000, ips: 379.51400 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 139200, eta: 81.36 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33730 sec, avg_samples: 64.00000, ips: 379.48577 images/sec
Training: 2023-10-18 02:43:56,945 - loss nan, lr: 0.025000, epoch: 3, step: 139200, eta: 81.36 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33730 sec, avg_samples: 64.00000, ips: 379.48577 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 139300, eta: 81.34 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33727 sec, avg_samples: 64.00000, ips: 379.51444 images/sec
Training: 2023-10-18 02:44:30,678 - loss nan, lr: 0.025000, epoch: 3, step: 139300, eta: 81.34 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33727 sec, avg_samples: 64.00000, ips: 379.51444 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 139400, eta: 81.32 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33725 sec, avg_samples: 64.00000, ips: 379.54578 images/sec
Training: 2023-10-18 02:45:04,408 - loss nan, lr: 0.025000, epoch: 3, step: 139400, eta: 81.32 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33725 sec, avg_samples: 64.00000, ips: 379.54578 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 139500, eta: 81.30 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33736 sec, avg_samples: 64.00000, ips: 379.42055 images/sec
Training: 2023-10-18 02:45:38,150 - loss nan, lr: 0.025000, epoch: 3, step: 139500, eta: 81.30 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33736 sec, avg_samples: 64.00000, ips: 379.42055 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 139600, eta: 81.28 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33722 sec, avg_samples: 64.00000, ips: 379.57723 images/sec
Training: 2023-10-18 02:46:11,878 - loss nan, lr: 0.025000, epoch: 3, step: 139600, eta: 81.28 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33722 sec, avg_samples: 64.00000, ips: 379.57723 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 139700, eta: 81.26 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33744 sec, avg_samples: 64.00000, ips: 379.32771 images/sec
Training: 2023-10-18 02:46:45,628 - loss nan, lr: 0.025000, epoch: 3, step: 139700, eta: 81.26 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33744 sec, avg_samples: 64.00000, ips: 379.32771 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 139800, eta: 81.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33731 sec, avg_samples: 64.00000, ips: 379.47827 images/sec
Training: 2023-10-18 02:47:19,364 - loss nan, lr: 0.025000, epoch: 3, step: 139800, eta: 81.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33731 sec, avg_samples: 64.00000, ips: 379.47827 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 139900, eta: 81.23 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33730 sec, avg_samples: 64.00000, ips: 379.48332 images/sec
Training: 2023-10-18 02:47:53,100 - loss nan, lr: 0.025000, epoch: 3, step: 139900, eta: 81.23 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33730 sec, avg_samples: 64.00000, ips: 379.48332 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 140000, eta: 81.21 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33740 sec, avg_samples: 64.00000, ips: 379.36849 images/sec
Training: 2023-10-18 02:48:26,846 - loss nan, lr: 0.025000, epoch: 3, step: 140000, eta: 81.21 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33740 sec, avg_samples: 64.00000, ips: 379.36849 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][140000]XNorm: 0.000000
Training: 2023-10-18 02:48:58,741 - [lfw][140000]XNorm: 0.000000
INFO:root:[lfw][140000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 02:48:58,741 - [lfw][140000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][140000]Accuracy-Highest: 0.81183
Training: 2023-10-18 02:48:58,741 - [lfw][140000]Accuracy-Highest: 0.81183
INFO:root:test time: 31.8942
Training: 2023-10-18 02:48:58,741 - test time: 31.8942
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][140000]XNorm: 0.000000
Training: 2023-10-18 02:49:35,739 - [cfp_fp][140000]XNorm: 0.000000
INFO:root:[cfp_fp][140000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 02:49:35,739 - [cfp_fp][140000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][140000]Accuracy-Highest: 0.61571
Training: 2023-10-18 02:49:35,739 - [cfp_fp][140000]Accuracy-Highest: 0.61571
INFO:root:test time: 36.9986
Training: 2023-10-18 02:49:35,739 - test time: 36.9986
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][140000]XNorm: 0.000000
Training: 2023-10-18 02:50:07,614 - [agedb_30][140000]XNorm: 0.000000
INFO:root:[agedb_30][140000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 02:50:07,614 - [agedb_30][140000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][140000]Accuracy-Highest: 0.58350
Training: 2023-10-18 02:50:07,614 - [agedb_30][140000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.8751
Training: 2023-10-18 02:50:07,615 - test time: 31.8751
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 140100, eta: 81.34 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33665 sec, avg_samples: 64.00000, ips: 380.21659 images/sec
Training: 2023-10-18 02:50:41,285 - loss nan, lr: 0.025000, epoch: 3, step: 140100, eta: 81.34 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33665 sec, avg_samples: 64.00000, ips: 380.21659 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 140200, eta: 81.32 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33736 sec, avg_samples: 64.00000, ips: 379.41457 images/sec
Training: 2023-10-18 02:51:15,028 - loss nan, lr: 0.025000, epoch: 3, step: 140200, eta: 81.32 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33736 sec, avg_samples: 64.00000, ips: 379.41457 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 140300, eta: 81.30 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33742 sec, avg_samples: 64.00000, ips: 379.35426 images/sec
Training: 2023-10-18 02:51:48,775 - loss nan, lr: 0.025000, epoch: 3, step: 140300, eta: 81.30 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33742 sec, avg_samples: 64.00000, ips: 379.35426 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 140400, eta: 81.29 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33726 sec, avg_samples: 64.00000, ips: 379.53477 images/sec
Training: 2023-10-18 02:52:22,506 - loss nan, lr: 0.025000, epoch: 3, step: 140400, eta: 81.29 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33726 sec, avg_samples: 64.00000, ips: 379.53477 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 140500, eta: 81.27 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33734 sec, avg_samples: 64.00000, ips: 379.43408 images/sec
Training: 2023-10-18 02:52:56,247 - loss nan, lr: 0.025000, epoch: 3, step: 140500, eta: 81.27 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33734 sec, avg_samples: 64.00000, ips: 379.43408 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 140600, eta: 81.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33736 sec, avg_samples: 64.00000, ips: 379.42061 images/sec
Training: 2023-10-18 02:53:29,988 - loss nan, lr: 0.025000, epoch: 3, step: 140600, eta: 81.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33736 sec, avg_samples: 64.00000, ips: 379.42061 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 140700, eta: 81.23 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33734 sec, avg_samples: 64.00000, ips: 379.43859 images/sec
Training: 2023-10-18 02:54:03,728 - loss nan, lr: 0.025000, epoch: 3, step: 140700, eta: 81.23 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33734 sec, avg_samples: 64.00000, ips: 379.43859 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 140800, eta: 81.21 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33729 sec, avg_samples: 64.00000, ips: 379.49018 images/sec
Training: 2023-10-18 02:54:37,464 - loss nan, lr: 0.025000, epoch: 3, step: 140800, eta: 81.21 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33729 sec, avg_samples: 64.00000, ips: 379.49018 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 140900, eta: 81.19 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33726 sec, avg_samples: 64.00000, ips: 379.52627 images/sec
Training: 2023-10-18 02:55:11,196 - loss nan, lr: 0.025000, epoch: 3, step: 140900, eta: 81.19 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33726 sec, avg_samples: 64.00000, ips: 379.52627 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 141000, eta: 81.17 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33733 sec, avg_samples: 64.00000, ips: 379.44781 images/sec
Training: 2023-10-18 02:55:44,935 - loss nan, lr: 0.025000, epoch: 3, step: 141000, eta: 81.17 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33733 sec, avg_samples: 64.00000, ips: 379.44781 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 141100, eta: 81.16 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33734 sec, avg_samples: 64.00000, ips: 379.43644 images/sec
Training: 2023-10-18 02:56:18,675 - loss nan, lr: 0.025000, epoch: 3, step: 141100, eta: 81.16 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33734 sec, avg_samples: 64.00000, ips: 379.43644 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 141200, eta: 81.14 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33743 sec, avg_samples: 64.00000, ips: 379.33950 images/sec
Training: 2023-10-18 02:56:52,423 - loss nan, lr: 0.025000, epoch: 3, step: 141200, eta: 81.14 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33743 sec, avg_samples: 64.00000, ips: 379.33950 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 141300, eta: 81.12 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33746 sec, avg_samples: 64.00000, ips: 379.29879 images/sec
Training: 2023-10-18 02:57:26,175 - loss nan, lr: 0.025000, epoch: 3, step: 141300, eta: 81.12 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33746 sec, avg_samples: 64.00000, ips: 379.29879 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 141400, eta: 81.10 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33751 sec, avg_samples: 64.00000, ips: 379.25246 images/sec
Training: 2023-10-18 02:57:59,931 - loss nan, lr: 0.025000, epoch: 3, step: 141400, eta: 81.10 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33751 sec, avg_samples: 64.00000, ips: 379.25246 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 141500, eta: 81.08 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33751 sec, avg_samples: 64.00000, ips: 379.24998 images/sec
Training: 2023-10-18 02:58:33,688 - loss nan, lr: 0.025000, epoch: 3, step: 141500, eta: 81.08 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33751 sec, avg_samples: 64.00000, ips: 379.24998 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 141600, eta: 81.07 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33742 sec, avg_samples: 64.00000, ips: 379.35317 images/sec
Training: 2023-10-18 02:59:07,435 - loss nan, lr: 0.025000, epoch: 3, step: 141600, eta: 81.07 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33742 sec, avg_samples: 64.00000, ips: 379.35317 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 141700, eta: 81.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33738 sec, avg_samples: 64.00000, ips: 379.38995 images/sec
Training: 2023-10-18 02:59:41,179 - loss nan, lr: 0.025000, epoch: 3, step: 141700, eta: 81.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33738 sec, avg_samples: 64.00000, ips: 379.38995 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 141800, eta: 81.03 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33736 sec, avg_samples: 64.00000, ips: 379.41939 images/sec
Training: 2023-10-18 03:00:14,920 - loss nan, lr: 0.025000, epoch: 3, step: 141800, eta: 81.03 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33736 sec, avg_samples: 64.00000, ips: 379.41939 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 141900, eta: 81.01 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33733 sec, avg_samples: 64.00000, ips: 379.44546 images/sec
Training: 2023-10-18 03:00:48,659 - loss nan, lr: 0.025000, epoch: 3, step: 141900, eta: 81.01 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33733 sec, avg_samples: 64.00000, ips: 379.44546 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 142000, eta: 80.99 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33746 sec, avg_samples: 64.00000, ips: 379.30450 images/sec
Training: 2023-10-18 03:01:22,411 - loss nan, lr: 0.025000, epoch: 3, step: 142000, eta: 80.99 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33746 sec, avg_samples: 64.00000, ips: 379.30450 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][142000]XNorm: 0.000000
Training: 2023-10-18 03:01:54,297 - [lfw][142000]XNorm: 0.000000
INFO:root:[lfw][142000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 03:01:54,297 - [lfw][142000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][142000]Accuracy-Highest: 0.81183
Training: 2023-10-18 03:01:54,297 - [lfw][142000]Accuracy-Highest: 0.81183
INFO:root:test time: 31.8866
Training: 2023-10-18 03:01:54,297 - test time: 31.8866
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][142000]XNorm: 0.000000
Training: 2023-10-18 03:02:31,293 - [cfp_fp][142000]XNorm: 0.000000
INFO:root:[cfp_fp][142000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 03:02:31,293 - [cfp_fp][142000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][142000]Accuracy-Highest: 0.61571
Training: 2023-10-18 03:02:31,293 - [cfp_fp][142000]Accuracy-Highest: 0.61571
INFO:root:test time: 36.9958
Training: 2023-10-18 03:02:31,293 - test time: 36.9958
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][142000]XNorm: 0.000000
Training: 2023-10-18 03:03:03,135 - [agedb_30][142000]XNorm: 0.000000
INFO:root:[agedb_30][142000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 03:03:03,135 - [agedb_30][142000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][142000]Accuracy-Highest: 0.58350
Training: 2023-10-18 03:03:03,135 - [agedb_30][142000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.8417
Training: 2023-10-18 03:03:03,135 - test time: 31.8417
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 142100, eta: 81.12 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33666 sec, avg_samples: 64.00000, ips: 380.20718 images/sec
Training: 2023-10-18 03:03:36,807 - loss nan, lr: 0.025000, epoch: 3, step: 142100, eta: 81.12 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33666 sec, avg_samples: 64.00000, ips: 380.20718 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 142200, eta: 81.10 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33741 sec, avg_samples: 64.00000, ips: 379.35504 images/sec
Training: 2023-10-18 03:04:10,554 - loss nan, lr: 0.025000, epoch: 3, step: 142200, eta: 81.10 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33741 sec, avg_samples: 64.00000, ips: 379.35504 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 142300, eta: 81.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33724 sec, avg_samples: 64.00000, ips: 379.54900 images/sec
Training: 2023-10-18 03:04:44,284 - loss nan, lr: 0.025000, epoch: 3, step: 142300, eta: 81.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33724 sec, avg_samples: 64.00000, ips: 379.54900 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 142400, eta: 81.07 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33745 sec, avg_samples: 64.00000, ips: 379.31948 images/sec
Training: 2023-10-18 03:05:18,035 - loss nan, lr: 0.025000, epoch: 3, step: 142400, eta: 81.07 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33745 sec, avg_samples: 64.00000, ips: 379.31948 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 142500, eta: 81.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33741 sec, avg_samples: 64.00000, ips: 379.35802 images/sec
Training: 2023-10-18 03:05:51,782 - loss nan, lr: 0.025000, epoch: 3, step: 142500, eta: 81.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33741 sec, avg_samples: 64.00000, ips: 379.35802 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 142600, eta: 81.03 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33743 sec, avg_samples: 64.00000, ips: 379.34288 images/sec
Training: 2023-10-18 03:06:25,531 - loss nan, lr: 0.025000, epoch: 3, step: 142600, eta: 81.03 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33743 sec, avg_samples: 64.00000, ips: 379.34288 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 142700, eta: 81.01 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33735 sec, avg_samples: 64.00000, ips: 379.42950 images/sec
Training: 2023-10-18 03:06:59,272 - loss nan, lr: 0.025000, epoch: 3, step: 142700, eta: 81.01 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33735 sec, avg_samples: 64.00000, ips: 379.42950 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 142800, eta: 80.99 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33733 sec, avg_samples: 64.00000, ips: 379.44485 images/sec
Training: 2023-10-18 03:07:33,011 - loss nan, lr: 0.025000, epoch: 3, step: 142800, eta: 80.99 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33733 sec, avg_samples: 64.00000, ips: 379.44485 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 3, step: 142900, eta: 80.98 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33734 sec, avg_samples: 64.00000, ips: 379.43706 images/sec
Training: 2023-10-18 03:08:06,751 - loss nan, lr: 0.025000, epoch: 3, step: 142900, eta: 80.98 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33734 sec, avg_samples: 64.00000, ips: 379.43706 images/sec
INFO:root:Save model to model/FresResNet101/3.
Training: 2023-10-18 03:08:22,695 - Save model to model/FresResNet101/3.
INFO:root:Remove checkpoint model/FresResNet101/2.
Training: 2023-10-18 03:08:22,695 - Remove checkpoint model/FresResNet101/2.
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 143000, eta: 80.96 hours, avg_reader_cost: 0.00900 sec, avg_batch_cost: 0.19764 sec, avg_samples: 35.84000, ips: 362.68445 images/sec
Training: 2023-10-18 03:08:42,602 - loss nan, lr: 0.025000, epoch: 4, step: 143000, eta: 80.96 hours, avg_reader_cost: 0.00900 sec, avg_batch_cost: 0.19764 sec, avg_samples: 35.84000, ips: 362.68445 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 143100, eta: 80.94 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33663 sec, avg_samples: 64.00000, ips: 380.24027 images/sec
Training: 2023-10-18 03:09:16,272 - loss nan, lr: 0.025000, epoch: 4, step: 143100, eta: 80.94 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33663 sec, avg_samples: 64.00000, ips: 380.24027 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 143200, eta: 80.92 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33673 sec, avg_samples: 64.00000, ips: 380.12424 images/sec
Training: 2023-10-18 03:09:49,951 - loss nan, lr: 0.025000, epoch: 4, step: 143200, eta: 80.92 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33673 sec, avg_samples: 64.00000, ips: 380.12424 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 143300, eta: 80.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33664 sec, avg_samples: 64.00000, ips: 380.22904 images/sec
Training: 2023-10-18 03:10:23,621 - loss nan, lr: 0.025000, epoch: 4, step: 143300, eta: 80.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33664 sec, avg_samples: 64.00000, ips: 380.22904 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 143400, eta: 80.89 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33708 sec, avg_samples: 64.00000, ips: 379.72643 images/sec
Training: 2023-10-18 03:10:57,336 - loss nan, lr: 0.025000, epoch: 4, step: 143400, eta: 80.89 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33708 sec, avg_samples: 64.00000, ips: 379.72643 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 143500, eta: 80.87 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33729 sec, avg_samples: 64.00000, ips: 379.50055 images/sec
Training: 2023-10-18 03:11:31,070 - loss nan, lr: 0.025000, epoch: 4, step: 143500, eta: 80.87 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33729 sec, avg_samples: 64.00000, ips: 379.50055 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 143600, eta: 80.85 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33723 sec, avg_samples: 64.00000, ips: 379.55739 images/sec
Training: 2023-10-18 03:12:04,799 - loss nan, lr: 0.025000, epoch: 4, step: 143600, eta: 80.85 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33723 sec, avg_samples: 64.00000, ips: 379.55739 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 143700, eta: 80.83 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33727 sec, avg_samples: 64.00000, ips: 379.52066 images/sec
Training: 2023-10-18 03:12:38,532 - loss nan, lr: 0.025000, epoch: 4, step: 143700, eta: 80.83 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33727 sec, avg_samples: 64.00000, ips: 379.52066 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 143800, eta: 80.81 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33732 sec, avg_samples: 64.00000, ips: 379.45776 images/sec
Training: 2023-10-18 03:13:12,270 - loss nan, lr: 0.025000, epoch: 4, step: 143800, eta: 80.81 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33732 sec, avg_samples: 64.00000, ips: 379.45776 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 143900, eta: 80.80 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33731 sec, avg_samples: 64.00000, ips: 379.47473 images/sec
Training: 2023-10-18 03:13:46,007 - loss nan, lr: 0.025000, epoch: 4, step: 143900, eta: 80.80 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33731 sec, avg_samples: 64.00000, ips: 379.47473 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 144000, eta: 80.78 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33726 sec, avg_samples: 64.00000, ips: 379.52924 images/sec
Training: 2023-10-18 03:14:19,739 - loss nan, lr: 0.025000, epoch: 4, step: 144000, eta: 80.78 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33726 sec, avg_samples: 64.00000, ips: 379.52924 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][144000]XNorm: 0.000000
Training: 2023-10-18 03:14:51,631 - [lfw][144000]XNorm: 0.000000
INFO:root:[lfw][144000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 03:14:51,631 - [lfw][144000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][144000]Accuracy-Highest: 0.81183
Training: 2023-10-18 03:14:51,632 - [lfw][144000]Accuracy-Highest: 0.81183
INFO:root:test time: 31.8923
Training: 2023-10-18 03:14:51,632 - test time: 31.8923
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][144000]XNorm: 0.000000
Training: 2023-10-18 03:15:28,635 - [cfp_fp][144000]XNorm: 0.000000
INFO:root:[cfp_fp][144000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 03:15:28,635 - [cfp_fp][144000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][144000]Accuracy-Highest: 0.61571
Training: 2023-10-18 03:15:28,635 - [cfp_fp][144000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.0038
Training: 2023-10-18 03:15:28,635 - test time: 37.0038
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][144000]XNorm: 0.000000
Training: 2023-10-18 03:16:00,530 - [agedb_30][144000]XNorm: 0.000000
INFO:root:[agedb_30][144000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 03:16:00,530 - [agedb_30][144000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][144000]Accuracy-Highest: 0.58350
Training: 2023-10-18 03:16:00,530 - [agedb_30][144000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.8951
Training: 2023-10-18 03:16:00,531 - test time: 31.8951
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 144100, eta: 80.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33687 sec, avg_samples: 64.00000, ips: 379.97299 images/sec
Training: 2023-10-18 03:16:34,223 - loss nan, lr: 0.025000, epoch: 4, step: 144100, eta: 80.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33687 sec, avg_samples: 64.00000, ips: 379.97299 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 144200, eta: 80.89 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33728 sec, avg_samples: 64.00000, ips: 379.50448 images/sec
Training: 2023-10-18 03:17:07,957 - loss nan, lr: 0.025000, epoch: 4, step: 144200, eta: 80.89 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33728 sec, avg_samples: 64.00000, ips: 379.50448 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 144300, eta: 80.87 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33710 sec, avg_samples: 64.00000, ips: 379.71181 images/sec
Training: 2023-10-18 03:17:41,673 - loss nan, lr: 0.025000, epoch: 4, step: 144300, eta: 80.87 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33710 sec, avg_samples: 64.00000, ips: 379.71181 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 144400, eta: 80.85 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33737 sec, avg_samples: 64.00000, ips: 379.40793 images/sec
Training: 2023-10-18 03:18:15,416 - loss nan, lr: 0.025000, epoch: 4, step: 144400, eta: 80.85 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33737 sec, avg_samples: 64.00000, ips: 379.40793 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 144500, eta: 80.83 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33726 sec, avg_samples: 64.00000, ips: 379.52421 images/sec
Training: 2023-10-18 03:18:49,148 - loss nan, lr: 0.025000, epoch: 4, step: 144500, eta: 80.83 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33726 sec, avg_samples: 64.00000, ips: 379.52421 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 144600, eta: 80.81 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33729 sec, avg_samples: 64.00000, ips: 379.49875 images/sec
Training: 2023-10-18 03:19:22,883 - loss nan, lr: 0.025000, epoch: 4, step: 144600, eta: 80.81 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33729 sec, avg_samples: 64.00000, ips: 379.49875 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 144700, eta: 80.80 hours, avg_reader_cost: 0.00003 sec, avg_batch_cost: 0.33676 sec, avg_samples: 64.00000, ips: 380.09059 images/sec
Training: 2023-10-18 03:19:56,567 - loss nan, lr: 0.025000, epoch: 4, step: 144700, eta: 80.80 hours, avg_reader_cost: 0.00003 sec, avg_batch_cost: 0.33676 sec, avg_samples: 64.00000, ips: 380.09059 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 144800, eta: 80.78 hours, avg_reader_cost: 0.00003 sec, avg_batch_cost: 0.33657 sec, avg_samples: 64.00000, ips: 380.30179 images/sec
Training: 2023-10-18 03:20:30,232 - loss nan, lr: 0.025000, epoch: 4, step: 144800, eta: 80.78 hours, avg_reader_cost: 0.00003 sec, avg_batch_cost: 0.33657 sec, avg_samples: 64.00000, ips: 380.30179 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 144900, eta: 80.76 hours, avg_reader_cost: 0.00003 sec, avg_batch_cost: 0.33659 sec, avg_samples: 64.00000, ips: 380.28119 images/sec
Training: 2023-10-18 03:21:03,900 - loss nan, lr: 0.025000, epoch: 4, step: 144900, eta: 80.76 hours, avg_reader_cost: 0.00003 sec, avg_batch_cost: 0.33659 sec, avg_samples: 64.00000, ips: 380.28119 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 145000, eta: 80.74 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33712 sec, avg_samples: 64.00000, ips: 379.69202 images/sec
Training: 2023-10-18 03:21:37,618 - loss nan, lr: 0.025000, epoch: 4, step: 145000, eta: 80.74 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33712 sec, avg_samples: 64.00000, ips: 379.69202 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 145100, eta: 80.72 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33721 sec, avg_samples: 64.00000, ips: 379.58396 images/sec
Training: 2023-10-18 03:22:11,344 - loss nan, lr: 0.025000, epoch: 4, step: 145100, eta: 80.72 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33721 sec, avg_samples: 64.00000, ips: 379.58396 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 145200, eta: 80.71 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33724 sec, avg_samples: 64.00000, ips: 379.54679 images/sec
Training: 2023-10-18 03:22:45,074 - loss nan, lr: 0.025000, epoch: 4, step: 145200, eta: 80.71 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33724 sec, avg_samples: 64.00000, ips: 379.54679 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 145300, eta: 80.69 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33740 sec, avg_samples: 64.00000, ips: 379.37631 images/sec
Training: 2023-10-18 03:23:18,819 - loss nan, lr: 0.025000, epoch: 4, step: 145300, eta: 80.69 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33740 sec, avg_samples: 64.00000, ips: 379.37631 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 145400, eta: 80.67 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33724 sec, avg_samples: 64.00000, ips: 379.55505 images/sec
Training: 2023-10-18 03:23:52,549 - loss nan, lr: 0.025000, epoch: 4, step: 145400, eta: 80.67 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33724 sec, avg_samples: 64.00000, ips: 379.55505 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 145500, eta: 80.65 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33753 sec, avg_samples: 64.00000, ips: 379.22574 images/sec
Training: 2023-10-18 03:24:26,307 - loss nan, lr: 0.025000, epoch: 4, step: 145500, eta: 80.65 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33753 sec, avg_samples: 64.00000, ips: 379.22574 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 145600, eta: 80.63 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33719 sec, avg_samples: 64.00000, ips: 379.61286 images/sec
Training: 2023-10-18 03:25:00,031 - loss nan, lr: 0.025000, epoch: 4, step: 145600, eta: 80.63 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33719 sec, avg_samples: 64.00000, ips: 379.61286 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 145700, eta: 80.62 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33714 sec, avg_samples: 64.00000, ips: 379.66842 images/sec
Training: 2023-10-18 03:25:33,750 - loss nan, lr: 0.025000, epoch: 4, step: 145700, eta: 80.62 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33714 sec, avg_samples: 64.00000, ips: 379.66842 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 145800, eta: 80.60 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33729 sec, avg_samples: 64.00000, ips: 379.49373 images/sec
Training: 2023-10-18 03:26:07,485 - loss nan, lr: 0.025000, epoch: 4, step: 145800, eta: 80.60 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33729 sec, avg_samples: 64.00000, ips: 379.49373 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 145900, eta: 80.58 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33738 sec, avg_samples: 64.00000, ips: 379.39197 images/sec
Training: 2023-10-18 03:26:41,229 - loss nan, lr: 0.025000, epoch: 4, step: 145900, eta: 80.58 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33738 sec, avg_samples: 64.00000, ips: 379.39197 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 146000, eta: 80.56 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33740 sec, avg_samples: 64.00000, ips: 379.37688 images/sec
Training: 2023-10-18 03:27:14,974 - loss nan, lr: 0.025000, epoch: 4, step: 146000, eta: 80.56 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33740 sec, avg_samples: 64.00000, ips: 379.37688 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][146000]XNorm: 0.000000
Training: 2023-10-18 03:27:46,890 - [lfw][146000]XNorm: 0.000000
INFO:root:[lfw][146000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 03:27:46,890 - [lfw][146000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][146000]Accuracy-Highest: 0.81183
Training: 2023-10-18 03:27:46,890 - [lfw][146000]Accuracy-Highest: 0.81183
INFO:root:test time: 31.9162
Training: 2023-10-18 03:27:46,890 - test time: 31.9162
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][146000]XNorm: 0.000000
Training: 2023-10-18 03:28:23,921 - [cfp_fp][146000]XNorm: 0.000000
INFO:root:[cfp_fp][146000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 03:28:23,921 - [cfp_fp][146000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][146000]Accuracy-Highest: 0.61571
Training: 2023-10-18 03:28:23,921 - [cfp_fp][146000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.0312
Training: 2023-10-18 03:28:23,921 - test time: 37.0312
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][146000]XNorm: 0.000000
Training: 2023-10-18 03:28:55,797 - [agedb_30][146000]XNorm: 0.000000
INFO:root:[agedb_30][146000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 03:28:55,797 - [agedb_30][146000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][146000]Accuracy-Highest: 0.58350
Training: 2023-10-18 03:28:55,797 - [agedb_30][146000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.8756
Training: 2023-10-18 03:28:55,797 - test time: 31.8756
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 146100, eta: 80.69 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33694 sec, avg_samples: 64.00000, ips: 379.88486 images/sec
Training: 2023-10-18 03:29:29,497 - loss nan, lr: 0.025000, epoch: 4, step: 146100, eta: 80.69 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33694 sec, avg_samples: 64.00000, ips: 379.88486 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 146200, eta: 80.67 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33727 sec, avg_samples: 64.00000, ips: 379.52191 images/sec
Training: 2023-10-18 03:30:03,230 - loss nan, lr: 0.025000, epoch: 4, step: 146200, eta: 80.67 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33727 sec, avg_samples: 64.00000, ips: 379.52191 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 146300, eta: 80.65 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33718 sec, avg_samples: 64.00000, ips: 379.62393 images/sec
Training: 2023-10-18 03:30:36,953 - loss nan, lr: 0.025000, epoch: 4, step: 146300, eta: 80.65 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33718 sec, avg_samples: 64.00000, ips: 379.62393 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 146400, eta: 80.63 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33717 sec, avg_samples: 64.00000, ips: 379.63064 images/sec
Training: 2023-10-18 03:31:10,676 - loss nan, lr: 0.025000, epoch: 4, step: 146400, eta: 80.63 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33717 sec, avg_samples: 64.00000, ips: 379.63064 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 146500, eta: 80.61 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33732 sec, avg_samples: 64.00000, ips: 379.46042 images/sec
Training: 2023-10-18 03:31:44,414 - loss nan, lr: 0.025000, epoch: 4, step: 146500, eta: 80.61 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33732 sec, avg_samples: 64.00000, ips: 379.46042 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 146600, eta: 80.60 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33720 sec, avg_samples: 64.00000, ips: 379.59751 images/sec
Training: 2023-10-18 03:32:18,140 - loss nan, lr: 0.025000, epoch: 4, step: 146600, eta: 80.60 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33720 sec, avg_samples: 64.00000, ips: 379.59751 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 146700, eta: 80.58 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33722 sec, avg_samples: 64.00000, ips: 379.57389 images/sec
Training: 2023-10-18 03:32:51,868 - loss nan, lr: 0.025000, epoch: 4, step: 146700, eta: 80.58 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33722 sec, avg_samples: 64.00000, ips: 379.57389 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 146800, eta: 80.56 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33741 sec, avg_samples: 64.00000, ips: 379.35642 images/sec
Training: 2023-10-18 03:33:25,615 - loss nan, lr: 0.025000, epoch: 4, step: 146800, eta: 80.56 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33741 sec, avg_samples: 64.00000, ips: 379.35642 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 146900, eta: 80.54 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33737 sec, avg_samples: 64.00000, ips: 379.40592 images/sec
Training: 2023-10-18 03:33:59,358 - loss nan, lr: 0.025000, epoch: 4, step: 146900, eta: 80.54 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33737 sec, avg_samples: 64.00000, ips: 379.40592 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 147000, eta: 80.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33735 sec, avg_samples: 64.00000, ips: 379.42853 images/sec
Training: 2023-10-18 03:34:33,099 - loss nan, lr: 0.025000, epoch: 4, step: 147000, eta: 80.52 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33735 sec, avg_samples: 64.00000, ips: 379.42853 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 147100, eta: 80.51 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33728 sec, avg_samples: 64.00000, ips: 379.50140 images/sec
Training: 2023-10-18 03:35:06,833 - loss nan, lr: 0.025000, epoch: 4, step: 147100, eta: 80.51 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33728 sec, avg_samples: 64.00000, ips: 379.50140 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 147200, eta: 80.49 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33742 sec, avg_samples: 64.00000, ips: 379.35002 images/sec
Training: 2023-10-18 03:35:40,581 - loss nan, lr: 0.025000, epoch: 4, step: 147200, eta: 80.49 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33742 sec, avg_samples: 64.00000, ips: 379.35002 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 147300, eta: 80.47 hours, avg_reader_cost: 0.00003 sec, avg_batch_cost: 0.33684 sec, avg_samples: 64.00000, ips: 380.00686 images/sec
Training: 2023-10-18 03:36:14,273 - loss nan, lr: 0.025000, epoch: 4, step: 147300, eta: 80.47 hours, avg_reader_cost: 0.00003 sec, avg_batch_cost: 0.33684 sec, avg_samples: 64.00000, ips: 380.00686 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 147400, eta: 80.45 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33699 sec, avg_samples: 64.00000, ips: 379.82794 images/sec
Training: 2023-10-18 03:36:47,978 - loss nan, lr: 0.025000, epoch: 4, step: 147400, eta: 80.45 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33699 sec, avg_samples: 64.00000, ips: 379.82794 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 147500, eta: 80.43 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33745 sec, avg_samples: 64.00000, ips: 379.31637 images/sec
Training: 2023-10-18 03:37:21,729 - loss nan, lr: 0.025000, epoch: 4, step: 147500, eta: 80.43 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33745 sec, avg_samples: 64.00000, ips: 379.31637 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 147600, eta: 80.42 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33730 sec, avg_samples: 64.00000, ips: 379.48769 images/sec
Training: 2023-10-18 03:37:55,464 - loss nan, lr: 0.025000, epoch: 4, step: 147600, eta: 80.42 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33730 sec, avg_samples: 64.00000, ips: 379.48769 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 147700, eta: 80.40 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33739 sec, avg_samples: 64.00000, ips: 379.38534 images/sec
Training: 2023-10-18 03:38:29,208 - loss nan, lr: 0.025000, epoch: 4, step: 147700, eta: 80.40 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33739 sec, avg_samples: 64.00000, ips: 379.38534 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 147800, eta: 80.38 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33732 sec, avg_samples: 64.00000, ips: 379.46275 images/sec
Training: 2023-10-18 03:39:02,946 - loss nan, lr: 0.025000, epoch: 4, step: 147800, eta: 80.38 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33732 sec, avg_samples: 64.00000, ips: 379.46275 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 147900, eta: 80.36 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33725 sec, avg_samples: 64.00000, ips: 379.54020 images/sec
Training: 2023-10-18 03:39:36,676 - loss nan, lr: 0.025000, epoch: 4, step: 147900, eta: 80.36 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33725 sec, avg_samples: 64.00000, ips: 379.54020 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 148000, eta: 80.35 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33732 sec, avg_samples: 64.00000, ips: 379.46247 images/sec
Training: 2023-10-18 03:40:10,413 - loss nan, lr: 0.025000, epoch: 4, step: 148000, eta: 80.35 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33732 sec, avg_samples: 64.00000, ips: 379.46247 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][148000]XNorm: 0.000000
Training: 2023-10-18 03:40:42,287 - [lfw][148000]XNorm: 0.000000
INFO:root:[lfw][148000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 03:40:42,287 - [lfw][148000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][148000]Accuracy-Highest: 0.81183
Training: 2023-10-18 03:40:42,287 - [lfw][148000]Accuracy-Highest: 0.81183
INFO:root:test time: 31.8733
Training: 2023-10-18 03:40:42,287 - test time: 31.8733
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][148000]XNorm: 0.000000
Training: 2023-10-18 03:41:19,277 - [cfp_fp][148000]XNorm: 0.000000
INFO:root:[cfp_fp][148000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 03:41:19,277 - [cfp_fp][148000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][148000]Accuracy-Highest: 0.61571
Training: 2023-10-18 03:41:19,277 - [cfp_fp][148000]Accuracy-Highest: 0.61571
INFO:root:test time: 36.9900
Training: 2023-10-18 03:41:19,277 - test time: 36.9900
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][148000]XNorm: 0.000000
Training: 2023-10-18 03:41:51,148 - [agedb_30][148000]XNorm: 0.000000
INFO:root:[agedb_30][148000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 03:41:51,148 - [agedb_30][148000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][148000]Accuracy-Highest: 0.58350
Training: 2023-10-18 03:41:51,148 - [agedb_30][148000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.8709
Training: 2023-10-18 03:41:51,148 - test time: 31.8709
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 148100, eta: 80.47 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33711 sec, avg_samples: 64.00000, ips: 379.70002 images/sec
Training: 2023-10-18 03:42:24,864 - loss nan, lr: 0.025000, epoch: 4, step: 148100, eta: 80.47 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33711 sec, avg_samples: 64.00000, ips: 379.70002 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 148200, eta: 80.45 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33723 sec, avg_samples: 64.00000, ips: 379.55939 images/sec
Training: 2023-10-18 03:42:58,594 - loss nan, lr: 0.025000, epoch: 4, step: 148200, eta: 80.45 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33723 sec, avg_samples: 64.00000, ips: 379.55939 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 148300, eta: 80.43 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33730 sec, avg_samples: 64.00000, ips: 379.48709 images/sec
Training: 2023-10-18 03:43:32,329 - loss nan, lr: 0.025000, epoch: 4, step: 148300, eta: 80.43 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33730 sec, avg_samples: 64.00000, ips: 379.48709 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 148400, eta: 80.41 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33707 sec, avg_samples: 64.00000, ips: 379.73757 images/sec
Training: 2023-10-18 03:44:06,043 - loss nan, lr: 0.025000, epoch: 4, step: 148400, eta: 80.41 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33707 sec, avg_samples: 64.00000, ips: 379.73757 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 148500, eta: 80.40 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33717 sec, avg_samples: 64.00000, ips: 379.63094 images/sec
Training: 2023-10-18 03:44:39,766 - loss nan, lr: 0.025000, epoch: 4, step: 148500, eta: 80.40 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33717 sec, avg_samples: 64.00000, ips: 379.63094 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 148600, eta: 80.38 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33728 sec, avg_samples: 64.00000, ips: 379.50457 images/sec
Training: 2023-10-18 03:45:13,500 - loss nan, lr: 0.025000, epoch: 4, step: 148600, eta: 80.38 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33728 sec, avg_samples: 64.00000, ips: 379.50457 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 148700, eta: 80.36 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33731 sec, avg_samples: 64.00000, ips: 379.46759 images/sec
Training: 2023-10-18 03:45:47,237 - loss nan, lr: 0.025000, epoch: 4, step: 148700, eta: 80.36 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33731 sec, avg_samples: 64.00000, ips: 379.46759 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 148800, eta: 80.34 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33734 sec, avg_samples: 64.00000, ips: 379.43354 images/sec
Training: 2023-10-18 03:46:20,978 - loss nan, lr: 0.025000, epoch: 4, step: 148800, eta: 80.34 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33734 sec, avg_samples: 64.00000, ips: 379.43354 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 148900, eta: 80.32 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33723 sec, avg_samples: 64.00000, ips: 379.56423 images/sec
Training: 2023-10-18 03:46:54,707 - loss nan, lr: 0.025000, epoch: 4, step: 148900, eta: 80.32 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33723 sec, avg_samples: 64.00000, ips: 379.56423 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 149000, eta: 80.31 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33729 sec, avg_samples: 64.00000, ips: 379.49580 images/sec
Training: 2023-10-18 03:47:28,441 - loss nan, lr: 0.025000, epoch: 4, step: 149000, eta: 80.31 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33729 sec, avg_samples: 64.00000, ips: 379.49580 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 149100, eta: 80.29 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33739 sec, avg_samples: 64.00000, ips: 379.37928 images/sec
Training: 2023-10-18 03:48:02,187 - loss nan, lr: 0.025000, epoch: 4, step: 149100, eta: 80.29 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33739 sec, avg_samples: 64.00000, ips: 379.37928 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 149200, eta: 80.27 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33738 sec, avg_samples: 64.00000, ips: 379.39391 images/sec
Training: 2023-10-18 03:48:35,931 - loss nan, lr: 0.025000, epoch: 4, step: 149200, eta: 80.27 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33738 sec, avg_samples: 64.00000, ips: 379.39391 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 149300, eta: 80.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33734 sec, avg_samples: 64.00000, ips: 379.43799 images/sec
Training: 2023-10-18 03:49:09,671 - loss nan, lr: 0.025000, epoch: 4, step: 149300, eta: 80.25 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33734 sec, avg_samples: 64.00000, ips: 379.43799 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 149400, eta: 80.24 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33738 sec, avg_samples: 64.00000, ips: 379.39232 images/sec
Training: 2023-10-18 03:49:43,415 - loss nan, lr: 0.025000, epoch: 4, step: 149400, eta: 80.24 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33738 sec, avg_samples: 64.00000, ips: 379.39232 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 149500, eta: 80.22 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33730 sec, avg_samples: 64.00000, ips: 379.48922 images/sec
Training: 2023-10-18 03:50:17,150 - loss nan, lr: 0.025000, epoch: 4, step: 149500, eta: 80.22 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33730 sec, avg_samples: 64.00000, ips: 379.48922 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 149600, eta: 80.20 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33730 sec, avg_samples: 64.00000, ips: 379.48872 images/sec
Training: 2023-10-18 03:50:50,886 - loss nan, lr: 0.025000, epoch: 4, step: 149600, eta: 80.20 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33730 sec, avg_samples: 64.00000, ips: 379.48872 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 149700, eta: 80.18 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33729 sec, avg_samples: 64.00000, ips: 379.49112 images/sec
Training: 2023-10-18 03:51:24,621 - loss nan, lr: 0.025000, epoch: 4, step: 149700, eta: 80.18 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33729 sec, avg_samples: 64.00000, ips: 379.49112 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 149800, eta: 80.16 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33711 sec, avg_samples: 64.00000, ips: 379.69602 images/sec
Training: 2023-10-18 03:51:58,338 - loss nan, lr: 0.025000, epoch: 4, step: 149800, eta: 80.16 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33711 sec, avg_samples: 64.00000, ips: 379.69602 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 149900, eta: 80.15 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33742 sec, avg_samples: 64.00000, ips: 379.35278 images/sec
Training: 2023-10-18 03:52:32,086 - loss nan, lr: 0.025000, epoch: 4, step: 149900, eta: 80.15 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33742 sec, avg_samples: 64.00000, ips: 379.35278 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 150000, eta: 80.13 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33718 sec, avg_samples: 64.00000, ips: 379.62307 images/sec
Training: 2023-10-18 03:53:05,810 - loss nan, lr: 0.025000, epoch: 4, step: 150000, eta: 80.13 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33718 sec, avg_samples: 64.00000, ips: 379.62307 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][150000]XNorm: 0.000000
Training: 2023-10-18 03:53:37,676 - [lfw][150000]XNorm: 0.000000
INFO:root:[lfw][150000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 03:53:37,676 - [lfw][150000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][150000]Accuracy-Highest: 0.81183
Training: 2023-10-18 03:53:37,676 - [lfw][150000]Accuracy-Highest: 0.81183
INFO:root:test time: 31.8664
Training: 2023-10-18 03:53:37,676 - test time: 31.8664
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][150000]XNorm: 0.000000
Training: 2023-10-18 03:54:14,680 - [cfp_fp][150000]XNorm: 0.000000
INFO:root:[cfp_fp][150000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 03:54:14,680 - [cfp_fp][150000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][150000]Accuracy-Highest: 0.61571
Training: 2023-10-18 03:54:14,680 - [cfp_fp][150000]Accuracy-Highest: 0.61571
INFO:root:test time: 37.0039
Training: 2023-10-18 03:54:14,680 - test time: 37.0039
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][150000]XNorm: 0.000000
Training: 2023-10-18 03:54:46,586 - [agedb_30][150000]XNorm: 0.000000
INFO:root:[agedb_30][150000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 03:54:46,586 - [agedb_30][150000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][150000]Accuracy-Highest: 0.58350
Training: 2023-10-18 03:54:46,586 - [agedb_30][150000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.9059
Training: 2023-10-18 03:54:46,586 - test time: 31.9059
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 150100, eta: 80.25 hours, avg_reader_cost: 0.00003 sec, avg_batch_cost: 0.33629 sec, avg_samples: 64.00000, ips: 380.62194 images/sec
Training: 2023-10-18 03:55:20,224 - loss nan, lr: 0.025000, epoch: 4, step: 150100, eta: 80.25 hours, avg_reader_cost: 0.00003 sec, avg_batch_cost: 0.33629 sec, avg_samples: 64.00000, ips: 380.62194 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 150200, eta: 80.23 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33727 sec, avg_samples: 64.00000, ips: 379.51514 images/sec
Training: 2023-10-18 03:55:53,957 - loss nan, lr: 0.025000, epoch: 4, step: 150200, eta: 80.23 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33727 sec, avg_samples: 64.00000, ips: 379.51514 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 150300, eta: 80.21 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33734 sec, avg_samples: 64.00000, ips: 379.43618 images/sec
Training: 2023-10-18 03:56:27,696 - loss nan, lr: 0.025000, epoch: 4, step: 150300, eta: 80.21 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33734 sec, avg_samples: 64.00000, ips: 379.43618 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 150400, eta: 80.20 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33738 sec, avg_samples: 64.00000, ips: 379.39131 images/sec
Training: 2023-10-18 03:57:01,440 - loss nan, lr: 0.025000, epoch: 4, step: 150400, eta: 80.20 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33738 sec, avg_samples: 64.00000, ips: 379.39131 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 150500, eta: 80.18 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33731 sec, avg_samples: 64.00000, ips: 379.47735 images/sec
Training: 2023-10-18 03:57:35,176 - loss nan, lr: 0.025000, epoch: 4, step: 150500, eta: 80.18 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33731 sec, avg_samples: 64.00000, ips: 379.47735 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 150600, eta: 80.16 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33733 sec, avg_samples: 64.00000, ips: 379.44597 images/sec
Training: 2023-10-18 03:58:08,915 - loss nan, lr: 0.025000, epoch: 4, step: 150600, eta: 80.16 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33733 sec, avg_samples: 64.00000, ips: 379.44597 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 150700, eta: 80.14 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33729 sec, avg_samples: 64.00000, ips: 379.49425 images/sec
Training: 2023-10-18 03:58:42,649 - loss nan, lr: 0.025000, epoch: 4, step: 150700, eta: 80.14 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33729 sec, avg_samples: 64.00000, ips: 379.49425 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 150800, eta: 80.12 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33727 sec, avg_samples: 64.00000, ips: 379.52069 images/sec
Training: 2023-10-18 03:59:16,382 - loss nan, lr: 0.025000, epoch: 4, step: 150800, eta: 80.12 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33727 sec, avg_samples: 64.00000, ips: 379.52069 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 150900, eta: 80.11 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33732 sec, avg_samples: 64.00000, ips: 379.46405 images/sec
Training: 2023-10-18 03:59:50,119 - loss nan, lr: 0.025000, epoch: 4, step: 150900, eta: 80.11 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33732 sec, avg_samples: 64.00000, ips: 379.46405 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 151000, eta: 80.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33742 sec, avg_samples: 64.00000, ips: 379.34901 images/sec
Training: 2023-10-18 04:00:23,866 - loss nan, lr: 0.025000, epoch: 4, step: 151000, eta: 80.09 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33742 sec, avg_samples: 64.00000, ips: 379.34901 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 151100, eta: 80.07 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33740 sec, avg_samples: 64.00000, ips: 379.37621 images/sec
Training: 2023-10-18 04:00:57,611 - loss nan, lr: 0.025000, epoch: 4, step: 151100, eta: 80.07 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33740 sec, avg_samples: 64.00000, ips: 379.37621 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 151200, eta: 80.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33743 sec, avg_samples: 64.00000, ips: 379.34298 images/sec
Training: 2023-10-18 04:01:31,359 - loss nan, lr: 0.025000, epoch: 4, step: 151200, eta: 80.05 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33743 sec, avg_samples: 64.00000, ips: 379.34298 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 151300, eta: 80.04 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33739 sec, avg_samples: 64.00000, ips: 379.38284 images/sec
Training: 2023-10-18 04:02:05,104 - loss nan, lr: 0.025000, epoch: 4, step: 151300, eta: 80.04 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33739 sec, avg_samples: 64.00000, ips: 379.38284 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 151400, eta: 80.02 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33725 sec, avg_samples: 64.00000, ips: 379.54257 images/sec
Training: 2023-10-18 04:02:38,834 - loss nan, lr: 0.025000, epoch: 4, step: 151400, eta: 80.02 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33725 sec, avg_samples: 64.00000, ips: 379.54257 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 151500, eta: 80.00 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33726 sec, avg_samples: 64.00000, ips: 379.52732 images/sec
Training: 2023-10-18 04:03:12,566 - loss nan, lr: 0.025000, epoch: 4, step: 151500, eta: 80.00 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33726 sec, avg_samples: 64.00000, ips: 379.52732 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 151600, eta: 79.98 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33731 sec, avg_samples: 64.00000, ips: 379.46823 images/sec
Training: 2023-10-18 04:03:46,303 - loss nan, lr: 0.025000, epoch: 4, step: 151600, eta: 79.98 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33731 sec, avg_samples: 64.00000, ips: 379.46823 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 151700, eta: 79.97 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33738 sec, avg_samples: 64.00000, ips: 379.39109 images/sec
Training: 2023-10-18 04:04:20,047 - loss nan, lr: 0.025000, epoch: 4, step: 151700, eta: 79.97 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33738 sec, avg_samples: 64.00000, ips: 379.39109 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 151800, eta: 79.95 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33736 sec, avg_samples: 64.00000, ips: 379.41733 images/sec
Training: 2023-10-18 04:04:53,788 - loss nan, lr: 0.025000, epoch: 4, step: 151800, eta: 79.95 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33736 sec, avg_samples: 64.00000, ips: 379.41733 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 151900, eta: 79.93 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33759 sec, avg_samples: 64.00000, ips: 379.16161 images/sec
Training: 2023-10-18 04:05:27,552 - loss nan, lr: 0.025000, epoch: 4, step: 151900, eta: 79.93 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33759 sec, avg_samples: 64.00000, ips: 379.16161 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 152000, eta: 79.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33710 sec, avg_samples: 64.00000, ips: 379.71095 images/sec
Training: 2023-10-18 04:06:01,268 - loss nan, lr: 0.025000, epoch: 4, step: 152000, eta: 79.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33710 sec, avg_samples: 64.00000, ips: 379.71095 images/sec
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[lfw][152000]XNorm: 0.000000
Training: 2023-10-18 04:06:33,155 - [lfw][152000]XNorm: 0.000000
INFO:root:[lfw][152000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 04:06:33,155 - [lfw][152000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][152000]Accuracy-Highest: 0.81183
Training: 2023-10-18 04:06:33,155 - [lfw][152000]Accuracy-Highest: 0.81183
INFO:root:test time: 31.8866
Training: 2023-10-18 04:06:33,155 - test time: 31.8866
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[cfp_fp][152000]XNorm: 0.000000
Training: 2023-10-18 04:07:10,144 - [cfp_fp][152000]XNorm: 0.000000
INFO:root:[cfp_fp][152000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 04:07:10,144 - [cfp_fp][152000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[cfp_fp][152000]Accuracy-Highest: 0.61571
Training: 2023-10-18 04:07:10,144 - [cfp_fp][152000]Accuracy-Highest: 0.61571
INFO:root:test time: 36.9895
Training: 2023-10-18 04:07:10,144 - test time: 36.9895
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:443: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:93: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  test_mask = np.zeros(_num_samples(X), dtype=np.bool)
INFO:root:[agedb_30][152000]XNorm: 0.000000
Training: 2023-10-18 04:07:42,129 - [agedb_30][152000]XNorm: 0.000000
INFO:root:[agedb_30][152000]Accuracy-Flip: 0.50000+-0.00000
Training: 2023-10-18 04:07:42,129 - [agedb_30][152000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[agedb_30][152000]Accuracy-Highest: 0.58350
Training: 2023-10-18 04:07:42,129 - [agedb_30][152000]Accuracy-Highest: 0.58350
INFO:root:test time: 31.9846
Training: 2023-10-18 04:07:42,129 - test time: 31.9846
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 152100, eta: 80.03 hours, avg_reader_cost: 0.00003 sec, avg_batch_cost: 0.33633 sec, avg_samples: 64.00000, ips: 380.57999 images/sec
Training: 2023-10-18 04:08:15,770 - loss nan, lr: 0.025000, epoch: 4, step: 152100, eta: 80.03 hours, avg_reader_cost: 0.00003 sec, avg_batch_cost: 0.33633 sec, avg_samples: 64.00000, ips: 380.57999 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 152200, eta: 80.01 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33702 sec, avg_samples: 64.00000, ips: 379.79397 images/sec
Training: 2023-10-18 04:08:49,480 - loss nan, lr: 0.025000, epoch: 4, step: 152200, eta: 80.01 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33702 sec, avg_samples: 64.00000, ips: 379.79397 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 152300, eta: 80.00 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33736 sec, avg_samples: 64.00000, ips: 379.41972 images/sec
Training: 2023-10-18 04:09:23,222 - loss nan, lr: 0.025000, epoch: 4, step: 152300, eta: 80.00 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33736 sec, avg_samples: 64.00000, ips: 379.41972 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 152400, eta: 79.98 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33728 sec, avg_samples: 64.00000, ips: 379.50834 images/sec
Training: 2023-10-18 04:09:56,956 - loss nan, lr: 0.025000, epoch: 4, step: 152400, eta: 79.98 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33728 sec, avg_samples: 64.00000, ips: 379.50834 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 152500, eta: 79.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33765 sec, avg_samples: 64.00000, ips: 379.08533 images/sec
Training: 2023-10-18 04:10:30,727 - loss nan, lr: 0.025000, epoch: 4, step: 152500, eta: 79.96 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33765 sec, avg_samples: 64.00000, ips: 379.08533 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 152600, eta: 79.94 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33763 sec, avg_samples: 64.00000, ips: 379.10764 images/sec
Training: 2023-10-18 04:11:04,497 - loss nan, lr: 0.025000, epoch: 4, step: 152600, eta: 79.94 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33763 sec, avg_samples: 64.00000, ips: 379.10764 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 152700, eta: 79.92 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33712 sec, avg_samples: 64.00000, ips: 379.68596 images/sec
Training: 2023-10-18 04:11:38,216 - loss nan, lr: 0.025000, epoch: 4, step: 152700, eta: 79.92 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33712 sec, avg_samples: 64.00000, ips: 379.68596 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 152800, eta: 79.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33689 sec, avg_samples: 64.00000, ips: 379.94822 images/sec
Training: 2023-10-18 04:12:11,912 - loss nan, lr: 0.025000, epoch: 4, step: 152800, eta: 79.91 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33689 sec, avg_samples: 64.00000, ips: 379.94822 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 152900, eta: 79.89 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33777 sec, avg_samples: 64.00000, ips: 378.95563 images/sec
Training: 2023-10-18 04:12:45,695 - loss nan, lr: 0.025000, epoch: 4, step: 152900, eta: 79.89 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33777 sec, avg_samples: 64.00000, ips: 378.95563 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 153000, eta: 79.87 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.70823 images/sec
Training: 2023-10-18 04:13:19,501 - loss nan, lr: 0.025000, epoch: 4, step: 153000, eta: 79.87 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33799 sec, avg_samples: 64.00000, ips: 378.70823 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 153100, eta: 79.85 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.71998 images/sec
Training: 2023-10-18 04:13:53,304 - loss nan, lr: 0.025000, epoch: 4, step: 153100, eta: 79.85 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33798 sec, avg_samples: 64.00000, ips: 378.71998 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 153200, eta: 79.84 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.60413 images/sec
Training: 2023-10-18 04:14:27,119 - loss nan, lr: 0.025000, epoch: 4, step: 153200, eta: 79.84 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33808 sec, avg_samples: 64.00000, ips: 378.60413 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 153300, eta: 79.82 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33718 sec, avg_samples: 64.00000, ips: 379.62222 images/sec
Training: 2023-10-18 04:15:00,844 - loss nan, lr: 0.025000, epoch: 4, step: 153300, eta: 79.82 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33718 sec, avg_samples: 64.00000, ips: 379.62222 images/sec
INFO:root:loss nan, lr: 0.025000, epoch: 4, step: 153400, eta: 79.80 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33712 sec, avg_samples: 64.00000, ips: 379.68238 images/sec
Training: 2023-10-18 04:15:34,563 - loss nan, lr: 0.025000, epoch: 4, step: 153400, eta: 79.80 hours, avg_reader_cost: 0.00002 sec, avg_batch_cost: 0.33712 sec, avg_samples: 64.00000, ips: 379.68238 images/sec
